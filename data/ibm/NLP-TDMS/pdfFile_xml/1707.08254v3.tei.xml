<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharif</forename><forename type="middle">Amit</forename><surname>Kamran</surname></persName>
							<email>sharifamit@iub.edu.bd</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Cognitive Skill Enhancement</orgName>
								<orgName type="institution">Independent University Bangladesh Dhaka</orgName>
								<address>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Shihab</forename><surname>Sabbir</surname></persName>
							<email>asabbir@iub.edu.bd</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Cognitive Skill Enhancement Independent University Bangladesh Dhaka</orgName>
								<address>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Yet Deep Convolutional Neural Networks for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Convolutional Neural Network</term>
					<term>Semantic Image Segmentation</term>
					<term>Skip Architectures</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic Segmentation using deep convolutional neural network pose more complex challenge for any GPU intensive task. As it has to compute million of parameters, it results to huge memory consumption. Moreover, extracting finer features and conducting supervised training tends to increase the complexity. With the introduction of Fully Convolutional Neural Network, which uses finer strides and utilizes deconvolutional layers for upsampling, it has been a go to for any image segmentation task. In this paper, we propose two segmentation architecture which not only needs one-third the parameters to compute but also gives better accuracy than the similar architectures. The model weights were transferred from the popular neural net like VGG19 and VGG16 which were trained on Imagenet classification data-set. Then we transform all the fully connected layers to convolutional layers and use dilated convolution for decreasing the parameters. Lastly, we add finer strides and attach four skip architectures which are element-wise summed with the deconvolutional layers in steps. We train and test on different sparse and fine data-sets like Pascal VOC2012, Pascal-Context and NYUDv2 and show how better our model performs in this tasks. On the other hand our model has a faster inference time and consumes less memory for training and testing on NVIDIA Pascal GPUs, making it more efficient and less memory consuming architecture for pixel-wise segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the introduction of convolutional neural network, the image recognition task has accelerated with great pace and has given state-of-the-art results for classification, detection and semantic segmentation alike. Classification is to recognize the the whole image to a certain class. Whereas in detection, each object has to be identified with a bounding box accurately. For semantic segmentation every pixel of the object in the image has to be classified to a corresponding class. Over the years many classification models yielded better results <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> in their independent task. Due to this success they have also been used as a base model for acing in extracting local features and giving finer output <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> for semantic segmentation tasks. Problem definition of the task in hand is to keep the global structure in contrast with the local context <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Here, the global structure means the shape of the objects as a whole and how they are placed in the image with respect to other objects. On the other hand, the local features means the small geometric shapes like the sharp edge, circles etc. For example, if we consider the bipedal humans as the object and its shape as the global structure, then the shape of the eyes, nose, color of the lips can be considered as the local features. Most of the time the local features tend to get lost while training the neural networks and global context seems to dominate throughout the segmentation mask. So for extracting those local fine features, Skip architectures were introduced <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref> with the preexisting segmentation architecture. This fine output from skip connections were then element-wise summed with the coarse semantic information on the top most layers of the neural net. By using Skip architectures the image representation becomes finer and less coarse.</p><p>The drawback for designing convolutional neural network with high level computing for pixel-wise classification seems to be the huge amount of memory required for the task. Firstly, orthodox ConvNets have rather large receptive fields because of their convolutional filters and generates coarse blob-like output map when it is redefined to produce pixelwise segmentation <ref type="bibr" target="#b3">[4]</ref>. Secondly, sub-sampling with max-pool in ConvNets diminishes the chance to get finer output <ref type="bibr" target="#b7">[8]</ref>. Furthermore, similar labeling in neighboring pixels tends to get lost in deeper layers where the upsampling <ref type="bibr" target="#b8">[9]</ref> takes place. So visual consistency and retaining spatial feature is one of the essential job for producing sharp segmentation mask. Falling short of producing such fine output can result in poor object portrayal and patch-like false regions in the segmentation mask <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>.</p><p>Using finer strides <ref type="bibr" target="#b3">[4]</ref> and replacing vanilla convolution with dilated convolution <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref> have shown better segmentation results while keeping the memory usage in check. Because with dilation the receptive fields can be increased exponentially <ref type="bibr" target="#b4">[5]</ref>. Whereas the filter of the convolution remains the same size as the previous filter. So with the expense of reducing the size of the filter and adding dilation between it, we can free up more memory for computing from the sixth convolutional layer in the architecture which is the most expensive layer.</p><p>Adding more Skip architectures seems to increase memory usage for the whole end-to-end network. But because additional memory has been freed up by using dilation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, extra skip connections can be added to upsample local features from other convolutional layers. In a feed forward network like Fully-Convolutional Neural Network <ref type="bibr" target="#b3">[4]</ref> (which is denoted by FCN) the size of the representation changes with each convolutions. As the structure is similar to an encoder-decoder network,the feature hierarchies from earlier layers have to be element-wise added with the upsampled <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref> layers in steps.</p><p>Our proposal in this paper is an efficient yet deep feed forward neural net for a strongly supervised image segmentation task. Our work tends to integrate both dilated and vanilla convolution to recreate a FCN (which stands for Fully-Convolutional Neural Network) architecture which generates better output while consuming less memory. In addition, we introduce four skip architectures which fetches more local information lost in the network in bottom layers. These features are then upsampled and element-wise summed with the global feature map in steps ih the top layers. Which in turn produces better segmentation mask while keeping GPU memory consumption in check. Most Importantly, with this changes in architecture, the end-to-end deep network can be trained on any type of data while utilizing the usual backpropagation algorithm with more efficient and finer results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LITERATURE REVIEW</head><p>Following section describe different procedures which has been proposed before for conducting semantic segmentation task using deep learning. Out of many approaches only few have been adopted for high computing pixel-wise segmentation.</p><p>Our proposed model was developed based on a particular neural net that was used for image classification task <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> and the weights were transferred from it <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Transfer learning was seen being performed in classification task, afterwards it was applied to object detection tasks, lately it has been adopted for instance aware segmentation <ref type="bibr" target="#b15">[16]</ref> and image segmentation models with a powerful classifier <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. We redesign and redefine the architecture and perform finetuning to get more sparse and accurate prediction for semantic segmentation. Furthermore. we compare different models with our one and show how it is more efficient and effective for semantic segmentation jobs.</p><p>Multi-digit recognition with neural network <ref type="bibr" target="#b18">[19]</ref>, an extension of LeNet <ref type="bibr" target="#b19">[20]</ref>, was such work where erratic range of values for input was first witnessed. Though the task was ordained for one dimensional data, Viterbi decoding was sufficient for such task. Three years later convolutional neural network was elongated for two dimensional feature output for processing postal address data <ref type="bibr" target="#b20">[21]</ref>. These historical breakthroughs were designed to conduct small yet powerful detection task. Additionally LeCun et al. <ref type="bibr" target="#b21">[22]</ref> using fully convolutional inference developed a CNN for sparse multiple class segmentation of embryo. We have also seen FCNs being used in many recent deep layered nets for high level computation. Using Sliding window for integrated object detection and localization by Eigen et al. <ref type="bibr" target="#b22">[23]</ref>, Recurrent neural network for scene labeling by Pinheiro et al. <ref type="bibr" target="#b23">[24]</ref> , and restoring dirt clad image using convNet by Eigen et al. <ref type="bibr" target="#b24">[25]</ref> is such remarkable example.</p><p>Training a FCN can be difficult, but has been used for detecting human parts and estimating pose efficiently by Tompson et al. <ref type="bibr" target="#b25">[26]</ref> Different approaches can be taken to get finer segmentation mask exploiting convolutional neural network. One such strategy could be to develop individual system for extracting dense features and detecting zoomed-in edges from images for finer semantic segmentation <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. A single step process can be, extract semantic feature with convnet and then using superpixels for figuring out the inner layout of the image. Another procedure can be to retrieve superpixels from the given image layout and then extracting features from images one by one <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The only drawback of this approach is that the erroneous super pixels may result into fallacious predictions, irrespective how powerful feature extraction took place. Zheng et al. <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref> designed a RNN model and used Conditional random field to get finer features by training an end-to-end network for semantic segmentation. They also proposed a disjointed version of the same model having less accuracy and consuming more memory to prove that an endto-end network always have an upper hand over two or even three stage effective segmentation retrieval procedure.</p><p>Another strategy could be to develop a model and train it using supervised image data and output the segmentation label map for each categories. Retaining the spatial information, one can replace the fully connected layers with convolutional layers in a deep convnet, which was shown by Eigen et al <ref type="bibr" target="#b30">[31]</ref>. The most groundbreaking work so far was by Shelhamer and Long et al <ref type="bibr" target="#b3">[4]</ref> where the idea was, FCN can be designed to harness features to help classify pixel from the top-most layers, whereas the bottom layers can be used for detecting shapes,contour and edges. With element-wise summation of earlier layers with latter layers they introduced the idea of skip architecture. On the other hand conditional random fields was used to refine semantic segmentation furthermore <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>. CRF was also used by Snavely et al. <ref type="bibr" target="#b31">[32]</ref> and Chen et al. <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b32">[33]</ref> for refining the existing segmentation mask. Snavely et al. conducted recognition for materials and it segmentation, on the other hand Chen et al. developed better ways to obtain finer semantic image segmentation. Though the previous procedures included disjointed CRF for conducting post-processing on the segmented output, the method developed by Torr et al. <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref> employed CRF as recurrent neural network and also developed higher order model which is an extension of CRFasRNN. Not only is the convnet is end-to-end but also it converges faster than the previous CRF models and produces finer segmentation mask.</p><p>Difference between dilated and vanilla convolution is the extra parameter called holes or dilation that affects the receptive fields of the convolution's filter. The whole idea of Atrous algorithm, which is based on wavelet decomposition <ref type="bibr" target="#b33">[34]</ref> is wholeheartedly based on dilated filter. In <ref type="bibr" target="#b4">[5]</ref> Fisher Yu et al. used the term "dilated convolution" instead of "convolution with a dilated filter" to formulate that no dilated filter weren't built or produced. Convolutional layer was modified instead to make way for a new parameter called dilation to alter the preexisting filter. In <ref type="bibr" target="#b34">[35]</ref> Chen et al. made use of dilation to modify the architecture of Shelhamer et al <ref type="bibr" target="#b3">[4]</ref> to make it suitable for his task. In contrast, Yu et al. <ref type="bibr" target="#b4">[5]</ref> developed a new range of feed forward neural net which exploits dilated convolutions and multi-scale context aggregation but get rid of the preexisting skip architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SEGMENTATION ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transfer Learning from Classification Net</head><p>VGGnet is a famous neural net which won ILSVRC14 <ref type="bibr" target="#b0">[1]</ref> for image classification. The neural net worked on the principal of using 3 × 3 sized filters for feature extraction and concurrently joins each convolutions to make the receptive field bigger. We transferred weights from the VGG 19-layer network , removed the classifier from the network and turned all the fully connected layers to convolutions as done by Shelhamer et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>In covolutional neural network all the tensors have three dimensions of size N × H × W , where H and W are defined as height and width, and N is the color channel or feature output map. At first layer the image is taken as input, where the pixel size is H × W , and the three color channel for RGB is N. As described by shelhamer et al. <ref type="bibr" target="#b3">[4]</ref> receptive fields is the locations in higher layers corresponds to the locations path-connected to the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Information and Receptive Field</head><p>The output feature map for each convolutions can be predefined, but the spatial dimension depends on the size of the filter, strides and padding. FCN architecture tends to keep the spatial dimension of each convolutions same before maxpooling (exception being Fc6 and Fc7 layers). Considering the output channel dimension be O ij and input channel dimension be I ij for any convolution layer, where i, j is the spatial dimension. Equation (1) can be used for getting the optimal output channel.</p><formula xml:id="formula_0">O ij = I ij + 2P ij − K ij S ij + 1<label>(1)</label></formula><p>Here, P stands for Padding, K is for Kernel or filter size and S is for stride. We choose a filter size of 3 × 3, single padding and stride 1 for convolutional layers. This helps us to retain the convolutional structure before pooling to reduce the spatial dimension.</p><p>For pooling we use stride of 2 and filter of 2 × 2, to lower the spatial length of the tensors. If we use the probable value for filter and stride, in equation 1, then we can see the output becomes half of the size of the input.</p><p>The tensors go through first convolution layer to second convolution layer and onward as it is fed forward through the net. In the first two convolution layers we have 3 × 3 filters. Therefore the first one has a receptive field of 3 × 3 and second one has receptive field of 5 × 5. From third to fifth set of convolutional layers we have four convolutions for each of them. So, the receptive field is quite larger than the earlier layers. Notice that receptive fields increases linearly after each convolutions. Let receptive field r, filter f and stride be s. If the filter and stride values remains the same for concurrent convolutions, (2) formulation can be used for computing the receptive field. r n × r n = (2r n−1 + 1) × (2r n−1 + 1)</p><p>where r n is the receptive field of the next convolution and r n−1 is the receptive field of the previous convolution.</p><p>The sixth layer which is the most expensive one comes next. It is a fully connected layer, having a spatial dimension of 4096 × 4096 with a filter size 7 × 7. The next is also a fully connected layer having a spatial dimension of 4096 × 1000 and a filter size of 1 × 1. We convert both of this layers to convolutional layers with filter size of 3 × 3 and 1 × 1 as done by Shelhamer et al. <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Decreasing Parameters using Dilation</head><p>Fihser et al. <ref type="bibr" target="#b4">[5]</ref> combined dilation with vanilla convolution throughout the network and emphasized multi-context aggregation. Whereas, we stick to the original structure of FCN <ref type="bibr" target="#b3">[4]</ref> but include dilation only in the sixth convolution, which is the most expensive layer and has the most amount of parameter computation. Similar work was done before in Parsenet, by Liu et al. <ref type="bibr" target="#b12">[13]</ref>, but they trained on a reduced version of VGG-16 net. We train on the original VGG-19 classification neural net and only change the filter size and enter dilation as parameter in Fc6 convolution (see <ref type="table" target="#tab_0">Table I</ref>) while retaining the same number of parameters across all the convolutions. The relation between filter size and dilation can formulated using (3).</p><formula xml:id="formula_2">K = K + (K − 1)(d − 1)<label>(3)</label></formula><p>where K' is the new filter size, K is the given filter size and d is the amount of dilation. A convolution with dilation 1 is the same as having no dilation. Using equation 3, the filter size of the sixth convolution layer can be changed from 7 × 7 to 3 × 3 with a dilation size of 3 and filter size of 3. Fisher et al <ref type="bibr" target="#b4">[5]</ref> has also defined (4) for calculating the receptive field of a dilated convolution considering the same condition as before.</p><formula xml:id="formula_3">r i+1 × r i+1 = (2 i+2 − 1) × (2 i+2 − 1)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deconvolution with Finer Strides</head><p>As upsampling with factor f means using convolution with a fractional input stride of 1/f <ref type="bibr" target="#b3">[4]</ref>. If f is an integer, we can reverse the forward and backward pass to make the upsampling work by replacing vanilla convolution with transpose convolution. So we use upsampling with finer stride as done by Shelhamer et al. <ref type="bibr" target="#b3">[4]</ref> for end-to-end calculation of pixel-wise semantic loss. But the author used strides of 32, 16 and 8, whereas we use a stride of 2 to upsample it in steps. This was done to element-wise summed local features from the bottom layers using skip architectures (discussed in the next section). <ref type="figure" target="#fig_0">Fig. 1</ref> shows the procedure in details. In <ref type="bibr" target="#b14">[15]</ref>, transpose convolution layers are called "deconvolution layers". The deconvolutional layers are used for "bilinear interpolation" This provides with finer segmentation and accurate pixel classification as described in <ref type="bibr" target="#b8">[9]</ref> rather than learning. It was witnessed by Shelhamer et al. <ref type="bibr" target="#b3">[4]</ref> and Torr et al. <ref type="bibr" target="#b6">[7]</ref> that upsampling in an end-to-end network is way faster and effective for learning dense prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multiple Skip Architectures</head><p>We adopt a similar procedure as fcn-8s-all-at-once <ref type="bibr" target="#b3">[4]</ref> for training rather than fcn-8s staged version Because it is more time-consuming to do in stages while predicting with nearly the same accuracy as the all-at-once version. Moreover, for all-at-once version each skip architectures scales with fixed constant and these constants are chosen in such a way that it equals to the average feature norms across all skip architectures <ref type="bibr" target="#b3">[4]</ref>. This helps to decrease inference time a lot. For our case the inference time was less than 200ms for each forward and backward pass combined, whereas fcn-8s had an inference time of 500ms. We use a total of four skip architectures. Also the skip architectures tends to consume less memory compared to the convolution layer due to its only operation being element-wise summation with other layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Metrics and Evaluation</head><p>We use four different metrics to score the pixel-accuracy <ref type="formula" target="#formula_4">(5)</ref>, mean-intersection-over-union denoted by mIOU <ref type="formula" target="#formula_6">(7)</ref>, mean accuracy <ref type="bibr" target="#b5">(6)</ref> and frequency weighted accuracy denoted by fw-IU <ref type="bibr" target="#b7">(8)</ref>. As background pixels numbers in majority, pixel accuracy is not preferable.For semantic segmentation and scene labeling mean-intersection-over union is the most optimum choice for bench-marking. pixel accuracy:</p><formula xml:id="formula_4">i N ii i j N ij<label>(5)</label></formula><p>mean accuracy:</p><formula xml:id="formula_5">(1/N class ) i N ii j N ij<label>(6)</label></formula><p>mIOU:</p><formula xml:id="formula_6">(1/N class ) i N ii ( j N ij + j N ji − N ii )<label>(7)</label></formula><p>fw-IU:</p><formula xml:id="formula_7">( k j N kj ) −1 i j N ij N ii ( j N ij + j N ji − N ii )<label>(8)</label></formula><p>where N ij is the number of pixels of class i predicted to belong to class j, N class is the number of classes and j P ij is the total number of pixels of class i. The data was used as it is provided by <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b17">[18]</ref>. No pre or post processing or augmentation was done to training or validation images to enhance the accuracy of the segmentation output.</p><p>In <ref type="table" target="#tab_0">Table II</ref>, Pixel Accuracy, Mean Accuracy, MeanIOU and FW Accuracy comparison between our model and Other FCN architecture. Both of our model outperforms the preexisting FCN structure for the reduced validation set.</p><p>B. Data-set and Procedure 1) Pascal VOC: Transfer learning was performed by copying weights separately from VGG-19 and VGG-16 classification nets for our two models, Dilated FCN-2s-VGG19 and Dilated FCN-2s-VGG16. We adopt the Back propagation  <ref type="bibr" target="#b19">[20]</ref> algorithm to train the network end-to-end with forward and backward pass. We used dilation for our most expensive layer, Fc6 as seen in <ref type="table" target="#tab_0">Table I</ref>. which reduced the number of parameters. Resulting into less computation by the machine yet faster inference time. The total time needed was 12 hours for both the networks to get the best mIOU using a single GPU. We used PASCAL VOC 2012 training data counting up to 1464 images. Validation was done on the reduced VOC2012 validation set of 346 images <ref type="bibr" target="#b6">[7]</ref> in which we got 58 percent meanIOU.</p><p>2) Semantic Boundaries Dataset: Extensive data was used to improve the pixel accuracy and mean-intersection-overunion score of both the models for which the training was done on Semantic Boundaries data-set <ref type="bibr" target="#b17">[18]</ref>. The set consists of 8498 training and 2857 validation data. Training was done on both the training and validation data summing up to 11355 images. The reduced set for validation was found by removing the common images in Augmented VOC2012 training set and VOC2012 validation set <ref type="bibr" target="#b35">[36]</ref>, resulting to 346 images. <ref type="table" target="#tab_0">Table  II</ref> to shows the comparative results of different FCN models. Our model, Dilated FCN-2s-VGG16 achieves a meanIOU of 64.1 percent and Dilated FCN-2s-VGG19 scores a meanIOU of 64.9 percent. Clearly, the deeper version of the model is more precise for pixel-wise-segmentation. Training was done with learning rate of 10e −11 with 200,000 iterations.</p><p>3) VOC2012 Test: The test results as shown on <ref type="table" target="#tab_0">Table  III</ref> indicates our models scoring better than similar FCN architecture in Pascal VOC2012 Segmentation Challenge. We didn't train on any additional data, neither did we add any graphical model like CRF or MRF <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref> to enhance the accuracy furthermore. Reason being it consumes 2× more GPU memory for training. Moreover, our model scores better than FCN model in NYUDv2 sets too (see <ref type="table">Table V</ref>). <ref type="figure">Fig.  2</ref> shows the segmentation mask compared to FCN-8s and the ground truth. Also </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Nets</head><p>MeanIOU FCN-8s <ref type="bibr" target="#b3">[4]</ref> 62.2 FCN-8s-heavy <ref type="bibr" target="#b3">[4]</ref> 67.2 DeepLab-CRF <ref type="bibr" target="#b7">[8]</ref> 66.4 DeepLab-CRF-MSc <ref type="bibr" target="#b7">[8]</ref> 67.1 VGG19 FCN <ref type="bibr" target="#b36">[37]</ref> 68.1 Dilated FCN-2s using VGG16 (ours) 67.6 Dilated FCN-2s using VGG19 (ours) 69 nets consume memory for training and testing with GPU. As one can see, the reduction in memory usage is more than 20 percent for training with FCN-2s Dilated VGG16. 4) Pascal Context Data-set: We train on more sparse data-set like Pascal Context which has 60 classes and pose more challenging pixel-wise prediction task <ref type="bibr" target="#b40">[41]</ref>. The dataset consists of 10103 images. We split the data set into 5105 validation images and rest are used as training set. <ref type="table" target="#tab_0">Table IV</ref> shows comparative results for Pascal Context Data-set. Our model, scores better mean-IOU of 42.6 percent than the other state-of-the-art models. Moreover, many deeper models with Higher Order CRF as post processing layer scored worse than our model. This clearly indicates that our model is better suited for pixel-wise prediction of sparse data-set. Training was done with learning rate of 10e −10 with 300,000 iterations. 5) NYUDv2 Data-set: We train on NYUD version 2, an RGB-D dataset collected with the Microsoft Kinect. It consists of 1,449 RGB-D images, with pixel-wise semantic labels that is divided into 40 semantic classes by Gupta et al. <ref type="bibr" target="#b41">[42]</ref>. The data is split into 795 training images and 654 testing images. In, <ref type="table">Table 5</ref> the comparison of between fcn and our model is given. We train with Dilated FCN-2s VGG19 with three channel RGB images. Then we add depth information and train on a new model upgraded to take four-channel RGB-D input. Though the performance doesn't increase. Long et al. <ref type="bibr" target="#b3">[4]</ref> describes this phenomenon happens due to having similar number of parameters or the failure to propagate all the semantic gradients through the net. Following the footstep of Gupta et al. <ref type="bibr" target="#b5">[6]</ref>, we next train on three-dimensional HHA encoding of depth. The results proves to be more precise and yields better score for our model. Training was done with learning rate of 10e −10 with 150,000 iterations. <ref type="table">Table   TABLE V</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Memory Efficiency</head><p>In terms of memory efficiency, three of our architecture has shown remarkable results across different GPUs. As shown in <ref type="figure">Fig. 3</ref>, training and testing time GPU usage has been decreased for all of the achitectures. Moreover, Dilated-Fcn2s-Vgg16 achieves better performance results by reducing 700 MB while training. For Dilated-Fcn2s-Context it has has shown slight improvement for both training and testing time while getting better even for more power consuming models.</p><p>In <ref type="table" target="#tab_0">Table VI</ref>, it can be seen that the GPU memory allocation for 20-class and 59-class segmentation task has been reduced by 10-20 percent. Additionally, the counterpart of Fcn-8s <ref type="bibr" target="#b3">[4]</ref> which is Dilated-Fcn2s, has shown remarkable results for both training and testing time GPU memory usage. On the other hand, the inference time required for three of our models are less than half of the other similar architectures. And for both 20 and 59 class segmentation it retains similar inference times. <ref type="figure">Fig. 4</ref>, shows the comparisons of different models in regard to Mean intersection-Over-Union Vs. Parameters (in millions). The number of parameters of Fcn-8s <ref type="bibr" target="#b3">[4]</ref>, VGG19-Fcn <ref type="bibr" target="#b36">[37]</ref> architectures for training VOC2012 <ref type="bibr" target="#b35">[36]</ref> and Pascal-context <ref type="bibr" target="#b40">[41]</ref> data are in hundred of millions. Whereas, all three of our models require less parameters for calculation, hence less memory usage by both CPU and GPU. It brings to light another prospect that, huge number of parameters are not needed to increase accuracy or finer potrayal. Furthermore, wide receptive fields and training on sparse data can also effectively give better results. All the models where trained and tested with Caffe [43] on Nvidia GTX1060 and GTX1070 separately. The code for this model can be found at: https://github.com/SharifAmit/ DilatedFCNSegmentation V. CONCLUSION Enhancing accuracy for pixel-wise segmentation requires huge amount of memory and time. Our benchmark result for PASCAL VOC2012 test data set set for 20 unique classes scored mean-IOU of 69 percent for Dilated-FCN-2s-VGG19 and 67.6 percent for Dilated-FCN-2s-VGG16. On the other hand, for sparse data-set like NYUDv2 for 39 unique classes and Pascal-Context for 59 unique classes our model scored pixel accuracy of 62.6 percent and mean IOU of 42.6 percent respectively. Fully convolutional networks can be used to transfer weights from pre-trained net, element-wise summing different layers to improve accuracy and to train end-to-end for entire images with extensive data. Dilation increases the receptive fields while decreasing parameters and inference time. The objective was to create efficient yet deep architectures for generating accurate output while using less computation resources. And the proposed models have remarkably produced accurate pixel-wise segmentation. Hopefully, this architectures  <ref type="bibr" target="#b40">[41]</ref> for 59-classes.The Fifth column of images show the output of our model which tends to be more accurate. On the second column O2P model's <ref type="bibr" target="#b38">[39]</ref>  <ref type="bibr" target="#b37">[38]</ref> output which has wrongly predicted in many instances.</p><p>can be further used for Semantic Segmentation tasks like Self-Driving cars, Medical Imaging and Robotics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The comparision between VGG-16, FCN-8s and Dilated Fully Convolutional Neural Network with Skip Architectures. Dilated FCn-2s upsample stride 2 predictions back to pixels in five steps. The pool-4 to pool-1 are element-wise summed with stride 2, 4, 8 and 16 in steps in reverse order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Results after testing on Pascal-Context dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc>The Third and fourth image shows the output of our model. Fourth one being more accurate Dilated FCN-2s VGG19. The second image shows the output of the previous best method by Shelhamer et al.<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell>Fig. 2.</cell></row><row><cell></cell><cell cols="2">PARAMETERS COMPARISON</cell></row><row><cell></cell><cell>FCN-8s</cell><cell>Dilated FCN-2s (our)</cell></row><row><cell>inference time</cell><cell>0.5s</cell><cell>0.2s</cell></row><row><cell>Fc6 Weights</cell><cell>4096x512x7x7</cell><cell>4096x512x3x3</cell></row><row><cell>Dilation</cell><cell>1(none)</cell><cell>3</cell></row><row><cell>Fc6 Parameters</cell><cell>102,760,448</cell><cell>18,874,368</cell></row><row><cell cols="2">Total Parameters 134,477,280</cell><cell>55,812,880</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II EVALUATION</head><label>II</label><figDesc></figDesc><table><row><cell cols="5">ON PASCAL VOC2012 REDUCED VALIDATION SET</cell></row><row><cell>Neural Nets</cell><cell>Pixel Accuracy</cell><cell>Mean Accuracy</cell><cell>Mean IOU</cell><cell>FW Accuracy</cell></row><row><cell>FCN-8s-all-at-once</cell><cell>90.8</cell><cell>77.4</cell><cell>63.8</cell><cell>84</cell></row><row><cell>FCN-8s</cell><cell>90.9</cell><cell>76.6</cell><cell>63.9</cell><cell>84</cell></row><row><cell>Dilated FCN-2s using VGG16 (ours)</cell><cell>91</cell><cell>78.3</cell><cell>64.15</cell><cell>84.4</cell></row><row><cell>Dilated FCN-2s using VGG19 (ours)</cell><cell>91.2</cell><cell>77.6</cell><cell>64.86</cell><cell>84.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc>VI demonstrates, how less our</figDesc><table><row><cell>TABLE III</cell></row><row><cell>PASCAL VOC 12 TEST RESULTS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV EVALUATION</head><label>IV</label><figDesc></figDesc><table><row><cell cols="5">OF PASCAL CONTEXT DATA-SET</cell></row><row><cell>Architectures</cell><cell>pixel accu.</cell><cell>mean accu.</cell><cell>mean IU</cell><cell>f.w. IU</cell></row><row><cell>O2P [38]</cell><cell>-</cell><cell>-</cell><cell>18.1</cell><cell>-</cell></row><row><cell>CFM [39]</cell><cell>-</cell><cell>-</cell><cell>18.1</cell><cell>-</cell></row><row><cell>FCN-32s</cell><cell>65.5</cell><cell>49.1</cell><cell>36.7</cell><cell>50.9</cell></row><row><cell>FCN-16s</cell><cell>66.9</cell><cell>51.3</cell><cell>38.4</cell><cell>52.3</cell></row><row><cell>FCN-8s</cell><cell>67.5</cell><cell>52.3</cell><cell>39.1</cell><cell>53.0</cell></row><row><cell>CRFasRNN [7]</cell><cell>-</cell><cell>-</cell><cell>39.28</cell><cell>-</cell></row><row><cell>HO-CRF [30]</cell><cell>-</cell><cell>-</cell><cell>41.3</cell><cell>-</cell></row><row><cell>DeepLab-LargeFOV-CRF [40]</cell><cell>-</cell><cell>-</cell><cell>39.6</cell><cell>-</cell></row><row><cell>Ours</cell><cell>69.9</cell><cell>54.9</cell><cell>42.6</cell><cell>56.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI GPU</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fig. 4. Number of parameters vs Mean IOU scores among different archi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tectures</cell></row><row><cell></cell><cell cols="3">MEMORY USAGE COMPARISON</cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell>GPU Memory Usage Training(MB)</cell><cell>GPU Memory Usage Training + Testing(MB)</cell><cell>Number of Parameters (millions)</cell><cell>Inference Time(ms)</cell><cell>Number of Classes</cell></row><row><cell>Fcn-8s</cell><cell>3759</cell><cell>4649</cell><cell>134</cell><cell>500</cell><cell>20</cell></row><row><cell>Dilated Fcn-2s VGG16(ours)</cell><cell>3093</cell><cell>4101</cell><cell>50.5</cell><cell>200</cell><cell>20</cell></row><row><cell>Dilated Fcn-2s VGG19(ours)</cell><cell>3367</cell><cell>4309</cell><cell>55.8</cell><cell>200</cell><cell>59</cell></row><row><cell>PascalContext Fcn-8s</cell><cell>4173</cell><cell>5759</cell><cell>136</cell><cell>500</cell><cell>59</cell></row><row><cell>Dilated Fcn-2s Context(ours)</cell><cell>3975</cell><cell>5333</cell><cell>56.2</cell><cell>200</cell><cell>59</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Evan Shelhamer for providing the evaluation scripts and Caffe users community for their advice and suggestions. We also would like to acknowledge the technical support "Center for Cognitive Skill Enhancement" has provided to us.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Associative hierarchical crfs for object class image segmentation. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-digit recognition using a space displacement neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Postal address block location using a convolutional locator network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="745" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward automatic phenotyping of developing embryos from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Delhomme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Piano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><forename type="middle">Emilio</forename><surname>Barbano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1360" to="1371" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rob Fergus, and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payman</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic segmentation using regions and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3378" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth map preddiction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Material recognition in the wild with the materials in context database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3479" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ph</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pascal visual object classes challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<ptr target="Availablefromwww.pascal-network.org" />
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Exploring deep features: deeper fully convolutional neural network for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabit Bin</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kabir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>BRAC University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Bachelor Thesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
