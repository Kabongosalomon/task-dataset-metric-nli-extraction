<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Li</surname></persName>
							<email>lixuan12@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
							<email>zhangchengquan@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
							<email>liutao32@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
							<email>hanjunyu@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
							<email>liujingtuo@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology(VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text image contains two levels of contents: visual texture and semantic information. Although the previous scene text recognition methods have made great progress over the past few years, the research on mining semantic information to assist text recognition attracts less attention, only RNN-like structures are explored to implicitly model semantic information. However, we observe that RNN based methods have some obvious shortcomings, such as time-dependent decoding manner and one-way serial transmission of semantic context, which greatly limit the help of semantic information and the computation efficiency. To mitigate these limitations, we propose a novel end-to-end trainable framework named semantic reasoning network (SRN) for accurate scene text recognition, where a global semantic reasoning module (GSRM) is introduced to capture global semantic context through multi-way parallel transmission. The state-of-the-art results on 7 public benchmarks, including regular text, irregular text and non-Latin long text, verify the effectiveness and robustness of the proposed method. In addition, the speed of SRN has significant advantages over the RNN based methods, demonstrating its value in practical use.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text has rich semantic information, which has been used in many computer vision based applications such as automatic driving <ref type="bibr" target="#b42">[43]</ref>, travel translator <ref type="bibr" target="#b37">[38]</ref>, product retrieval, etc. Scene text recognition is a crucial step in scene text reading system. Although sequence-to-sequence recognition has made several remarkable breakthroughs in the past decades <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, text recognition in the wild is still a big challenge, caused by the significant variations of scene text in color, font, spatial layout and even uncontrollable background.</p><p>Most of the recent works have attempted to improve the performance of scene text recognition from the perspective of extracting more robust and effective visual features, such as upgrading the backbone networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>, adding rectification modules <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> and improving attention mechanisms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Nevertheless, it is a fact that, for a human, the recognition of scene text is not only dependent on visual perception information, but also affected by the high-level text semantic context understanding. As some examples shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, it is very difficult to distinguish each character in those images separately when only visual features are considered, especially the characters highlighted with red dotted boxes. Instead, taking semantic context information into consideration, human is likely to infer the correct result with the total word content.</p><p>Unfortunately, for the semantic information, the mainstream text recognition methods consider it in the way of one-way serial transmission, such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>, which recursively perceive the character semantic information of the last decoding time step, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (a). There are several obvious drawbacks in this manner: First, it can only perceive very limited semantic context from each decoding time step, even no useful semantic information for the first decoding time step. Second, it may pass the wrong semantic information down and cause a error accumulation when the wrong decoding is raised at an earlier time step. Meanwhile, the serial mode is hard to be paralleled, thus it is always time-consuming and inefficient.</p><p>In this paper, we introduce a sub-network structure named global semantic reasoning module (GSRM) to tackle these disadvantages. The GSRM considers global semantic context in a novel manner of multi-way parallel transmission. As is shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>, the multi-way parallel transmission can simultaneously perceive the semantic information of all characters in a word or text line, which is much more robust and effective. Besides, the wrong semantic content of the individual character can only cause quite limited negative impact on other steps. Furthermore, we propose a novel framework named semantic reasoning network (SRN) for accurate scene text recognition, which integrates not only global semantic reasoning module (GSRM) but also parallel visual attention module (PVAM) and visual-semantic fusion decoder (VSFD). The PVAM is designed to extract visual features of each time step in a parallel attention mechanism, and the VSFD aims to develop an effective decoder with the combination of visual information and semantic information. The effectiveness and robustness of the SRN are confirmed by extensive experiments, which are discussed in Sec. 4.</p><p>The major contributions of this paper are threefold. First, we propose a global semantic reasoning module (GSRM) to consider global semantic context information, which is more robust and efficient than one-way serial semantic transmission methods. Second, a novel framework named semantic reasoning network (SRN) for accurate scene text recognition is proposed, which combines both visual context information and semantic context information effectively. Third, SRN can be trained in an end-to-end manner, and achieve the state-of-the-art performance on several benchmarks including regular text, irregular text and non-Latin long text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The research of scene text recognition has made significant progress in the past few years. In this section, we will first review some recent text recognition methods, and summarize them into two categories: semantic context-free and semantic context-aware, according to whether semantic information is used. Besides, some popular context modeling structures will also be presented.</p><p>Semantic context-free methods regard scene text recognition as a purely visual classification task, without using any semantic information explicitly. CRNN <ref type="bibr" target="#b28">[29]</ref> firstly combined CNN and RNN to extract sequential visual features of a given text image, and then directly fed them into a CTC decoder to predict the best character category of each time step, where CTC <ref type="bibr" target="#b7">[8]</ref> only maximized the probability of all the paths that can reach the ground truth according to the visual classification of each position. In order to alleviate the back-propagating computation burden of CTC loss, Xie et al. <ref type="bibr" target="#b38">[39]</ref> proposed the aggregation crossentropy (ACE) loss to optimize the statistical frequency of each character along the time dimension, enhancing the efficiency greatly. Inspired by the success of visual segmentation, Liao et al. <ref type="bibr" target="#b21">[22]</ref> used FCN to predict the character categories of each position by pixel-level classification, and gathered characters into text lines by heuristic rules. However, this method requires expensive character-level annotation. Instead of optimizing the decoding accuracy of each step, Jaderberg <ref type="bibr" target="#b12">[13]</ref> directly used CNNs to classify 90k kinds of text image, each of which represented a word. In general, these methods ignore to take semantic context into account.</p><p>Semantic context-aware methods try to capture semantic information to assist the scene text recognition. Most of those methods follow the one-way semantic transmission manner, for example, Lee et al. <ref type="bibr" target="#b18">[19]</ref> encoded the input text image horizontally into 1D sequential visual features, and then guided visual features to attend the corresponding region with the help of semantic information of last time step. As we mentioned before, some of the latest works focus on how to mine more effective visual features, especially for irregular text. In order to eliminate the negative effects brought by perspective distortion and distribution curvature, Shi et al. <ref type="bibr" target="#b30">[31]</ref> added a rectification module before sequence recognition, in which a spatial transform network <ref type="bibr" target="#b14">[15]</ref> with multiple even control point pairs was adopted. Zhan et al. <ref type="bibr" target="#b43">[44]</ref> employed a line fitting transformation with iterative refinement mechanism to rectify the irregular text image. Furthermore, Yang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a symmetry-constrained rectification network based on the rich local attributes to generate better rectification results. There are some methods alleviating the challenge of irregular text recognition by enhancing spatial visual features. Cheng et al. <ref type="bibr" target="#b5">[6]</ref> extracted scene text features in four directions and designed a filter gate to control the contribution of features from each direction. Wojna et al. <ref type="bibr" target="#b36">[37]</ref> introduced additional encoding of spatial coordinates on 2D  feature maps to increase the sensitivity to sequential order. However, these works do not fully exploit semantic context information, which is exactly what we want to focus on in this paper.</p><p>Context modeling structures are designed to capture information in a certain time or spatial range. RNN is good at capturing dependencies of sequence data, but its inherent sequential behavior hinders parallel computation <ref type="bibr" target="#b36">[37]</ref> during the training and inference. To solve those issues, ByteNet <ref type="bibr" target="#b15">[16]</ref> and ConvS2S <ref type="bibr" target="#b6">[7]</ref> directly used CNNs as encoder. These methods can be fully parallelized during training and inference to make better use of the hardware, but cannot flexibly capture the global relations, due to the limitation of receptive field size. Recently, the structure of transformer <ref type="bibr" target="#b23">[24]</ref> has been proposed to capture global dependencies and relate two signals at arbitrary positions with constant computation complexity. In addition, transformer has been proved to be effective in many tasks of computer vision <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref> and natural language processing <ref type="bibr" target="#b33">[34]</ref>. In this paper, we not only adopt transformer to enhance the visual encoding features, but also use the similar structure to reason semantic content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The SRN is an end-to-end trainable framework that consists of four parts: backbone network, parallel visual attention module (PVAM), global semantic reasoning module (GSRM), and visual-semantic fusion decoder (VSFD). Given an input image, the backbone network is first used to extract 2D features V . Then, the PVAM is used to generate N aligned 1-D features G, where each feature corresponds to a character in the text and captures the aligned visual information. These N 1-D features G are then fed into our GSRM to capture the semantic information S. Finally, the aligned visual features G and the semantic information S are fused by the VSFD to predict N characters. For text string shorter than N , 'EOS' are padded. The detailed structure of SRN is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone Network</head><p>We use FPN <ref type="bibr" target="#b22">[23]</ref> to aggregate hierarchical feature maps from the stage-3, stage-4 and stage-5 of ResNet50 <ref type="bibr" target="#b9">[10]</ref> as the backbone network. Thus, the feature map size of ResNet50+FPN is 1/8 of the input image, and the channel number is 512. Inspired by the idea of non-local mechanisms <ref type="bibr" target="#b3">[4]</ref>, we also adopt the transformer unit <ref type="bibr" target="#b33">[34]</ref> which is composed of a positional encoding, multi-head attention networks and a feed-forward module to effectively capture the global spatial dependencies. 2D feature maps are fed into two stack transformer units, where the number of heads in multi-head attention is 8 and the feed-forward output dimension is 512. After that, the final enhanced 2D visual fea-</p><formula xml:id="formula_0">tures are extracted, denoted as V , v ij ∈ R d , where d = 512.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parallel Visual Attention Module</head><p>Attention mechanism is widely used in sequence recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>. It can be regarded as a form of feature alignment where relevant information in the input is aligned to the corresponding output. Therefore, attention mechanism is used to generate N features where each feature corresponds to a character in the text. Existing attention based methods are inefficient because of some time-dependent terms. In this work, a new attention method named parallel visual attention (PVA) is introduced to improve the efficiency by breaking down these barriers.</p><p>Generally, attention mechanism can be described as follows: Given a key-value set (k i , v i ) and a query q, the similarities between the query q and all keys k i are computed. Then the values v i are aggregated according to the similarities. Specifically, in our work, the key-value set is the input 2D features (v ij , v ij ). Following the Bahdanau attention <ref type="bibr" target="#b1">[2]</ref>, the existing methods use the hidden state H t−1 as the query to generate the t-th feature. To make the computation parallel, the reading order is used as the query instead of the time-dependent term H t−1 . The first character in the text has reading order 0. The second character has reading order 1, and etc. Our parallel attention mechanism can be summarized as:</p><formula xml:id="formula_1">       e t,ij = W T e tanh(W o f o (O t ) + W v v ij ) α t,ij = exp(e t,ij ) ∀i,j exp(e t,ij )<label>(1)</label></formula><p>where, W e , W o , and W v are trainable weights. O t is the character reading order whose value is in the list of [0, 1, ..., N − 1], and f o is the embedding function. Based on the idea of PVA, we design the parallel visual attention module (PVAM) to align all visual features of all time steps. The aligned visual feature of the t-th time step can be represented as:</p><formula xml:id="formula_2">g t = ∀i,j α t,ij v ij<label>(2)</label></formula><p>Because the calculation method is time independent, PVAM outputs aligned visual features (G, g t ∈ R d ) of all time steps in parallel.</p><p>As some attention maps shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the obtained attention maps can pay attention to the visual areas of corresponding characters correctly and the effectiveness of PVAM is verified well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global Semantic Reasoning Module</head><p>In this section, we propose the global semantic reasoning module (GSRM) that follows the idea of multi-way parallel transmission to overcome the drawbacks of one-way semantic context delivery. Firstly, we review the probability formula to be maximized in the Bahdanau attention mechanism, a typical RNN-like structure. It can be expressed as the following:</p><formula xml:id="formula_3">p(y 1 y 2 · · · y N ) = N t=1 p(y t |e t−1 , H t−1 , g t )<label>(3)</label></formula><p>where e t is regarded as the word embedding of t-th label y t . At each time step, RNN-like methods can refer to the previous labels or predicted results during the training or inference. So they work in a sequential way since the previous information like e t−1 and H t−1 can only be captured at time step t, which limits the ability of semantic reasoning and causes low efficiency during inference.</p><p>To overcome the aforementioned problems, our key insight is that instead of using the real word embedding e, we use an approximated embedding e which is timeindependent. Several benefits can be made from this improvement. 1) First, the hidden states value of last steps H t−1 are able to be removed from the Eq. 3 and thus the serial forward process will be upgraded to a parallel one with high efficiency because all time-dependent terms are eliminated. 2) Second, the global semantic information, including all the former and the latter characters, is allowed to be combined together and to reason the appropriate semantic content of the current time. Hence, the probability expression can be upgraded as:</p><formula xml:id="formula_4">p(y 1 y 2 · · · y N ) = N t=1 p(y t |f r (e 1 · · · e t−1 e t+1 · · · e N ), g t ) ≈ N t=1 p(y t |f r (e 1 · · · e t−1 e t+1 · · · e N ), g t ) (4)</formula><p>where e t is the approximate embedding information of e t at the t-th time step. The f r in Eq. 4 means a function that can build the connection between the global semantic context and current semantic information. If we denote the s t = f r (e 1 · · · e t−1 e t+1 · · · e N ) and s t is the t-th feature of semantic information S, the Eq. 4 can be simplified to the following one:</p><formula xml:id="formula_5">p(y 1 y 2 · · · y N ) ≈ N t=1 p(y t |s t , g t )<label>(5)</label></formula><p>Inheriting from the above spirit, we propose the GSRM, by which the function f r in Eq. 4 is modeled, to make the supposition come true and benefit from it. The structure of GSRM is composed of two key parts: visual-to-semantic embedding block and semantic reasoning block. Visual-to-semantic embedding block is used for the purpose of generating e , and the detailed structure is shown in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>. Thanks to the PVAM, the features we get are already aligned to every time step or every target character. The aligned visual features G are fed to a fully-connection layer with softmax activation first, and the embedding loss L e , where cross entropy loss is utilized, is added to make them more concentrate on the target characters.</p><formula xml:id="formula_6">L e = − 1 N N t=1 log p(y t |g t )<label>(6)</label></formula><p>Next, embedding vector e t is calculated based on the most likely output characters of g t by the argmax operation and an embedding layer. Semantic reasoning block serves to realize the global semantic reasoning, that is to model the function f r in Eq. 4. Specially, the structure of GSRM is illustrated in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>. Several transformer units are followed with the masked e allowing the model to perceive the global context information with high efficiency. Meanwhile, both first-order relations and higher-order relations, such as word semantic, can be implicitly modeled by multiple transformer units. Finally, the semantic features of every step is output through this module, which is defined as S, s t ∈ R d , d = 512. Meanwhile, the reasoning loss L r is added on the s. The objective function can be defined as</p><formula xml:id="formula_7">L r = − 1 N N t=1 log p(y t |s t )<label>(7)</label></formula><p>The cross entropy loss is performed to optimize the objective probability from the perspective of semantic information, which also helps to reduce the convergence time. It is noticeable that the global semantic is reasoned in a parallel way in the GSRM, making SRN run much faster than the traditional attention based methods, especially in the cases of long text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Visual-Semantic Fusion Decoder</head><p>As we discussed in the Sec. 1, it is very important to consider both visual aligned features G and semantic information S for scene text recognition. However, G and S belong to different domains, and their weights for the final sequence recognition in different cases should be different. Inspired by the gated unit <ref type="bibr" target="#b0">[1]</ref>, we introduce some trainable weights to balance the contribution of features from different domains in our VSFD. The operation can be formulated as the following:</p><formula xml:id="formula_8">z t = σ(W z · [g t , s t ]) f t = z t * g t + (1 − z t ) * s t<label>(8)</label></formula><p>where W z is trainable weight, f t is the t-th fused feature vector, t ∈ [1, N ]. All fused features can be denoted as F , f t ∈ R d , and are used to predict the final characters in a non-recursive manner, the objective function is as follows:</p><formula xml:id="formula_9">L f = − 1 N N t=1 log p(y t |f t )<label>(9)</label></formula><p>Combing all constraint functions in GSRM and VSFD, the final objective function is summarized as follows:</p><formula xml:id="formula_10">Loss = α e L e + α r L r + α f L f<label>(10)</label></formula><p>where L e , L r and L f represent embedding loss, reasoning loss and final decoder loss, respectively. The weights of α e , α r and α f are set to 1.0, 0.15 and 2.0 to trade off these three constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>There are six Latin scene text benchmarks chosen to evaluate our method.</p><p>ICDAR 2013 (IC13) <ref type="bibr" target="#b17">[18]</ref> contains 1095 testing images. Using the protocol of <ref type="bibr" target="#b34">[35]</ref>, we discard images that contain non-alphanumeric characters or less than three characters.</p><p>ICDAR 2015 (IC15) <ref type="bibr" target="#b16">[17]</ref> is taken with Google Glasses without careful position and focusing. We follow the same protocol of <ref type="bibr" target="#b4">[5]</ref> and use only 1811 test images for evaluation without some extremely distorted images.</p><p>IIIT 5K-Words (IIIT5k) <ref type="bibr" target="#b25">[26]</ref> is collected from the website and comprises of 3000 testing images.</p><p>Street View Text (SVT) <ref type="bibr" target="#b34">[35]</ref> has 647 testing images cropped form Google Street View. Many images are severely corrupted by noise, blur, and low resolution.</p><p>Street View Text-Perspective (SVTP) <ref type="bibr" target="#b26">[27]</ref> is also cropped form Google Street View. There are 639 test images in this set and many of them are perspectively distorted.</p><p>CUTE80 (CUTE) is proposed in <ref type="bibr" target="#b27">[28]</ref> for curved text recognition. 288 testing images are cropped from full images by using annotated words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Model Configurations The details of backbone are introduced in Sec.3.1. For PVAM, the size of W e , W o and W v is 512, and the embedding dim of f o is 512 in Eq.1. The embedding dim in GSRM is also set to 512. The semantic reasoning block consists of 4 stacked transformer units, where the number of heads is 8 and the number of hidden units is 512. For fair comparison, the same backbone as our SRN is adopted in the CTC, 1D-Attention and 2D-Attention based methods. The number of both attention units and hidden units in 1D-Attention and 2D-Attention are set to 512.</p><p>Data Augmentation The size of input images is 64 × 256. We randomly resize the width of original image to 4 scales (e.g., 64, 128, 192, and 256), and then pad them to 64 × 256. Besides, some image processing operations <ref type="bibr" target="#b20">[21]</ref>, such as rotation, perspective distortion, motion blur and Gaussian noise, are randomly added to the training images. The number of class is 37, including 0-9, a-z, and 'EOS'. And the max length of output sequence N is set to 25.</p><p>Model Training The proposed model is trained only on two synthetic datasets, namely Synth90K <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> and SynthText <ref type="bibr" target="#b8">[9]</ref> without finetuning on other datasets. The ResNet50 pre-trained on ImageNet is employed as our initialized model and the batch size is 256. Training is divided into two stages: warming-up and joint training. At first stage, we train the SRN without the GSRM for about 3 epochs. ADAM optimizer is adopted with the initial learning rate 1e −4 . At joint training stage, we train the whole pipeline end-to-end with the same optimizer until convergence. All experiments are implemented on a workstation with 8 NVIDIA P40 graphics cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Discussions about Backbone Network and PVAM</head><p>Our SRN utilizes transformer units <ref type="bibr" target="#b33">[34]</ref> in the backbone and adds the character reading order information in the PVAM to capture global spatial dependencies. As depicted in Tab. 1, our backbone with transformer units outperforms the one without it on all benchmarks by at least 3% in accuracy, demonstrating that the importance of global visual context captured by transformer unit. As depicted in the in Tab. 1, the gain of using char reading order is obtained in the most public datasets, especially for CUTE. The performance on some easy tasks is slightly improved, since the attention mechanism without this term is actually able to implicitly capture the reading order through data-driven training. <ref type="table">Table 1</ref>. Ablation study of backbone and PVAM. "Base" means the backbone; "TU" means the transformer units; "CRO" means the character reading order information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Discussions about GSRM</head><p>To evaluate the effectiveness of GSRM in semantic reasoning, we compare the results yielded by the experiments with/without GSRM. Besides, the number of transformer units in GSRM is also explored. As shown in Tab. 2, the GSRM achieves successive gains of 1.5%, 0.2%, 0.8%, 0.3% in IIIT5K and 4.2%, 0.9%, 0.1%, 0.0% in IC15 with the number of Transformer Units set to 1, 2, 4, and 6. This suggests that the semantic information is important to text recognition and GSRM is able to take advantage of these information. Since the performance of 4-GSRM is similar to that of 6-GSRM, the 4-GSRM is adopted in the remaining experiments to preserve controllable computation. To demonstrate the benefits of global semantic reasoning strategy, we compare our approach with two variants: one only runs forward and the other runs backward to capture one-way semantic information. Moreover, the two typical text recognition methods, CTC and 2D-Attention based <ref type="table">Table 3</ref>. Ablation study of semantic reasoning strategy. "2D-ATT" means 2D-Attention; "FSRM" and "BSRM" mean forward and backward one-way semantic reasoning module respectively. methods, are also included in the comparison to prove our superiority to both the existing semantic context-free methods and semantic context-aware methods. As illustrated in Tab. 3, all the semantic context-aware methods outperform the semantic context-free methods (CTC based methods), which highlights the importance of semantic information. Furthermore, the GSRM with global semantic reasoning outperforms those with the one-way semantic reasoning by about 1% in accuracy on most of the benchmarks, verifying the effectiveness of the multi-way semantic reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Discussions about Feature Fusion Strategy</head><p>In this paper, we introduce a novel feature fusion strategy, namely gated unit, which is described in Sec. 3.4. In this section, we conduct experiments to compare our VSFD with three different feature fusion methods, including add, concatenate and dot. Tab. 4 indicates that the other three fusion operations will lead to degradation of performance on benchmarks to a certain extent. Thus, the VSFD is utilized in our approach as default. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Analysis of Right/Failure Cases</head><p>To illustrate how semantic information helps SRN to improve the performance, we collect some individual cases from the benchmarks to compare the predictions of SRN with/without GSRM. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, for example, because the character "r" is visually similar to character "c" in the image with word "Vernon", the prediction without GSRM wrongly gives the character "c", while the prediction with GSRM correctly infers the character "r" with the help of global semantic context. The character "e" in "sale", the character "r" in "precious", and the character "n" in "herbert" are handled by the same working pattern. The failure cases of SRN is shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, which can be divided into two categories: 1) SRN cannot insert or remove characters, but only modifies wrong characters. If the visual features are aligned wrongly or some characters are missed, SRN cannot infer the correct characters, such as "snout"  and "shining". 2) The SRN may not work when both visual and semantic context fail to recognize the image, since the gain of SRN is mainly due to the complementation in visual features and semantic features. When the image suffers from special fonts or low quality and the words in it are rarely appeared in training data, it is difficult for the SRN to get the visual context and semantic dependencies, as the second line in <ref type="figure" target="#fig_6">Fig. 7</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with State-of-the-Arts</head><p>The comparison of our method with previous outstanding methods is shown in Tab. 5. We only compare the results without any lexicon, because the lexicon is always unknown before recognition in practical use. The contextaware methods perform better than context-free methods in general, and the proposed SRN achieves superior performance across the six public datasets compared with stateof-the-art approaches with the help of GSRM, which proves that this particular designed module can make better use of semantic information than existing techniques. For regular datasets, we get a 0.2%, 0.4%, 0.9% improvement on IC13, IIIT5K and SVT respectively. The gain of SVT is quite larger than the other two sets, and we claim that semantic information will play a more significant role, especially for recognition of the low-quality images.</p><p>Although our method does not take special measures into consideration, such as rectification module, to handle the irregular words, like ASTER <ref type="bibr" target="#b30">[31]</ref> and ESIR <ref type="bibr" target="#b43">[44]</ref>, it is worth noting that the SRN achieves comparable or even better per-formance on those distorted datasets. As is shown in Tab. 5, there are increase of 4.0% and 2.8% on IC15 and SVTP respectively and comparable results on CUTE, comparing with State-of-the-Arts methods with rectification modules. Similar to the explanation of the gain on SVT, we believe this is mainly due to the fact that global semantic information will be an important supplementation to the visual information in text recognition, and it will show more effectiveness when facing difficult cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on non-Latin Long Text</head><p>To evaluate the performance on long text, we set up two additional experiments: Attention and CTC based methods with the same configuration. We generate a synthetic long text dataset with the engine in <ref type="bibr" target="#b8">[9]</ref>, which includes 3 million images. Besides, we also use the training set of RCTW <ref type="bibr" target="#b31">[32]</ref> and LSVT <ref type="bibr" target="#b32">[33]</ref> as training data. Following the configuration described in Sec. 4.2, we just change the max decoding length N to 50 and the number of classes to 10784. We evaluated our model on ICDAR2015 Text Reading in the Wild Competition dataset (TRW15) <ref type="bibr" target="#b44">[45]</ref> by character-level accuracy. TRW15 dataset contains 484 test images. We crop 2997 horizontal text line images as the first test set (TRW-T) and select the images whose length is more than 10 as the second test set (TRW-L).</p><p>The results are shown in Tab. 6. Compared with CTC and attention based methods, the proposed approach without GSRM achieved 6.8% and 8.4% boost in TRW-T. Because our method could model 2D spatial information and conquer the error accumulation when the wrong decoding is raised at a certain time step. Compared with SCCM <ref type="bibr" target="#b41">[42]</ref>, our SRN achieves 4.9% improvement over the SRN without GSRM, while LM model in SCCM obtains 4.7% improvement. This shows that GSRM could integrate semantic features with visual features very well, which is important to recognition long text. Compared with the accuracy of TRW-T and TRW-L in the table, the improvement of GSRM increases from 4.9% to 6.8%. We can also find that 2D-Attention has a much lower recognition rate in the TRW-L, which is approximately caused by the accumulation of errors As shown in <ref type="figure" target="#fig_7">Fig. 8</ref>, there are several cases selected from the test set. It's obvious that semantic information can better distinguish two characters, when they are easily confused. For example, "责" is visually similar to "素", while "素 材" is a common Chinese phrase, so the SRN with GSRM correctly infer the character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Inference Speed</head><p>To explore the efficiency of our proposed approach, we evaluate the speed of our method with/without GSRM and compare it with CTC, 1D-Attention and 2D-Attention based recognizers in both the short and long text datasets. The test <ref type="table">Table 5</ref>. Comparisons of scene text recognition performance with previous methods on several benchmarks. All results are under NONE lexicon. "90K" and "ST" mean Synth90K and SynthText; "word" and "char" means the word-level or character-level annotations are adopted; "self" means self-designed convolution network or self-made synthetic datasets are used. SRN w/o GSRM means that SRN cuts down GSRM, and thus loses semantic information.    set is IC15 and TRW-L, of which the average length is 5 and 15 respectively. For a fair comparison, we test all methods with the same backbone network on the same hardware (NVIDIA Tesla K40m). Each method runs 3 times on the test set, and the average time consumed by a single image is listed in Tab. 7.</p><p>Benefiting from the parallel framework in SRN, our model with GSRM is 1.7 times and 1.8 times faster than the 1D and 2D-Attention based method in the IC15, and the acceleration will be enlarged to 2.0 times and 2.2 times in the TRW-L. Meanwhile, the computational efficiency of our approach without GSRM is similar to that of CTC-based method, due to its parallelism and simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we claim that semantic information is of great importance for a robust and accurate scene text recognizer. Given the characters (Latin or non-Latin) of a text line, we use the GSRM to model its semantic context, which includes both first-order relations among characters and higher-order relations. Integrating with GSRM, we propose a novel end-to-end trainable framework named semantic reasoning network (SRN) for recognizing text in the wild, which also contains backbone network, parallel visual attention module and fusion decoder module. SRN achieves SOTA results in almost 7 public benchmarks including regular text, irregular text and non-Latin long text, and extensive experiments are conducted to show the significant superiority over the existing methods. Additionally, since all modules of SRN are time independent, SRN can run in parallel and is more practical than other semantic modeling methods. In the future, we are interested in improving the efficiency of GSRM, and making it adaptive to CTC-based methods to boost its value in practical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of text in the wild. (a) are some difficult scene text images, (b) are individual characters extracted separately from (a), and (c) are the corresponding semantic word contents. The characters with red dashed boxes in (b) are easy to be misclassified, only based on visual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Two different manners of semantic context delivery. (a) is one-way serial transmission, and (b) is multi-way parallel transmission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The pipeline of the semantic reasoning network (SRN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Attention maps calculated by PVAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The detailed structure of GSRM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Right cases of SRN with/without GSRM. The predictions are placed along the right side of images. The top string is the prediction of SRN without the GSRM; The bottom string is the prediction of SRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Failure cases of SRN. The predictions and ground-truth labels are placed along the right side of images. The top string is the prediction of SRN without the GSRM; The middle string is the prediction of SRN; The bottom string is the ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Right cases for non-Latin long text. There are two predictions under the image. The left string is the prediction of SRN without the GSRM; The right string is the prediction of SRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of GSRM configuration. "n-GSRM" means the GSRM has n transformer units.</figDesc><table><row><cell></cell><cell>IC13</cell><cell>IC15</cell><cell>IIIT5K</cell><cell>SVT</cell><cell>SVTP</cell><cell>CUTE</cell></row><row><cell>no GSRM</cell><cell>93.2</cell><cell>77.5</cell><cell>92.3</cell><cell>88.1</cell><cell>79.4</cell><cell>84.7</cell></row><row><cell>1-GSRM</cell><cell>94.7</cell><cell>81.7</cell><cell>93.8</cell><cell>88.5</cell><cell>82.6</cell><cell>88.9</cell></row><row><cell>2-GSRM</cell><cell>95.6</cell><cell>82.6</cell><cell>94.0</cell><cell>91.0</cell><cell>83.9</cell><cell>87.8</cell></row><row><cell>4-GSRM</cell><cell>95.5</cell><cell>82.7</cell><cell>94.8</cell><cell>91.5</cell><cell>85.1</cell><cell>87.8</cell></row><row><cell>6-GSRM</cell><cell>95.0</cell><cell>82.7</cell><cell>95.1</cell><cell>90.6</cell><cell>84</cell><cell>86.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of feature fusion strategy.</figDesc><table><row><cell></cell><cell>IC13</cell><cell>IC15</cell><cell>IIIT5K</cell><cell>SVT</cell><cell>SVTP</cell><cell>CUTE</cell></row><row><cell>Add</cell><cell>95.2</cell><cell>81.7</cell><cell>93.8</cell><cell>90.9</cell><cell>84.3</cell><cell>87.8</cell></row><row><cell>Concat</cell><cell>95.0</cell><cell>82.0</cell><cell>93.8</cell><cell>91.5</cell><cell>82.9</cell><cell>88.1</cell></row><row><cell>Dot</cell><cell>94.8</cell><cell>81.0</cell><cell>92.0</cell><cell>89.7</cell><cell>84.5</cell><cell>88.1</cell></row><row><cell>Gated unit</cell><cell>95.5</cell><cell>82.7</cell><cell>94.8</cell><cell>91.5</cell><cell>85.1</cell><cell>87.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">Recognition accuracies (character-level) on non-Latin</cell></row><row><cell>long text dataset</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>TRW-T(%)</cell><cell>TRW-L(%)</cell></row><row><cell>CASIA-NLPR[45]</cell><cell>72.1</cell><cell>-</cell></row><row><cell>SCCM w/o LM[42]</cell><cell>76.5</cell><cell>-</cell></row><row><cell>SCCM[42]</cell><cell>81.2</cell><cell>-</cell></row><row><cell>2D-Attention</cell><cell>72.2</cell><cell>59.8</cell></row><row><cell>CTC</cell><cell>73.8</cell><cell>70.9</cell></row><row><cell>SRN w/o GSRM</cell><cell>80.6</cell><cell>77.5</cell></row><row><cell>SRN</cell><cell>85.5</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Comparion of speed during inference</figDesc><table><row><cell>Method</cell><cell>IC15</cell><cell>TRW-L</cell></row><row><cell>CTC</cell><cell>128.6ms</cell><cell>131.8ms</cell></row><row><cell>1D-Attention</cell><cell>323.3ms</cell><cell>431.1ms</cell></row><row><cell>2D-Attention</cell><cell>338.8ms</cell><cell>486.9ms</cell></row><row><cell>SRN w/o GSRM</cell><cell>131.5ms</cell><cell>137.3ms</cell></row><row><cell>SRN</cell><cell>191.6ms</cell><cell>216.8ms</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01992</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1508" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aon: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangliu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep structured output learning for unconstrained text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5903</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mas</surname></persName>
		</author>
		<title level="m">Icdar 2013 robust reading competition. In ICDAR</title>
		<meeting><address><addrLine>David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mask textspotter: An end-toend trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scene text recognition from two-dimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhang</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05708</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>2d attentional irregular scene text recognizer</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahnakote</forename><surname>Trung Quy Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangxuan</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhar</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palaiahankote</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Chee Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Icdar2017 competition on reading chinese text in the wild (rctw-17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1429" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chinese street view text: Large-scale chinese text reading with partially supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9086" to="9095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-based extraction of structured information from street view imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar-Shyang</forename><surname>Alexander N Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ibarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="844" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Editing text in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1500" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregation cross-entropy for sequence recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoxiong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6538" to="6547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Symmetry-constrained rectification network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushuo</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaigui</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9147" to="9156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu-Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01727</idno>
		<title level="m">Scene text recognition with sliding convolutional character models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An end-to-end video text detector with online tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07135</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03184</idno>
		<title level="m">Icdar 2015 text reading in the wild competition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
