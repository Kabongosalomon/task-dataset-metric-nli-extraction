<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Micro-Net: A unified model for segmentation of various objects in microscopy images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-01-22">22 Jan 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><forename type="middle">E</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Raza</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Division of Molecular Pathology</orgName>
								<orgName type="institution">The Institute of Cancer Research</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Cheung</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Life Sciences</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shaban</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Graham</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Epstein</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Pelengaris</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Life Sciences</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Khan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Life Sciences</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
							<email>nasir.rajpoot@ieee.orgnasirm.rajpoot</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Pathology</orgName>
								<orgName type="institution">University Hospitals Coventry and Warwickshire</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">The Alan Turing Institute</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<addrLine>Shan E Ahmed Raza)</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Micro-Net: A unified model for segmentation of various objects in microscopy images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-01-22">22 Jan 2019</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Medical Image Analysis January 24, 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cell segmentation</term>
					<term>nuclear segmentation</term>
					<term>gland segmentation</term>
					<term>convolution neural networks</term>
					<term>microscopy image analysis</term>
					<term>digital pathology * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object segmentation and structure localization are important steps in automated image analysis pipelines for microscopy images. We present a convolution neural network (CNN) based deep learning architecture for segmentation of objects in microscopy images. The proposed network can be used to segment cells, nuclei and glands in fluorescence microscopy and histology images after slight tuning of input parameters. The network trains at multiple resolutions of the input image, connects the intermediate layers for better localization and context and generates the output using multi-resolution deconvolution filters.</p><p>The extra convolutional layers which bypass the max-pooling operation allow the network to train for variable input intensities and object size and make it robust to noisy data. We compare our results on publicly available data sets and show that the proposed network outperforms recent deep learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In automated microscopic image analysis pipelines, segmentation of key structures such as tumours, glands and cells is an important step <ref type="bibr" target="#b3">(Awan et al. (2017)</ref>; <ref type="bibr" target="#b41">Yuan et al. (2012)</ref>; <ref type="bibr">Qaiser et al. (2017)</ref>). Recent advances in deep learning have helped to achieve accurate segmentation of these structures. A major strength of deep learning is that the same network architecture can be used to segment various structures across different modalities by retraining and slight tuning of the input parameters <ref type="bibr" target="#b31">(Shelhamer et al. (2017)</ref>; <ref type="bibr" target="#b28">Ronneberger et al. (2015)</ref>).</p><p>In this paper, we propose a CNN with additional layers in the downsampling path, bypassing the max-pooling operation in order to learn the parameters for segmentation ignored during the max-pooling operation. By doing so, we retain contextual information, make the network interpret the output at multiple resolutions and train the model at multiple input image resolutions in the downsampling path to learn the model parameters for variable cell/nucleus/gland sizes and shapes in the presence of variable intensities and texture. There are two main features of the proposed architecture: (a) it learns image features at multiple input resolutions for better understanding of tissue components and (b) it bypasses the max-pooling operation through extra layers to retain information from weak features may be missed during max-pooling. This makes the network robust to noise and helps to learn the context at multiple resolutions.  <ref type="bibr" target="#b28">(Ronneberger et al. (2015))</ref> and <ref type="figure" target="#fig_2">Figure 2</ref>(c) the result of the proposed approach. It can be observed that U-Net failed to learn the features due to the presence of a dark cytoplasmic region and segmented most of the cellular region instead of just the nucleus, whereas the proposed approach learned the context at multiple resolutions and successfully located the nuclei despite high levels of noise. We discuss this in detail in Section 4. This paper is an extension of our previous work on cell <ref type="bibr" target="#b23">(Raza et al. (2017a)</ref>) 1 and gland segmentation <ref type="bibr">(Raza et al. (2017b)</ref>) with the following novel contributions: ground truth, (b) U-Net <ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>) (c) proposed. U-Net clearly misses the boundary and is inclined towards strong contrast whereas the proposed method segments nuclei instead of strong contrast with the background. This is because the proposed approach learns the features at multiple input resolutions and learns for weaker boundaries.</p><p>1. A unified framework for segmentation of various types of objects (nuclei, cells, glands) in two different types of image modalities (histology and fluorescence microscopy).</p><p>2. We discuss in detail the challenges faced for training a CNN for segmentation and present a solution to overcome those challenges.</p><p>3. Detailed results to show the robustness of the method to high levels of noise and comparative evaluation with the state-of-the-art.</p><p>4. We propose how the proposed network architecture can be modified/extended for different applications.</p><p>5. In order to justify and clearly demonstrate the effect of additional layers, we present results for Micro-Netafter removing the multi-resolution input and the bypass layers while keeping the rest of the architecture the same as Micro-Net.</p><p>6. Addition of another data set to our analysis where we compare our results with those in the MICCAI 2017 computational precision medicine (CPM) challenge contest dataset for nuclear segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>The existing literature on segmentation methods can be broadly classified into two main categories: handcrafted feature based approaches and deep learning based methods. Most of the existing handcrafted feature based approaches to cell/nuclear segmentation employ a combination of thresholding, filtering, morphological operations, region accumulation, marker controlled watershed <ref type="bibr" target="#b40">(Yang et al. (2006)</ref>; <ref type="bibr" target="#b35">Veta et al. (2013)</ref>), deformable model fitting <ref type="bibr">(Bergeest and</ref> Rohr <ref type="formula" target="#formula_0">(2012)</ref>), graph cut <ref type="bibr" target="#b8">(Dimopoulos et al. (2014)</ref>) and feature classification <ref type="bibr" target="#b15">(Li et al. (2015)</ref> where the morphology of glands is quite complex. <ref type="bibr" target="#b19">Nguyen et al. (2012)</ref> grouped the nuclei, cytoplasm and lumen using colour space analysis and grew the lumen region with constraints to achieve segmentation. <ref type="bibr" target="#b11">Gunduz-Demir et al. (2010)</ref> represented each tissue component as a circular disc and constructed a graph with nearby discs joined by an edge. They performed region growing on lumen discs that were constrained by lines joining the nuclear discs. <ref type="bibr" target="#b20">Nosrati and Hamarneh (2014)</ref> and <ref type="bibr" target="#b7">Cohen et al. (2015)</ref> first classify tissue regions into different constituents and then employ a constrained level set algorithm to segment the glands. <ref type="bibr" target="#b33">Sirinukunwattana et al. (2015)</ref> identified epithelial superpixels and used epithelial regions as vertices of a polygon approximating the boundary of a gland. Most of the methods discussed above first distinguish tissue regions and then employ region growing or level sets to segment glandular regions. Recently, <ref type="bibr" target="#b14">Li et al. (2017)</ref> proposed a slightly different approach where they first determine potential epithelial regions using lumen/background information and then identify connected epithelial cells to segment the glands using a multi-resolution cell orientation descriptor.</p><p>In this paper, we focus on deep learning based approaches using convolu- for segmentation is considered to be a benchmark for segmentation tasks using deep learning <ref type="bibr" target="#b31">(Shelhamer et al. (2017)</ref>). The network performs pixel-wise classification to obtain the segmentation mask for a given input and consists of downsampling and upsampling paths. The downsampling path consists of convolution and max-pooling and the upsampling path consists of convolution and deconvolution (convolution transpose) layers. U-Net <ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>)</p><p>is inspired by FCN but connects intermediate downsampling and upsampling paths to conserve the context information. Recently, <ref type="bibr" target="#b29">Sadanandan et al. (2017)</ref> used the CellProfiler pipeline <ref type="bibr" target="#b5">(Carpenter et al. (2006)</ref>) as an automatic way of generating ground truth to train the network and employed a variation of fully convolutional network inspired by the improvements in U-Net <ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>) and residual network architecture <ref type="bibr" target="#b12">(He et al. (2016)</ref>) for cell segmentation. <ref type="bibr" target="#b13">Kraus et al. (2016)</ref> use multiple instance learning (MIL) to simultaneously segment and classify cells in microscopy images. The binary instance classifier generates the predictions which are combined through an aggregate function in the MIL layer of the proposed network. However, this approach can be computationally expensive for solving a segmentation problem as multiple feature maps need to be aggregated using the global pooling function. DCAN ) employs a modified FCN that simultaneously segments both the objects and contours to assist separating clustered object instance. Another recently proposed multi-scale convolutional neural network <ref type="bibr" target="#b34">(Song et al. (2017))</ref> trains the network at different scales of the Laplacian pyramid and merges the network in the upsampling path to perform segmentation. <ref type="bibr">Xu et al. (2016</ref><ref type="bibr" target="#b39">Xu et al. ( , 2017</ref> proposed a network that performs side supervision of boundary maps in addition to the foreground. <ref type="bibr" target="#b16">Manivannan et al. (2018)</ref> combined handcrafted features with deep learning for segmentation, but this approach is computationally expensive as it not only requires calculation of features using classical approaches but also a support vector machine (SVM) classifier to predict local label patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Sets and Challenges</head><p>The data sets that we use in this paper come from two different sources. The first data set contains images acquired using a multiplexed fluorescence microscope, capable of acquiring images of multiple tags in a cyclic manner <ref type="bibr" target="#b30">(Schubert et al. (2006)</ref>), where our task was cell segmentation. The other two data sets are</p><p>Haematoxylin and Eosin (H&amp;E) stained microscopic images collected as part of open challenge contests. We use one of the data sets to evaluate nuclear segmentation in four different tumour types (CPM) and the other one for gland segmentation in colon cancer histology images ).</p><p>In this way, we demonstrate that the proposed network is capable of dealing with diverse datasets and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multiplexed Fluorescence Imaging Data</head><p>We first focus on segmentation of individual cells in multiplexed fluorescence images using nuclear and membrane markers. In the fluorescence microscopy images, this task is challenging for various reasons, for example relatively large variation in intensity of captured signal and difficulty with separating neighbouring cells. It requires careful tuning of the algorithm to make it robust to intensity, shape, size and fusion of individual cellular regions. That process can require experimentation with a variety of features and can be time consuming.</p><p>Membrane markers such as E-cadherin (or Ecad) mark the boundary of individ-ual cells, but the intensity of the membrane markers varies depending on type and orientation of each cell which makes segmentation difficult.</p><p>A multi-channel fluorescence microscope known as the Toponome Imaging System (TIS) <ref type="bibr" target="#b30">(Schubert et al. (2006)</ref>), acquired images of tissue samples from mouse pancreata. The TIS microscope is capable of capturing signals from multiple biomarkers, but for cell segmentation we employ only two channels corresponding to Ecad (membrane marker using FITC channel) and DAPI (nuclear marker). After segmentation work is completed, the other channels are available to study individual cells, and to group similar cells together for statistical purposes. We performed alignment and normalization of the multi-channel images using protocols designed for pre-processing of the TIS data <ref type="bibr" target="#b25">(Raza et al. (2012</ref><ref type="bibr" target="#b26">(Raza et al. ( , 2016</ref>). Next, ground truth for image segmentation, marked by an expert biologist, was used for training.</p><p>Sample images of mouse pancreatic exocrine cells and endocrine cells are shown in <ref type="figure" target="#fig_4">Figure 3</ref> as RGB composite images (enhanced for display), where membrane marker is shown in green, nuclear marker in blue and ground truth is overlaid in red with black boundaries. One can observe the variation in intensities of cell boundaries and that the nuclei are not always present and, if present, are not always positioned at the centre of the cell. This is because a tissue is a three-dimensional structure which is finely cut into multiple sections to obtain a two-dimensional image, which may or may not contain part of the cell containing the nucleus. Pancreatic cells are either endocrine cells, seen in the islets, or exocrine cells. Endocrine cells are more tightly packed and are smaller than exocrine cells. In addition, images with varying levels of signalto-noise ratio (SNR) are expected in fluorescence microscopy images where not only the imaging apparatus but also antibody concentration, temperature and incubation times contribute to noise. These variations make segmentation a challenging task.  Bottom row: lower grade glioma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Gland Segmentation (GLaS) Challenge Data Set</head><p>Histological assessment of glands is one of the key factors in colon cancer grading ). This requires a highly trained pathologist, is labour intensive, suffers from inter and intra-observer variability and has limited reproducibility. Due to complex nature of the problem, sophisticated algorithms are needed for successful automatic segmentation. Automatic segmentation of glands is challenging due to high variation in texture, size and structure of glands especially in malignant tissue. The third data set we use in this paper is the publicly available Warwick-QU data set published as part of the GLand Segmentation (GLaS) challenge ).</p><p>The data set consists of 165 images with the associated ground truth marked by expert pathologists. The composition of the data set is detailed in <ref type="table" target="#tab_1">Table 1</ref>, whereas a few sample images from the data set are shown in <ref type="figure" target="#fig_6">Figure 5</ref>. In <ref type="figure" target="#fig_6">Figure   5</ref>, the top row shows sample images from benign cases, and the bottom row shows sample images from malignant cases. <ref type="figure" target="#fig_6">Figure 5</ref> (c) has been taken from a moderately differentiated colon cancer tissue and (d) has been taken from a poorly differentiated colon cancer tissue section. It is evident from these images that there is a large variation in the size, texture and structure of glands in both malignant and benign cases although the variation is greater in malignant cases.  3. The Proposed Network</p><p>The architecture of proposed Micro-Net is shown in <ref type="figure" target="#fig_7">Figure 6</ref>. In the case of fluorescence images, the input to the network consists of two features, i.e., membrane and nuclear marker images, whereas in the case of H&amp;E images the input to the network is a stain normalised RGB image. We perform stain normalisation 2 using the method proposed by <ref type="bibr" target="#b27">(Reinhard et al. (2001)</ref>) to reduce the effect of stain variation from different labs and staining conditions. In both cases, the network performes batch normalisation at the input layer. The network is divided into five groups and thirteen branches, the division depending on their function and the set of layers/filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Group 1: Downsampling</head><p>The first group, which consists of four branches with output B1-B4, constructs the downsampling path. Each branch in Group 1 consists of convolution, max-pooling, resize and concatenation layers. The convolution and maxpooling layers perform standard operations as in conventional CNNs. We use tanh activation after each convolution layer as our experiments showed that the network converges faster with tanh activation than with ReLU. The resize layer resizes the image using bicubic interpolation so that the resized image dimension matches the corresponding dimension of the max-pooling output. We add the lower resolution input to retain the information from pixels that do not have the maximum response, because they are in the vicinity of a noisy neighbourhood. This is particularly useful when we are trying to retain tiny feature details ignored during the max-pooling operation, for example, when trying to detect cells with boundary markers having extreme intensities, even for individual cells as shown in <ref type="figure" target="#fig_4">Figure 3</ref>. Another aspect of the resizing operation is to train the network on different sized cells/nuclei and glands as explained in Section 2. The output of branch 1 (B1) has feature depth of size 128 where the first half <ref type="formula">(64)</ref> of the features are the result of the max-pooling operation and the next half (64) are obtained by performing convolutions only on the resized image. The following branches in Group 1 double the feature depth of the previous branch but follow the same protocol in generating the branch output. The only difference is that B1 performs batch normalisation at the input and the resize layer whereas B2-B4 perform batch normalisation at the resize layer only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group 2: Bridge</head><p>Group 2, consisting of B5, bridges the connection between the downsampling and upsampling paths, whose architecture is very similar to conventional CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Group 3: Upsampling</head><p>Group 3 forms the upsampling path and consists of branches B6, B7, B8 &amp; B9. Each of these branches take two inputs, one from the previous branch and one from the branch with the closest feature dimension in the downsampling path. The output of each branch is double in height and width and half the depth of the previous branch. The second input is added from the downsampling path for better localization and to capture context information as in <ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>). It also passes the convolution only features to the upsampling path, which helps to learn from features which do not have maximum response in the downsampling path. Compared to U-Net <ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>), we add additional deconvolution layers instead of cropping the feature from the downsampling path. This allows us to produce a segmentation map of the same size as the input image and an overlap-tile strategy is not required. It also reduces the number of patches required to produce the desired segmentation output thus removing computational steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Group 4 &amp; 5: Auxiliary and Main Output</head><p>Group 4 &amp; 5 generate the auxiliary and main output and calculate the loss function. Group 4 consists of three branches where each branch takes output from one of B7-B9 and generates three auxiliary feature masks, which are fed into the main output branch. The output branch concatenates feature masks and performs convolution followed by softmax classification to get the segmentation output map p o (x) where x represents a pixel location. The output of branches B7-B9 are of different resolutions and so the deconvolution layer in each of the auxiliary branches is set to generate the output of the same size ). The deconvolution is followed by a convolution layer which produces the auxiliary feature mask. Each of the auxiliary feature masks is followed by a dropout layer (set to 50%) and the convolution layer followed by softmax classification to get the auxiliary outputs (p a1 (x), p a2 (x), p a3 (x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Modifications for Gland Segmentation</head><p>For gland segmentation we slightly modified the network to train on a bigger patch size as shown in <ref type="figure" target="#fig_7">Figure 6(b)</ref>. We doubled the input size to incorporate larger context to take account of the larger size of glands as compared individual cells. This modified architecture consists of five groups and fourteen branches where all five groups and the corresponding branches perform the same tasks as in <ref type="figure" target="#fig_7">Figure 6(a)</ref>. However, the architecture of group 2 was slightly modified to learn deep features by adding an additional branch that performs deconvolution followed by convolution. The additional branch in group 2 was added so that the smallest feature patch size is (8 × 8) in line with the Micro-Net 252 architecture.</p><p>The rest of the architecture remains the same except for the size of input/output for each branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Loss Function</head><p>For training, we calculate weighted cross entropy loss for the main output (l o ) and the auxiliary outputs (l a1 , l a2 , l a3 ) as</p><formula xml:id="formula_0">l k = x∈Ω w(x) log(p k(x) (x))<label>(1)</label></formula><p>where k ∈ {o, a1, a2, a3}, as explained in Subsection 3.4 and Ω is the set of pixel locations in the input image. The weight function w(x) gives higher weights to pixels that are at the merging cell boundaries, leading to a higher penalty <ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>). The total loss (l) is calculated by combining auxiliary and main loss by using l = l o + (l a1 + l a2 + l a3 )/epoch where epoch &gt; 0</p><p>represents the number of training passes already made through the data. This strategy reduces exponentially the contribution of auxiliary losses for a higher epoch, avoiding reduction of the contribution by large steps ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Data Augmentation</head><p>As deep learning algorithms require large amounts of data for training, we augment the data using barrel, pincushion and moustache distortion. While adjusting parameters we made sure by visual examination that the distortions created by these parameters were realistic and not too strong. For cell segmentation on fluorescence imaging data, we augmented the data by adding white</p><p>Gaussian noise with mean 0 and variance in the range 0.0007 to 0.001, where for each patch the value of variance was randomly selected. For nuclear and gland segmentation we introduced Gaussian blur with a Gaussian filter of size 12 × 12, with σ ranging from 0.2 to 2. The value σ was randomly selected for each patch. In addition we rotate, and flip the images left, right, up and down. To train the network for cell/nuclear (gland) segmentation, we first extract 300 × 300 (600 × 600) patches from the training data. If the size of image is smaller than 300 (600) in height or width, we symmetrically pad the image to increase its size. During training the network picks these patches in a random order for each epoch, choosing centres for the patches at random locations, and then cropping them to a size of 252 × 252 (508 × 508) patch before inputting.</p><p>The proposed network was implemented using TensorFlow v0.12 <ref type="bibr">(Abadi et al. (2015)</ref>). We start with a learning rate (lr = 0.001) and reduce it according to lr = 0.001/(10 (epoch/5) ), which reduces the learning rate by a factor of 10 for every fifth epoch.       <ref type="figure" target="#fig_9">Figure 8</ref> shows the performance of the proposed method on CPM data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Computational Precision Medicine (CPM) Data Set</head><p>Row 1-4 show sample images from head and neck squamous cell carcinoma  In GBM, the proposed algorithm misses a few cells with fainter nuclei. Overall the performance of the algorithm seems good; however it seems to struggle with nuclei under multiple shades. U-Net on the other hand shows more red in all cases demonstrating oversegmentation. This is particularly noticeable in NSCLC where there seems to be a strong cytoplasmic shade. Micro-Net struggles in these circumstances but performs much better than U-Net due to its robustness. We quantitatively measure the performance of segmentation using two evaluation metrics as selected by the contest organisers (CPM), namely Traditional Dice (Dice 1) and Ensemble Dice (Dice 2). Dice 1 measures the overlap between the ground truth and the prediction, whereas Dice 2 also penalises the prediction if there is a mismatch in the way segmentation regions are split. The overall score is then computed as the average of the two Dice coefficients. We compare our results with FCN8 <ref type="bibr" target="#b31">(Shelhamer et al. (2017)</ref>), U-Net <ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>), SAMS-Net <ref type="bibr" target="#b10">(Graham and Rajpoot (2018)</ref>) and the submissions in the competition. The results in <ref type="table" target="#tab_10">Table 8</ref> show that our method not only outperforms the results of contest winners but also recent deep learning methods such as SAMS-Net <ref type="bibr" target="#b10">(Graham and Rajpoot (2018)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Gland Segmentation Challenge (GLaS) Data Set</head><p>We <ref type="formula">used</ref>     ). In the case of Hausdorff distance lower values are better; for other measures higher are better. The quantitative results are given in <ref type="table" target="#tab_12">Table 9</ref> which shows that our method produces competitive results compared to the state-of-the-art algorithms from the contest and ranks third after the recently proposed <ref type="bibr" target="#b39">(Xu et al. (2017)</ref>; <ref type="bibr" target="#b16">Manivannan et al. (2018)</ref>) according the rank sum criteria set by the organisers. <ref type="bibr" target="#b39">Xu et al. (2017)</ref> and <ref type="bibr" target="#b16">Manivannan et al. (2018)</ref>   in terms of qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In multi-channel fluorescence microscopy, cell segmentation can help to build molecular profiles of individual cells. However, images captured using fluorescence microscopy contain very weak and variable intensities that make it difficult to segment cells in these types of images. The variable size of the cells makes  it even more challenging for image processing algorithms to perform cell segmentation. In tumour histology slides, the morphology of nuclei and nuclear pleomorphism can help in making a diagnosis and in studying the tumour microenvironment but nuclear segmentation is difficult due to varying shape, size, chromatin structure and clumped nuclei. Similarly morphology of glands can help the pathologist to grade the cancer but it is also very challenging due to texture, size and structure of the glands. All these tasks require sophisticated segmentation algorithms. We have presented a deep learning architecture named Micro-Net that can be used to segment cells/nuclei and glands in fluorescence and H&amp;E stained images with slight tuning of the input parameters. The proposed architecture allows the network to visualise input and output at multiple resolutions. The extra convolutional layers bypass the max-pooling layer, thus allowing the network to better train its parameters for weak features in addition to the strongly observed feature sets. This has been demonstrated by the robustness of algorithm to varying level of noise in fluorescence image data.</p><p>Intermediate connections between the layers allow context and localization to be retained. The qualitative and quantitative results show that the Micro-Net architecture outperforms recently published deep learning approaches. We will make the fluorescence image data set publicly available subject to publication of this manuscript. The other two data sets we used in this paper are already publicly available. We showed that the proposed algorithm produces competitive results compared to the state-of-the-art. The results produced by the algorithm can be extended to build molecular profile in multiplexed fluorescence images, grade cancer or study tumour microenvironment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We are grateful to the BBSRC UK for supporting this study through project grant BB/K018868/1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 &amp;</head><label>1</label><figDesc>2 demonstrate the impact of these design changes. In Figure 1, solid lines represent training accuracy/loss for Micro-Net and Micro-Net -, whereas dashed lines represent validation accuracy/loss for Micro-Net and Micro-Netduring training. Accuracy is defined in terms of pixel-wise agreement with the ground truth and loss is defined in Section 3.6. In Micro-Net -, we removed the multi-resolution input and the bypass layers while keeping the rest of the architecture the same. The improved accuracy and loss values demonstrate the importance of the proposed design changes. To emphasise this further, Figure 2(a) shows an H &amp; E image where nuclei are outlined with green boundaries by an expert, Figure 2(b) outlines the result of U-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Solid lines represent training accuracy/loss for Micro-Net and Micro-Netwhile the dashed lines represent validation accuracy/loss calculated for 25 epochs on fluorescence imaging data for cell segmentation. Micro-Netwas obtained by removing multi-resolution input and the bypass layers from Micro-Net architecture. The accuracy and loss curves clearly show the importance of the multi-resolution input and bypass layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Nuclear segmentation on a sample H &amp; E image from lung outlined in green (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>tional neural networks (CNNs). These have recently received a wealth of attention, due to state-of-the-art performance in recent computer vision tasks, including segmentation (Shelhamer et al. (2017); Ronneberger et al. (2015); Chen et al. (2017); Song et al. (2017)). The fully convolutional network (FCN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Top row: Membrane marker (Ecad-FITC) is shown in green and nuclear marker (DAPI) in blue. Bottom row: ground truth is overlaid in red with black boundaries. Left: Exocrine Cells. Right: Endocrine Cells. 2.2. The Computational Precision Medicine (CPM) Data Set for nuclear segmentation Nuclear segmentation can help understand the tumour microenvironment by studying features such as nuclear pleomorphism and nuclear morphology. Segmentation of nuclei in histology images is difficult, especially within tumour cells due to their heterogeneous nature with high variation in shape, size and chromatin pattern. The data set we use in this paper was published as part of a challenge contest at Medical Image Computing and Computer Assisted Interventions (MICCAI) 2017. The data set contains 32 training and 32 testing image tiles along with ground truth marking for nuclear segmentation, extracted from multi-tissue H&amp;E stained histology slides. There is an equal representation of glioblastoma multiforme (GBM), lower grade glioma (LGG), head and neck squamous cell carcinoma (HNSCC) and non-small cell lung cancer (NSCLC). A couple of example images (left) with corresponding ground truth outlined with green boundary (right) are shown in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Left: Sample Images from the CPM data set. Right: Ground truth marking outlined in green colour on the sample images. Top row: head and neck squamous cell carcinoma.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Sample images from the GLaS data set). Theimages are shown in pairs, where the sample image on the left is overlaid on the right with the ground truth. The top row shows sample images from benign cases and the bottom row shows sample images from malignant cases. (a) &amp; (b) show variation in size and structure of glands in benign cases, whereas (c) &amp; (d) show variation in malignant colon cancer, where (c) is taken from a moderately differentiated sample and (d) is taken from a poorly differentiated (higher grade) cancerous sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>The proposed Micro-Net architecture. (a) Micro-Net-252 &amp; (b) Micro-Net-508.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Segmentation results, ground truth in red, output of the algorithm in green and overlap between ground truth and output of algorithm in yellow. Top row: Exocrine region. Bottom row: Endocrine region. Columns (left to right) are output from FCN8, U-Net, DCAN and Micro-Net architectures.4. Results and Discussion4.1. Multiplexed Fluorescence Imaging DataOur image data consists of 10 images of size 2048 × 2048 pixels(11,163 cells)    of which 6 images (with approximately 60% i.e., 6,641 cells) are used for training and 4 images (with the remaining 40% i.e., 4,522 cells) for testing. During training, we used 20% of the data for validation. We compare our results with the state-of-the-art FCN8<ref type="bibr" target="#b31">(Shelhamer et al. (2017)</ref>), DCAN) and U-Net<ref type="bibr" target="#b28">(Ronneberger et al. (2015)</ref>) networks. To remove the bias we trained all the networks on the same training data obtained after augmentation. We used the authors' implementation of FCN8 and trained it for our data, whereas DCAN and U-Net were implemented in TensorFlow. The weights for the proposed network were initialised with truncated Gaussian and the network was trained for 25 epochs. The checkpoint was chosen based on the minimum validation loss. For U-Net and DCAN, we ran the network for 30 epochs but the criteria for choosing the trained checkpoint file was the same (i.e., best checkpoint was based on minimum validation loss.) The results(Figure 7)show that FCN8 identified cellular regions but was not able to segment individual cells.DCAN is designed to learn the contour features and performed better segmentation of the cells in the exocrine region but performed poorly with smaller sized cells in the endocrine region. U-Net performed better than both FCN and DCAN but missed the cells with weaker boundaries. The proposed Micro-Net method, performed better in the presence of variable intensities and variable size/shape of the cells. The output inFigure 7was post-processed for all the algorithms using area opening (100 pixels) and hole filling operations to get the final output score in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Top to Down: (1) HNSCC, (2) LGG, (3) NSCLC, (4) GBM. Left to Right: (1) H &amp; E image with ground truth outlined for nuclear segmentation, (2) RGB composite image with ground truth in green and output of U-Net in red, (3) RGB composite image with ground truth in green and output of the proposed algorithm in red. Yellow is the overlap between the ground truth and the output of the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Results of the proposed network for sample images in Figure 5. First and second column show RGB composite images with output of the DCAN (Chen et al. (2017)) and the proposed algorithm respectively in red, ground truth in green and overlap of ground truth and output in yellow. The image in the third column shows the output of the proposed method overlaid on the sample image. The results for DCAN were obtained from the contest organisers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Composition of Warwick-QU data set.</figDesc><table><row><cell></cell><cell cols="2">Number of images</cell><cell></cell></row><row><cell>Histologic Grade</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Training Test A Test B</cell></row><row><cell>Benign</cell><cell>37</cell><cell>33</cell><cell>4</cell></row><row><cell>Malignant</cell><cell>48</cell><cell>27</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Dice, pixel accuracy and object Hausdorff but failed to increase the Dice coefficient. In addition, we add results for Micro-Netwhere we removed the multi-resolution input and the bypass layers. The results are slightly better compared to U-Net but not better than Micro-Net supporting the suggested changes in the design.In addition to the above experiments we tested all the network architectures for their robustness to various levels of white Gaussian noise by controlling the SNR and generating the output for various network architectures. For this purpose we did not retrain the networks but used the already trained models as above. The values for Dice, F1, object Dice, pixel accuracy and object Hausdorff</figDesc><table><row><cell>. For quantitative analysis, we used measures</cell></row><row><cell>which include Dice coefficient, F1 score, object Dice, pixel accuracy and object</cell></row><row><cell>Hausdorff (Sirinukunwattana et al. (2017)). Better results correspond to smaller</cell></row><row><cell>Hausdorff distance and all other measures larger. The quantitative results are</cell></row><row><cell>shown in Table 2 which show that the proposed Micro-Net method outperforms</cell></row><row><cell>the state-of-the-art deep learning approaches with at least 3-4% margin in terms</cell></row><row><cell>of average Dice, F1 score, object Dice, pixel accuracy and object Hausdorff. We</cell></row><row><cell>modified the FCN8 algorithm (FCN8W) by introducing weighted loss (Ron-</cell></row><row><cell>neberger et al. (2015)) to improve segmentation of individual cells. FCN8W</cell></row><row><cell>improved F1, object for various SNR values are given in Table 3, 4, 5, 6 and 7 respectively. For Dice</cell></row><row><cell>coefficient, FCN8 and U-Net drop to 73% whereas the proposed method drops</cell></row><row><cell>to 78%. However, DCAN shows a different behaviour and increases the Dice</cell></row></table><note>values by 2% taking the top spot at 1dB. In terms of the F1 score, all the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results for cell segmentation in terms of Dice coefficient, F1 score, The trend was similar with object Dice as well where the first position was retained by the proposed network. In terms of pixel accuracy, the proposed method dropped roughly 4% from 20dB to 1dB. DCAN however showed similar trend to Dice values i.e., increased pixel accuracy with increasing noise content. This is an interesting result, but if we carefully observe Dice and pixel accuracy both depend only on the pixels that are in agreement, whereas object Dice only takes into account the pixels which belong to the ground truth.Similarly, the F1 score takes into account not only true positives but also false positives. With increasing noise content, DCAN lost some of the false positives causing more pixels not belonging to the cellular region to agree. This increased pixel accuracy and Dice while simultaneously decreasing object Dice and the F1 score. Object Hausdorff is a measure of similarity of the shape of cell boundaries and is lower for better performance. Here the author's implementation of FCN produced very high values, whereas with weighted loss (FCN8W) the values were in a comparable range. Additionally FCN8W values improved with increasing noise due to the resulting loss of many cell segmentations. U-Net increased 55 points from 20dB to 1dB noise which shows it's sensitivity to noise. DCAN performed better as it is designed to match the contours and only increased 12 points with increased noise content. However, the proposed network only increased 6 points showing the stability of the network and it's robustness to the noise. It is important to note here that Micro-Netshows the steepest</figDesc><table><row><cell cols="4">Object Dice (OD), Pixel Accuracy (Acc) &amp; Object Hausdorff (OH).</cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell>Dice</cell><cell>F1</cell><cell>OD</cell><cell>PAcc</cell><cell>OH</cell></row><row><cell>FCN8</cell><cell>78.54%</cell><cell>11.69%</cell><cell>7.49%</cell><cell cols="2">77.68% 1349.51</cell></row><row><cell>(Shelhamer et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN8W</cell><cell>71.36%</cell><cell>50.53%</cell><cell>50.86%</cell><cell>74.61%</cell><cell>91.77</cell></row><row><cell>DCAN</cell><cell>76.03%</cell><cell>61.41%</cell><cell>63.80%</cell><cell>78.67%</cell><cell>42.31</cell></row><row><cell>(Chen et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U-Net</cell><cell>78.39%</cell><cell>66.43%</cell><cell>67.35%</cell><cell>80.28%</cell><cell>40.49</cell></row><row><cell>(Ronneberger et al. (2015))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro-Net -</cell><cell>80.74%</cell><cell>69.87%</cell><cell>71.52%</cell><cell>82.22%</cell><cell>30.61</cell></row><row><cell>Proposed</cell><cell cols="4">82.43% 71.79% 74.12% 83.53%</cell><cell>27.53</cell></row><row><cell cols="6">networks drop to below 65% at 1dB where the proposed network retains the</cell></row><row><cell>top position.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative results for cell segmentation of state-of-the-art algorithms in terms of Dice coefficient for various SNR values.</figDesc><table><row><cell>Network</cell><cell>20dB</cell><cell>15dB</cell><cell>10dB</cell><cell>5dB</cell><cell>3dB</cell><cell>1dB</cell></row><row><cell>FCN8</cell><cell>77.55%</cell><cell>76.24%</cell><cell>76.97%</cell><cell>76.47%</cell><cell>75.26%</cell><cell>73.66%</cell></row><row><cell>(Shelhamer et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN8W</cell><cell>71.56%</cell><cell>71.53%</cell><cell>71.39%</cell><cell>70.56%</cell><cell>69.36%</cell><cell>66.88%</cell></row><row><cell>DCAN</cell><cell>77.18%</cell><cell>77.95%</cell><cell>78.92%</cell><cell>79.82%</cell><cell>79.91%</cell><cell>79.40%</cell></row><row><cell>(Chen et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U-Net</cell><cell>76.58%</cell><cell>76.61%</cell><cell>76.29%</cell><cell>75.59%</cell><cell>74.99%</cell><cell>73.45%</cell></row><row><cell>(Ronneberger et al. (2015))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro-Net -</cell><cell>81.35%</cell><cell>81.42%</cell><cell>80.72%</cell><cell>78.30%</cell><cell>76.04%</cell><cell>72.33%</cell></row><row><cell>Proposed</cell><cell cols="6">82.62% 82.69% 82.49% 81.37% 80.23% 78.52%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results for cell segmentation of state-of-the-art algorithms in terms of F1 score for various SNR values.</figDesc><table><row><cell>Network</cell><cell>20dB</cell><cell>15dB</cell><cell>10dB</cell><cell>5dB</cell><cell>3dB</cell><cell>1dB</cell></row><row><cell>FCN8</cell><cell>16.69%</cell><cell>26.34%</cell><cell>15.25%</cell><cell>5.34%</cell><cell>4.33%</cell><cell>4.06%</cell></row><row><cell>(Shelhamer et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN8W</cell><cell>49.41%</cell><cell>49.54%</cell><cell>49.07%</cell><cell>47.75%</cell><cell>48.24%</cell><cell>44.92%</cell></row><row><cell>DCAN</cell><cell>62.33%</cell><cell>63.15%</cell><cell>63.65%</cell><cell>62.18%</cell><cell>61.58%</cell><cell>60.16%</cell></row><row><cell>(Chen et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U-Net</cell><cell>65.51%</cell><cell>65.01%</cell><cell>65.05%</cell><cell>63.18%</cell><cell>61.19%</cell><cell>58.27%</cell></row><row><cell>(Ronneberger et al. (2015))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro-Net -</cell><cell>70.43%</cell><cell>69.99%</cell><cell>68.95%</cell><cell>66.07%</cell><cell>63.23%</cell><cell>58.68%</cell></row><row><cell>Proposed</cell><cell cols="6">71.63% 71.59% 70.65% 69.19% 67.93% 64.67%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative results for cell segmentation of state-of-the-art algorithms in terms of Object Dice for various SNR values.</figDesc><table><row><cell>Network</cell><cell>20dB</cell><cell>15dB</cell><cell>10dB</cell><cell>5dB</cell><cell>3dB</cell><cell>1dB</cell></row><row><cell>FCN8</cell><cell>12.30%</cell><cell>23.47%</cell><cell>13.26%</cell><cell>5.27%</cell><cell>4.55%</cell><cell>4.50%</cell></row><row><cell>(Shelhamer et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN8W</cell><cell>50.78%</cell><cell>50.77%</cell><cell>50.57%</cell><cell>50.28%</cell><cell>50.97%</cell><cell>51.01%</cell></row><row><cell>DCAN</cell><cell>64.46%</cell><cell>65.36%</cell><cell>65.54%</cell><cell>64.22%</cell><cell>62.99%</cell><cell>61.61%</cell></row><row><cell>(Chen et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U-Net</cell><cell>65.61%</cell><cell>65.04%</cell><cell>64.42%</cell><cell>61.89%</cell><cell>60.37%</cell><cell>58.55%</cell></row><row><cell>(Ronneberger et al. (2015))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro-Net -</cell><cell>71.60%</cell><cell>71.28%</cell><cell>70.57%</cell><cell>68.28%</cell><cell>66.45%</cell><cell>63.11%</cell></row><row><cell>Proposed</cell><cell cols="6">73.90% 73.79% 72.98% 71.70% 70.42% 68.03%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results for cell segmentation of state-of-the-art algorithms in terms of pixel accuracy for various SNR values.</figDesc><table><row><cell>Network</cell><cell>20dB</cell><cell>15dB</cell><cell>10dB</cell><cell>5dB</cell><cell>3dB</cell><cell>1dB</cell></row><row><cell>FCN8</cell><cell>77.33%</cell><cell>77.54%</cell><cell>77.50%</cell><cell>75.19%</cell><cell>73.63%</cell><cell>72.14%</cell></row><row><cell>(Shelhamer et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN8W</cell><cell>74.94%</cell><cell>74.92%</cell><cell>74.80%</cell><cell>74.10%</cell><cell>73.24%</cell><cell>71.67%</cell></row><row><cell>DCAN</cell><cell>79.57%</cell><cell>80.18%</cell><cell>80.93%</cell><cell>81.42%</cell><cell>81.30%</cell><cell>80.55%</cell></row><row><cell>(Chen et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U-Net</cell><cell>78.64%</cell><cell>78.65%</cell><cell>78.30%</cell><cell>77.40%</cell><cell>76.80%</cell><cell>75.28%</cell></row><row><cell>(Ronneberger et al. (2015))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro-Net -</cell><cell>82.63%</cell><cell>82.65%</cell><cell>82.11%</cell><cell>80.37%</cell><cell>78.83%</cell><cell>76.39%</cell></row><row><cell>Proposed</cell><cell cols="6">83.69% 83.71% 83.48% 82.49% 81.52% 80.10%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Quantitative results for cell segmentation of state-of-the-art algorithms in terms of object Hausdorff for various SNR values.</figDesc><table><row><cell>Network</cell><cell>20dB</cell><cell>15dB</cell><cell>10dB</cell><cell>5dB</cell><cell>3dB</cell><cell>1dB</cell></row><row><cell>FCN8</cell><cell cols="6">1171.08 604.18 1176.88 1618.90 1700.83 1589.12</cell></row><row><cell>(Shelhamer et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN8W</cell><cell>88.55</cell><cell>90.42</cell><cell>86.99</cell><cell>84.24</cell><cell>81.03</cell><cell>66.30</cell></row><row><cell>DCAN</cell><cell>42.20</cell><cell>41.16</cell><cell>42.32</cell><cell>47.72</cell><cell>50.92</cell><cell>54.21</cell></row><row><cell>(Chen et al. (2017))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>U-Net</cell><cell>55.53</cell><cell>63.53</cell><cell>62.50</cell><cell>85.48</cell><cell>97.49</cell><cell>100.15</cell></row><row><cell>(Ronneberger et al. (2015))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro-Net -</cell><cell>31.42</cell><cell>32.13</cell><cell>32.85</cell><cell>33.62</cell><cell>33.91</cell><cell>35.13</cell></row><row><cell>Proposed</cell><cell>27.98</cell><cell cols="2">28.35 29.64</cell><cell>30.73</cell><cell>31.69</cell><cell>33.84</cell></row><row><cell cols="6">(HNSCC), lower grade glioma (LGG), non-small cell lung cancer (NSCLC) &amp;</cell><cell></cell></row><row><cell cols="6">glioblastoma multiforme (GBM) respectively. Column 1 shows H &amp; E image</cell><cell></cell></row><row><cell cols="6">with ground truth outlined for nuclear segmentation. Column 2 shows an RGB</cell><cell></cell></row><row><cell cols="6">composite image with ground truth in green and output of U-Net in red and</cell><cell></cell></row><row><cell cols="6">overlap in yellow. Column 3 is similar to column 2. It shows an RGB com-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Cell segmentation results on CPM contest data set using evaluation metrics Dice 1 and Dice 2 as selected by the organisers.</figDesc><table><row><cell>Method</cell><cell cols="3">Dice 1 Dice 2 Score</cell><cell>Rank</cell></row><row><cell>Proposed</cell><cell cols="4">0.857 0.796 0.827 1</cell></row><row><cell cols="2">SAMS-Net (Graham and Rajpoot (2018)) 0.855</cell><cell>0.769</cell><cell>0.812</cell><cell>2</cell></row><row><cell>U-Net (Ronneberger et al. (2015))</cell><cell>0.837</cell><cell>0.741</cell><cell>0.789</cell><cell>3</cell></row><row><cell>vuquocdang</cell><cell>-</cell><cell>-</cell><cell>0.783</cell><cell>4</cell></row><row><cell>brisker</cell><cell>-</cell><cell>-</cell><cell>0.773</cell><cell>5</cell></row><row><cell>FCN8 (Shelhamer et al. (2017))</cell><cell>0.829</cell><cell>0.697</cell><cell>0.763</cell><cell>6</cell></row><row><cell>Schwarz</cell><cell>-</cell><cell>-</cell><cell>0.703</cell><cell>7</cell></row><row><cell>object Hausdorff</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>employ optimised handcrafted features in addition to deep learning.<ref type="bibr" target="#b39">Xu et al. (2017)</ref> on the other hand is heavily optimised for gland segmentation. Our algorithm employs only deep learning and is being proposed as a generic approach for cell, nuclear and gland segmentation, easily implemented using existing libraries optimised for GPU usage, which reduces the computational cost. Nevertheless, the proposed method performs competitively against recently proposed approaches and beats the conference version ofXu   et al. (2016). Compared to the results of the contest on test A, the proposed algorithm performed best in terms of F1 and object Dice but ranked third and fifth on test B. In terms of object Hausdorff, which measures the shape similarity, it ranked second after CUMedVision2) on test A and Xu et al.(2016) on test B. Lower F1 and object Dice on test B suggests that our method missed more glands in malignant cases, whereas lower Hausdorff suggests higher shape similarity to the ground truth extracted by our method. Qualitative results of the algorithm for sample images inFigure 5are shown inFigure 9, where for each pair the image on first and second column show ground truth in green, output of DCAN (contest winner, the results were obtained from the contest organisers) and the proposed algorithm respectively in red and overlap of ground truth and output of algorithm in yellow. The image on the right shows the output of the proposed algorithm overlaid on the sample image. These results show that our algorithm clearly misses a few glands on the boundary of the image for which there is insufficient information. In the second row, it merges the glands at the bottom of the image and misses one gland. In malignant cases the algorithm seems to be rather 'conservative' in its approach when marking the boundary of glands. It can be observed in the third row for the two large glands at the bottom and in the fourth row for the smaller gland in the middle.All these glands show significant green inside the ground truth boundary, which at first suggests that the algorithm segmented the gland well inside the ground truth marking for the gland. However, when carefully observed in the overlay with the sample images the algorithm is faithfully following the boundary with tumor cells. For the large gland on the top right in third row the algorithm 'oversegments' the gland compared to the ground truth but again, looking at the overlay with the sample image, the algorithm has included tumor cells in the segmentation. Overall the algorithm performs a good job in segmenting the glands but needs to improve on the glands at the boundary of a patch. This limitation could be overcome by using overlapping patches from the whole slide and then merging the results. Compared to DCAN (first column), the proposed algorithm shows better overlap with the ground truth. It can be observed that DCAN is sensitive to white spaces and certain architecture in benign cases where it segments false regions. This can be clearly observed in cases from row 2, 3 &amp; 4 and column 1. In the fourth row, it joins two glands together and under segments the smaller gland in the middle. It can also be observed that similar to the proposed algorithm, DCAN misses the glands at the boundary due to insufficient information. Overall the proposed algorithm performs better</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Quantitative comparison with the state-of-the-art methods. S and R in the table correspond to score and rank. It is important to note that<ref type="bibr" target="#b39">Xu et al. (2017)</ref> and<ref type="bibr" target="#b16">Manivannan et al. (2018)</ref> employ optimised handcrafted features in addition to deep learning.<ref type="bibr" target="#b39">Xu et al. (2017)</ref> on the other hand is optimised for gland segmentation. The proposed generic CNN framework managed to compete with recently proposed algorithms optimised for gland segmentation which demonstrates its robustness.</figDesc><table><row><cell></cell><cell>Rank Sum</cell></row><row><cell>Object Hausdorff</cell><cell>Test A Test B</cell></row><row><cell>Object Dice</cell><cell>Test A Test B</cell></row><row><cell>F1 score</cell><cell>Test A Test B</cell></row><row><cell></cell><cell>Method</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We will publish our fluorescence cell segmentation data set along with ground truth annotations subject to the publication of this manuscript at go.warwick.ac.uk/tialab/data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www2.warwick.ac.uk/fac/sci/dcs/research/tia/software/sntoolbox</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computational precision medicine nuclei segmentation challenge website</title>
		<ptr target="http://miccai.cloudapp.net/competitions/57" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<pubPlace>Steiner B, Sutskever</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Talwar</forename><forename type="middle">K</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<ptr target="https://www.tensorflow.org/;softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Glandular morphometrics for objective grading of colorectal adenocarcinoma histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jefferyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Qidwai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Aftab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mujeeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-16516-w</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16852</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Efficient globally optimal segmentation of cells in fluorescence microscopy images using level sets and convex energy functionals. Medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergeest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2012.05.012</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1436" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cellprofiler: image analysis software for identifying and quantifying cell phenotypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Friman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Guertin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moffat</surname></persName>
		</author>
		<idno type="DOI">10.1186/gb-2006-7-10-r100</idno>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dcan: Deep contour-aware networks for object instance segmentation from histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2016.11.004</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="135" to="181" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory based active contour algorithm using pixel-level classified images for colon crypt segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sabo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2014.12.006</idno>
		<idno>doi:10.1016/j. compmedimag.2014.12.006</idno>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="150" to="64" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate cell segmentation in microscopy images using membrane patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dimopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rudolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stelling</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btu302</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2644" to="51" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image analysis approach for automatic malignancy determination of prostate pathological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farjam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltanian-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jafari-Khouzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Zoroofi</surname></persName>
		</author>
		<idno type="DOI">10.1002/cyto.b.20162</idno>
	</analytic>
	<monogr>
		<title level="j">Cytometry Part B: Clinical Cytometry</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="227" to="267" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sams-net: Stain-aware multi-scale network for instance-based nuclei segmentation in histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2018.8363645</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="590" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic segmentation of colon glands using object-graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gunduz-Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tosun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sokmensuer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2009.09.001</idno>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classifying and segmenting microscopy images with deep multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Z</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btw252</idno>
		<idno>doi:10. 1093/bioinformatics/btw252</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-resolution cell orientation congruence descriptors for epithelium segmentation in endometrial histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sea</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2017.01.006</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel multitarget tracking algorithm for myosin vi protein molecules on actin filaments in tirfm sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1111/jmi.12299</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Microscopy</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="337" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure prediction for gland segmentation with hand-crafted and deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manivannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2017.2750210</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="210" to="231" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cell segmentation: 50 years down the road</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meijering</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2012.2204190</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="140" to="145" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Signal Processing Magazine</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated gland and nuclei segmentation for grading of prostate and breast cancer histopathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomaszewski</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2008.4540988</idno>
	</analytic>
	<monogr>
		<title level="m">5th IEEE International Symposium on. IEEE</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="284" to="291" />
		</imprint>
	</monogr>
	<note>Biomedical Imaging: From Nano to Macro</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structure and context in prostatic gland segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33415-3_15</idno>
		<idno>doi:10.1007/ 978-3-642-33415-3_15</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="115" to="138" />
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local optimization based segmentation of spatiallyrecurring, multi-region objects with part configuration constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Nosrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2014.2323074</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1845" to="59" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tumor segmentation in whole slide images using persistent homology and deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Medical Image Understanding and Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-319-60964-5_28</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="320" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mimonet: A multi-input multi-output convolutional neural network for cell segmentation in fluorescence microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sea</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pelengaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2017.7950532</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 14th International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="337" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sea</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pelengaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mi-Monet</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-60964-5_61</idno>
		<title level="m">Gland Segmentation Using Multi-Input-Multi-Output Convolutional Neural Network</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="698" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RAMTaB: robust alignment of multi-tag bioimages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sea</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abouna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Nattkemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0030894</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">30894</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust normalization protocols for multiplexed fluorescence bioimage analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sea</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Langenkämper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Nattkemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13040-016-0088-2</idno>
		<idno>doi:10.1186/ s13040-016-0088-2</idno>
	</analytic>
	<monogr>
		<title level="j">BioData Mining</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated training of deep convolutional neural networks for cell segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sadanandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranefall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wählby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-07599-6</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7860</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing proteome topology and function by automated multidimensional fluorescence microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bonnekoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Pommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Philipsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Böckelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gollnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Friedenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Dress</surname></persName>
		</author>
		<idno type="DOI">10.1038/nbt1250</idno>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1270</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2572683</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="51" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: The glas challenge contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2016.08.008</idno>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A stochastic polygons model for glandular structures in colon histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2015.2433900</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2366" to="78" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accurate cervical cell segmentation from overlapping clumps in pap smear images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2016.2606380</idno>
		<idno>doi:10.1109/ TMI.2016.2606380</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="300" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic Nuclei Segmentation in H&amp;E Stained Breast Cancer Histopathology Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kornegoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0070221</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">70221</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segmentation of intestinal gland images with iterative region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harpaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gil</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1365-2818.2005.01531.x</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Microscopy</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gland instance segmentation by deep multichannel side supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-319-46723-8_57</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gland instance segmentation using deep multichannel neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eic</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBME.2017.2686418</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2901" to="2913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nuclei segmentation using marker-controlled watershed, tracking using mean-shift, and kalman filter in time-lapse microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSI.2006.884469</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2405" to="2419" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Quantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Failmezger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Rueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gräf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bardwell</surname></persName>
		</author>
		<idno type="DOI">10.1126/scitranslmed.3004330</idno>
	</analytic>
	<monogr>
		<title level="j">Science translational medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">157</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
