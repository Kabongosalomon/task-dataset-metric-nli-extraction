<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Caglayan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevrez</forename><surname>Imamoglu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><forename type="middle">Burak</forename><surname>Can</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Nakamura</surname></persName>
						</author>
						<title level="a" type="main">When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional neural networks</term>
					<term>recursive neural networks</term>
					<term>randomized neural networks</term>
					<term>transfer learning</term>
					<term>RGB-D object recognition</term>
					<term>RGB-D scene recognition !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing objects and scenes are two challenging but essential tasks in image understanding. In particular, the use of RGB-D sensors in handling these tasks has emerged as an important area of focus for better visual understanding. Meanwhile, deep neural networks, specifically convolutional neural networks (CNNs), have become widespread and have been applied to many visual tasks by replacing hand-crafted features with effective deep features. However, it is an open problem how to exploit deep features from a multi-layer CNN model effectively. In this paper, we propose a novel two-stage framework that extracts discriminative feature representations from multi-modal RGB-D images for object and scene recognition tasks. In the first stage, a pretrained CNN model has been employed as a backbone to extract visual features at multiple levels. The second stage maps these features into high level representations with a fully randomized structure of recursive neural networks (RNNs) efficiently. In order to cope with the high dimensionality of CNN activations, a random weighted pooling scheme has been proposed by extending the idea of randomness in RNNs. Multi-modal fusion has been performed through a soft voting approach by computing weights based on individual recognition confidences (i.e. SVM scores) of RGB and depth streams separately. This produces consistent class label estimation in final RGB-D classification performance. Extensive experiments verify that fully randomized structure in RNN stage encodes CNN activations to discriminative solid features successfully. Comparative experimental results on the popular Washington RGB-D Object and SUN RGB-D Scene datasets show that the proposed approach significantly outperforms state-of-the-art methods both in object and scene recognition tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C ONVOLUTIONAL neural networks (CNNs) have attracted researchers to handle many visual recognition tasks since their breakthrough emergence. However, building an effective model can be quite challenging due to the lack of labeled training data, limited time and computational resources, and the need for well defined hyperparameter settings for a good generalization capability. Especially in many real-world tasks, it is not preferable to train a model from scratch. Luckily, CNNs offer highly efficient solutions with their transferable off-the-shelf features. Consequently, many approaches take advantage of these features to propose new solutions for object recognition (e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>), scene recognition (e.g. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>), object detection (e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>), and semantic segmentation (e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>) due to their high representation ability and capability of generalization among different tasks when trained with large scale datasets. The most common and straightforward strategy among these methods is to utilize the features obtained from final layers which provide semantically rich information with smaller dimensions comparing to the earlier layers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, one of the concerns about this semantics is the fact that as features evolve towards the final layers, they are increasingly dependent on the chosen dataset and task <ref type="bibr" target="#b7">[8]</ref>, which might diminish the generalization capabilities of these features when transferred. Moreover, this strategy ignores the locally activated distinctive information of the earlier layers which is less sensitive to semantics <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. One of the main challenges in earlier layers of deep CNNs is the high dimensionality of extracted features. In addition, when these features are used as is, it makes the feature space untraceable. Eventually, while features are transformed from low-level general to high-level specific representations throughout the network, the relational information is distributed across the network at different levels <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, it remains unclear how to exploit the information effectively.</p><p>In this paper, we aim to present an effective deep feature extraction framework to derive powerful image representations through transfer learning. The proposed pipeline relies on two key insights. The first one is to employ a pretrained CNN as the backbone model and exploit activations at different layers of the network to cover the predominant information of the underlying localities. The second one is to implement multiple random recursive neural networks (RNNs) on top of CNNs to encode the CNN activations into a robust representation with reduced dimensionality and sufficient descriptiveness. <ref type="figure">Fig. 1</ref>: General overview of the proposed framework. The framework accepts RGB and depth images and it first colorizes depth inputs. In the CNN-Stage activations at different levels of a pretrained model are extracted. In the RNN-Stage, first, CNN activations are converted to reasonable dimensions and appropriate input requirements for RNNs by preprocessing operations. Then, multiple random RNNs are applied to map these inputs into high level representations. Finally, multiple level fusion and classification steps are deployed for recognition tasks.</p><p>In developing our framework, we particularly deal with the RGB-D object and scene recognition problems, which are challenging yet crucial tasks especially with the todays wider application of robotics technologies. Moreover, the multi-modality of the RGB-D sensors arises additional difficulties in representation of input data such as handling different modalities and devising solutions that captures complementary information from both RGB and depth data effectively. Besides these challenges, alleviating limitations on time and memory consumption is another challenge to deal with. To address these challenges, we propose a novel framework that gathers feature representations at different levels in a compact and representative feature vector for both of RGB and depth data. After obtaining CNN activations, we first apply a preprocessing operation to the activation maps of each level through reshaping or randomized pooling. This not only provides a generic structure for each level by fixing an RNN tree but also it allows us to improve recognition accuracy through multilevel fusion. We then give the outputs of these operations to multiple random RNNs <ref type="bibr" target="#b10">[11]</ref> to acquire higher level compact feature representations. Incorporating multiple fixed RNNs together with the pre-trained CNN models allows feature transition at different levels to preserve both semantic and spatial structure of objects. In order to transfer learning from a pre-trained CNN model for depth modality, we embed depth data into the RGB domain with a highly efficient depth colorization technique based on surface normals. As for the multi-modal fusion of RGB and depth modalities, we explore different fusion techniques. Moreover, we present an approach that provides a decisive fusion of RGB and depth modalities based on the modality importance through a weighting scheme (see Sec. <ref type="bibr">3.4)</ref>. Our implementation is in Python using PyTorch 1 and numpy 2 libraries. All the source codes together with system requirements and documentations will be opened to the community on Github.</p><p>The proposed framework is evaluated with exhaustive experiments on two popular public datasets (i) Washington RGB-D Object dataset <ref type="bibr" target="#b11">[12]</ref> for RGB-D object recognition task and (ii) Sun RGB-D Scene dataset <ref type="bibr" target="#b12">[13]</ref> for RGB-D scene recognition task. The experimental results demonstrate the effectiveness of our approach in terms of accuracy by achieving superior performance over the current stateof-the-art methods. A preliminary version of this work appeared in <ref type="bibr" target="#b13">[14]</ref> for RGB-D object recognition. In this work, we present an extended and enhanced version of our work in <ref type="bibr" target="#b13">[14]</ref> with a novel framework and contribute to the task of RGB-D object and scene recognition tasks as follows:</p><p>• We present a novel framework for deep features with two-stage organization where information at differ-ent levels is encoded by incorporation of multiple random RNNs with a pre-trained CNN model for RGB-D object and scene recognition (see Sect. 3). The framework is applicable to a variety of pre-trained CNN models including AlexNet <ref type="bibr" target="#b14">[15]</ref>, VGGNet <ref type="bibr" target="#b15">[16]</ref>, ResNet <ref type="bibr" target="#b16">[17]</ref>, and DenseNet <ref type="bibr" target="#b17">[18]</ref>. The overall structure has been designed in a modular and extendable way through a unified CNN and RNN process. Thus, it offers easy and flexible use. These also can easily be extended with new capabilities and combined with different setups and other models for implementing new ideas. In fact, our preliminary approach has been already successfully applied to another challenging robotics task in a SLAM system <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr">•</ref> We extend the idea of randomness in RNNs as a novel pooling strategy to cope with the high dimensionality of CNN activations from different levels (see Sec. 3.3.1). This strategy has been applied as a preprocessing stage before RNNs and it allows us to evaluate and utilize multiple level information in deep models such as ResNet <ref type="bibr" target="#b16">[17]</ref> and DenseNet <ref type="bibr" target="#b17">[18]</ref> models. In addition, we give the experimental results of different pooling strategies in terms of accuracy and show the effectiveness of our pooling strategy over other pooling methods (see Sec. 4.2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We study several aspects of transfer learning through an empirical investigation including comparative profiling results of different baselines (see Sec. 4.2.1), level-wise analysis of different baselines (see Sec. 4.2.3), the effects of finetuning over fixed pretrained CNN models (see Sec. 4.2.6), and different approaches to multi-level and multi-modality data fusion (see Sec. 4.2.7). In regard to multi-model fusion, unlike our previous work using concatenation of features, we propose a soft voting approach based on individual SVM confidences of RGB and depth streams (see Sec. 3.4) and show the strength of our approach experimentally (see Sec. 4.2.7). We also give; (i) empirical evaluation of the randomness to see if random RNNs are stable enough (see Sec. 4.2.2), (ii) experimental analysis of multi-level RNNs (see Sec. 4.2.5), and (iii) comparative results of different pooling strategies over the proposed random pooling (see Sec. 4.2.4). Finally, we provide experimental results demonstrating that our approach improves the stateof-the-art results on two the most comprehensive and challenging real-world public datasets; Washington RGB-D Object dataset for RGB-D object recognition (see Sec. 4.3) and SUN RGB-D scene dataset for RGB-D scene recognition (see Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The proposed work can be related with different areas, such as multi-modal CNN based approaches, transfer learning based approaches, and random recursive neural networks. In this section, we narrow our focus to RGB-D based recognition and give a brief review of the relevant approaches with stating the current work in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-Modal CNN based Approaches</head><p>Following their success in computer vision, CNN-based solutions have replaced conventional methods such as the works in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and <ref type="bibr" target="#b21">[22]</ref> in the field of RGB-D object recognition, as in many other areas. For instance, Wang et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> present CNN-based multi-modal learning systems motivated by the intuition of common patterns shared between RGB and depth modalities. They enforce their systems to correlate features of the two modalities in a multi-modal fusion layer with a pretrained model <ref type="bibr" target="#b22">[23]</ref> and their custom network <ref type="bibr" target="#b23">[24]</ref> respectively. Li et al. <ref type="bibr" target="#b24">[25]</ref> extends the idea of considering multi-modal intrinsic relationship with intra-class and inter-class similarities for indoor scene classification by providing a two-stage training approach. In <ref type="bibr" target="#b25">[26]</ref>, a three-streams multi-modal CNN architecture has been proposed in which depth images are represented with two different encoding methods in two-streams and the remaining stream is used for RGB images. Despite the extra burden, this naturally has increased the depth accuracy in particular. Similar multi-representational approach has been proposed by Zia et al. in <ref type="bibr" target="#b26">[27]</ref> where a hybrid 2D/3D CNN model initialized with pre-trained 2D CNNs is employed together with 3D CNNs for depth images. Cheng et al. <ref type="bibr" target="#b27">[28]</ref> propose convolutional fisher kernel (CFK) method which integrates a single CNN layer with fisher kernel encoding and utilizes Gaussian mixture models for feature distribution. The drawback of their approach is the very high dimensional of the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transfer Learning based Approaches</head><p>Deep learning algorithms require a significant amount of annotated training data and obtaining such data can be difficult and expensive. Therefore, it is important to leverage transfer learning for enhancing high-performance learner on a target domain and the task at hand. Especially, applying a trained deep network and then fine-tuning the parameters can speed up the learning process or improve the classification performance <ref type="bibr" target="#b28">[29]</ref>. Furthermore, many works show that a pre-trained CNN on a large-scale dataset can generate good generic representations that can effectively be used for other visual recognition tasks as well <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. This is particularly important in vision tasks on RGB-D datasets, which is hard to collect with labeled data and generally amount of data is much less than that of the labeled images in RGB datasets.</p><p>There are many successful approaches that use transfer learning in the field of RGB-D object recognition. Schwarz et al. use the activations of two fully connected layers, a.k.a. fc7 and fc8, extracted from the pre-trained AlexNet <ref type="bibr" target="#b14">[15]</ref> for RGB-D object recognition and pose estimation. Gupta et al. <ref type="bibr" target="#b32">[33]</ref> study the problem of object detection and segmentation on RGB-D data and present a depth encoding approach referred as HHA to utilize a pre-trained CNN model on RGB datasets. Asif et al. introduce a cascaded architecture of random forests together with the use of the fc7 features of the pre-trained models of <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b15">[16]</ref> to encode the appearance and structural information of objects in their works of <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b35">[36]</ref>, respectively. Carlucci et al. <ref type="bibr" target="#b36">[37]</ref> propose a colorization network architecture and use a pretrained model as feature extractor after fine-tuning it. They also make use of the final fully-connected layer in their approach. So, these above-mentioned studies mainly focus on the outputs of the fully-connected layers.</p><p>On the other hand, many studies <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> have concluded that using fully connected layers from pretrained or fine-tuned networks might not be the optimum approach to capture discriminating properties in visual recognition tasks. Moreover, combining the activations obtained in different levels of the same modal enhances recognition performance further, especially for multi-modal representations, where earlier layers capture modality-specific patterns <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Hence, utilizing information at different levels in the works of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> yields better performances. More recent approach of Loghmani et al. <ref type="bibr" target="#b42">[43]</ref> utilizes the pre-trained model of residual networks <ref type="bibr" target="#b16">[17]</ref> to extract features from multiple layers and combines them through a recurrent neural network. Their experimental results also verify that multi-level feature fusion provide better performance than single-level features. While their approach is based on a gated recurrent unit (GRU) <ref type="bibr" target="#b43">[44]</ref> with a number of memory neurons, our approach employs multiple random neural networks with no necessarily need for training. A different related approach is proposed by Asif et al. in <ref type="bibr" target="#b44">[45]</ref>. They handle the classification task by dividing it into image-level and pixel-level branches and fusing through a Fisher encoding branch. Eitel et al. <ref type="bibr" target="#b45">[46]</ref> and Tang et al. <ref type="bibr" target="#b46">[47]</ref> employ two-stream CNNs, one for each modality of RGB and depth channels and each stream uses the pre-trained model of <ref type="bibr" target="#b14">[15]</ref> on the ImageNet. In both works <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, the two-streams are finally connected by a fully-connected fusion layer and a canonical correlation analysis (CCA) module, respectively. While feature fusion approaches (e.g. concatenation) may provide good accuracy for the visual recognition task, feature fusion may not be the only solution for multi-level decision process since increased feature space may not be good for recognition with small number of data. We experiment and show that voting on the SVM confidence scores for selected levels can also provide reliable and improved performance. Moreover, this also enables us to use confidence score based importance to RGB and depth domains in multi-modal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Random Recursive Neural Networks</head><p>Randomization in neural networks has been researched for a long time in various studies <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> due to its benefits, such as simplicity and computationally cheapness over optimization <ref type="bibr" target="#b53">[54]</ref>. Since a complete overview of these variations is beyond the scope of this paper, we give an overview specifically with the focus of random recursive neural networks <ref type="bibr" target="#b10">[11]</ref>. Recursive neural networks (RNNs) <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> are graphs that process a given input into recursive tree structures to make a highlevel reasoning possible in a part-whole hierarchy by repeating the same process over the trees. RNNs have been employed for various research purposes in computer vision including image super-resolution <ref type="bibr" target="#b57">[58]</ref>, semantic segmentation <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b58">[59]</ref>, and RGB-D object recognition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, Socher et al. have introduced a two-stage RGB-D object recognition architecture where the first stage is a single CNN layer using a set of k-means centroids as the convolution filters and the second stage is multiple random recursive neural networks to process outputs of the first stage. Bai et al. <ref type="bibr" target="#b59">[60]</ref> propose a subset based approach of the pioneer work in <ref type="bibr" target="#b10">[11]</ref> where they use a sparse auto-encoder instead of the k-means clustering for convolution filters. Cheng et al. <ref type="bibr" target="#b60">[61]</ref> employ the same architecture of Socher et al. <ref type="bibr" target="#b10">[11]</ref> for a semi-supervised learning system with a modification by adding a spatial pyramid pooling to prevent a potential performance degradation during resizing input images. Bui et al. <ref type="bibr" target="#b61">[62]</ref> have replaced the single CNN layer in <ref type="bibr" target="#b10">[11]</ref> with a pre-trained CNN model for RGB object recognition and achieved impressive results. Following their success, in our preliminary work <ref type="bibr" target="#b13">[14]</ref>, we propose an approach that aims to improve on this idea by gathering feature representations at different levels in a compact and representative feature vector for both of RGB and depth data. To this end, we reshape CNN activations in each layer that provides a generic structure for each layer by fixing the tree structure without hurting performance and it allows us to improve recognition accuracy by combining feature vectors at different levels. In this work, we propose a pooling strategy to handle large dimensional CNN activations by extending the idea of randomness in RNNs. This can be related with the stochastic pooling by Zeiler and Fergus in <ref type="bibr" target="#b62">[63]</ref>, which picks the normalized activations of a region according to a multinomial distribution by computing the probabilities within the region. Instead of using probabilities, our pooling approach here is a form of averaging based on uniform distributed random weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>The proposed pipeline has two main stages. In the first stage, a pre-trained CNN model has been employed as the underlying feature extractor. In this work, we have examined several models in this stage. The second stage transforms convolutional features through a randomized recursive neural network based structure that aims to acquire more compact representations. In order to cope with the high dimensionality of CNN activations, a pooling strategy based on random weights has been proposed. The final representative outcomes have been passed through a linear SVM classifier for categorization of objects and scenes. The overall pipeline can be related as a deeper analogy to <ref type="bibr" target="#b63">[64]</ref> where a proper architecture with random weights for object recognition task has been explored. In the following, we describe each stage of our approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preparation</head><p>In order to use pre-trained CNN models, it is important to process input images appropriately. To this end, following common practices for preprocessing, we resize RGB images to 256x256 dimensions according to bilinear transformation and apply center cropping to get 224x224 dimensional images. Then, we apply commonly used z-score standardization on the input data by using mean and standarddeviation of the ImageNet <ref type="bibr" target="#b64">[65]</ref>. We do not perform any other practices such as data augmentation.</p><p>As for the depth domain, we first need appropriate RGBlike representation of depth data to leverage the power of   pre-trained CNN models over the large-scale RGB dataset of the ImageNet. To do so, there are several ways to represent depth data as RGB-like images such as HHA method of Gupta et al. <ref type="bibr" target="#b32">[33]</ref> (i.e. using horizontal and vertical observation values and angle of the normal to common surface), ColorJet work by Eitel et al. <ref type="bibr" target="#b45">[46]</ref> (i.e. mapping depth values to different RGB color values), or commonly used surface normal based colorization as in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b65">[66]</ref>. In this work, we prefer to use the colorization technique based on surface normals, as it confirms its effectiveness in our previous work <ref type="bibr" target="#b13">[14]</ref>. However, unlike surface normal estimation from depth maps without camera parameters in <ref type="bibr" target="#b13">[14]</ref>, we improve this in a more accurate way by estimating surface normals on 3D point clouds that has been computed using depth maps and camera intrinsic values. To address the issue of missing depth values, we first apply a fast vectorized depth interpolation by applying a median filter through a 5 × 5 neighborhood to reconstruct missing values in noisy depth inputs. Then, 3D point cloud estimation by using camera intrinsic constant values and surface normal calculation on point clouds are followed, respectively. After this, the common approach is scaling surface normals to map values to the 0−255 range to fit RGB image processing. However, since such an approach of mapping from floating point to integer values leads to a loss of information, we use these normal vectors as is without performing further quantization or scaling. Furthermore, unlike in RGB input processing, we apply resizing operation on these RGB-like depth data using the nearest neighborhood based interpolation rather than bilinear interpolation. Because the latter may lead to more distortion in geometric structure of a scene. Moreover, nearest neighbor interpolation is more suitable to the characteristics of depth data by providing a better separability between foreground and background in a scene. When applying z-score standardization to depth domain, we use the standard-deviation of the ImageNet as in RGB domain. However, we use zero-mean instead of the ImageNet mean as normal vectors are in the range of [−1, 1] without the need for zero-mean shifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CNN-Stage</head><p>The backbone of our approach is a pre-trained CNN model. Since size of available RGB-D datasets are much smaller than that of RGB's, it is important to make use of an efficient knowledge transfer from pre-trained models on large RGB datasets. In addition, it saves time by eliminating the need for training from scratch. In the previous work <ref type="bibr" target="#b13">[14]</ref>, the available pre-trained CNN model of <ref type="bibr" target="#b33">[34]</ref>, named VGG f, in MatConvNet toolbox <ref type="bibr" target="#b66">[67]</ref> has been used. In this work, we employ several available pre-trained models of PyTorch including AlexNet <ref type="bibr" target="#b14">[15]</ref>, VGGNet <ref type="bibr" target="#b15">[16]</ref> (specifically VGGNet-16 model with batch normalization), ResNet <ref type="bibr" target="#b16">[17]</ref> (specifically ResNet-50 and ResNet-101 models), and DenseNet <ref type="bibr" target="#b17">[18]</ref>. We extract features from seven different levels of CNN models. The models investigated in this study with the feature extraction levels are shown in <ref type="figure" target="#fig_1">Fig.  2</ref>. For AlexNet, outputs of the five successive convolutional layers and the following two fully-connected (FC) layers have been considered, while for VGGNet, the first two FC layers are taken into account together with the outputs of each convolution block that includes several convolutions and a final max pooling operations. Unlike AlexNet and VGGNet, ResNet and DenseNet models consist of blocks such as residual, dense or transition blocks where there are multiple layers. While ResNet extends the sequential behaviour of AlexNet and VGGNet with the introduction of the skip-connections, DenseNet takes one step further by concatenating the incoming activations rather than summing up them. The ResNet models consist of five stages and a following average pooling and an FC layer. Therefore, each output of the five successive stages and the output of the final average pool have been considered for the six of the seven extraction points. As for the remaining extraction level for these models (ResNet-50 and ResNet-101), the middle point of the third block (which is the largest block) has been taken. Similarly, for DenseNet model, the output of all the four dense blocks (for the last dense block, the output of normalization that follows the dense block has been taken) and the transition blocks between them have been considered as the extraction points. Since common and straightforward model of AlexNet has a minimum depth with a seven layer stack-ups, the above-mentioned CNN extraction points for each model are selected to evaluate and compare level-wise model performances. In addition, these levels are also related to the CNN model in the previous work <ref type="bibr" target="#b13">[14]</ref> that we improve on by considering their intrinsic reasoning behind the use of blocks and the approximate distance differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RNN-Stage</head><p>Random recursive neural networks offer a feasible solution by randomly fixing the network connections and eliminate the need for selection in the parameter space. Motivated by this, we employ multiple random RNNs, whose inputs are the activation maps of a pre-trained CNN model. RNNs map a given 3D matrix input into a vector of higher level representations of it by applying the same operations recursively in a tree structure. In each layer, adjacent blocks are merged into a parent vector with tied weights where the objective is to map inputs C ∈ R K ×s×s into a lower dimensional space p ∈ R K in the end through multiple levels. Then, the output of a parent vector is passed through a nonlinear function. A typical choice for this purpose is the tanh function. In our previous work <ref type="bibr" target="#b13">[14]</ref>, we give the comparative results of different activation functions in terms of accuracy success and show hyperbolic functions work well. Therefore, in this work, we employ tanh activation function as in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref> shows a graphical representation of a pooled CNN output with the size K × 8 × 8 and an RNN structure with 3 levels and blocks of 2 × 2 = 4 child nodes (Note that this figure is inspired by the RNN graphical representation of <ref type="bibr" target="#b10">[11]</ref>).</p><formula xml:id="formula_0">∈ ℝ ×8×8 (1) ∈ ℝ ×4×4 (2) ∈ ℝ ×2×2 ∈ ℝ ∈ ℝ × 2 × (1) = g( ) (2) = g( (1) ) = g(<label>(2)</label></formula><p>In our case, inputs of RNNs are activation maps obtained from different levels of the underlying CNN model. Let x be an input image that pass through f (x ) l a given CNN model, where l = 1 , .., 7 are the extraction levels and f (x ) l = C l , where the output convolution maps are either a 3D matrix C l ∈ R K ×s×s for l convolutional layers or a 1D vector of C l ∈ R M for l FC layers/global average pooling. Since RNN requires a 3D input of C ∈ R K ×s×s , we first process the convolution maps at each level to ensure the required form. Moreover, by applying this step, we ensure that RNNs are able to handle inputs fast and effectively by reducing the receptive field area and/or the number of activation maps of high-dimensional feature levels (e.g. the outputs of early levels for models such as VGGNet <ref type="bibr" target="#b15">[16]</ref>, ResNet <ref type="bibr" target="#b16">[17]</ref>, DenseNet <ref type="bibr" target="#b17">[18]</ref> etc). In addition, we apply preprocessing to obtain similar output structures with the previous work <ref type="bibr" target="#b13">[14]</ref>. However, it was enough to apply only reshaping in the previous work due to less dimensional size of layers in VGG f model. In this work, we introduce random weighted pooling that copes with high dimensionality of layers in the underlying deeper models such as ResNet <ref type="bibr" target="#b16">[17]</ref> and DenseNet <ref type="bibr" target="#b17">[18]</ref>. Our pooling mechanism can downsample CNN activations in both number and spatial dimension of maps. After applying the preprocessing step to obtain suitable forms for RNNs, we compute parent vector as</p><formula xml:id="formula_1">p = g (W C l )<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">C l =    c 1 . . . c s 2  </formula><p> for each CNN extraction level l = 1 , ..., 7 , g is a nonlinearity function which is tanh in this study, s is block size of an RNN. Instead of a multilevel structured RNN, an RNN in this study is of onelevel with a single parent vector. In fact, our experiments have shown that the single-level structure provides better or comparable results over the multi-level structure in terms of accuracy (see Sec. 4.2.5). Moreover, the single-level is more efficient with less computational burden. Thus, s block size is actually the receptive field size in an RNN. In Eq. 1, the parameter weight matrix is W ∈ R K ×s 2 K and it is randomly generated from a predefined distribution that satisfies the following probability density function</p><formula xml:id="formula_3">W ∼ h ⇒ b a h(w)dw = P (a ≤ W ≤ b)<label>(2)</label></formula><p>where h is a predefined distribution and a and b are boundaries of the distribution. In our case, the weights are set to be uniform random values in [−0 .1 , +0 .1 ], which have been assigned by following our previous work <ref type="bibr" target="#b13">[14]</ref> and specifically with the assumption of preventing possible explosion of tensor values due to our aggregating pooling strategy. Keeping in mind that in order to obtain sufficient descriptive power from the randomness, we need to generate enough samples from the range. In <ref type="bibr" target="#b10">[11]</ref>, it has been demonstrated experimentally that increasing the number of random RNNs up to 64 improves performance and gives the best result with 128 RNNs. In <ref type="bibr" target="#b13">[14]</ref>, it has also been verified that K = 128 number of RNN weights can be generated for feature encoding with high performance in classification on both of RGB and depth data. Therefore, as a standard usage in this work, we do feature encoding on CNN features using 128 random RNNs with 64 channel representations, leading us to 8192 dimensional feature vector at each level in a model. The reason why random weights work well for object recognition tasks seems to lie in the fact that particular convolutional pooling architectures can naturally produce frequency selective and translational invariant features <ref type="bibr" target="#b67">[68]</ref>. As stated before, in analogy to the convolutional-pooling architecture in <ref type="bibr" target="#b63">[64]</ref>, our approach intuitively incorporates both selectivity due to the CNN stage and translational invariance due to the RNN stage. Moreover, we have to point out that there is biological plausibility lies in the use of randomness as well. In <ref type="bibr" target="#b68">[69]</ref>, Rigotti et al. have shown that random connections between inter-layer neurons are needed to implement mixed selectivity for optimal performance during complex cognitive tasks. Before concluding this section, we give details of our random pooling approach, where we extend the idea of random RNN as a downsampling mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Random Weighted Pooling</head><p>In our previous work <ref type="bibr" target="#b13">[14]</ref>, we give CNN outputs to RNNs after a reshaping process. However, due to the high dimensional output size of the models used in this study, it is necessary to process CNN activations further. In this work, we propose a random pooling strategy to reduce the dimension in either size of the activation maps (s block size or receptive field area of an RNN) or number of maps (K) at CNN levels where reshaping is insufficient. In our random weighted pooling approach, we aggregate the CNN activation maps by sampling from a uniform distribution as in Eq. 2 from each pooling area. More precisely, for l extraction level, the pooling reduces C l activations by mapping into S l region as P : C l → S l where C l ∈ R K ×s×s and S l ∈ R K ×s ×s in Eq. 3.</p><formula xml:id="formula_4">S l = i∈S l W (i) l C (i) l<label>(3)</label></formula><p>where S l is pooling region, C l convolutional activations, i is the index of each element within the pooling, and W l is random weights. K &lt; K and s = s when pooling is over number of maps whereas K = K and s &lt; s when pooling is over size of maps. <ref type="figure">Fig. 4</ref> illustrates proposed random weighted pooling for both of downsampling in number of maps and size of maps. In this work, by extending the randomness in RNNs along the pipeline with the proposed pooling strategy, we aim to show that randomness can actually work quite effectively. In fact, as we can see in the comparative results (see Sec. 4.2.4), this randomness in our approach works generally better comparing to the other common pooling methods such as max pooling and average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fusion and Classification</head><p>After obtaining encoded features from the RNN-Stage, we investigate multi-level fusions to capture more distinctive information at different levels for further recognition performance. In order to minimize the cross entropy error between output predictions and the target values, we could give multi-level outputs to fully connected layers and backpropagate through them. However, following the success in our previous study <ref type="bibr" target="#b13">[14]</ref>, we perform classification by employing linear SVM with the scikit-learn 3 <ref type="bibr" target="#b69">[70]</ref> implementation. To this end, in our previous work <ref type="bibr" target="#b13">[14]</ref>, we have performed the straightforward feature concatenation on various combinations of the best mid-level representations. In this work, in addition to the feature concatenation, we also apply soft voting by averaging SVM confidence scores on these best trio of levels. Finally, RGB and depth features are fused to evaluate combined RGB-D accuracy performance. Shiny, transparent, or thin surfaces may cause corruption in depth information since depth sensors do not properly handle reflections from such surfaces, resulting better performance in favor of RGB in such cases. On the other hand, depth sensors work well in a certain range and are insensitive to changes in lighting conditions. Therefore, to take full advantage of both modalities in a complementary way, a compact multi-modal combination based on the success of input type is important in devising the best performing fusion. To this end, we present a decision mechanism using weighted soft voting based on the confidence scores obtained from RGB and depth streams. Modality weighting in this way is used to compensate imbalance and complement decision in different data modalities. Once the modality-specific branches proceed, we combine the predictions through the weighted SVM as follows. Let S i represents SVM confidence scores of each category class n = 0...N − 1, where N is number of classes, and i ∈ {rgb, depth} indicates RGB and depth modalities. Then, weights w i are computed as in Eq. 4.</p><formula xml:id="formula_5">w i = e m i i e m i<label>(4)</label></formula><p>where m i is normalized squared magnitudes for each modality and defined as:</p><formula xml:id="formula_6">m i = S i 2 max( S rgb 2 , S depth 2 )<label>(5)</label></formula><p>Finally, multi-modal RGB-D predictions are estimated as follows, in Eq. 6:ŷ</p><formula xml:id="formula_7">RGBD = arg max n i w i S i<label>(6)</label></formula><p>where n is a category class. Concretely, if RGB and depth results are balanced in confidence scores, then the final soft voting decision is based on equal contribution from each stream similar to averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>The proposed framework has been evaluated on two challenging benchmarks (Sec   <ref type="bibr" target="#b12">[13]</ref> to evaluate the proposed work for scene recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Ablation</head><p>We first have analysed and validated the proposed framework with extensive experiments with a variety of architectural configurations on the popular benchmark of Washington RGB-D dataset. In this section, the analysis and evaluations of the model ablative investigations are presented. The developmental experiments are carried out on two splits of Washington RGB-D Object dataset for both modalities in order to evaluate on more stable results. The average results are analysed. However, in some experiments, more runs have been carried out, which are clearly stated in the related sections. Then, the best performing models are compared with the state-of-the-art methods with the exact provided evaluation setups. We assess the proposed framework on a desktop PC with AMD Ryzen 9 3900X 12-Core Processor, 3.8 GHz Base, 128 GB DDR4 RAM 2666 MHz, and NVIDIA GeForce GTX 1080 Ti graphics card with 11 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Computation Time and Memory Profiling on Different Models</head><p>We first evaluate different baseline CNN models within our framework in terms of computational time and memory  requirements. We evaluate the proposed framework in two parts: (i) Feature extraction containing CNN-RNN stages and (ii) Classification where a model based on the extracted features is learnt to distinguish the different classes. The batch size is set to 64 for all the models. <ref type="table" target="#tab_2">Table 1</ref> reports computational times and memory workspaces for the whole data processing (41, 877 images) on Washington RGB-D dataset. The results here are the average results of two splits on RGB images. There is additional cost for depth data processing as it is required to colorize them. The results on this table cover the overall processing and classification of all 7 level features. Moreover, it should be noted that classification time covers both training and testing processes, in which training takes the main computational burden. Therefore, the main cost in terms of processing time comes from training SVM models that works on CPU for 7 times.</p><p>The process for only a single optimum level would reduce the computational time to a ratio of seven approximately. Hence, using a single optimum level or fusion of selected levels can be efficient enough in terms of time and memory requirements while presenting sufficient representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Empirical Evaluation of the Effect of Randomness</head><p>The use of random weights both in pooling and RNN structures leads to the question of how stable are the results. Thus, we experimentally investigate to see whether there is a decisive difference between different runs that generate and use new random weights. We run the pipeline with different random weights on two splits, 5 times for each. <ref type="figure" target="#fig_5">Fig. 5</ref> reports average results with their standard deviations for each level. The figure clearly shows that randomness does not cause any instability in the model and produces similar results with very small deviations. <ref type="figure" target="#fig_6">Fig. 6</ref> shows level-wise average accuracy performances of all the baseline models for both of RGB and depth modalities on all the 10 evaluation splits. The graphs show a similar performance trend line with a clear upward at the beginning and a downward at the end. Although the levels at which optimum performance is obtained vary according to the model, what is common to all models in general is that instead of final level representations, intermediate level representations present the optimal results. These experiments also verify that while deep models transform attributes from general to specific through the network eventually <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b70">[71]</ref>, intermediate layers present the optimal representations. This makes sense because while early layers response to low-level raw features such as corners and edges, late layers extract more object-specific features of the trained datasets. This is more clear on the depth plot in <ref type="figure" target="#fig_6">Fig. 6</ref>, where the dataset difference is obvious due to the domain difference.  We should state that RNN encoding on features extracted from FC layers with less than 8192 dimension might not be efficient since they are already compact enough. Therefore, encoding outputs of these layers to a larger feature space through RNNs might lead to redundancy in representations. This might be another reason why there is a drop in accuracy of these layers (e.g. see L7 in <ref type="figure" target="#fig_6">Fig. 6</ref>). In addition, depth plot contains more fluctuations and irregularities comparing to the RGB plot, since the pretrained models of the RGB ImageNet are used as fixed extractors without finetuning. As for the different baseline model comparison, ResNet-101 and DenseNet-121 models perform similarly in terms of accuracy and are better than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Level-wise Performance of Different Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Comparative Results of Random Weighted Pooling</head><p>In our approach, we extend the idea of randomness into a pooling strategy to cope with the high dimensionality of CNN activations. We particularly employ random pooling to confirm that randomness works greatly in overall RNN-Stage even in such a pooling strategy together with random RNNs. To this end, we investigate the comparative accuracy performances of random pooling together with average pooling and max pooling. We use the DenseNet-121 model, where pooling is used extensively on each level (except in level 4), and we conduct experiments using the same RNN weights for fair comparison. <ref type="figure" target="#fig_7">Fig. 7</ref> shows average accuracy results of two splits for each pooling on both RGB and depth data. As seen from the figure, random weighted pooling generally performs similar to average pooling, while it performs better than max pooling. Moreover, it is seen that random pooling acquires better results especially in middle/late levels(L4-L7), which presents more stable and meaningful representations comparing to the early levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Effect of Multi-Level RNN Structure</head><p>An RNN in this study is of one-level structure with a single parent computation, which is obviously computationally fast comparing to the multi-level structural RNNs. Furthermore, in this way, it provides an ease of use with no need of further processing for fixing the required input forms. However, in order to testify the performance of single-level RNNs over multiple-level RNNs, we analyze the comparative accuracy performances of 1-level RNNs together with 3-levels RNNs (see <ref type="figure" target="#fig_2">Fig. 3</ref>). To this end, we conduct experiments on two CNN activation levels with highest semantic information (L6 and L7) of the baseline model of AlexNet. The average results of two splits for both of RGB and depth data are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. The results show that RNN with 1-level performs better than RNN with 3-levels on RGB data, while 3-levels of RNN is better than 1-level of RNN on depth data. The better performance of RNN with 3-levels on depth data might be due to the use of a pretrained CNN model based on the RGB data of ImageNet. Hence, further processing might provide more representative information for depth data in that way. Therefore, this difference might be diminished or turn in favor of 1-level RNNs in the use of finetuned CNNs for depth modality as well. Overall, considering both RGB and depth data together, RNNs with 1-level are better in terms of accuracy performance as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6">Contribution of Fine-tuning</head><p>We have not used any training or fine-tuning in our approach to feature extraction in the experiments so far. Although impressive results are obtained on RGB data, the same success is not achieved on depth data. The reason for this difference is that the baseline CNN models are pretrained models on RGB dataset of the ImageNet. Therefore, as the next step, we analyze the changes in accuracy performance of RGB and depth data modalities by fine-tuning the baseline CNN models in our approach.</p><p>To this end, we first carry out a systematic inquiry to find optimal fine-tuning hyper-parameters on a predefined set of values using only one split of Washington RGB-D dataset as a validation set for AlexNet and DenseNet-121 models. Then, fine-tuning of the models are performed by stochastic gradient descent (SGD) with momentum. The hyper-parameters of momentum, learning rate, batch size, learning rate decay factor and decay step size, and number of epochs, respectively are used as following; (0.9, 0.001, 32, 0.01, 10, 40) and (0.9, 0.0001, 8, 0.1, 10, 40) are used for AlexNet on RGB and depth data, respectively, whereas (0.95, 0.0001, 16, 0.1, 10, 40) and (0.95, 0.001, 8, 0.1, 10, 40) are used for DenseNet-121. Apart from these two models, we also perform fine-tuning on the ResNet-101 model. We use the same fine-tuning hyperparameters of DenseNet-121 for ResNet-101, since they are in a similar architectural structure. <ref type="figure" target="#fig_9">Fig. 9</ref> shows average accuracy performance of finetuned CNN models together with fixed models on all the 10 evaluation splits of Washington RGB-D object dataset. The plot shows a clear upward in performance on depth data as expected. However, there is a loss of accuracy in general, when fine-tuning is performed on RGB data. Washington RGB-D object dataset contains a subset of the categories in ImageNet. Accordingly, pretrained models of ImageNet are already satisfy highly correlated distribution on RGB data. Therefore, there is no need for fine-tuning on RGB data. In contrast, in order to ensure coherence and relevance, fine-tuning is required for depth data due to domain difference of the inputs with the pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.7">Empirical Performance of Different Fusion Strategies</head><p>We have shown that a fixed pretrained CNN model together with random RNN already achieves impressive results on a single level. Likewise, when such pretrained models are fine-tuned on depth data, the results are boosted greatly. The best single levels for RGB and depth data, respectively, are L4, L5 for AlexNet; L5, L6 for ResNet-101; and L6, L7 for DenseNet-121. Next, to further improve accuracy performances, we investigate empirical accuracy analysis of multi-level fusions using fixed pretrained CNN models on RGB data and fine-tuned CNN models on depth data. In this work, in addition to the feature concatenation as in our previous work <ref type="bibr" target="#b13">[14]</ref>, we also apply average voting based on SVM confidence scores on the best performing levels. <ref type="table" target="#tab_3">Table 2</ref> reports the average accuracy on the all 10 train/test Finally, we provide RGB-D combined results for all three models as shown in <ref type="table" target="#tab_4">Table 3</ref> based on the SVM confidences. The table reports average results for fusion of the best levels of RGB and depth, and the best trio levels. We evaluate two variants of soft voting, our proposed weighted vote and average vote. The proposed weighted vote increases accuracy comparing to average vote for all the models both on the multi-modal fusion of the best single and best trio levels of RGB and depth streams. The results also confirm the strength of our multi-modal voting approach that combines RGB and depth modalities effectively.  <ref type="table" target="#tab_5">Table 4</ref> shows average accuracy performance of our approach along with the state-of-the-art methods for object recognition on Washington RGB-D object benchmark. Our approach greatly improves the previous state-of-the-art results for both of RGB and depth modalities with a margin   <ref type="bibr" target="#b42">[43]</ref>, which is slightly better than ours (0.3%). These results emphasize the importance of deep features in a unified framework based on the incorporation of CNNs and random RNNs. We also present average accuracy performance of individual object categories on the 10 evaluation splits of Washinton RGB-D Object dataset using the best-performing structure, ResNet101-RNN. As shown in <ref type="figure" target="#fig_10">Fig. 10</ref>, our approach is highly accurate in recognition of the most of the object categories. Categories with lower accuray results are mushroom, peach, and pitcher. The common reason that leads to the lower performance in these categories seems to be due to their less number of instances. In particular, these categories have only 3 instances, which is the minimum number for any category in the dataset. Considering the other categories with up to 14 instances, this imbalance of the data may have biased the learning to favor of categories with more examples. Moreover, the accuracy of our combined RGB and depth based on weighted confidences of modalities reflects that the fusion of RGB and depth data in this way can provide strong discrimination capability for object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Object Recognition Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Scene Recognition Performance</head><p>To test the generalization ability of our approach, we also carry out comparative analysis of our best-performing model, namely ResNet101-RNN, on SUN RGB-D Scene <ref type="bibr" target="#b12">[13]</ref> dataset for scene recognition as a more challenging task of scene understanding. To this end, we first apply ResNet101 pretrained model without finetuning, namely Fixed ResNet101-RNN, for both of RGB and depth modalities. Then, we finetune the pretrained CNN model on SUN RGB-D Scene dataset using the same hyper-parameters of object recognition task (see Sec. 4.2.6). The results of these experiments together with the-state-of-the-art results on this dataset are reported in <ref type="table" target="#tab_6">Table 5</ref>. Our best system outperforms the-state-of-the-art methods for all of the data types with impressive improvement of 8%, 6%, and 5.2% for RGB, depth, and RGB-D, respectively, over the previous best performing results. It is worth mentioning that we use the pretrained CNN model on object-centric dataset of ImageNet <ref type="bibr" target="#b64">[65]</ref>, which is less commonly used for scene recognition task than the pretrained models on scene-centric datasets such as Places <ref type="bibr" target="#b72">[73]</ref>. Nevertheless, our approach outperforms existing state-of-the-art methods for RGB-D scene recognition task. Moreoever, it is interesting that our system even with fixed pretrained CNN model is already discriminative enough and achieves impressive accuracy performances. Contrary to our findings on Washington RGB-D Object dataset, finetuning provides much better results not only for depth domain but also for the RGB domain as well. This is what we expect as scene recognition is a cross-domain task for our approach that has the pretrained CNN model of the object-centric ImageNet as the backbone. Specifically, finetuning on depth data boosts the accuracy greatly by providing both domain and modality adaptation. <ref type="figure" target="#fig_11">Fig. 11</ref> shows the confusion matrix of our approach with fine-tuning over the 19 categories of SUN RGB-D Scene dataset for RGB-D. The matrix demonstrates the degree of confusion between pairs of scene categories and implies the similarity between scenes on this dataset. The largest misclassification errors happen to be between extremely similar scene categories such as computer roomoffice, conference room-classroom, discussion area-rest space, lecture theatreclassroom, study space-classroom, lab-office, etc. In addition to  the inter-class similarity, other reasons for poor performance might be intra-class variations of the scenes and lack of getting enough representative knowledge transfer from the ImageNet models. To further analyse the performance of our system, we give top-3 and top-5 classification accuracy together with top-1 results as in <ref type="table" target="#tab_7">Table 6</ref>. While the top-1 accuracy shows the percentage of test images that exactly matches with the predicted classes, the top-3 and top-5 indicates the percentage of test images that are among the top ranked 3 and 5 predictions, respectively. The top-3 and top-5 results demonstrate the effectiveness of our system more closely by overcoming ambiguity among scene categories greatly. <ref type="figure" target="#fig_1">Fig.  12</ref> depicts some test examples of scene categories confused with each other frequently on SUN RGB-D Scene dataset. As shown in the figure, these scene categories have similar appearances that make them hard to distinguish even for a human expert without sufficient context knowledge in the evaluation. Nevertheless, our approach is able to identify scene category labels among the top-3 and top-5 predictions with high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>Our framework presents an effective solution for deep feature extraction in an efficient way by integrating a pretrained CNN model with random weights based RNNs.</p><p>Randomization throughout our RNN-Stage raises the question of whether the results are stable enough. The carefully implemented experiments in Sec. 4.2.2 are an empirical justification for the stability of random weights. On the other hand, our multi-level analysis shows that the optimum performance gain from a single level always comes from an intermediate level for all the models with/without finetuning for both of RGB and depth modalities. The only exception is in the use of finetuned DenseNet-121 model on depth data. This is an interesting finding, because one expects better representation capabilities of final layers, especially in the use of finetuned models. Yet, as expected, performance generally increases from the first level to the last level throughout the networks when the underlying CNN models are finetuned. Since Washington RGB-D Object <ref type="bibr" target="#b11">[12]</ref> dataset includes a subset of object categories in the ImageNet <ref type="bibr" target="#b64">[65]</ref>, finetuning does not improve accuracy success on RGB data. In contrast, accuracy gain is significant due to the need for domain adaptation in depth data. This also shows that using an appropriate technique to handle depth data as in our approach (Sec. 3.1), leads impressive performance improvement by knowledge transfer between modalities.</p><p>In this study, although we have explored different techniques to fuse representations of multiple levels to further increase the classification success, a single optimum level may actually be sufficient enough for many tasks. In this way, especially for tasks where computational time is more critical, results can be obtained much faster without sacrificing accuracy success. Another point of interest is that the data imbalance in Washington RGB-D Object dataset results in poor performance for the individual categories with less instances and consequently leads to a drop in the overall success of the system. Hence, this imbalance might be overcome by applying data augmentation on the categories with less instances. However, it is worth to note that we do not perform any data augmentation in this study for both tasks.</p><p>The success of our approach for RGB-D scene recognition confirms the generalization ability of the proposed framework. Unlike object recognition, when the underlying CNN models are finetuned, success in both RGB and depth modalities increases significantly in scene recognition task. This is due to the need for cross-domain task adaptation of object-centric based pretrained models. Therefore, similar findings in object recognition could be observed if scenecentric based pretrained models are employed for scene recognition (e. g. Places <ref type="bibr" target="#b72">[73]</ref>). Moreover, such pretrained models could improve the results further with our framework. Another potential that could improve the success for scene recognition is embedding contextual knowledge by jointly employing attention mechanism such as <ref type="bibr" target="#b77">[78]</ref> in our structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have presented a framework that incorporates pretrained CNN models together with multiple random recursive neural networks. The proposed approach greatly improves RGB-D object and scene recognition performances over the-state-of-the-art results in the literature on the widely used Washington RGB-D Object and SUN RGB-D Scene datasets. The proposed randomized pooling schema allows us to deal with high-dimensional activations of CNN models effectively. The extensive experimental analysis of various parameters and setup properties show that the incorporation of multiple random RNNs with a pretrained CNN model provides a robust and effective general solution for both of RGB-D object and scene recognition tasks. Utilizing depth data by mapping it into RGB-like image domain allows knowledge transfer from RGB pretrained CNN models effectively. The generic design and the generalization capability of the proposed framework allow to utilize it for other visual recognition tasks. Thus, we will open our code along with models to the community in order to help future studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Schematic overview of CNN models and their level-wise extraction points based structures. Each level of schematic view shows name of the level, operations performed in the level with the number of them if exist (for ResNet<ref type="bibr" target="#b16">[17]</ref> and DenseNet<ref type="bibr" target="#b17">[18]</ref> models), and dimensions of the activation output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)Fig. 3 :</head><label>3</label><figDesc>Graphical representation of a single recursive neural network (RNN). The same random weights have been applied to compute each node and level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 Fig. 4 :</head><label>24</label><figDesc>where , ∈ ℝ × × ′ pooling result where ′ ∈ ℝ3 × × tensor product where , ∈ ℝ × × ′ pooling result where ′ ∈ ℝ × 2 × Illustration of random weighted pooling over number of maps (top) and size of maps (below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Effect of randomness on the accuracy results for each level (L1 to L7). Values indicate standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Level-wise average accuracy performance of different baseline models on all the 10-splits of Washington RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Average accuracy performance of different pooling methods on RGB and depth data for the baseline model of DenseNet-121 on two splits of Washington RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of single-level and multi-level RNNs on two different CNN activations (L6 and L7) of AlexNet. The horizontal axis shows average accuracy performances (%) on two splits of Washington RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Level-wise average accuracy performance of finetuned CNN models together with fixed models on all the 10-splits of Washington RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Per-category average accuracy performances of ResNet101-RNN on Washington RGB-D Object dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>RGB-D confusion matrix of ResNet101-RNN on SUN RGB-D Scene dataset (best viewed with magnification).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 :</head><label>12</label><figDesc>Top-5 RGB-D predictions of our system using sample test images of frequently confused scene categories on SUN RGB-D Scene dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Level 2 layer2 conv2 maxpool2 192 × 13 × 13 Level 3 layer3 conv3 384 × 13 × 13 Level 4 layer4 conv4 256 × 13 × 13 Level 5 layer5 conv5 maxpool5 256 × 6 × 6 Level 6 layer6 fc6 4096 Level 7 layer7 fc7 4096 VGGNet-16 Level 1 block1 conv1.1 conv1.2 maxpool1 64 × 112 × 112 Level 2 block2 conv2.1 conv2.2 maxpool2 128 × 56 × 56 Level 3 block3 conv3.1 conv3.2 conv3.3 maxpool3 256 × 28 × 28 Level 4 block4 conv4.1 conv4.2 conv4.3 maxpool4 512 × 14 × 14 Level 5 block5 conv5.1 conv5.2 conv5.3 maxpool5 512 × 7 × 7 Level 6 fc6 fc6 4096 Level 7 fc7 fc7 4096 ResNet-50 Level 1 basic_block conv1 maxpool1 64 × 56 × 56 Level 228 × 28 block2 conv3.1 conv3.2 512 × 28 × 28 ×12 transition2 conv4 avgpool4 256 × 14 × 14 block3 conv5.1 conv5.2 1024 × 14 × 14 ×24 transition3 conv6 avgpool6 512 × 7 × 7 block4 conv7.1 conv7.2 1024 × 7 × 7 ×16</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Level 3</cell><cell></cell><cell>Level 4</cell><cell></cell><cell>Level 5</cell><cell></cell><cell>Level 6</cell><cell>Level 7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>block1</cell><cell></cell><cell>block2</cell><cell></cell><cell cols="2">block3 (p1)</cell><cell cols="2">block3 (p2)</cell><cell>block4</cell><cell>avgpool</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conv2.1</cell><cell></cell><cell>conv3.1</cell><cell></cell><cell>conv4.1</cell><cell></cell><cell>conv4.1</cell><cell></cell><cell>conv5.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conv2.2</cell><cell>×3</cell><cell>conv3.2</cell><cell>×4</cell><cell>conv4.2</cell><cell>×3</cell><cell>conv4.2</cell><cell>×3</cell><cell>conv5.2</cell><cell>×3</cell><cell>avgpool</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conv2.3</cell><cell></cell><cell>conv3.3</cell><cell></cell><cell>conv4.3</cell><cell></cell><cell>conv4.3</cell><cell></cell><cell>conv5.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">256 × 56 × 56</cell><cell cols="2">512 × 28 × 28</cell><cell cols="2">1024 × 14 × 14</cell><cell cols="2">1024 × 14 × 14</cell><cell>2048 × 7 × 7</cell><cell>2048</cell></row><row><cell></cell><cell>Level 1</cell><cell></cell><cell>Level 2</cell><cell></cell><cell>Level 3</cell><cell></cell><cell>Level 4</cell><cell></cell><cell>Level 5</cell><cell></cell><cell>Level 6</cell><cell>Level 7</cell></row><row><cell>ResNet-101</cell><cell cols="2">basic_block conv1 maxpool1</cell><cell>block1 conv2.1 conv2.3 conv2.2</cell><cell>×3</cell><cell>block2 conv3.1 conv3.3 conv3.2</cell><cell>×4</cell><cell cols="2">block3 (p1) conv4.1 conv4.3 conv4.2 ×12</cell><cell cols="2">block3 (p2) conv4.1 conv4.3 conv4.2 ×11</cell><cell>block4 conv5.1 conv5.3 conv5.2</cell><cell>×3</cell><cell>avgpool avgpool</cell></row><row><cell></cell><cell cols="2">64 × 56 × 56</cell><cell cols="2">256 × 56 × 56</cell><cell cols="2">512 × 28 × 28</cell><cell cols="2">1024 × 14 × 14</cell><cell cols="2">1024 × 14 × 14</cell><cell>2048 × 7 × 7</cell><cell>2048</cell></row><row><cell></cell><cell>Level 1</cell><cell></cell><cell>Level 2</cell><cell></cell><cell>Level 3</cell><cell></cell><cell>Level 4</cell><cell></cell><cell>Level 5</cell><cell></cell><cell>Level 6</cell><cell>Level 7</cell></row><row><cell>DenseNet-121</cell><cell>block1 conv1.1 conv1.2</cell><cell>×6</cell><cell cols="2">transition1 conv2 avgpool2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">256 × 56 × 56</cell><cell>128 ×</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Washington RGB-D Object Dataset Washington RGB-D object dataset includes a total of 41, 877 images for each modality under 51 object categories and 300 category instances. Categories are commonly used household objects such as cups, camera, keyboards, vegetables, fruits, etc. Each instance of a category has images taken from 30 • , 45 • and 60 • elevation angles. The dataset provides 10 train/test splits where in each split, one instance for each category is used for testing and the remaining instances are for training. Thus, for a single split run, a total of 51 category instances (roughly 7, 000 images) are used at testing and the remaining 249 instances (roughly 35, 000 images) are used at training phase. We evaluate the proposed work on the provided cropped images with the same setup in<ref type="bibr" target="#b11">[12]</ref> for the 10 splits and average accuracy results are reported for the comparison to the related works.</figDesc><table><row><cell>4.1) for two tasks: (i) RGB-D</cell></row><row><cell>object recognition (Sec. 4.3) using Washington RGB-D object</cell></row><row><cell>dataset [12] and (ii) RGB-D scene recognition (Sec. 4.4)</cell></row><row><cell>using SUN RGB-D scene dataset [13]. In order to evaluate</cell></row><row><cell>effects of various model parameters and setup properties</cell></row></table><note>in our framework, we carry out extensive experiments (Sec. 4.2) on the challenging Washington RGB-D object dataset, which is a larger-scale RGB-D dataset comparing to other RGB-D benchmarks. Finally, we compare our results with state-of-the-art results for both benchmarks. Results of other methods are taken from the original papers.3. https://github.com/scikit-learn/scikit-learn 4.1 Dataset and Setup 4.1.14.1.2 SUN RGB-D Scene Dataset SUN RGB-D scene dataset is the largest real-world RGB-D scene understanding benchmark to the date and contains RGB-D images of indoor scenes. Following the publicly available configuration of the dataset, we choose 19 scene categories with a total of 4, 845 images for training and 4, 659 images for testing. We use the same train/test split of Song et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Average computational time and memory overhead for overall data processing and model learning on two splits of Washington RGB-D dataset. Results cover both of train and test phases together.</figDesc><table><row><cell></cell><cell cols="2">Time (hh:mm:ss)</cell><cell></cell><cell></cell><cell>Memory</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Feature Extraction (CNN-RNN Stages)</cell><cell>Classification (SVMs)</cell><cell>Overall</cell><cell>CNN-Stage (GPU)</cell><cell cols="2">RNN-Stage (CPU) Pool Weights RNN Weights</cell><cell>Overall (CPU)</cell></row><row><cell>AlexNet</cell><cell>00:07:41</cell><cell>00:28:33</cell><cell>00:36:14</cell><cell>1115 MB</cell><cell>772.1 kB</cell><cell>4.2 GB</cell><cell>12.6 GB</cell></row><row><cell>VGGNet-16</cell><cell>00:21:21</cell><cell>00:36:42</cell><cell>00:58:03</cell><cell>9259 MB</cell><cell>8.6 MB</cell><cell>4.8 GB</cell><cell>11.8 GB</cell></row><row><cell>ResNet-50</cell><cell>00:16:23</cell><cell>00:38:36</cell><cell>00:54:59</cell><cell>6067 MB</cell><cell>9.6 MB</cell><cell>5.1 GB</cell><cell>10.8 GB</cell></row><row><cell>ResNet-101</cell><cell>00:19:08</cell><cell>00:40:33</cell><cell>00:59:41</cell><cell>8795 MB</cell><cell>9.6 MB</cell><cell>5.1 GB</cell><cell>11.8 GB</cell></row><row><cell>DenseNet-121</cell><cell>00:17:02</cell><cell>00:26:47</cell><cell>00:43:49</cell><cell>8821 MB</cell><cell>8.3 MB</cell><cell>5.4 GB</cell><cell>13.3 GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="7">Average accuracy performance of different fusion</cell></row><row><cell cols="8">combinations on the best three levels using Washington</cell></row><row><cell cols="2">RGB-D dataset (%).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">AlexNet</cell><cell cols="2">DenseNet-121</cell><cell cols="2">ResNet-101</cell></row><row><cell></cell><cell></cell><cell>RGB</cell><cell>Depth</cell><cell>RGB</cell><cell>Depth</cell><cell>RGB</cell><cell>Depth</cell></row><row><cell></cell><cell>LB1</cell><cell>81.4 ± 1.8</cell><cell>83.5 ± 2.2</cell><cell>89.7 ± 1.0</cell><cell>85.0 ± 2.1</cell><cell>89.2 ± 1.3</cell><cell>85.5 ± 2.2</cell></row><row><cell>Single</cell><cell>LB2</cell><cell>81.1 ± 2.1</cell><cell>83.3 ± 2.2</cell><cell>91.0 ± 1.2</cell><cell>86.2 ± 2.3</cell><cell>91.1 ± 1.0</cell><cell>87.1 ± 2.7</cell></row><row><cell></cell><cell>LB3</cell><cell>79.2 ± 2.4</cell><cell>83.2 ± 2.3</cell><cell>89.5 ± 1.5</cell><cell>86.8 ± 2.1</cell><cell>90.5 ± 1.6</cell><cell>86.9 ± 2.6</cell></row><row><cell></cell><cell>LB1 + LB2</cell><cell>83.0 ± 1.9</cell><cell>84.0 ± 2.4</cell><cell>90.4 ± 1.0</cell><cell>85.5 ± 2.1</cell><cell>91.1 ± 1.1</cell><cell>87.1 ± 2.7</cell></row><row><cell>Concats</cell><cell>LB1 + LB3 LB2 + LB3</cell><cell>82.2 ± 2.0 81.0 ± 2.0</cell><cell>83.8 ± 2.4 83.4 ± 2.3</cell><cell>90.0 ± 1.5 89.6 ± 1.5</cell><cell>86.9 ± 2.1 86.8 ± 2.1</cell><cell>91.1 ± 1.4 91.1 ± 1.5</cell><cell>86.9 ± 2.6 87.0 ± 2.7</cell></row><row><cell></cell><cell>LB1 + LB2 + LB3</cell><cell>82.5 ± 2.0</cell><cell>83.8 ± 2.3</cell><cell>90.0 ± 1.5</cell><cell>86.9 ± 2.1</cell><cell>91.5 ± 1.3</cell><cell>87.0 ± 2.7</cell></row><row><cell></cell><cell>LB1 + LB2</cell><cell>82.8 ± 1.9</cell><cell>84.1 ± 2.3</cell><cell>91.2 ± 1.0</cell><cell>86.0 ± 2.2</cell><cell>91.0 ± 1.1</cell><cell>87.0 ± 2.5</cell></row><row><cell>SVM Avg Voting</cell><cell>LB1 + LB3 LB2 + LB3</cell><cell>82.5 ± 2.0 81.1 ± 2.0</cell><cell>84.1 ± 2.4 83.4 ± 2.3</cell><cell>91.2 ± 1.0 91.3 ± 1.3</cell><cell>86.8 ± 2.2 86.8 ± 2.2</cell><cell>91.8 ± 1.2 92.2 ± 1.0</cell><cell>87.1 ± 2.5 87.0 ± 2.7</cell></row><row><cell></cell><cell>LB1 + LB2 + LB3</cell><cell>82.7 ± 2.1</cell><cell>84.0 ± 2.4</cell><cell>91.5 ± 1.1</cell><cell>86.7 ± 2.2</cell><cell>92.3 ± 1.0</cell><cell>87.2 ± 2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">Average accuracy performance of RGB-D (RGB +</cell></row><row><cell cols="6">Depth) with different fusion combinations on Washington</cell></row><row><cell cols="2">RGB-D dataset (%).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>AlexNet</cell><cell>DenseNet-121</cell><cell>ResNet-101</cell></row><row><cell>Avg Vote</cell><cell>RGBLB1</cell><cell>+ Depth LB1</cell><cell>90.2 ± 1.3</cell><cell>92.9 ± 1.4</cell><cell>92.7 ± 1.6</cell></row><row><cell>Weighted Vote</cell><cell>RGBLB1</cell><cell>+ Depth LB1</cell><cell>90.2 ± 1.2</cell><cell>93.5 ± 1.0</cell><cell>93.8 ± 1.1</cell></row><row><cell>Avg Vote</cell><cell cols="2">RGBLB1+LB2+LB3 + Depth LB1+LB2+LB3</cell><cell>90.6 ± 1.6</cell><cell>92.6 ± 1.4</cell><cell>93.0 ± 1.3</cell></row><row><cell>Weighted Vote</cell><cell cols="2">RGBLB1+LB2+LB3 + Depth LB1+LB2+LB3</cell><cell>90.9 ± 1.3</cell><cell>93.5 ± 1.0</cell><cell>94.1 ± 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Average accuracy comparison of our approach</cell></row><row><cell>with the related methods on Washington RGB-D Object</cell></row><row><cell>dataset (%). Red: Best result, Blue: Second best result, Green:</cell></row><row><cell>Third best result.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>Accuracy comparison of our approach with the related methods on SUN RGB-D Scene dataset (%). Red: Best result, Blue: Second best result, Green: Third best result.</figDesc><table><row><cell>Method</cell><cell>RGB</cell><cell>Depth</cell><cell>RGB-D</cell></row><row><cell>Places CNN-Lin SVM [73]</cell><cell>35.6</cell><cell>25.5</cell><cell>37.2</cell></row><row><cell>Places CNN-RBF SVM [73]</cell><cell>38.1</cell><cell>27.7</cell><cell>39.0</cell></row><row><cell>SS-CNN-R6 [3]</cell><cell>36.1</cell><cell>-</cell><cell>41.3</cell></row><row><cell>DMFF [74]</cell><cell>37.0</cell><cell>-</cell><cell>41.5</cell></row><row><cell>Places CNN-RCNN [75]</cell><cell>40.4</cell><cell>36.3</cell><cell>48.1</cell></row><row><cell>MSMM [40]</cell><cell>41.5</cell><cell>40.1</cell><cell>52.3</cell></row><row><cell>RGB-D-CNN [76]</cell><cell>42.7</cell><cell>42.4</cell><cell>52.4</cell></row><row><cell>MDSI-CNN [45]</cell><cell>39.6</cell><cell>35.2</cell><cell>45.2</cell></row><row><cell>DF 2 Net [25]</cell><cell>-</cell><cell>-</cell><cell>54.6</cell></row><row><cell>HP-CNN-T [42]</cell><cell>38.8</cell><cell>28.5</cell><cell>42.2</cell></row><row><cell>RGB-D-OB [4]</cell><cell>-</cell><cell>42.4</cell><cell>53.8</cell></row><row><cell>G-L-SOOR [77]</cell><cell>50.5</cell><cell>44.1</cell><cell>55.5</cell></row><row><cell>This work -Fix ResNet101-RNN</cell><cell>50.8</cell><cell>38.6</cell><cell>53.1</cell></row><row><cell>This work -Finetuned ResNet101-RNN</cell><cell>58.5</cell><cell>50.1</cell><cell>60.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>Scene recognition accuracy of top-1, top-3, and top-5 on SUN RGB-D Scene dataset (%).</figDesc><table><row><cell>Accuracy</cell><cell>RGB</cell><cell>Depth</cell><cell>RGB-D</cell></row><row><cell>top-1</cell><cell>58.5</cell><cell>50.1</cell><cell>60.7</cell></row><row><cell>top-3</cell><cell>81.0</cell><cell>71.5</cell><cell>83.6</cell></row><row><cell>top-5</cell><cell>88.5</cell><cell>80.9</cell><cell>89.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition and pose estimation based on pre-trained convolutional neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1329" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understand scene categories by objects: A semantic regularized scene classifier using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2318" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning effective rgb-d representations for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="980" to="993" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional hypercube pyramid for accurate rgb-d object category and instance recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multiview rgb-d object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting multi-layer features using a cnn-rnn approach for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burak Can</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rgb-d indoor mapping using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Guclu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burak Can</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Depth kernel descriptors for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="821" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object recognition with hierarchical kernel descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Histogram of oriented normal vectors for object recognition with a depth sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Skubic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="525" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mmss: Multi-modal sharable and specific feature learning for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-margin multi-modal deep learning for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1887" to="1898" />
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Df2net: Discriminative feature learning and fusion network for rgb-d indoor scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition with multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yemez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional fisher kernels for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge guided disambiguation for large-scale scene classification with multi-resolution cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Factors of transferability for a generic convnet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient rgb-d object categorization using cascaded ensembles of randomized decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1295" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition and grasp detection using hierarchical cascaded forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="547" to="564" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">(de) 2 co: Deep depth colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2386" to="2393" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The treasure beneath convolutional layers: Cross-convolutional-layer pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4749" to="4757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a deeply supervised multi-modal rgb-d embedding for semantic scene and object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining models from multiple sources for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/631</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/631" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4523" to="4529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale recognition with dag-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1215" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Viewpoint invariant semantic object and scene categorization with rgb-d sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1005" to="1022" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent convolutional fusion for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Loghmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Planamente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2878" to="2885" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A multi-modal, discriminative and spatially invariant cnn for rgb-d object labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2051" to="2065" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for robust rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis regularization: An effective deep multiview learning baseline for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="118" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Feed forward neural networks with random weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kraaijveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IAPR International Conference on Pattern Recognition</title>
		<meeting>the 11th IAPR International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Functional-link net computing: theory, system architecture, and functionalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="76" to="79" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning and generalization characteristics of the random vector functional-link net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sobajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stochastic choice of basis functions in adaptive function approximation and the functional-link net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Igelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1320" to="1329" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mapping part-whole hierarchies into connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="47" to="75" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recursive context propagation network for semantic scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2447" to="2455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Subset based deep learning for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="280" to="292" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semi-supervised learning and feature evaluation for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="149" to="160" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Object recognition using deep convolutional features transformed by a recursive network structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Burnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="10" to="059" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Depth kernel descriptors for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="821" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Internal representation of task rules by recurrent dynamics: the importance of the diversity of neural responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rigotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Ben Dayan Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multi-modal local receptive field extreme learning machine for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="page" from="4" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Discriminative multi-modal feature fusion for rgbd indoor scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Weibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Modality and component aware feature fusion for rgb-d scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Depth cnns for rgb-d scene recognition: Learning from scratch better than transferring from rgb-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Image representations with spatial object-to-object relations for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="525" to="537" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Attention branch network: Learning of attention mechanism for visual explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
