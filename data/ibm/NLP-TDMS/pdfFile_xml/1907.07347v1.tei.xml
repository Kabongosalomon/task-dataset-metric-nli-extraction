<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fake News Detection as Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 11-15, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Chou</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Niven</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
							<email>hykao@mail.ncku.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Chou</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Niven</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Cheng Kung University Tainan</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Cheng Kung University Tainan</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Cheng Kung University Tainan</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fake News Detection as Natural Language Inference</title>
					</analytic>
					<monogr>
						<title level="m">WSDM &apos;19</title>
						<meeting> <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">February 11-15, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>. 2019. Fake News De-tection as Natural Language Inference. In WSDM &apos;19, February 11-15, 2019, Melbourne, Australia. ACM, New York, NY, USA, 4 pages. https:// ACM ISBN 978-1-4503-9999-9/18/06. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Overview of our method. High performing NLI models are independently trained and ensembled with a fine-tuned BERT model to determine soft labels, which are then used to fine-tune the original NLI models, BERT, and the Decomposable Attention model. These are then ensembled and combined with predictions made via observing transitivity relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>This report describes the entry by the Intelligent Knowledge Management (IKM) Lab in the WSDM 2019 Fake News Classification challenge. We treat the task as natural language inference (NLI). We individually train a number of the strongest NLI models as well as BERT. We ensemble these results and retrain with noisy labels in two stages. We analyze transitivity relations in the train and test sets and determine a set of test cases that can be reliably classified on this basis. The remainder of test cases are classified by our ensemble. Our entry achieves test set accuracy of 88.063% for 3rd place in the competition. KEYWORDS fake news, natural language inference, natural language processing ACM Reference Format:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The WSDM 2019 Fake News Classification challenge presents pairs of sentences requiring three-class prediction. The first sentence is the title of an article already known to be fake news. The second sentence is the title of another article, and the task is to decide whether it agrees with the original fake news, disagrees with it, or is unrelated. Sentences are in Mandarin and are drawn from news sources in China. English translations are provided, however they are noisy machine translations and we did not obtain good results using them. For this reason, we ignored the English sentences and just used the Mandarin. <ref type="bibr" target="#b0">1</ref> This task can be viewed as a natural language inference task <ref type="bibr" target="#b1">[2]</ref>, where the first sentence corresponds to the "premise" (P) and the second to the "hypothesis" (H). An example of a contradiction (disagreed) case taken from the training data: P Over 1,000 foreigners dissect children and steal their organs H</p><p>The rumour of 1,000 foreigners harvesting children's organs strikes again Based on this observation we focused on the best performing neural models for SNLI 2 as well as BERT <ref type="bibr" target="#b4">[5]</ref>. We used these models in an ensemble and fine-tuned with soft labels in a process summarized in figure 1, that will be outlined in the following section.</p><p>We additionally explored transitive relations in the train and test datasets and found that classification on this basis can be used to reliably increase our accuracy as compared to using our final classifier's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>Our overall method is summarized in <ref type="figure">figure 1</ref> and is explained in detail in this section, which aims to provide step-by-step instructions to accompany our published code 3 for reproducing of our results.</p><p>In general we optimize with Adam <ref type="bibr" target="#b6">[7]</ref>, use early stopping conditioned on validation set accuracy, and dropout <ref type="bibr" target="#b11">[12]</ref> for regularization. As we have a large number of models and hyperparameter settings, we refer the reader to our published code for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embeddings</head><p>We prepare multiple kinds of embedding at both the word and character level. We train models on all types independently and ensemble the results. This approach is motivated by the fact that Chinese word segmentation is a hard problem often resulting in noise and many out-of-vocabulary tokens. Ensembling the different information captured by these embedding techniques is also expected to reduce bias and variance.</p><p>At the word level, we use Tencent <ref type="bibr" target="#b10">[11]</ref> word embedding and the SGNS version of Chinese word embeddings <ref type="bibr" target="#b7">[8]</ref>. These word embeddings are trained in an unsupervised manner on a large corpus drawn from different sources and have acceptable word coverage for this task. At the character level, we train a skip-gram model <ref type="bibr" target="#b8">[9]</ref>, CBOW model, and FastText model <ref type="bibr" target="#b0">[1]</ref> with window sizes 5, 7, and 3, respectively. The training data for character embedding training is composed of the set of sentences in train and test datasets. These three embeddings are then concatenated to form the final character-level embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">First Level NLI Models</head><p>We independently train a number of relatively high performing NLI models:</p><formula xml:id="formula_0">• Dense RNN • Dense CNN • ESIM [3] • Gated CNN [4] • Decomposable Attention [10]</formula><p>The dense NLI architecture is an abstract version of <ref type="bibr" target="#b5">[6]</ref>, which densely concatenates the features from different levels and repeats the comparison loop several times. The dense CNN shares the same general architecture as the dense RNN (figure 2) but uses a CNN encoder instead. For ESIM, Gated CNN and decomposable attention, we slightly modified the architectures to make them fit well on this task. Due to space requirements, we refer the reader to corresponding researches for details about their architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">First Level Ensemble</head><p>To ensemble the first level weak models, we use LightGBM 4 and a densely connected feed-forward network. For each of our 10 models, we concatenate the 3 class label probabilities, yielding a 30-dimensional input vector for each training and testing data point. For preventing data leakage, we choose the same validation set for  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">BERT</head><p>We train the BERT base Chinese model 5 for three epochs on the full training set. The learning rate was 5e-5, maximum sequence length was 128, and batch size was 32. This achieved a test set accuracy of 86.689%. We did not perform an exhaustive search of hyperparameter space due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Blending</head><p>For the final output in the first level, we blend the predictions of (2.3) and (2.4). The blended predictions are the weighted sum of the BERT and ensemble predictions, with weights 0.42 and 0.58, respectively. We chose these weights by threshold search to make sure the blended result is an optimal mixture of both predictions. The test set accuracy is 86.963%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Second Level: Fine Tune NLI Models</head><p>We took the predictions generated by step (2.5) as soft pseudo-labels and used them to fine-tune the pretrained NLI models in step (2.1). The training set was concatenated with the pseudo-labeled test set and the same validation set was used for early stopping. The finetuned results are reported in table 1. Note that the Decomposable attention model was added at this stage, initialized with random  weights. Due to time constraints we did not train every combination of models and embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Second Level Ensemble</head><p>Once again, we used LightGBM and a multi-layer perceptron to perform ensembling with the output of the second level NLI models in the same manner as described in section <ref type="bibr">(2.3)</ref>. This achieved a test set accuracy of 87.990%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">BERT Pseudo-Label Training</head><p>We used the psueod-labels to continue fine tuning the BERT model from step <ref type="bibr">(2.4)</ref>. Whereas we tuned the NLI models on the pseudolabeled test set only, BERT was trained on the whole concatenation of the entire training set and the pseudo-labeled test set, without any validation. We trained BERT for three epochs with the same hyperparameter settings as in step <ref type="bibr">(2.4)</ref>. This model achieved a test set score of 87.484%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Final Blending</head><p>After fine-tuning BERT and the based NLI models, we once again blended their predictions to obtain our final predictions. The blending weights were 0.79 and 0.21, respectively. Just as in step (2.5) these weights were determined by threshold search. The blended result was 88.019%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Post-Processing: Transitive Relations</head><p>We investigated transitivity relations in the data and found they were reliable enough to use as test set predictions. <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates the two types of relation we considered. For positive relations, we observed that if sentence A agrees with sentence B, and B agrees with sentence C, then A should also agree with C. In the negative case, if A disagrees with B, and B agrees with C, the A should also disagree with C. We found that the positive case held for 99.9% of the training data. The negative case held 99.7% of the time. As there are sentence overlaps between the train and test sets, we are able to apply these rules recursively to generate predictions for 6, 888 data points in the test set. We expected these should be more reliable than our trained classifiers. We made a late submission with these predictions, labeling all other test set samples with a "fake" prediction label so only the samples labeled by transitivity rules were considered by the scorer. This submission achieved 93% accuracy, a level of performance dramatically higher than any trained classifier in the competition, validating our expectation. Overall, we observed a 0.04% increase in our test set accuracy using this method as compared to using our best classifier for all test set data points. The final accuracy was 88.063%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.11">Results</head><p>The results from all levels are summarized in table 2. We see a consistent improvement from successive ensembling and pseudolabel fine tuning. We note again that, due to time constraints we were unable to fully exploit all models we identified, nor conduct a suitably thorough hyperparameter search. We therefore expect our method could easily improve upon the results presented here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONCLUSION</head><p>NLI models appear to be generally effective for this task. Ensemble via gradient boosting and fine-tuning with noisy labels proved to be very beneficial. However, due to time constraints we were unable to cover all the combinations we targeted and expect we could improve further. As the train and test sets have overlapping sentences, we were able to exploit transitive relations between them to reliably improve our performance.</p><p>For future work we intend to further investigate the transitivity method for data augmentation. Initial investigation revealed we can create an additional 700, 000 sentence "agreed" pairs and 19, 000 "disagreed". That is 7 and 2 times the number of original training data points, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3 https://github.com/zake7749/WSDM-Cup-2019 4 https://github.com/Microsoft/LightGBM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The general architecture of the Dense RNN and Dense CNN models. early-stopping. The ensemble result achieves a test set accuracy of 86.741%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Positive and negative transitivity relations in the labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test set accuracy of for NLI models</figDesc><table><row><cell>Model</cell><cell cols="4">First Level Tencent SGNS Character Tencent SGNS Character Fine-tuned</cell></row><row><cell>Decomposable Attention</cell><cell></cell><cell></cell><cell>0.86730 0.86721</cell><cell></cell></row><row><cell>Dense RNN</cell><cell>0.85529 0.85620</cell><cell>0.85170</cell><cell>0.87704 0.87248</cell><cell>0.86809</cell></row><row><cell>Dense CNN</cell><cell>0.85082 0.84803</cell><cell>0.85287</cell><cell>0.87479 0.87114</cell><cell>0.86302</cell></row><row><cell>ESIM</cell><cell>0.85738 0.85622</cell><cell>0.86053</cell><cell>0.87788 0.87334</cell><cell>0.87436</cell></row><row><cell>Gated CNN</cell><cell>0.84711</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Test set accuracy of for ensembles, BERT, blended</cell></row><row><cell>models, and transitivity rules</cell><cell></cell></row><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>Ensemble (1)</cell><cell>0.86741</cell></row><row><cell>BERT (1)</cell><cell>0.86689</cell></row><row><cell>Blended (1)</cell><cell>0.86963</cell></row><row><cell>Ensemble (2)</cell><cell>0.87990</cell></row><row><cell>BERT (2)</cell><cell>0.87484</cell></row><row><cell>Blended (2)</cell><cell>0.88019</cell></row><row><cell>Blended (2) + Transitivity</cell><cell>0.88063</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Examples given in this report are in English and come from our own translations. 2 https://nlp.stanford.edu/projects/snli/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/huggingface/pytorch-pretrained-BERT</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<ptr target="http://arxiv.org/abs/1508.05326" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<ptr target="http://arxiv.org/abs/1609.06038" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language Modeling with Gated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08083</idno>
		<ptr target="http://arxiv.org/abs/1612.08083" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hyuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11360</idno>
		<ptr target="http://arxiv.org/abs/1805.11360" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analogical Reasoning on Chinese Morphological and Semantic Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renfen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P18-2023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Decomposable Attention Model for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<ptr target="http://arxiv.org/abs/1606.01933" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2028</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-2028" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
