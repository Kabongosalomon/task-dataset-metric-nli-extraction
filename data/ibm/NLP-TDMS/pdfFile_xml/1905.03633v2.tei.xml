<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intra-frame Object Tracking by Deblatting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
							<email>kotera@utia.cas.cz</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Rozumnyi</surname></persName>
							<email>rozumden@cmp.felk.cvut.cz</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UTIA</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CMP</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">FilipŠroubek UTIA</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Jiří Matas CMP</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intra-frame Object Tracking by Deblatting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objects moving at high speed along complex trajectories often appear in videos, especially videos of sports. Such objects elapse non-negligible distance during exposure time of a single frame and therefore their position in the frame is not well defined. They appear as semitransparent streaks due to the motion blur and cannot be reliably tracked by standard trackers.</p><p>We propose a novel approach called Tracking by Deblatting based on the observation that motion blur is directly related to the intra-frame trajectory of an object. Blur is estimated by solving two intertwined inverse problems, blind deblurring and image matting, which we call deblatting. The trajectory is then estimated by fitting a piecewise quadratic curve, which models physically justifiable trajectories. As a result, tracked objects are precisely localized with higher temporal resolution than by conventional trackers.</p><p>The proposed TbD tracker was evaluated on a newly created dataset of videos with ground truth obtained by a high-speed camera using a novel Trajectory-IoU metric that generalizes the traditional Intersection over Union and measures the accuracy of the intra-frame trajectory. The proposed method outperforms baseline both in recall and trajectory accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The field of visual object tracking has progressed significantly in recent years. The area encompasses a wide range of problems, including single object modelfree short-term tracking where a single target is localized in a video sequence given a single training example <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>, long-term tracking methods requiring redetection and learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>, multitarget multi-camera tracking <ref type="bibr" target="#b26">[27]</ref>, multi-view methods [20] and methods targeting specific objects such as cars <ref type="bibr" target="#b2">[3]</ref>, humans <ref type="bibr" target="#b23">[24]</ref>, or animals <ref type="bibr" target="#b9">[10]</ref>. Many variants of the problems have been considered -static or dynamic cameras or environments, RGBD input, use of inertial measurement units to name a few.</p><p>Recently, Rozumnyi et al . <ref type="bibr" target="#b27">[28]</ref> have shown that the performance of standard state-of-the-art trackers drops significantly when applied to Fast Moving Objects (FMO), apparently due to the effect of blur -such objects appear only as semi-transparent streaks. Examples of applications with FMOs include tracking of balls and ball-like objects in sports videos, particles in scientific experiments, and flying birds and insects.</p><p>Standard trackers, both long and short term, provide information about the object location in a frame in the from of a single rectangle. The true, continuous trajectory of the object center is thus sampled with the frequency equal to the video frame rate. For slow moving objects, such sampling is adequate. For fast moving objects, especially if their trajectory is not linear (due to bounces, gravity, friction), a single location estimate per frame cannot represent the true trajectory well, even if the fast moving object is inside the reported bounding box. Moreover, standard trackers typically fail even in achieving that <ref type="bibr" target="#b27">[28]</ref>.</p><p>We propose a novel methodology for tracking fastmoving, blurred objects. The approach untangles the image formation by solving two inverse problems: motion deblurring and image matting. We therefore call the method Tracking by Deblatting, TbD in short.</p><p>The deblatting procedure simultaneously recovers the trajectory of the object, its shape and appearance. We introduce a strong prior on the blur kernel and force it to lie on a 1D manifold. The corresponding curve models the object trajectory within a frame. Unlike a standard general tracker, TbD does not need a template of the object, since the representation of the shape and appearance of the object is recovered on the fly. Experiments show that the estimated trajectory is often highly accurate; see <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Object tracking methods are based on diverse principles, such as correlation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>, feature point tracking <ref type="bibr" target="#b34">[35]</ref>, mean-shift <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref>, and trackingby-detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b11">12]</ref>. In addition, several surveys of object tracking have been compiled <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11]</ref>. Excellent performance in visual object tracking has been shown by discriminative correlation filters <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22</ref>], yet all the methods fail when the tracked object is blurred as demonstrated in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Methods proposed for object motion deblurring try to estimate sharp images from photos or videos without considering the tracking goal. Early methods worked with a transparency map (the alpha matte) caused by the blur, and assumed linear motion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref> or rotation <ref type="bibr" target="#b29">[30]</ref>. Blind deconvolution of the transparency map is better posed, since the latent sharp map is a binary image. Accurate estimation of the transparency map by alpha matting algorithms, such as <ref type="bibr" target="#b20">[21]</ref>, is necessary and this is not tractable for large blurs. Other methods are based on the observation that autocorrelation increases in the direction of blur <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32]</ref>. Autocorrelation techniques require a relatively large neighborhood to estimate blur parameters and such methods are not suitable for small moving objects. More recently, deep learning has been applied to motion deblurring of videos <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31]</ref> and to the generation of intermediate short-exposure frames <ref type="bibr" target="#b13">[14]</ref>. The proposed convolutional neural networks are trained only on small blurs; blur parameters are not available as they are not directly estimated.</p><p>Tracking methods that consider motion blur has been proposed in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23]</ref>, yet there is an important distinction between models therein and the FMO problem considered here. The blur is assumed to be caused by camera motion and not by the object motion, which results in blur affecting the whole image and in the absence of alpha blending of the tracked object with the background.</p><p>To our knowledge, the only method that tackles the similar problem of tracking motion-blurred objects remains the work in <ref type="bibr" target="#b27">[28]</ref>. The authors assume linear motion and the trajectories are calculated by fitting a line segment to a morphologically thinned difference image between the given frame and the estimated background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tracking By Deblatting</head><p>The proposed method formulates tracking as an inverse problem to the video formation model. Suppose that within a single video frame I an object F moves along the trajectory C in front of background B. Frame I is then formed as</p><formula xml:id="formula_0">I = H * F + (1 − H * M )B,<label>(1)</label></formula><p>where * denotes convolution, H is the Point Spread Function (PSF) of the object motion blur corresponding to trajectory C, and M is the binary mask of the object shape (i.e. the indicator function of F ). We refer to the pair (F, M ) as the object model. The first term is the tracked object blurred by its own motion, the second term is the background partially occluded by the object, and the blending coefficients are determined by H * M . Inference under the assumption of this formation model consists of solving simultaneously two inverse problems: blind deblurring and image matting. The solution is the estimated PSF H and the object model F and M . Motion blur in (1) is modeled by convolution, which implies the following assumption about the object motion: The object shape and appearance remain constant during the frame exposure time. Scenarios that satisfy the assumption precisely are, e.g., an object of arbitrary shape undergoing only translational motion or a spherical object of uniform color undergoing arbitrary motion under spatially-uniform illumination. In addition, the motion must be in a plane parallel to the camera image plane to guarantee constant size of the object. For the purpose of tracking and trajectory estimation we claim that the formation model <ref type="bibr" target="#b0">(1)</ref> with convolution is sufficient as long as the assumption   holds at least approximately, which is experimentally validated on the presented dataset. The proposed method is iterative and causal processing of a new frame I i+1 using only knowledge acquired from earlier frames I 1 , . . . , I i ; <ref type="figure" target="#fig_1">Fig. 2</ref>   <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Step 1 stops after reaching either a given relative tolerance or a maximum number of iterations. Steps 1 and 2 are repeated only if the newly fitted C touches the boundary of D -in this case the new D is the d−neighborhood of C where d is the object diameter. Adjusting D this way helps to eliminate the detrimental influence of other moving objects to correct estimation of H.</p><p>If the consistency check (CC) passes, we extrapolate the estimated trajectory to the next frame and D i+1 is again d-neighborhood of this extrapolation. To update the appearance model we use exponential forgetting</p><formula xml:id="formula_1">F i+1 = γF i + (1 − γ)F i+1 ;<label>(2)</label></formula><p>M is updated analogically.</p><p>To enable long-term tracking, the FMO detector (FMOd) from <ref type="bibr" target="#b27">[28]</ref> determines the new input if CC fails. First, FMOd tries detecting the object in an gradually enlarged D. If it succeeds, the main TbD pipeline is reinitialized with D set as a neighborhood of the FMOd detection. If FMOd fails, TbD returns the extrapolation of trajectory C i as the best guess of C i+1 and tracking is restarted anew on the next frame. The background B i is estimated as a temporal median of frames B i−1 , B i−2 , . . ., optionally including video stabilization if necessary. The first detection is also performed automatically by FMOd. The object appearance model is either learned "on the fly" starting trivially with F 0 ≡ 1, M 0 ≡ 1, or the user provides a template of the tracked object, e.g. a rectangular region from one of the frames where the object is still.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deblatting</head><p>The core step of TbD is the extraction of motion information H from the input frame, which we formulate as a blind deblurring and matting problem. Inputs are the frame I, domain D, background B, and the object appearance modelF . The inverse problem corresponding to (1) is formulated as min F,M,H</p><formula xml:id="formula_2">1 2 H * F + (1 − H * M )B − I 2 2 + λ 2 F − MF 2 2 + α F ∇F 1 + α H H 1 (3) s.t. 0 ≤ F ≤ M ≤ 1 and H ≥ 0 in D, H ≡ 0 else- where.</formula><p>The primary unknown is H, but F and M are estimated as by-products. The first term in <ref type="formula">(3)</ref> is the fidelity to the model <ref type="bibr" target="#b0">(1)</ref>. The second λ-weighted term is a form of "template-matching", an agreement with a prescribed appearance. The templateF is multiplied by M because ifF is initially supplied by user as a rectangular region from a video frame, it contains the object and partially also the surrounding background. When processing the i-th frame, we setF = F i−1 as the updated appearance estimate (2) from the previous frame. The first L 1 term is the total variation that promotes smoothness of the recovered object appearance.</p><p>The second L 1 regularization enforces sparsity of the blur and reduces small nonzero values. If M is a binary mask then the condition F ≤ M states that F cannot be nonzero where M is zero -pixels outside the object must be zero. For computational reasons, we relax the binary restriction and allow M to attain values in the range [0, 1]. The correct constraint corresponding to this relaxation is then exactly F ≤ M , assuming F alone is bounded in [0, 1]. The inequality constraint H ≥ 0 prohibits negative values in H, which are physically implausible for motion blur, and H is estimated only within the domain D.</p><p>We solve <ref type="formula">(3)</ref> in an alternating manner, fix (F, M ) and solve for H and vice versa, until convergence.</p><formula xml:id="formula_3">Minimizing (3) w.r.t. H with (F, M ) fixed becomes min H 1 2 H * F + (1 − H * M )B − I 2 2 + α H H 1 (4) s.t. H ≥ 0.</formula><p>We use ADMM to solve <ref type="bibr" target="#b3">(4)</ref>, which leads to the linear system</p><formula xml:id="formula_4">(F − BM) T (F − BM) + ρ H = (F − BM) T (I − B) + ρ(z − u),<label>(5)</label></formula><p>where F and M are the convolution operator given by F (i.e. convolution with F ) and M , respectively. B is the pixelwise multiplication by background B and z, u, ρ are related to ADMM variable splitting for the non-smooth L 1 term and the inequality constraint.</p><formula xml:id="formula_5">Minimizing (3) w.r.t. the joint unknown (F, M ) with H fixed is min F,M 1 2 H * F + (1 − H * M )B − I 2 2 + λ 2 F − MF 2 2 + α F ∇F 1 (6) s.t. 0 ≤ F ≤ M ≤ 1.</formula><p>We again solve this problem using ADMM, which leads to the linear system</p><formula xml:id="formula_6">H T H + ρ 1 ∇ T ∇ + λ + ρ 2 −H T B − λF −H T B − λF −H T B 2 H + λF 2 + ρ 2 F M = [H, −BH] T (I − B) + ρ 1 ∇ T (z 1 − u 1 ) + ρ 2 (z 2 − u 2 ),<label>(7)</label></formula><p>where H is the convolution operator given by H, and z 1 , u 1 , ρ 1 are related to ADMM variable splitting due to the nonsmooth regularization. To enforce the constraint (F, M ) ∈ C where C is a convex set defined by 0 ≤ F ≤ M ≤ 1, we use the ADMM splitting z 2 := (F, M ) and then each ADMM iteration requires projecting z 2 onto C. Note that C ⊂ R 4 and correspondingly z 2 ∈ R 4 since each pixel in F has three (RGB) channels and M is a single-channel mask. Since C is an intersection of half-spaces, we can use iterative Dykstra's projection algorithm <ref type="bibr" target="#b4">[5]</ref>. The rest of the minimization is standard.</p><p>To summarize, the alternating H-(F, M ) estimation loop for the i-th frame proceeds as follows: </p><formula xml:id="formula_7">1. Initialize M := M i−1 (if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Trajectory fitting</head><p>Fitting the PSF H, which is a gray-scale image, with a trajectory C(t) : [0, 1] → R 2 serves three purposes. First, we use the error of the fit in the Consistency Check to determine if H is the motion blur induced by the tracked object and thus whether to proceed with tracking, or to declare the deblatting step a failure and to reinitialize it with different parameters. Second, the trajectory as an analytic curve can be used for motion prediction whereas H cannot. Third, C defines the intra-frame motion, which is the desired output of the proposed method.</p><p>The fitting is analogous to vectorization of raster images. It is formulated as the maximum a posteriori estimation of C, given H, with the physical plausibility of the trajectory used as a prior. Let C be a curve defined by a set of parameters θ (e.g. polynomial coefficients) and H C be a raster image of the corresponding C (i.e. blur PSF). We say that the curve C is the trajectory fit of H if θ minimizes</p><formula xml:id="formula_8">min θ H C − H s.t. C ∈ Ψ,<label>(8)</label></formula><p>where Ψ is the set of admissible curves.</p><p>Our main tracking targets are balls and similar freefalling objects, therefore our assumption is that between impulses from other moving objects (e.g. players), tracked objects remain in free flight or bounce off static rigid bodies. We then define Ψ as a set of piecewise quadratic continuous curves -quadratic to account for deacceleration due to gravity and piecewise to account for abrupt change of motion during bounces. C ∈ Ψ is defined as</p><formula xml:id="formula_9">C(t) = 2 k c k,1 t k 0 ≤ t ≤t, 2 k c k,2 t kt ≤ t ≤ 1,<label>(9)</label></formula><p>s.t.  <ref type="formula" target="#formula_8">(8)</ref> is non-convex and thus a good initial guess is necessary for gradient-descent optimization to perform well. To this end, we employed a four-step procedure:</p><p>I and H</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RANSAC</head><p>I and C 1. Identify the most salient linear and quadratic segments in H by RANSAC.</p><p>2. Connect segments to form a curve C of the kind (9).</p><p>3. Refine C to be a locally optimal fit of H in terms of pointwise distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Calculate the loss (8) and choose the best candidate.</head><p>See <ref type="figure" target="#fig_4">Fig. 4</ref> for illustrations of the above steps. Let us view the blur H as a set of pixels with coordinates x i and intensities w i &gt; 0. Sequential RANSAC finds line segments as follows: sample two points, find inliers of the corresponding line, find the most salient consecutive run of points on this line and in each round remove the winner from the sampling pool. The saliency is defined as w i for x i in the inlier set and "consecutive" means that the distance between neighboring points is bounded by a threshold. The search stops when the saliency drops bellow a specified threshold or there are no more points. We denote the set of collected linear segments as M 1 . Parabolic arcs are found similarly. We sample four points, find two corresponding parabolas, project the remaining points on the parabolas to determine the distance and inlier set as well as the arc-length parametrization of inliers (required for correct ordering and mutual distance calculation of inliers) and again find the most salient consecutive run. We denote the set of collected parabolic segments as M 2 .</p><p>The solution will be close to a curve formed from one or two segments (linear or parabolic) found so far. Let C 1 , C 2 ∈ M 1 be two linear segments. If the intersection P of the corresponding lines is close to the segments (w.r.t. some threshold), the curve connecting C 1 → P → C 2 is a candidate for the piecewise linear trajectory fit. This way we construct a set M 3 of all candidate and similarly M 4 with candidates of parabolic pairs. Curves in M 0 = M i are approximate candidates for the final trajectory, yet we first refine them to be locally optimal robust fits to H. We say that a curve C defined by a set of parameters θ is locally optimal fit to {x i } if θ is the minimizer of the problem min θ xi∈K <ref type="figure">C)</ref> is the distance of the point x to the curve C and dist(C(t), {x i }) is the distance of the curve point C(t) to the set {x i }.</p><formula xml:id="formula_10">w i dist(x i , C) + λ 1 0 dist(C(t), {x i })dt (10) where K = {x i | dist(x i , C) &lt; ρ}, dist(x,</formula><p>In the first term, K is a set of inliers defined by the distance threshold ρ and then C is the distance-optimized fit to inliers. The second term restricts curve length. The gradient of (10) is intractable since the distance of a point x to a non-convex set (in our case the curve C) is intractable. We therefore resort to a procedure similar to the Iterative Closest Point (ICP) algorithm. In each iteration, we fix the currently closest curve counterpart y i = C(t i ) for each point x i by solving t i = argmin t dist(x i , C(t)), and in (10) we approximate dist(x i , C) ≈ x i − y i and analogically for dist(C(t), {x}). Then (10) becomes a tractable function of θ. We find the solution using the Iteratively Reweighted Least Squares algorithm and proceed with the next iteration of ICP. The algorithms converges in a few iterations and the optimization is fast.</p><p>We then refine every curve C 0 ∈ M 0 by solving (10) with the ICP-like algorithm and denote the set of solutions as M. Finally, for each curve C ∈ M we construct H C , measure the error H C − H and choose the best candidate as the trajectory fit. In TbD, the Consistency Check of the trajectory fit C is performed by evaluating the criterion H C − H / H &lt; τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We show the results of Tracking by Deblatting and compare it with other trackers on the task of long-term tracking of motion-blurred objects in real-life video sequences. As a baseline, we chose the FMO detector (FMOd, <ref type="bibr" target="#b27">[28]</ref>), specifically proposed for tracking fast moving objects, and the Discriminative Correlation Filter with Channel and Spatial Reliability (CSR-DCF, <ref type="bibr" target="#b21">[22]</ref>), which performs well on standard benchmarks such as VOT <ref type="bibr" target="#b16">[17]</ref>. CSR-DCF was not designed to track objects undergoing large changes in velocity within a single sequence and would perform poorly in the comparison. We therefore augmented CSR-DCF by FMOd reinitialization every time it outputs the same  . Trajectory recovery for selected sequences from the FMO dataset <ref type="bibr" target="#b27">[28]</ref>. Intersection over Union (IoU) with the ground truth occupancy mask is color coded using the scale from <ref type="figure">Figure 5</ref>. Arrows indicate the direction of the motion.</p><p>bounding box in consecutive frames, which is considered a fail. We use FMOd for automatic initialization of both TbD and CSR-DCF to avoid manual input and we skip the first two frames of every sequence to establish background B and initialize CSR-DCF. The rest of the sequence is processed causally, B is estimated as a moving median of the past 3 -5 frames.</p><p>The goal of TbD is to produce a precise intra-frame motion trajectory, not only a single position per frame in the form of a bounding box. <ref type="figure" target="#fig_4">Fig. 4</ref> shows examples of trajectory estimation. The left column is the input image with the estimated PSF superimposed in white and the right column shows the estimated mo-tion trajectory. The efficacy of trajectory fitting is a crucial part of the framework, the estimated blur can contain various artifacts (e.g. in the top example due to the ball shadow) and the trajectory fit still recovers the actual motion.</p><p>The comparison with baseline methods was conducted on a new dataset consisting of 12 sequences with different objects in motion and setting (different kinds of sports, objects in flight or rolled on the ground, indoor/outdoor). The sequences contain abrupt changes of motion, such as bounces and interactions with players, and a wide range of speeds. The dataset is annotated with the ground-truth trajectory for each frame, Input I</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High FPS</head><p>Blur H F M obtained from a high-speed camera footage. We compare the method performance in predicting the motion trajectory in each frame. We therefore generalize IoU, the standard measure of position accuracy, to trajectories and define a new measure Trajectory-IoU (TIoU):</p><formula xml:id="formula_11">TIoU(C, C * ; M * ) = t IoU M * C(t) , M * C * (t) dt,<label>(11)</label></formula><p>where C is the predicted trajectory, C * is the groundtruth trajectory, M * is a disk mask with true object diameter obtained from the ground truth, and M x denotes M placed at location x. TIoU can be regarded as the standard IoU averaged over each position on the estimated trajectory. In practice, we discretize the exposure time into evenly spaced timestamps and calculate IoU of the ground-truth object location and prediction by the tracker at the timestamps and average these measurements. CSR-DCF tracker only outputs positions, so in this case we estimate linear trajectories from positions in neighboring frames and then calculate TIoU.</p><p>The results of the comparison are presented in <ref type="table">Table 1</ref>. We evaluated three flavors of TbD that differ in the presence of the initial user-supplied templateF and the learning rate γ of the object model in <ref type="bibr" target="#b1">(2)</ref>. The presented flavors are:</p><p>• TbD-T0,0: Object template not available, model update is instantaneous (memory-less), γ = 0. • TbD-T0,0.5: Object template not available, model is updated with the learning rate γ = 0.5. • TbD-T1,1: Object template available, model remains constant and equal to the template, γ = 1. The TbD outperforms baseline methods on average by a wide margin, both in the traditional recall measure (a detection is called true positive if it overlaps with the ground truth) as well as in trajectory accuracy TIoU. FMOd is less accurate and more prone to false positives as it lacks any prediction step and by design ignores slow objects. CSR-DCF, despite reinitializations by FMOd, fails to detect fast moving objects accurately. Among TbD flavors, it is no surprise that availability of the object template is beneficial and outperforms other versions. However, even if the template is not available, TbD can learn the object model and updating the appearance model gradually during tracking is preferable to instantaneous updates.</p><p>To evaluate the performance of the core part of TbD that consists of deblatting and trajectory fitting alone, we provide results of a special version of the proposed method called "TbD with oracle", TbD-O. This behaves like regular TbD but with a perfect trajectory prediction step. We use the ground-truth trajectory to supply the region D to the deblatting step exactly as if it were predicted by the prediction step, effectively bypassing the long-term tracking logic of TbD. The rest is identical to TbD-T1,1. TbD with oracle tests the performance and potential of the deblatting and trajectory estimation alone because failures do not cause long-term damage -success in one frame is independent of success in the previous frame. <ref type="table">Table 2</ref> shows aggregated results for the FMO dataset <ref type="bibr" target="#b27">[28]</ref>. This dataset does not contain groundtruth trajectory data, we therefore report traditional precision/recall measure, which is derived from the detection and ground-truth bounding-box IoU. On this dataset, the proposed TbD method is slightly better in recall, owing to the fact that initial detection is done by FMOd and if FMOd fails then TbD cannot start the tracking, but significantly better in terms of precision. Results on individual sequences are in the supplementary.</p><p>A visual demonstration of the tracking by the proposed method on the TbD dataset and the FMO dataset is shown in Figs. <ref type="bibr" target="#b4">5</ref>   <ref type="table">Table 2</ref>. Precision and recall of the TbD tracker (setting: TbD without template and with exponential forgetting factor (2) γ = 0.5) and the FMO method <ref type="bibr" target="#b27">[28]</ref>, average on the 16 sequences of the FMO dataset.</p><p>results of tracking in one sequence from the evaluation dataset superimposed on a single image from the sequence. Arrows depict trajectories detected in a particular frame and the color encodes the corresponding TIoU from green=1 to red=0 (false positive). We can see that the trajectory is estimated successfully with the exception of frames where the object is in direct contact with other moving objects, which throws off the local estimation of background. Examples of the deblatting alone are in Figs. 7 and 8. <ref type="figure">Fig. 7</ref> contains from left to right the input frame (crop), corresponding frame from the high-speed camera, estimated motion PSF H, estimated object F and object shape M . In the top row, we see that the shape of the badminton shuttlecock, though not circular, is estimated correctly. In the bottom row, we see that if the non-uniform object undergoes only small rotation during motion, the appearance estimation can also be good. In this case, the shape estimation is difficult due to the mostly homogeneous background similar to the object. <ref type="figure">Fig. 8</ref> is another interesting example of the deblatting behavior. The input frame is in the top left corner and the corresponding part from the high-speed camera is bellow. The object casts significant shadow. If we set the size of F too small, the model cannot cope with the shadow and the estimated blur will contain artifacts in the locations of the shadow as is visible in the top row. If instead we make the support of F sufficiently large, the estimated mask compensates for the shadow and the estimated blur is clean as shown in the bottom row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel approach -Tracking by Deblatting -intended for sequences in which the object of interest undergoes non-negligible motion within a single frame, which needs to be specified by intra-frame trajectory rather than a single position. The method is based on the observation that motion blur is directly related to the motion trajectory of the object. Blur is estimated by a complex method combining blind deblurring, image matting and shape estimation, followed by fitting a piecewise linear or quadratic curve that models physically plausible trajectories. As a result, we can precisely localize the object with higher temporal resolution than by conventional trackers.</p><p>The proposed TbD tracker was evaluated on a newly created dataset of videos with ground truth obtained by a high-speed camera using a novel Trajectory-IoU metric that generalizes the traditional Intersection over Union and measures the accuracy of the intra-frame trajectory. The proposed method outperforms baseline techniques both in recall and trajectory accuracy.</p><p>Due to the complexity of blind deblurring, the method is currently limited to objects that do not significantly change their perceived shape and appearance within a single frame, the method works best for approximately round and uniform objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Tracking by Deblatting (TbD) successfully recovers trajectory on a pingpong sequence from the proposed TbD dataset. Color encodes Trajectory Intersection over Union (TIoU) with ground truth trajectories from highspeed camera. Arrows indicate the direction of the motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Long-term Tracking by Deblatting (Sec. 3). The FMO detector is activated during initialization or if the consistency check fails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Deblatting, i.e. deblurring and matting -Sec. 3.1, with trajectory fitting -Sec. 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 k c k,1t k = 2 k</head><label>22</label><figDesc>c k,2t k . Single linear or quadratic curves are included as special cases whent = 1. The problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Trajectory fitting. Left input image with estimated blur superimposed in white, middle linear and parabolic segments found by RANSAC, right final fitted trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 1 2 0Figure 5 .</head><label>125</label><figDesc>Trajectory recovery for selected sequences from the TbD dataset. Trajectory Intersection over Union (TIoU) with ground truth trajectories from a high-speed camera, color coded. Arrows indicate the direction of the motion. The trajectory over the whole sequence can be obtained by fitting a continuous piecewise quadratic curve (9) on all frames jointly, as shown in the bottom right in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>Figure 6. Trajectory recovery for selected sequences from the FMO dataset [28]. Intersection over Union (IoU) with the ground truth occupancy mask is color coded using the scale from Figure 5. Arrows indicate the direction of the motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Deblatting examples. From left to right: the input image, corresponding high-speed camera frame, estimated blur H, estimated appearance F and shape M .I (top)/high FPS H F M Shadow and blur estimation. Top: the domain of F is set too small and the shadow causes artifacts in H. Bottom: the domain of F is larger, M can compensate for the shadow and the blur H is estimated correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(shaded area) provides the overview. Inputs are the current estimates of the object model F i and M i , background B i , and a region of interest D i in I i+1 , which is the neighborhood of the predicted object location. Three main steps are performed in TbD: 1. Deblatting: Iteratively solve blind deblurring and matting in the image region D i with the model (1) and estimate F i+1 , M i+1 , and H i+1 ; see Sec. 3.1 2. Trajectory fitting: Estimate physically plausible motion trajectory (parametric curve) C i+1 corresponding to H i+1 and optionally adjust D i according to C i+1 ; see Sec. 3.2. 3. Consistency check &amp; model update: Verify that the error of the mapping H → C is below threshold τ , predict the new region of interest D i+1 for the next frame, and update the object model to F i+1 and M i+1 . A more detailed illustration of Steps 1 and 2 is in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>available from previous detection) or M ≡ 1; initializeF := F i−1 , F := MF . 2. Calculate H by solving (4). 3. Check convergence, exit if satisfied. 4. Calculate (F, M ) by solving (6), go to 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The highest TIoU for each sequence is highlighted in blue color and the highest recall in cyan color. TbD-O shows the highest attainable TIoU for TbD as a reference point when predictions are precise. The number of frames is indicated by #.</figDesc><table><row><cell>Sequence name</cell><cell>#</cell><cell cols="2">CSR-DCF [22] TIoU Rcl</cell><cell cols="8">FMO [28] TIoU Rcl TIoU Rcl TIoU Rcl TIoU Rcl TbD-T0, 0 TbD-T0, 0.5 TbD-T1, 1</cell><cell>TbD-O TIoU</cell></row><row><cell>badminton white</cell><cell>40</cell><cell>.275</cell><cell>0.39</cell><cell>.242</cell><cell>0.34</cell><cell>.673</cell><cell>0.92</cell><cell>.674</cell><cell>0.95</cell><cell>.711</cell><cell>0.95</cell><cell>.792</cell></row><row><cell cols="2">badminton yellow 57</cell><cell>.047</cell><cell>0.11</cell><cell>.236</cell><cell>0.31</cell><cell>.615</cell><cell>0.89</cell><cell>.623</cell><cell>0.89</cell><cell>.633</cell><cell>0.85</cell><cell>.788</cell></row><row><cell>pingpong</cell><cell>58</cell><cell>.060</cell><cell>0.14</cell><cell>.064</cell><cell>0.12</cell><cell>.583</cell><cell>0.89</cell><cell>.587</cell><cell>0.89</cell><cell>.536</cell><cell>0.91</cell><cell>.697</cell></row><row><cell>tennis</cell><cell>38</cell><cell>.249</cell><cell>0.83</cell><cell>.596</cell><cell>0.78</cell><cell>.577</cell><cell>0.81</cell><cell>.573</cell><cell>0.81</cell><cell>.633</cell><cell>0.86</cell><cell>.827</cell></row><row><cell>volleyball</cell><cell>41</cell><cell>.373</cell><cell>0.69</cell><cell>.537</cell><cell>0.72</cell><cell>.552</cell><cell>0.87</cell><cell>.587</cell><cell>0.90</cell><cell>.741</cell><cell>0.92</cell><cell>.836</cell></row><row><cell>throw floor</cell><cell>40</cell><cell>.262</cell><cell>0.74</cell><cell>.272</cell><cell>0.37</cell><cell>.746</cell><cell>1.00</cell><cell>.768</cell><cell>1.00</cell><cell>.817</cell><cell>1.00</cell><cell>.864</cell></row><row><cell>throw soft</cell><cell>60</cell><cell>.470</cell><cell>0.93</cell><cell>.377</cell><cell>0.57</cell><cell>.585</cell><cell>0.90</cell><cell>.539</cell><cell>0.90</cell><cell>.641</cell><cell>0.95</cell><cell>.707</cell></row><row><cell>throw tennis</cell><cell>45</cell><cell>.347</cell><cell>0.91</cell><cell>.507</cell><cell>0.65</cell><cell>.688</cell><cell>1.00</cell><cell>.781</cell><cell>1.00</cell><cell>.852</cell><cell>1.00</cell><cell>.872</cell></row><row><cell>roll golf</cell><cell>16</cell><cell>.406</cell><cell>1.00</cell><cell>.187</cell><cell>0.71</cell><cell>.414</cell><cell>1.00</cell><cell>.346</cell><cell>1.00</cell><cell>.851</cell><cell>1.00</cell><cell>.898</cell></row><row><cell>fall cube</cell><cell>20</cell><cell>.422</cell><cell>0.89</cell><cell>.408</cell><cell>0.78</cell><cell>.553</cell><cell>0.89</cell><cell>.669</cell><cell>0.89</cell><cell>.704</cell><cell>0.89</cell><cell>.744</cell></row><row><cell>hit tennis</cell><cell>30</cell><cell>.316</cell><cell>0.93</cell><cell>.381</cell><cell>0.68</cell><cell>.564</cell><cell>0.93</cell><cell>.570</cell><cell>0.93</cell><cell>.662</cell><cell>0.93</cell><cell>.828</cell></row><row><cell>hit tennis2</cell><cell>26</cell><cell>.289</cell><cell>0.79</cell><cell>.414</cell><cell>0.71</cell><cell>.459</cell><cell>0.83</cell><cell>.493</cell><cell>0.83</cell><cell>.627</cell><cell>0.83</cell><cell>.738</cell></row><row><cell>Average</cell><cell>39</cell><cell>.293</cell><cell>0.70</cell><cell>.352</cell><cell>0.56</cell><cell>.584</cell><cell>0.91</cell><cell>.601</cell><cell>0.92</cell><cell>.701</cell><cell>0.93</cell><cell>.799</cell></row><row><cell cols="13">Table 1. Trajectory Intersection over Union (TIoU) and Recall (Rcl) on the TbD dataset -comparison of the TbD, CSR-</cell></row><row><cell cols="13">DCF[22] trackers and the Fast Moving Object method [28]. CSR-DCF is a standard, well-performning [18], near-real time</cell></row><row><cell cols="13">tracker. TbD tracker settings: TbD without template and with exponential forgetting factors (2) γ = 0 (TbD-T0, 0) and</cell></row><row><cell cols="10">FMO [28] γ = 0.5 (TbD-T0, 0.5), TbD with template and γ = 1 (TbD-T1, 1), TbD with oracle (TbD-O). FMO dataset TbD-T0, 0.5 Prec. Recall Prec. Recall</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell>59.2</cell><cell cols="2">35.5 81.6</cell><cell>41.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">and 6. Each image shows</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ensemble tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time multiple vehicle detection and tracking from a moving vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="83" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correlation-based self-correcting tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Biresaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomput</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A method for finding projections onto the intersection of convex sets in hilbert spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Dykstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Order Restricted Statistical Inference</title>
		<meeting><address><addrLine>New York, NY; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="28" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Motion from blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tracking of flying insects using pan-tilt cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="67" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hough-based tracking of non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2096" to="2109" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image motion deblurring using transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to extract a video sequence from a single motion-blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6334" to="6342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segmentation-free dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2766" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Visual Object Tracking VOT2016 Challenge Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018 Workshops</title>
		<editor>L. Leal-Taixé and S. Roth</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view tracking of multiple targets with dynamic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual tracking under motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5867" to="5876" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">M 2 tracker: a multi-view approach to segmenting and tracking people in a cluttered scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moudgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gandhi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01358</idno>
		<title level="m">Long-term visual object tracking benchmark</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The world of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rozumnyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4838" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Model-based motion blur estimation for the improvement of motion tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="45" to="56" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rotational motion deblurring of a rigid object from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-speed tracking with multi-kernel correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4874" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tracking for half an hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10217</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Detection and tracking of point features. School of Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ. Pittsburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Robust Scale-Adaptive Mean-Shift for Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="652" to="663" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blurred target tracking by blur-driven tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1100" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Robust Tracking via Multiple Experts Using Entropy Minimization</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="188" to="203" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
