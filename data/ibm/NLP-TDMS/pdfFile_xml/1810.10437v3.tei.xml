<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Semi-supervised Aspect-term Sentiment Analysis via Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
							<email>weidi.xwd@antfin.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
							<email>taifeng.wang@antfin.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Huang</surname></persName>
							<email>weipeng.hwp@antfin.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlong</forename><surname>Chen</surname></persName>
							<email>kunlong.ckl@antfin.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Semi-supervised Aspect-term Sentiment Analysis via Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-term sentiment analysis (ATSA) is a long-standing challenge in natural language processing. It requires fine-grained semantical reasoning about a target entity appeared in the text. As manual annotation over the aspects is laborious and time-consuming, the amount of labeled data is limited for supervised learning. This paper proposes a semisupervised method for the ATSA problem by using the Variational Autoencoder based on Transformer. The model learns the latent distribution via variational inference. By disentangling the latent representation into the aspect-specific sentiment and the lexical context, our method induces the underlying sentiment prediction for the unlabeled data, which then benefits the ATSA classifier. Our method is classifier-agnostic, i.e., the classifier is an independent module and various supervised models can be integrated. Experimental results are obtained on the SemEval 2014 task 4 and show that our method is effective with different five specific classifiers and outperforms these models by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect based sentiment analysis (ABSA) has two sub-tasks, namely aspect-term sentiment analysis (ATSA) and aspect-category sentiment analysis (ACSA). ACSA is to infer the sentiment polarity with regard to the predefined categories, e.g., the aspect f ood, price, ambience. On the other hand, ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text. For example, given a review about a restaurant "the [pizza] aspect is the best if you like thin crusted pizza, however, the [service] aspect is awful.", the sentiment implications with regard to "pizza" and "service" are contrary. For the aspect * * : equal contribution "pizza", the sentiment polarity is "positive" while "negative" for the aspect "service". In contrast to document-level sentiment analysis, ATSA requires more fine-grained reasoning about the textual context. The task is worthy of investigation as it can obtain the attitude with regard to a specific entity which we are interested in. The task is widely applicated in analyzing the comments, such as opinion generation. Recently, many attempts <ref type="bibr" target="#b23">(Tang et al., 2016b;</ref><ref type="bibr" target="#b18">Pan and Wang, 2018;</ref><ref type="bibr" target="#b14">Liu et al., 2018;</ref><ref type="bibr" target="#b11">Li et al., 2018a)</ref> focus on supervised learning and pay much attention to the interaction between the aspect and the context. However, the amount of labeled data is quite limited as the annotation about the aspects is laborious. Currently available data sets, e.g. Se-mEval, only has around 2K unique sentences and 3K sentence-aspect pairs, which is insufficient to fully exploit the power of the deep models. Fortunately, a large amount of unlabeled data is available for free and can be accessed easily from the websites. It will be of great significance if numerous unlabeled samples can be utilized to further facilitate the supervised ATSA classifier. Therefore, the semi-supervised ATSA is a promising research topic.</p><p>In ATSA, achieving the sentiment of the aspectterm is semantically complicated and it is nontrivial for a model to capture sentimental similarity of the aspects, which causes the difficulties for semi-supervised learning. In this paper, we proposed a classifier-agnostic framework which named Aspect-term Semi-supervised Variational Autoencoder (Kingma and Welling, 2014) based on Transformer (ASVAET). The variational autoencoder offers the flexibility to customize the model structure. In other words, the proposed framework is compatible with other supervised neural networks to boost their performance. Our proposed model learns the latent representation arXiv:1810.10437v3 [cs.CL] 5 Sep 2019 of the input data and disentangles the representations into two independent parts, i.e., the aspectterm sentiment and the representation of the lexical context. By regarding the aspect sentiment polarity of the unlabeled data as the discrete latent variable, the model implicitly induces the sentiment polarity via the variational inference. Specifically, the representation of the lexical context is extracted by the encoder and the aspect-term sentiment polarity is inferred from the specific ATSA classifier. The decoder takes these two representations as inputs and reconstructs the original sentence by two unidirectional language models. In contrast to the conventional auto-regressive models, the latent representations have their specific meanings and are obtained from the encoder and the classifier to the input examples. Therefore, it is also possible to condition the sentence generation on the sentiment and lexical information w.r.t. a certain target entity. In addition, by separating the representation of the input sentence, the classifier becomes an independent module in our framework, which endows the method with the ability to integrate different classifiers. The method is presented in detail in Sec. 3.</p><p>Experimental results are obtained on the two classical datasets from SemEval 2014 task 4 <ref type="bibr" target="#b21">(Pontiki et al., 2014)</ref>. Five recent available models are implemented as the classifier in ASVAET. Our method is able to utilize the unlabeled data and consistently improve the performance against the supervised models. Compared with other semisupervised methods, i.e., in-domain word embedding pre-training and self-training, the proposed method also demonstrates better performance. We also evaluate the effectiveness of labeled data and sharing embeddings, and show that the structure can provide the separation between lexical context and sentiment polarity in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentiment analysis is a traditional research hotspot in the NLP field <ref type="bibr" target="#b25">(Wang and Manning, 2012)</ref>. Rather than obtaining the sentimental inclination of the entire text, ATSA instead aims to extract the sentimental expression w.r.t. a target entity. With the release of online completions, abundant methods were proposed to explore the limits of current models. <ref type="bibr" target="#b22">Tang et al. (Tang et al., 2016a)</ref> proposed to make use of bidirectional Long Short-Term Memory (LSTM) <ref type="bibr">(Hochreiter and Schmid-huber, 1997)</ref> to encode the sentence from the left and right to the aspect-term. This model primarily verifies the effectiveness of deep models for ABSA <ref type="bibr" target="#b23">Tang et al. (Tang et al., 2016b)</ref> then put forward a neural reasoning model in analogy to the memory network to perform the reasoning in many steps. There are also many other works dedicating to solve this task (Pan and <ref type="bibr" target="#b18">Wang, 2018;</ref><ref type="bibr" target="#b14">Liu et al., 2018;</ref><ref type="bibr" target="#b30">Zhang and Liu, 2017)</ref>.</p><p>Another related topic is semi-supervised learning for the text classification. Recently, Data augmentation methods <ref type="bibr" target="#b27">(Xie et al., 2019;</ref><ref type="bibr" target="#b5">Berthelot et al., 2019)</ref> achieve a greate success on lowresource datasets. Moreover, A simple but efficient method is to use pre-trained modules, e.g., initializing the word embedding or bottom layers with pre-training. Word embedding technique has been wildly used in NLP models, e.g., Glove <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref> and ELMo <ref type="bibr" target="#b20">(Peters et al., 2018)</ref>. Recently, Bidirectional Encoder Representations from Transformer (BERT) <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> replaces the embedding layer to contextdependent layer with the pre-trained bidirectional language model to capture the contextual representation. BERT is complementary to the encoder of the proposed method. To keep our framework neat, these pre-training investigations are not conducted in this paper.</p><p>VAE-based semi-supervised methods, on the other hand, are able to cooperate with various kinds of classifiers. VAE has been applied in many semi-supervised NLP tasks, ranging from text classification <ref type="bibr" target="#b28">(Xu et al., 2017)</ref>, relation extraction <ref type="bibr" target="#b16">(Marcheggiani and Titov, 2016)</ref> to sequence tagging <ref type="bibr" target="#b6">(Chen et al., 2018)</ref>. Different from text classification where sentiment polarity is related to an entire sentence, ATSA just interested in related information of a given aspect-term. To circumvent this problem, a novel structure is proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method Description</head><p>In this section, the problem definition is provided and then the model framework is presented in detail.</p><p>The ATSA task aims to classify a data sample with input sentence x = {x 1 , ..., x n } and corresponding aspect 1 a = {a 1 , ..., a m }, where a is a subsequence of x, into a sentiment polarity y, where y ∈ {P, O, N }. P, O, N denotes "positive", "neutral", "negative". For the semisupervised ATSA, we consider the following scenario. Given a dataset consisting of labeled samples S l and unlabeled samples S u , where the</p><formula xml:id="formula_0">S l = {(x (i) l , a (i) l , y (i) l )} N l i=1 and S u = {(x (i) u , a (i) u } N l i=1</formula><p>, the goal is to utilize S u to improve the classification performance over the supervised model using S l only.</p><p>The architecture is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. The method consists of three main components, i.e., the classifier, the encoder, and the decoder. The classifier can be any differentiable supervised ATSA model, which takes x and a as input, and outputs the prediction about y. The encoder transform the data into a latent space that is independent of the label y. And the decoder combines the outputs from the classifier and the encoder to reconstruct the input sentence. For the labeled data, the classifier and the autoencoder are trained with the given label y. For the unlabeled data, the y is regarded as the latent discrete variable and it is induced by maximizing the generative probability. As the classifier can be implemented by various models, the description of the classifier will be omitted. We present a autoencoder structure based on Transformer <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>. In the following, the objective functions are clarified, followed by the model description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational Inference</head><p>Using generative models is a common approach for semi-supervised learning, which tries to extract the information from the unlabeled data by modeling the data distribution. In VAE, the data distribution is modeled by optimizing the evidence lower bound (ELBO) of data log-likelihood, which leads to two objectives for labeled data and unlabeled data respectively. For the labeled data, VAE maximizes the ELBO of p(x, y|a). For the unlabeled data, it optimizes the ELBO of p(x|a), where the y is latent and integrated. Specifically, the dependency between variables is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The ELBO of log p(x, y|a) can be given as follows:</p><formula xml:id="formula_1">log p θ (x, y|a) ≥ E q φ (z|x,a,y) [log p θ (x|y, a, z)] − D KL (q φ (z|x, a, y)||p θ (z)) + log p θ (y) = L(x, a, y) ,<label>(1)</label></formula><p>where z is the latent variable which represents lexical information over the sentence and D KL is the KullbackLeibler divergence.</p><p>In terms of the unlabeled data, the ELBO of log p(x|a) can be extended from Eq. 1.</p><formula xml:id="formula_2">log p θ (x|a) ≥ y q φ (y|x, a)(L(x, a, y)) + H(q φ (y|x, a)) = U(x, a) ,<label>(2)</label></formula><p>where H is the entropy function and q φ (y|x, a) is the classification function.</p><p>And q φ (y|x, a) can also be trained in the supervised manner using the labeled data. Combining the above objectives, the overall objective for the entire data set is:</p><formula xml:id="formula_3">J = (x,a,y)∈S l −L(x, a, y) + x∈Su −U(x, a) + γ (x,a,y)∈S l − log q φ (y|x, a) ,<label>(3)</label></formula><p>where γ is a hyper-parameter which controls the weight of the additional classification loss.</p><p>To implement this objective, three components are required to model the q φ (y|x, a), q φ (z|x, a, y) and p θ (x|y, a, z) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifier</head><p>Various currently available models can be used as the classifier. For the unlabeled data, the classifier is used to predict the distribution of label y for the decoder, i.e., y ∼ q φ (y|x, a). The distribution q φ (y|x, a) will be tuned during maximizing the objective in Eq. 2. In this work, five classifiers are implemented in ASVAET and they are also used as the supervised baselines for the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transformer Encoder</head><p>The encoder plays the role of q φ (z|x, a, y). This module attempts to extract the lexical feature that is independent of the label y when given data sample (x, a). In this way, the z and y jointly form the representative vector for the input data.</p><p>In our implementation, we use a bidirectional encoder to construct sentences embeddings. It is referred as the Transformer encoder that is actually a sub-graph of the Transformer architechture <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>, the architecture is shown in the left part of the <ref type="figure" target="#fig_1">Fig. 2</ref>. The encoder employs residual connections around each of the multi-head attention sub-layers, followed by  layer normalization. To capture the aspect-term, we treat the aspect-term and its context differently by segment embeddings. To further emphasize the position of the conditional aspect, the position tag is also included for each token. The position tag indicates the distance from the token to the aspect. And then the position tag is transformed into a vector as defined in <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref>, which is added with the word embedding and segment embedding as the input of the Transformer encoder. Let g denote the output of the Transformer encoder after pooling which simply averaging the hidden states of the aspect-terms (the number of tokens is equal or greater than one) of the last layer, y is the indicator vector of the polar-ity. Then the distribution of z can be given as:</p><formula xml:id="formula_4">z ∼ N (µ(x, y), diag(σ 2 (x, y))) , µ(x, y) = tanh(W µ [g : y] + b µ ) , σ(x, y) = tanh(W σ [g : y] + b σ ) .</formula><p>The sequences are divided into two parts by using segment embedding, the encoder can be aware of the position and the content of the aspect-term a by multi-head attention operation in the Transformer encoder. The information from two sides are aggregated into the aspect-term a, and therefore the resulting z can gather the information related to the aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Transformer Decoders</head><p>The decoder is also a sub-graph of Transformer architechture <ref type="bibr" target="#b24">(Vaswani et al., 2017)</ref> which focus on reconstructing original text. The main difference from the Transformer encoder is that the Transformer decoder is unidirectional by modifying the self-attention sub-layer to prevent positions from paying attention to subsequent positions. The textual sequence is well-known to be semantically complex and it is non-trivial for a Transformer decoder to capture the high-level semantics. Here we investigate two questions. How to implement p θ (x|y, a, z) without losing the information of a and how to capture the semantic polarity by a sequential model. For the first question, denoting that x is composed of three parts (x l , a, x r ), we use two Transformer decoders to model the left and right content. For the second question, we let each token is generated conditioned on the summation of the variables z and embedding y.</p><p>One way to achieve p θ (x|y, a, z) is to separate the sequence into two parts, reversing the process in the two unidirectional decoder. For each decoder, the input state is represented by the summation of the four input i.e., the polarity indicator vector y from the classifier or the labeled dataset, the context vector z from the encoder, the input token embedding e xt and the position embedding p xt :</p><formula xml:id="formula_5">← − h trm t = ← − − f trm (e [xt:a] , p xt , y, z), x t ∈ [x l : a] p(x t−1 |·) = softmax(W p ← − h trm t + b p ) , log p θ (x l |a, y, z) = xt log p(x t |·), x t ∈ x l , − → h trm t = − − → f trm (e [a:xt] , p xt , y, z), x t ∈ [a : x r ] p(x t+1 |·) = softmax(W p − → h trm t + b p ) , log p θ (x r |a, y, z) = xt log p(x t |·), x t ∈ x r .</formula><p>It is equivalent to generate two sequences using two decoders. When decoding left part (or right part), the aspect will first get processed by the decoder and hence the decoder is aware of the aspect-terms. The position tag is also used in the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Preparation</head><p>The models are evaluated on two benchmarks: Restaurant (REST) and Laptop (LAPTOP) datasets from the SemEval ATSA challenge <ref type="bibr" target="#b21">(Pontiki et al., 2014)</ref>. The REST dataset contains the reviews in the restaurant domain, while the LAPTOP dataset contains the reviews of Laptop products. The statistics of these two datasets are listed in <ref type="table" target="#tab_1">Table  1</ref>. When processing these two datasets, we follow the same procedures as in another work <ref type="bibr" target="#b10">(Lam et al., 2018)</ref>. The dataset has a few samples that are labeled as "conflict" and these samples are removed. All tokens in the samples are lowercased without other preprocess, e.g., removing the stop words, symbols or digits.   In terms of the unlabeled data, we obtained samples in the same domain for the REST and LAPTOP datasets. For the REST, the unlabeled samples are obtained from a sentiment analysis competition in Kaggle 2 . The competition consists of 82K training samples and 34K test samples. For the LAPTOP, the unlabeled samples are obtained from the "Six Categories of Amazon Product Reviews" 3 , which has 412K samples. The reviews about the laptops are used among six product categories.</p><p>The NLTK sentence tokenizer is utilized to extract the sentences from the raw comments. And each sentence is regarded as a sample in our model for both REST and LAPTOP. To obtain the aspects in the unlabeled samples, an open-sourced aspect extractor 4 is pre-trained using labeled data. The resulting test F1 score is 88.42 for the REST and 80.12 for the LAPTOP. Then the unlabeled data is processed by the pre-trained aspect extractor to obtain the aspects. The sentences that have no aspect are removed. And the sentences are filtered with maximal sentence length 80. The statistic of the resulting sentences is given in <ref type="table">Table.</ref> 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Configuration &amp; Classifiers</head><p>In the experiments, the model is fixed with a set of universal hyper-parameters. The number of units in the encoder and the decoder is 100 and the latent variable is of size 50 and the number of layers of both Transformer blocks is 2, the number of selfattention heads is 8. The KL weight klw should be carefully tuned to prevent the model from trapping in a local optimum, where z carries no useful in-  <ref type="table">Table 3</ref>: Experimental results (%). For each classifier, we performed five experiments, i.e., the supervised classifier, the supervised classifier with pre-trained embedding using unlabeled data and our model with the classifier. The results are obtained after 5 runs, and we report the mean and the standard deviation of the test accuracy, and the Macro-averaged F1 score. Better results are in bold. denotes that the results are extracted from the original paper.</p><p>formation. In this work, the KL weight is set to be 1e-4. In term of word embedding, the pre-trained GloVe <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref> is used as the input of the encoder and the decoder 5 and the outof-vocabulary words are excluded. And it is fixed during the training. The γ is set to be 10 across the experiments.</p><p>We implemented and verified four kinds of mainstream ATSA classifiers integrated into our model, i.e., TC-LSTM <ref type="bibr" target="#b22">(Tang et al., 2016a)</ref>, Mem-Net <ref type="bibr" target="#b23">(Tang et al., 2016b)</ref>, BILSTM-ATT-G <ref type="bibr" target="#b30">(Zhang and Liu, 2017)</ref>, IAN <ref type="bibr" target="#b15">(Ma et al., 2017)</ref> and TNet <ref type="bibr" target="#b13">(Li et al., 2018b)</ref>.</p><p>• TC-LSTM: Two LSTMs are used to model the left and right context of the target separately, then the concatenation of two representations is used to predict the label.</p><p>• MemNet: It uses the attention mechanism over the word embedding over multiple rounds to aggregate the information in the sentence, the vector of the final round is used for the prediction.</p><p>• IAN: IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer. 5 http://nlp.stanford.edu/data/glove.8B.300d.zip</p><p>• BILSTM-ATT-G: It models left and right contexts using two attention-based LSTMs and makes use of a special gate layer to combine these two representations. The resulting vector is used for the prediction.</p><p>• TNet-AS: Without using an attention module, TNet adopts a convolutional layer to get salient features from the transformed word representations originated from a bidirectional LSTM layer. Among current supervised models, TNet is currently one of the in-domain state-of-the-art methods and the TNet-AS is one of the two variants of TNet.</p><p>The configuration of hyper-parameters and the training settings are the same as in the original papers. Various classifiers are tested here to demonstrate the robustness of our method and show that the performance can be consistently improved for different classifiers. <ref type="table">Table 3</ref> shows the experimental results on the REST and LAPTOP datasets. Two evaluation metrics are used here, i.e., classification accuracy and Macro-averaged F1 score. The latter is more sensitive when the dataset is class-imbalance. In this table, the semi-supervised results are obtained with 10K unlabeled data. We didn't observe fur-ther improvement with more unlabeled data. The mean and the standard deviation are reported over 5 runs. For each classifier clf, we conducted the following experiments:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>• clf : The classifier is trained using labeled data only.</p><p>• clf (EMB): We use CBOW <ref type="bibr" target="#b17">(Mikolov et al., 2013)</ref> to train the word embedding vectors using both labeled and unlabeled data. And the resulting vectors, instead of pre-trained GloVe vectors, are used to initialize the embedding matrix of the classifier. This is the embedding-level semi-supervised learning as the embedding layer is trained using in-domain data.</p><p>• clf (ST): The self-training (ST) method is a typical semi-supervised learning method. We performed the self-training method over each classifier. At each epoch, we select the 1K samples with the best confidence and give them pseudo labels using the prediction. Then the classifier is re-trained with the new labeled data. The procedure loops until all the unlabeled samples are labeled.</p><p>• clf (ASVAET): The proposed method that uses clf as the classifier. Note again that the classifier is an independent module in the proposed model, and the same configuration is used as in the supervised learning.</p><p>Besides, we also include the results of several supervised models in the first block, i.e., CNN-ASP <ref type="bibr" target="#b10">(Lam et al., 2018)</ref>, AE-LSTM, ATAE-LSTM <ref type="bibr" target="#b26">(Wang et al., 2016)</ref>, GCAE , from the original paper.</p><p>From the <ref type="table">Table 3</ref>, the ASVAET is able to improve supervised performance consistently for all classifiers. For the MemNet, the test accuracy can be improved by about 2% by the TSSVAE, and so as the Macro-averaged F1. The TNet-AS outperforms the other three models.</p><p>Compared with the other two semi-supervised methods, the ASVAET also shows better results. The ASVAET outperforms the compared semisupervised methods evidently. The adoption of indomain pre-trained word vectors is beneficial for the performance compared with the Glove vectors.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effect of Labeled Data</head><p>Here we investigated whether the ASVAET works with less labeled data. Without loss of generality, the MemNet is used as the basic classifier. We sampled different amount of labeled data to verify the improvement by using ASVAET. The test accuracy curve w.r.t. the amount of labeled data used is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>  <ref type="table">Table 5</ref>: Nice sentences that are generated by controlling the sentiment polarity y using the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effect of Sharing Embeddings</head><p>In previous works, the word embedding is shared among all the components. In other words, the word embedding is also tuned in learning to reconstruct the data. It is questionable whether the improvement is obtained by using VAE or multitask learning (text generation and classification). In the aforementioned experiments, the embedding layer is not shared between the classifier and autoencoder. This implementation guarantees that the improvement does not come from learning to generate. To verify if sharing embedding will benefit, we also conducted experiments with sharing embedding, as illustrated in <ref type="table">Table.</ref> 4. The results indicate that the joint training for the embedding layer is negative for improving the performance in this task. The gradient from the autoencoder may collide with the gradients from the classifier and therefore, interferes with the optimization direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of the Latent Space</head><p>Transformer encodes the data into two representations, i.e., y and z. These two latent variable represented sentiment polarity and lexical context individually from the input text. We expect the y and z are fully disentangled and represent different meanings. The scatters of latent variable z (cf. <ref type="figure" target="#fig_3">Fig. 4</ref>) helps us have a better understanding. As shown in the figure, the distributions of three different polarities are very similar, which indicates that the lexical context reprensetation z is independent of the polarity y.</p><p>The generation ability of the decoder is also in-vestigated. Several sentences are generated and selected in the <ref type="table">Table 5</ref>. By controlling the sentiment polarity y with the same z, the decoder can generate sentences with different sentiment in a similar format. This indicates that the decoder is trained successfully to perceive the y and model the relationship between the y and x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>A VAE-based framework has been proposed for the ATSA task. In this work, the encoder and decoder are constructed from the Transformers. Both analytical and experimental work has been carried out to show the effectiveness of the ASVAET. The method is verified with various kinds of classifiers. For all tested classifiers, the improvement is obtained when equipped with ASVAET, which demonstrates its universality. In this paper, the aspect-term is assumed to be known and there is an error accumulation problem when using the pre-trained aspect extractor. According to this, in future work, it is also interesting to show if it is possible to learn the aspect and sentiment polarity jointly for the unlabeled data. It will be of great importance if detailed knowledge can be extracted from the unlabeled data, which will shed light on other related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>This is the sketch of our model with bidirectional encoder and decoder. Assuming the aspect-term starts at the k-th position in x. Bottom: When using unlabeled data, the distribution of y ∼ q φ (y|x, a) is provided by the classifier. Left: The sequence is encoded by a Transformer block, which receives the summation of three embeddings, i.e., segment (used to distinguish aspect words) s xn , position p xn and word e xn . The encoding and the label y are used to parameterize the posterior q φ (z|x, a, y). Right: A sample z from the posterior q φ (z|x, a, y) and label y are passed to the generative network which estimates the probability p θ (x|y, a, z) by two unidirectional Transformer decoders. The number of aspect tokens is l a .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of ASVAET as a directed graph. Left: Dashed lines are used to denote variational approximation q φ (y|x, a)q φ (z|x, a, y). Right: Solid lines are used to denote generative model p θ (x|y, a, z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The test accuracy w.r.t. the number of labeled samples on the REST dataset with MemNet classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The distribution of the REST dataset in latent space z using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Avg. Length Std. Length</cell></row><row><cell>REST</cell><cell>Labeled Unlabeled</cell><cell>20.06 22.70</cell><cell>10.38 12.38</cell></row><row><cell>LAPTOP</cell><cell>Labeled Unlabeled</cell><cell>21.95 29.89</cell><cell>11.80 17.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The statistics of the reviews.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison between with or without sharing embedding on the REST dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Positive ... the best food i 've ever had !!! ... ... the lox is very tasty ... ... the rice is a great value ... Negative ... the worst food i 've ever had !!! ... ... the lox is a bit of boring ... ... the rice is awful ... Neutral ... had the food in the restaurant ... ... lox with a glass of chilli sauce ... ... the rice with a couple of olives salad ...</figDesc><table><row><cell>. With fewer labeled samples,</cell></row><row><cell>the test accuracy decreases, however, the improve-</cell></row><row><cell>ment becomes more evident. When using 500 la-</cell></row><row><cell>beled samples, the improvement is about 3.2%.</cell></row><row><cell>With full 3591 labeled samples, 1.5% gain can be</cell></row><row><cell>obtained. This illustrates that our method can im-</cell></row><row><cell>prove the accuracy with limited data.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If an input sentence has n aspect-terms, then n data samples are generated.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://inclass.kaggle.com/c/restaurant-reviews 3 http://times.cs.uiuc.edu/ wang296/Data/ 4 https://github.com/guillaumegenthial/sequence tagging</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tc-Lstm</surname></persName>
		</author>
		<idno>ST) 78.19 (0.36) 67.65 (0.43) 68.47 (0.47) 62.54 (0.74</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memnet</surname></persName>
		</author>
		<idno>EMB) 79.47 (0.38) 69.06 (0.21) 72.17 (0.44) 65.06 (0.73</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memnet</surname></persName>
		</author>
		<idno>ST) 78.83 (0.20) 68.92 (0.20) 69.52 (0.36) 64.39 (0.67</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memnet</surname></persName>
		</author>
		<idno>ASVAET) 80.58 (0.23) 70.06 (0.53) 73.21 (0.55) 65.88 (0.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tnet-As</surname></persName>
		</author>
		<idno>ST) 80.76 (0.23) 71.32 (0.56) 76.88 (0.41) 71.74 (0.63</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Avital Oliver, and Colin Raffel</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational sequential labelers for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="215" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical attention based position-aware network for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lishuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqiao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transformation networks for targetoriented sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01086</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recurrent entity networks with delayed memory update for targeted aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="278" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/568</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discretestate variational autoencoders for joint discovery and factorization of relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="231" to="244" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recursive neural structural correspondence network for crossdomain aspect and opinion co-extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2171" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>NAACL-. Long Papers</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>SemEval@COLING; Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<title level="m">Unsupervised data augmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational autoencoder for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoze</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9<address><addrLine>San Francisco, California, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3358" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07043</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention modeling for targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="572" to="577" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
