<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Web Images for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyi</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Web Images for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the performance of object detection has advanced significantly with the evolving deep convolutional neural networks. However, the state-of-the-art object detection methods still rely on accurate bounding box annotations that require extensive human labelling. Object detection without bounding box annotations, i.e, weakly supervised detection methods, are still lagging far behind. As weakly supervised detection only uses image level labels and does not require the ground truth of bounding box location and label of each object in an image, it is generally very difficult to distill knowledge of the actual appearances of objects. Inspired by curriculum learning, this paper proposes an easy-to-hard knowledge transfer scheme that incorporates easy web images to provide prior knowledge of object appearance as a good starting point. While exploiting large-scale free web imagery, we introduce a sophisticated labour free method to construct a web dataset with good diversity in object appearance. After that, semantic relevance and distribution relevance are introduced and utilized in the proposed curriculum training scheme. Our end-to-end learning with the constructed web data achieves remarkable improvement across most object classes especially for the classes that are often considered hard in other works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid growth of computational power and dataset size and the development of deep learning algorithms, object detection, one of the core problems in computer vision, has achieved promising results <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>. However, state-of-the-art object detection methods still require bounding box annotations which cost extensive human labour. To alleviate this problem, weakly super- vised object detection approaches <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">19]</ref> have attracted many attentions. These approaches aim at learning an effective detector with only image level labels, so that no labour-extensive bounding box annotations are needed. Nevertheless, as objects in common images can appear in different sizes and locations, only making use of image level labels are often not specific enough to learn good object detectors, and thus the performance of most weakly supervised methods are still subpar compared to their strongly supervised counterparts, especially for small objects with occlusions, such as "bottle" or "potted plant". As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, images containing small objects or with very complicated contexts are hard to learn. In contrast, images containing a single object with very clean background provides very good appearance priors for learning object detectors. Particularly, for these easy images, the difficulty of localizing the objects is much lower than complicated images. With correct localization, the appearance model can be better learned. Therefore, easy images can provide useful information of the object appearance for learning the model for more complicated images. Unfortunately, such easy images are rarely available in object detection datasets, such as PASCAL VOC or MS COCO, as images in these multiobject datasets usually contain cluttered objects and very complicated background. On the other hand, there are a large number of easy web images available online and we can exploit these web images for the weakly supervised detection (WSD) task.</p><p>However, to construct a suitable auxiliary dataset and appropriately design an algorithm to utilize the knowledge from the dataset are non-trivial tasks. In this paper, we intend to provide a practical and effective solution to solve both problems.</p><p>Specifically, as various image search engines like Bing, Google, Flickr provide access to freely available web data of high quality images. Recent researches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b11">12]</ref> have already utilized these large-scale web data in various vision tasks. However, as object detection tasks impose specific requirements for auxiliary web data, we need to carefully design a labour-free way to obtain suitable images for the task.</p><p>First of all, when constructing the web dataset, we need to consider the relevance of web images in order to effectively transfer the knowledge of easy web images to the target detection dataset. In this paper, we break down this relevance into two parts, namely semantic relevance, which refers to the relevance between web images and the target labels, and the distribution relevance, which refers the relevance between web images and target images. As we will shown in later section, the semantic relevance focuses on a larger picture in the semantic space, while the distribution relevance measures more fine-grain differences in the feature distributions. To give an example, for category "chair", the semantic relevance measures whether a certain web image is "chair" or not, and the distribution relevance measures whether this web image lies on the manifold formed by the specific "chairs" in the target dataset.</p><p>Secondly, apart from the relevance problem, we also need to consider the diversity of the web images. As sub-categories, poses as well as backgrounds are crucial for the success of object detection, and thus our web images should not only be easy and related to the target dataset, but also contain a variety of different images even for the same category. With single text query, commonly used image search engines are not able to produce images with large intra-category diversity, especially in top ranked results. Therefore, inspired by <ref type="bibr" target="#b7">[8]</ref>, which uses ngram to retrieve the fine-grained dataset, we propose a multi-attribute web data generation scheme to enhance the diversity of web data. Specifically, we construct a general attribute table with common attributes that can easily be propagated to other target datasets as well. With the attribute table, we are able to build a hassle-free web dataset with proper category-wise diversity for the coarsely labeled dataset.</p><p>Once we have an appropriate web dataset, we need to consider how to transfer the knowledge from the easy web images to more complex multi-object target datasets. During the recent years, easy web images have been used in other weakly supervised tasks, such as weakly supervised segmentation <ref type="bibr" target="#b22">[22]</ref>. To the best of our knowledge, we are the first work bringing in web images for improving the weakly supervised object detection task.</p><p>Inspired by curriculum learning <ref type="bibr" target="#b0">[1]</ref>, we propose a simple but effective hierarchical curriculum learning scheme. Specifically, with the hierarchical curriculum structure, all web images are considered easier than target images, which we refer as the first level of curriculum, followed by the second level of curriculum that includes all target images. Extensive experimental results show that our constructed web image dataset and the adopted curriculum learning can significantly improve the WSD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is related to several areas in computer vision and machine learning.</p><p>Weakly Supervised Object Detection (WSD): Traditional WSD methods like <ref type="bibr" target="#b5">[6]</ref> address this problem with multiple instance learning (MIL) <ref type="bibr" target="#b6">[7]</ref>, which treats each image as a bag and each proposal/window in the image as an instance in the bag. A positive image contains at least one positive instance whereas a negative image contains only negative instances. Since MIL approaches alternate the processes between selecting a region of objects and using the selected region to learn the object appearance model, they are often sensitive to initialization and often get stuck in local optima. <ref type="bibr" target="#b3">[4]</ref> proposed a two-stream CNN structure named WSDDN to learn localization and recognition in dedicated streams respectively. These two streams share the common features from the earlier convolutional layers and one fully connected layer. It learns one detection stream to find the high responsive windows and one recognition stream to learn the appearance of the objects. In this way, the localization and recognition processes are decoupled. Similarly, <ref type="bibr" target="#b10">[11]</ref> also uses the two stream structure and involves the context feature in the localization stream.</p><p>In this research, we use WSDDN <ref type="bibr" target="#b3">[4]</ref> as an example to evaluate our learning method. Since WSDDN separates recognition and localization into two individual streams, it introduces additional degrees of freedom while optimizing the model, and hence it is hard to train at the early stage. It is also sensitive to initialization. Thus, in this work we propose to explicitly provide good initialization during the training process in an easy-to-hard manner. Note that although we utilize WSDDN as our baseline, our learning scheme is general and can be applied to other WSD methods as well.</p><p>Curriculum Learning: Our work is inspired by curriculum learning <ref type="bibr" target="#b0">[1]</ref> scheme. Curriculum learning was initially proposed to solve the shape recognition problem, where the recognition model is first trained to recognize the basic shapes and then trained on more complicated geoshapes. Recently, Tudor et al. <ref type="bibr" target="#b20">[20]</ref> used this easy-tohard learning scheme in MIL problem but mainly focused on learning a model to rank images with difficulty that matches the human perspective. In our work, we propose a hierarchical curriculum scheme that incorporates easy web images in early training stage to provide prior knowledge for the subsequent training on complicated images.</p><p>Learning from Weak or Noisy Labels: This paper is also related to those works on learning from weak or noisy labels <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>. In <ref type="bibr" target="#b7">[8]</ref>, they proposed a classifier-based cleaning process to deal with the noisy labels. They first train a classification model on images with higher confidence and then use this model to filter the outliers in the rest of images. Later, with incorporation of CNN, novel loss layer is introduced to the deep network in <ref type="bibr" target="#b17">[17]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, web images are separated into easy images (Google) and hard images (Flickr). They build a knowledge graph on easy web images and use the graph as a semantic constraint to deal with the possible label-flip noises during training of harder web images. Similarly, <ref type="bibr" target="#b8">[9]</ref> learns the mutual relationship to suppress the feedback of noises during the back propagation. These works emphasize their methods to lessen the impact by outliers during the training process. In our work, apart from the outliers, we also consider distribution mismatch problem since we acquire web data that are from completely different information source with discrepant distribution compared to target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this part, we introduce the methodology on constructing the web dataset and the hierarchical curriculum learning to transfer the knowledge of web images to target dataset. We will use state-of-ther-art weakly supervised objection algorithm WSDDN <ref type="bibr" target="#b3">[4]</ref> as an example to show the effectiveness of our scheme. Note that our scheme is general and can be adapted to any other available algorithms if necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">WSDDN</head><p>We first introduce weakly supervised deep detection network, or WSDDN <ref type="bibr" target="#b3">[4]</ref>, which is utilized as baseline for our experiments. WSDDN provides an end-to-end solution that breaks the cycle of training of classification and localization alternatively by decoupling them into two separate streams.</p><p>Particularly, WSDDN replaces the last pooling layer with spatial pyramid pooling layer <ref type="bibr" target="#b12">[13]</ref> to obtain SPP feature of each region of interest (RoI). As shown in <ref type="figure" target="#fig_3">Figure  4</ref>, the SPP features are passed to a classification stream and a localization stream which individually learns the appearance and location of the objects. In the classification stream, the score for each RoI from f c8 layer is normalized across classes to find the correct label of RoIs. In the localization stream, the scores of all RoIs are normalized category-wise to find most respondent RoIs for each category. Then the probability outputs from both softmax layers are multiplied as the final detection scores for each RoI. Finally, detection scores of all RoIs are summed up to one vector as the image level score to optimize the loss function <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">L(y ci , x i |w) = −log(y ci (Φ c (x i |w) − 1 2 ) + 1 2 ) (1)</formula><p>In the binary log loss function L(y ci , x i |w) , x i is the input image i, and y is the binary image level label where y c i = {−1, 1} for class c in image i. Output from the last sum pooling layer is denoted as Φ y c (x i |w) which is a vector in range of 0 to 1 with the dimension equal to number of category. For each class c, if the label y ci is 1, L(y ci , x i |w) = −log(p(y ci = 1)) and if y ci is −1, L(y ci , x i |w) = −log(1 − p(y ci = 1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing Multi-Attribute Web Dataset</head><p>In this section, we describe our method to construct a diversified and robust web dataset by introducing an expand-to-condense process. Specifically, we first introduce multiple attributes on top of the given target labels when crawling for web images to improve the generalization ability of the obtained dataset. Then we introduce both semantic relevance and distribution relevance to condense the dataset by filtering out irrelevant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Expand to Diversify</head><p>Free web images are abundantly available and accessible. Many image search engines can provide high quality images by searching the object names, such as Google, Flickr and Bing. In our preliminary study, we observe that images searched from Bing are generally easier than images from other search engines. Since easier images are intuitively better for learning object appearance, we choose Bing as the search engine to crawl web images. However, for most search engines, we observed that if we just use the given target labels as keywords, the resulting images are very similar in object appearances, poses or sub-categories. Moreover, the number of good quality images returned per query is very limited and lower ranked images are generally very noisy and unrelated to the queries.</p><p>To solve the problem of lacking diversity as well as limited number of high quality images, we introduce multiple attributes to each category. Based on the general knowledge of object detection, we define a set of attributes in three general aspects: namely viewpoints, poses or habitats of the objects.</p><p>First of all, adding viewpoint attributes such as "front view" and "side view" not only provides extensive amount of high quality images for artificial objects like "aeroplane", "car" and "bus", but also enhances the appearance knowledge of these objects, which will eventually make the detector more robust. Note that for categories without clear discrepancy between front view and side view such as "bottle" and "potted plant", as well as flat objects like "tv monitor", we do not include these attributes. Secondly, for animals like "cat" and "dog", we add pose attributes. As their appearances vary significantly in different poses, adding such attributes will also be beneficial towards more robust detector. In particular, we add poses such as "sitting", "jumping" and "walking" to these animal categories. Last but not least, for category "bird" which resides in different habitats, we add habitat attributes of "sky" and "water". The set of attributes is summarized in <ref type="table" target="#tab_0">Table 1</ref>. Note that following the same spirit, the table can be easily expanded to other categories.</p><p>Moreover, to overcome the limitation of limited available clean images in the top ranking, we also crawl related images. Related images are the images retrieved with similar visual appearance by using each of the previously retrieved top ranked images as query to the search engine. These related image can expand the size of the web dataset by more than 20 times and also introduce more variations to the dataset. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the process of expanding the dataset by the multi-attribute per-category expansion and the per-image expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Condense to Transfer</head><p>Once we obtain a large scale web image dataset, we are facing with the relevance problem. As free web data often contain many noisy images, to effectively make use of these web images, we need to analyse the image relevance to condense the noisy data. In this paper, we break down the image relevance to two parts: semantic relevance and distribution relevance. In detail, semantic relevance indicates whether a image contains the correct objects and distribution relevance measures how well a web image matches the the distribution of the target dataset.</p><p>Firstly, to measure the semantic relevance, we train a web-to-web outlier detector to find images with wrong labels in the web dataset. Specifically, we select top 80 images from queries of each target label and top 20 images from queries of each attribute + label combination. As we only use high ranked images as seed images, the "cleanness" of the images can be guaranteed, and thus we are able to learn a more robust outlier detector.</p><p>The outlier detector is trained iteratively with the expansion of the seed images. Similar to the idea of active learning, we train a CNN classifer with softmax loss with the seed images. Then it is applied to the whole set of the web images. The highly confident positive samples are then used as the second batch of training images for next iteration. After a few iterations, the classification scores from the final stabilized model are used to measure semantic relevance. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, our model can provide very solid semantic relevance measurement. Most of the non-meaningful images have negative scores, outliers with wrong objects have very low scores and images with correct objects have high scores.</p><p>Secondly, since semantic relevance condenses images purely based on their semantic meaning regardless of the distribution matching with target dataset, we also consider the distribution relevance for more fine-grain measurements. To align the diversified web dataset into the distribution of target dataset, we search in the neighborhood of the target dataset to find similar web images. Particularly, for each single-label image in the target dataset, we select k nearest web images in the feature space. The distance between images is defined as the Euclidean distance between their corresponding CNN features. Specifically, we use the L2 normalized f c7 feature from a pretrained vgg-f model with PCA dimension reduction to represent each image. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, our method is capable to extend the target dataset with web images having very similar object appearances and poses.</p><p>We expect both relevance metrics to be effective for this task since it is intuitive to eliminate noises and unrelated data during the training. Nevertheless, our experiment result shows that matching the web data to target distribution is not as helpful as using a clean but diversified web dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relevance Curriculum Regularizer</head><p>Incorporating a good quality web dataset to the target dataset does not automatically guarantee better performance. Based on our experiments, we find out that simply appending these web images to target dataset is unhelpful or even harmful. These easy web images could lead to skew training models due to the distribution misalignment problem of the two datasets.</p><p>Therefore, instead of simply appending web data to target dataset, we propose a hierarchical curriculum structure. Specifically, we first consider a coarser curriculum with web images as easy and all target images as hard. If necessary, we could also add a fine curriculum to each dataset for full curriculum learning. Moreover, in addition to the normal curriculum or self-paced learning <ref type="bibr" target="#b9">[10]</ref>, we also consider adding an extra relevance term. As an analogy, we could consider web images as extracurricular activities. In order to help students with their learning, extracurricular activities need to be relevant to the course, in the same way that we should learn from easy images and relevant images.</p><p>In particular, to incorporate both curriculum and relevance constraints in training, we propose a relevance curriculum regularizer to the base detection structure:</p><formula xml:id="formula_1">E(w) = n i=1 C c=1 L(y i , x i |w) · f (u i , v i ),<label>(2)</label></formula><formula xml:id="formula_2">f (u i , v i ) = σ(u i ) · ψ(v i ),<label>(3)</label></formula><p>where u i is the relevance variable indicating whether the training sample is relevant as discussed in 3.2. v i is the curriculum regulation variable which indicates difficulty score of each image. σ is the relevance region function that only relevant samples can be learned every epoch. If a sample is in the relevance region, the value of σ(u) is 1 and otherwise 0. ψ is the curriculum region. It controls the pace of learning that allows only easy samples to be learned at early stage and gradually adding harder samples along the training process. If the difficulty score of sample image is within the curriculum region, ψ(v) is 1 and otherwise, ψ(v) is 0. As described previously, we implemented a hierarchical curriculum, where ψ(v) for all web images are consider as 1 first, then we gradually expand it to include target images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the effectiveness of our proposed weakly supervised object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Model &amp; Setting</head><p>For experimental setting, similar to the original WSDDN work, we use Edgebox <ref type="bibr" target="#b25">[25]</ref> as the proposal method to generate around 2000 bounding boxes. To train the network, we use the vgg-f model pretrained on ImageNet as the initial model. For fairness, our results are compared with the baseline method trained on vgg-f as well.</p><p>We evaluate our method on PASCAL VOC2007 and VOC2012 datasets with 20 object categories. During the training, we use only image-level labels of the training images. The evaluation metric is the commonly used detection mAP with IoU threshold of 0.5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results Regarding Curriculum Learning</head><p>We first evaluate the effectiveness of applying the curriculum learning method on PASCAL VOC 2007 trainval set itself, without using our web data. The curriculum is designed by the ranking of the mean edge strength of each image. The mean edge strength of an image is defined as the number of edge pixels over the total number of pixels. This is a simple yet intuitive method because images with more edges tend to have more complicated background or contain more cluttered objects, and thus it is reasonable to consider them as hard samples. <ref type="figure" target="#fig_4">Fig. 5</ref> gives some examples, which show that the mean edge length represents the relative difficulty of the images well. Specifically, we use the classical LoG edge detector to detect edges. For each curriculum region, we add 1 5 of more difficult images from each category. This is to balance the number of positive samples from each category in every iteration. In this way, the curriculum consists  of five overlapped regions with gradually increased image complexity. <ref type="table" target="#tab_1">Table 2</ref> shows the detection result ('CurrWS-DDN') of applying the curriculum regularization term to train VOC 2007 trainval set only, compared with the result of the baseline ('WSDDN'). We can see that using curriculum learning on VOC 2007 training images alone already improves the performance. This suggests that for training weakly supervised object detector, it is beneficial to train the network in an easy-to-hard manner. Note that the baseline WSDDN result is obtained by running the original WSDDN codes released in Github with the same setting 1 , which is slightly different from the result of 34.5 reported in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results Regarding Constructed Web Dataset</head><p>We now evaluate the usefulness of our constructed web image dataset for WSD. As mentioned in Section 3.2, we construct a web image dataset of 34k images using Bing image search engine with attributes and related images. Considering that many selected web images are of high resolution, which causes huge complexity in the proposal generation process, we resize the longer side of all images to 600 pixels and keep the aspect ratios. We treat all web images as easy images and all VOC images as hard images. Simple web images are trained first followed by more complicated VOC images. <ref type="table" target="#tab_1">Table 2</ref> shows the detection result of our method 'WebETH(Bing)' that exploits our constructed Bing dataset and trains the network in an easy-to-hard manner. Comparing <ref type="table" target="#tab_1">Tables 2 and 3</ref>, we can see that our method 'WebETH(Bing)' significantly improves the baseline 'WSDDN', increasing mAP from 33.9% mAP to 36%, and also outperforms the VOC curriculum method 'CurrWSDDN'. We also conduct experiments on another publicly available web dataset, STC Flickr clean dataset <ref type="bibr" target="#b22">[22]</ref>, which contains more than 40k super clean images and has been proven to have good performance in generating good saliency maps to train weakly supervised segmentation networks. Surprisingly, by involving STC Flickr clean, although its result (see <ref type="table" target="#tab_1">Table 2</ref>) is much better than the baseline using only VOC images, it has no improvement over the VOC curriculum method 'CurrWSDDN'. In contrast, using our noisy Bing dataset 'WebETH(Bing)' beats both the VOC curriculum method 'CurrWSDDN' and the Flickr clean dataset 'WebETH(Flickr clean)'. This suggests that our approach of constructing a multi-attribute web dataset with large diversity is practically useful in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results Regarding Relevance Metrics</head><p>Here we conduct experiments to study the effectiveness of using semantic relevance and distribution relevance. <ref type="figure" target="#fig_2">Fig. 3</ref> gives some examples of the two relevance metrics. For the semantic relevance, we use the classification scores by the outlier detector described in Section 3.2.2, whose values vary from negative to more than 20. We set a semantic relevance threshold of 8 so that web images with scores lower than 8 are excluded. This prevents from mixing in noisy images without target objects into the early stage of training. For the distribution relevance, its relevance region includes web images which are members of top k-th nearest neighbors of one of VOC images, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows the results using the two relevance metrics. We can see that with the semantic relevance, the detection result increases from 36.0% to 36.8%, whereas the kNN based distribution relevance gives a slightly lower result, which suggests that similar images might not be always preferred. As a non-convex optimization problem,    <ref type="table" target="#tab_4">Table 5</ref> might not be exhaustive since there might be some very recent WSD methods that report better performance. Since our solution is general, which can be added on top of any WSD baseline, it is more meaningful to evaluate our methods w.r.t the baseline. Based on WSDDN, we consider five variants: using only VOC images with the curriculum regularizer (Cur-rWSDDN), simply combining our web images with VOC images with the semantic relevance for training (WebRel), combining our web images with VOC images for easy-tohard training (WebETH), combining our web images with VOC images with the semantic relevance for easy-to-hard training (WebRelETH), and combining our web images with VOC images with the semantic relevance for easy-tocurriculum training (WebRelETC), where we train easy web images first and then train VOC images in a more detailed curriculum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">More Comparison Results</head><p>The results of CurrWSDDN, WebETH and We-bRelETH have been discussed previously w.r.t. <ref type="table" target="#tab_1">Tables 2,  3</ref> and 4, which demonstrate the effectiveness of the curriculum regularizer, the constructed web dataset, the proposed relevance metrics, respectively. For WebRel, its result is even worse than the baseline WSDDN, which suggests that it is not an effective way to simply combine data from two sources. In our case, a large number of easy images dominate the training so that the model cannot be well trained for hard samples. For WebRelETC, we expect that web images to have similar difficulty level but VOC images need to be partitioned in more levels of difficulty. We first train on easy web images and adopt five-level curriculum regions for VOC images. It is found that its average precision performance is slightly worse than WebRelETH. This suggest that it is not always good to further break down the higher level curriculum for every class if the lower-level curriculum of simple web images have been used. Overall, our WebRelETH achieves the best mAP of 36.8%, outperforming the baseline by 2.9%. <ref type="table" target="#tab_5">Table 6</ref> shows the experiment results for VOC2012. Our method also achieved up to 3.8% improvement in this dataset. Similar to VOC2007, WebRelETH outperforms WebRelETC, although WebRelETC excels largely in "dining table" by more than 10%. <ref type="figure">Fig. 6</ref> gives some visual comparisons of the detection results using WSDDN and our best model (WebRelETH). It can be seen that our model can refine the bounding boxes (see the top two rows of <ref type="figure">Fig. 6</ref>), and missing objects in WSDDN can also be detected by our model in some test images (see the bottom rows of <ref type="figure">Fig. 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper have addressed the two questions: how to construct a large, diverse and relevant web image dataset and how to use it to help weakly supervised object detection. Particularly, for constructing the web dataset, we introduced a sophisticated expand-to-condense process to first expand web data with attributes and related images and then condense the dataset with semantic relevance or distribution relevance. For helping the target dataset, we applied an easy-to-hard learning scheme. Extensive results have validated that our easy-to-hard learning with web data is effective and the multi-attribute web data do help in training a weakly supervised detector.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Easy web images and VOC images. Web images have clean background while VOC images are more difficult with cluttered instances and complicated background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Multi-attribute related dataset. Aeroplane category is expanded with multi-view attributes including front view and side view. Each multi-attribute web image is then expanded by the related images obtained from Bing image search engine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of relevance metrics including semantic relevance and distribution relevance. Top: semantic relevance by the scores from web-to-web classifier for motorbike images, where non-meaningful images have negative scores, outliers with wrong objects have very low scores, and images containing correct objects have high scores. Bottom: distribution relevance by k nearest neighbors of each motorbike image in VOC dataset, where images in the neighborhood of VOC images with small feature distances are considered relevant to the target dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>WSDDN with relevance curriculum regularizer. The relevance curriculum regularizer suppresses backpropagation from samples which do not fit in the relevance region and curriculum region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Curriculum metric by the mean edge strength for web images (Top: table / Bottom: potted plant). The mean edge strength can reasonably represent the difficulty of images. Images with clean background and single object usually have small mean edge strength and images with complicated background and cluttered objects usually have large edge strength.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Attribute table.</figDesc><table><row><cell>Category</cell><cell>Viewpoint</cell><cell>Pose</cell><cell>Habitat</cell></row><row><cell>aeroplane; bicycle;</cell><cell></cell><cell></cell><cell></cell></row><row><cell>boat; bus; car; motorbike; train;</cell><cell>front view; side view</cell><cell>-</cell><cell>-</cell></row><row><cell>chair; diningtable; sofa</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bird</cell><cell>front view; side view</cell><cell>-</cell><cell>water; sky</cell></row><row><cell>cat; dog</cell><cell>front view; side view</cell><cell>sitting; jumping walking;</cell><cell>-</cell></row><row><cell>cow; sheep</cell><cell>front view; side view</cell><cell>walking;</cell><cell>-</cell></row><row><cell>horse</cell><cell>front view; side view</cell><cell>walking; jumping</cell><cell>-</cell></row><row><cell>person</cell><cell>front view; side view</cell><cell>sitting; walking standing;</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of the detection mAP on VOC2007 test set by using the curriculum regularization term to train VOC 2007 trainval set.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell></row><row><cell>WSDDN (baseline)</cell><cell>33.9</cell></row><row><cell>CurrWSDDN</cell><cell>35.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of the detection mAP on VOC2007 test set by using our constructed Bing dataset or the 'Flickr clean' dataset as easy images and VOC images as hard images for easy-to-hard training.</figDesc><table><row><cell>Web dataset</cell><cell>mAP</cell></row><row><cell>WebETH(Flickr clean)</cell><cell>35.5</cell></row><row><cell>WebETH(Bing)</cell><cell>36.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Results of the detection mAP on VOC2007 test</cell></row><row><cell cols="2">set with different relevance metrics and using our con-</cell></row><row><cell cols="2">structed Bing dataset with the easy-to-hard training.</cell></row><row><cell>Transfer metrics</cell><cell>mAP</cell></row><row><cell>WebRelETH(Dist-Rel)</cell><cell>35.9</cell></row><row><cell>WebRelETH(Semantic-Rel)</cell><cell>36.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of the detection average precision results (%) on VOC2007 test set with training on VOC2007 trainval set. We use vgg-f model pretrained on ImageNet. WSDDN results are obtained using the published code on Github with the same setting stated in<ref type="bibr" target="#b3">[4]</ref>. Results of other methods are from their papers.</figDesc><table><row><cell></cell><cell cols="4">aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv</cell><cell>mean</cell></row><row><cell>Bilen et al. [2]</cell><cell cols="2">42.2 43.9 23.1 9.2 12.5 44.9 45.1 24.9 8.3 24.0 13.9 18.6 31.6 43.6</cell><cell>7.6</cell><cell cols="2">20.9 26.6 20.6 35.9 29.6 26.4</cell></row><row><cell>Bilen et al. [3]</cell><cell cols="2">46.2 46.9 24.1 16.4 12.2 42.2 47.1 35.2 7.8 28.3 12.7 21.5 30.1 42.4</cell><cell>7.8</cell><cell cols="2">20.0 26.8 20.8 35.8 29.6 27.7</cell></row><row><cell>Cinbis et al. [6]</cell><cell cols="2">39.3 43.0 28.8 20.4 8.0 45.5 47.9 22.1 8.4 33.5 23.6 29.2 38.5 47.9</cell><cell cols="3">20.3 20.0 35.8 30.8 41.0 20.1 30.2</cell></row><row><cell>Wang et al. [21]</cell><cell cols="2">48.8 41.0 23.6 12.1 11.1 42.7 40.9 35.5 11.1 36.6 18.4 35.3 34.8 51.3</cell><cell cols="3">17.2 17.4 26.8 32.8 35.1 45.6 30.9</cell></row><row><cell>Teh et al. [19]</cell><cell cols="2">48.8 45.9 37.4 26.9 9.2 50.7 43.4 43.6 10.6 35.9 27.0 38.6 48.5 43.8</cell><cell cols="3">24.7 12.1 29.0 23.2 48.8 41.9 34.5</cell></row><row><cell cols="3">ContextLocNet(contrastive S) [11] 57.1 52 31.5 7.6 11.5 55 53.1 34.1 1.7 33.1 49.2 42 47.3 56.6</cell><cell cols="3">15.3 12.8 24.8 48.9 44.4 47.8 36.3</cell></row><row><cell>WSDDN [4]</cell><cell cols="2">41.8 57.7 31.8 16.2 9.2 59.2 53.0 39.1 3.6 34.6 14.2 33.5 50.2 53.5</cell><cell>9.8</cell><cell cols="2">15.6 37.3 21.0 53.1 43.3 33.9</cell></row><row><cell>WSDDN(Flickr clean only)</cell><cell cols="2">31.4 26.6 22.0 10.0 1.5 43.0 38.1 36.6 1.7 12.3 19.7 32.8 34.1 38.6</cell><cell>8.4</cell><cell cols="2">5.7 17.6 29.5 32.0 18.2 23.0</cell></row><row><cell>WSDDN(Bing rel only)</cell><cell>37.4 22.6 18.5 6.9</cell><cell>1.7 42.2 38.0 29.9 1.0 14.9 1.7 37.1 34.2 33.9</cell><cell>11.7</cell><cell cols="2">4.4 17.0 16.3 27.7 12.5 20.5</cell></row><row><cell>CurrWSDDN</cell><cell cols="2">40.4 54.6 28.2 15.4 10.4 57.4 53.0 44.5 1.2 35.3 30.9 41.5 51.3 53.0</cell><cell cols="3">11.6 16.3 34.5 39.0 46.0 45.0 35.5</cell></row><row><cell>WebRel(ours)</cell><cell cols="2">40.7 51.5 31.0 10.7 10.0 61.0 43.2 39.4 1.8 30.1 35.5 46.4 52.3 50.6</cell><cell>9.0</cell><cell cols="2">13.4 30.4 31.8 41.2 42.3 33.6</cell></row><row><cell>WebETH(ours)</cell><cell cols="2">40.2 51.6 33.3 13.5 13.0 62.8 54.5 38.7 11.8 34.8 25.1 42.2 50.5 55.3</cell><cell cols="3">13.1 19.0 31.4 34.6 49.3 44.6 36.0</cell></row><row><cell>WebRelETH(ours)</cell><cell cols="2">44.4 52.1 38.1 10.2 12.3 61.5 54.4 33.5 7.6 37.2 30.2 37.6 55.4 57.3</cell><cell>9.1</cell><cell cols="2">18.3 35.9 43.0 47.6 50.0 36.8</cell></row><row><cell>WebRelETC(ours)</cell><cell cols="2">38.9 52.4 33.4 11.2 10.5 59.9 53.8 36.4 3.0 38.5 41.8 38.8 53.9 56.0</cell><cell cols="3">11.9 18.9 35.1 43.2 46.2 47.2 36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of the detection average precision (%) on VOC2012 test set with training on VOC2012 training set. We use vgg-f model pretrained on ImageNet. WSDDN results are obtained using the published code on Github with the same setting stated in [4]. aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean WSDDN [4] 53.4 53.2 36.2 7.9 16.4 57.2 35.3 24.8 6.5 29.0 13.7 31.1 47.1 57.2 11.0 18.9 28.6 19.4 42.3 39.6 31.4 WebRelETH (ours) 57.6 55.1 38.5 8.6 20.4 59.4 36.4 33.6 14.0 34.8 21.7 39.4 51.3 62.8 11.5 19.2 30.2 23.9 41.2 44.5 35.2 WebRelETC (ours) 55.6 56.2 35.3 7.4 20.5 55.6 32.6 34.8 9.7 32.9 32.1 34.6 48.4 61.6 15.5 18.9 27.3 15.7 41.2 43.5 34.0Figure 6: Visual results of WSDDN and our best model (WebRelETH). Our model can refine the bounding boxes as shown in the top two rows. Missing objects in the original model can also be detected in some test images as shown in the bottom rows.the training of WSD tends to drift to optimize small clus-ters of training samples. Although additional training in-stances with a similar distribution can help achieve lower training loss, it is not as helpful as involving new training samples with larger diversity, which leads to better generalization ability. This may also explain why STC Flickr clean dataset is not so helpful since the images in the Flickr clean dataset also have a similar distribution as VOC dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>lists out the per-category average precision re-</cell></row><row><cell>sults of different WSD methods on VOC2007 test set with</cell></row><row><cell>training on VOC2007 trainval set. It can be seen that</cell></row><row><cell>compared with other existing WSD methods, the base-</cell></row><row><cell>line method WSDDN achieves reasonably good perfor-</cell></row><row><cell>mance. We would like to point out that our list in</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hbilen/WSDDN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1081" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00949</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3270" to="3277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relaxing from vocabulary: Robust weakly-supervised deep learning for vocabulary-free image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1985" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hauptmann. Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attention networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How hard can it be? estimating the difficulty of visual search in an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2157" to="2166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Augmenting strong supervision using web data for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2524" to="2532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
