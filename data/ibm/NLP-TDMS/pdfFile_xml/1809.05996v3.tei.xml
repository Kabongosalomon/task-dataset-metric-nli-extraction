<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Devil in the Details: Towards Accurate Single and Multiple Human Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ruan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">â€ </forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Devil in the Details: Towards Accurate Single and Multiple Human Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human parsing has received considerable interest due to its wide application potentials. Nevertheless, it is still unclear how to develop an accurate human parsing system in an efficient and elegant way. In this paper, we identify several useful properties, including feature resolution, global context information and edge details, and perform rigorous analyses to reveal how to leverage them to benefit the human parsing task. The advantages of these useful properties finally result in a simple yet effective Context Embedding with Edge Perceiving (CE2P) framework for single human parsing. Our CE2P is end-to-end trainable and can be easily adopted for conducting multiple human parsing. Benefiting the superiority of CE2P, we won the 1st places on all three human parsing tracks in the 2nd Look into Person (LIP) Challenge. Without any bells and whistles, we achieved 56.50% (mIoU), 45.31% (mean AP r ) and 33.34% (AP p 0.5 ) in Track 1, Track 2 and Track 5, which outperform the state-of-the-arts more than 2.06%, 3.81% and 1.87%, respectively. We hope our CE2P will serve as a solid baseline and help ease future research in single/multiple human parsing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Human parsing is a fine-grained semantic segmentation task which aims at identifying the constituent parts (e.g. body parts and clothing items) for a human image on pixel-level. Understanding the contents of the human image makes sense in several potential applications including ecommerce, human-machine interaction, image editing and virtual reality to name a few. Currently, human parsing has gained remarkable improvement with the development of fully convolutional neural networks on semantic segmentation.</p><p>For the semantic segmentation, researchers have developed many solutions from different views to tackle the challenging dense prediction task. In general, current solutions can be grossly divided into two types: 1) High-resolution Maintenance This kind of approaches are attempting to obtain high-resolution features to recover the desired detailed information. Due to consecutive spatial pooling and convolution strides, the resolution of the final feature maps is reduced significantly that the finer image information is lost. To generate high-resolution features, there are two typical solutions, i.e. cutting several down-sampling (e.g. max pooling) operations <ref type="bibr" target="#b1">(Chen et al. 2016a</ref>) and introducing details from low-level feature maps <ref type="bibr" target="#b3">(Chen et al. 2018b)</ref>. For the latter case, it is usually embedded in an encoder-decoder architecture, in which the high-level semantic information is captured in the encoder and the details and spatial information are recovered in the decoder. 2) Context Information Embedding This kind of approaches is devoting to capture rich context information to handle the objects with multiple scales. Feature pyramid is one of the effective ways to mitigate the problem caused by various scales of objects, and atrous spatial pyramid pooling (ASPP) based on dilated convolution <ref type="bibr" target="#b2">(Chen et al. 2018a</ref>) and pyramid scene parsing (PSP) ) are two popular structures. ASPP utilizes parallel dilated convolution layers with different rates to incorporate multi-scale context. PSP designs pyramid pooling operation to integrate the local and global information together for more reliable prediction. Beyond these two typical types, some other works also propose to advantage the segmentation performance by introducing additional information such as edge  or more effective learning strategy such as cascaded training <ref type="bibr" target="#b8">(Li et al. 2017b)</ref>.</p><p>Compare with the general semantic segmentation tasks, the challenges of human parsing is to produce finer predictions for every detailed region belonging to a person. Besides, the arms, legs and shoes are further divided into the left side and right side for more precise analysis, which makes the parsing more difficult. Despite the approaches mentioned above showing impressive results in semantic segmentation, it remains unclear how to develop an accurate human parsing system upon these solutions and most previous works did not explore and analyze how to leverage them to unleash the full potential in human parsing. In this work, we target on answering such a question: can we simply formulate a powerful framework for human parsing by exploiting the recent advantages in the semantic segmentation area?</p><p>To answer such a question, we conduct a great deal of rigorous experiments to clarify the key factors affecting the performance of human parsing. In particular, we perform an analysis of potential gains in mIoU score with different properties. The evaluated useful properties include feature resolution, context information and edge details. Based on the analysis, we present a simple yet effective Context Embedding with Edge Perceiving (CE2P) framework for single human parsing. The proposed CE2P consists of three key modules to learn for parsing in an end-to-end manner: 1) A high-resolution embedding module used to enlarge the feature map for recovering the details; 2) A global context embedding module used for encoding the multi-scale context information; 3) An edge perceiving module used to integrate the characteristic of object contour to refine the boundaries of parsing prediction. Our approach achieves state-of-the-art performance on all three human parsing benchmarks. Those results manifest that the proposed CE2P can provide consistent improvements over various human parsing tasks. The main contributions of our work are as follows:</p><p>â€¢ We analyze the effectiveness of several properties for human parsing, and reveal how to leverage them to benefit the human parsing task.</p><p>â€¢ We design a simple yet effective CE2P framework by leveraging the useful properties to conduct human parsing in a simple and efficient way.</p><p>â€¢ Our CE2P brings a significant performance boost to all three human parsing benchmarks, outperforming the current state-of-the-art method by a large margin.</p><p>â€¢ Our code is available, which can serve as a solid baseline for the future research in single/multiple human parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Human Parsing</p><p>The study of human parsing has drawn more and more attention due to the wide range of potential application. The early works <ref type="bibr" target="#b17">(Yamaguchi et al. 2012;</ref><ref type="bibr" target="#b3">Dong et al. 2013;</ref><ref type="bibr" target="#b13">Simo-Serra et al. 2014;</ref><ref type="bibr" target="#b6">Ladicky, Torr, and Zisserman 2013)</ref> performed the parsing with CRF framework and utilized the human pose estimation to assist the parsing. A Co-CNN  architecture was proposed to hierarchically integrate the local and global context information and improved the performance greatly. Recently, SSL (Gong et al. 2017) introduced a self-supervised structure-sensitive loss, which was used for enforcing the consistency between parsing results and the human joint structures, to assist the parsing task. Following previous work, JPPNet ) incorporated the human parsing and pose estimation task into a unified network. With multi-scale feature connections and iterative refinement, the parsing and pose tasks boosted each other simultaneously. Considering the practical application, several current works <ref type="bibr" target="#b7">(Li, Arnab, and Torr 2017;</ref><ref type="bibr" target="#b8">Li et al. 2017a;</ref><ref type="bibr" target="#b20">Zhao et al. 2018</ref>) focus on handling the scenario with multiple persons. Usually, it consisted of three sequential steps: object detection (He et al. 2017), object segmentation and part segmentation. Besides, many research efforts <ref type="bibr" target="#b4">(Girshick et al. 2014;</ref><ref type="bibr" target="#b13">Wang et al. 2015;</ref><ref type="bibr" target="#b1">Chen et al. 2016b;</ref><ref type="bibr" target="#b5">Hariharan et al. 2015)</ref> have been devoted into the object parts segmentation, which was similar to human parsing. Most of those works leveraged the multi-scale features to enhance the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation</head><p>Human parsing is a fine-grained semantic segmentation task. Hence, the methods used in human parsing is similar to semantic segmentation. Since the fully convolutional network (FCN) <ref type="bibr" target="#b12">(Long, Shelhamer, and Darrell 2015)</ref> has shown numerous improvements in semantic segmentation, many researchers <ref type="bibr" target="#b1">(Chen et al. 2016a;</ref><ref type="bibr" target="#b6">JÃ©gou et al. 2017;</ref>) have made efforts based on the FCN. Several pieces of work <ref type="bibr" target="#b0">(Badrinarayanan, Kendall, and Cipolla 2017;</ref><ref type="bibr" target="#b12">Ronneberger, Fischer, and Brox 2015;</ref>, leveraged an encoderdecoder architecture with skip connection to recover the dense feature responses. Another literatures <ref type="bibr" target="#b1">(Chen et al. 2016a;</ref><ref type="bibr" target="#b18">Yu and Koltun 2015;</ref><ref type="bibr" target="#b3">Chen et al. 2018b</ref>) exploited the dilated convolution for higher resolution output. Besides, the local and global information are integrated for generating more reliable prediction, such as <ref type="bibr" target="#b2">(Chen et al. 2018a;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Embedding with Edge Perceiving</head><p>In this section, we first provide the architecture of our Context Embedding with Edge Perceiving(CE2P) approach. Within CE2P, we analyze the effectiveness of several key modules motivated from the previous state-of-the-art semantic segmentation models and reveal how they can work together to accomplish the single human parsing task. Then, we give the details of applying the CE2P to address the more challenging multiple human parsing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key Modules of CE2P</head><p>Our CE2P integrates the local fine details, global context and semantic edge context into a unified network. The overview of our framework is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, it consists of three key components to learn for parsing in an end-to-end manner, i.e. context embedding module, high-resolution embedding module and edge perceiving module. ResNet-101 is adopted as the feature extraction backbone.</p><p>Context Embedding Module Global context information is useful to distinguish the fine-grained categories. For instance, the left and right shoe have relatively high similarity in appearance. To differentiate between the left and right shoe, the global information, like the orientation of leg and body, provides an effective context prior. As we know, feature pyramid is a powerful way to capture the context information. Draw on the previous work PSP ), we utilize a pyramid pooling module to incorporate the global representations. We perform four average pooling operations on the features extracted from ResNet-101 to generate multiple scales of context features with size 1Ã—1, 2Ã—2, 3Ã—3, 6Ã—6 respectively. Those context features are upsampled to keep the same size with the original feature map by bilinear interpolation, which are further concatenated with the original feature. Then, the 1Ã—1 convolution is employed to reduce the channels and better integrate the multi-scale context information. Finally, the output of context embedding module is fed into the following high-resolution module as global context prior.</p><p>High-resolution Embedding Module In human parsing, there exist several small objects to be segmented, e.g. socks, shoes, sunglasses and glove. Hence, high-resolution feature for final pixel-level classification is essential to generate an accurate prediction. To recover the lost details, we adopt a simple yet effective method which embeds the low-level visual features from intermediate layers as complementary to the high-level semantic features. We exploit the feature from the conv2 to capture the high-resolution details. The global context feature is upsampled by factor 4 with bilinear interpolation, and concatenated with local feature after channel reduced by 1Ã—1 convolution. Finally, we conduct two sequential 1Ã—1 convolution on the concatenated feature to better fuse the local and global context feature. In this manner, the output of high-resolution module simultaneously acquires high-level semantic and high-resolution spacial information.</p><p>Edge Perceiving Module This module aims at learning the representation of contour to further sharp and refine the prediction. We introduce three branches to detect multi-scale semantic edges. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> Our CE2P consisting of the three modules is learned with an end-to-end manner. The outputs of CE2P consist of two parsing results and edge prediction. Hence, the loss can be formulated as:</p><formula xml:id="formula_0">L = L P arsing + L Edge + L Edgeâˆ’P arsing ,<label>(1)</label></formula><p>where L Edge denotes the weighted cross entropy loss function between the edge map detected by edge module and the binary edge label map; L P arsing denotes the cross entropy loss function between the parsing result from high resolution module and the parsing label; L Edgeâˆ’P arsing denotes the cross entropy loss function between the final parsing result, which is predicted from the edge perceiving branch, and the parsing label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Human Parsing (MHP)</head><p>MHP is a more challenging task, which not only needs to classify the semantics of pixels but also identify the instance (i.e. one unique person) that these pixels belong to. To achieve high-quality parsing results in the scenario of multiple persons, we design a framework called M-CE2P upon our CE2P and Mask R-CNN (He et al. 2017). As shown in <ref type="figure">Fig. 2</ref> Considering that a human instance obtained from the ground-truth instance mask is more approximate to the real single human image, we introduce the branch B l 2 to train a model with the data generated from ground-truth instance mask. This branch is rather similar with B l 1 , the only difference is that we obtain person patches with the guide of the ground truth bounding boxes in the training stage. With the B l 2 , the performance of local parsing can be further boosted. Finally, the predictions generated by the three branches are fused by element-wise summation to obtain the final instance-agnostic parsing result. The predicted instance-agnostic parsing results are further fed into the following process to make the instance-level parsing.</p><p>Instance-level Parsing and Label Refinement With the instance-agnostic parsing result obtained from M-CE2P, we consider two aspects to generate the instance-level label, i.e. instance assignment for predicting instance-aware results and label refinement for solving the shortage of the undersegmentation phenomenon of Mask R-CNN. For instance assignment, we directly apply human masks generated by Mask R-CNN to assign instance-level part label of global body parts. Concretely, parts will be assigned with different part instance labels when they are same category while belonging to different masks. Through the experiments, we find that the parsing mask predicted from our CE2P is more reliable than human instance map. To further validate the reliability of parsing results, we introduce the label refinement by expanding the area of intersection with the neighbor pixels which have same parsing label while exceeding the human instance. For example, some regions of marginal parts(e.g. hair, hands) are very likely to be outside of the area of predicted human masks. We use searching based method to alleviate this problem. Specifically, for each border pixel of the part obtained from the assignment step, we use Breadth-First Search to find the pixel that is endowed with an instance-agnostic class label but no part label due to the inaccuracy of segmentation prediction. With the proposed refinement, the body parts excluded by human mask can be effectively included in the final instance-level result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Metrics</head><p>We compare the performance of single human parsing of our proposed approach with other state-of-the-arts on the LIP <ref type="formula">(</ref>  For data augmentation, we apply the random scaling (from 0.5 to 1.5), cropping and left-right flipping during training. Note that the edge annotation used in the edge perceiving module is directly generated from the parsing annotation by extracting border between different semantics.  <ref type="figure" target="#fig_1">Fig. 3</ref>. Observing the per-classes performance and the visualized results, there exist the following problems. 1) Big-size objects have the discontinuous prediction. For instance, the dress is always parsed as a upper-clothes and skirt, and the jumpsuit is separated into a upper-clothes and pants.</p><p>2) The confusable categories are hard to distinguish. i.e. the coat and upper-clothes.</p><p>3) The left and right parts are easily confused, which frequently occurs in the back-view human body and the front-view body with legs crossed. Global Context Embedding Module To evaluate the effectiveness of each module, we first conduct experiments by introducing a global context embedding module. In our architecture, we leverage the pyramid pooling  to generate multi-scale context information. As shown in Tab. 1, we can find it brings about 1.5% improvements on mIoU, which demonstrates that the multi-scale context information can assist the fine-grained parsing. Particularly, it shows significant boosts (nearly 7%) in the class of dress.</p><p>Since the long-range context information can provide the more discriminated characteristic, the global context embedding module is helpful for the big-size objects.</p><p>High Resolution Embedding Module To figure out the importance of the high resolution, we conduct experiments by further introducing a high resolution module. From Tab. 1, we can find that the performance gains nearly 2% improvement with the high resolution embedding module. As human parsing is a fine-grained pixel-wise classification, it requires a lot of detailed information to identify the various small-size parts accurately. With high resolution embedding module, the features from shallow and high-resolution layers provide more details not available in deep layers. The results demonstrate the effectiveness as well. Edge Perceiving Module Finally, we report the performance with the edge perceiving module in Tab. 1. Based on the above two modules, appending edge perceiving module still brings nearly 1% boosts. That's the contours of the parts can be underlying constraints during separating the semantic parts from a human body. In addition, the features from multiple edge branches carrying various details of the objects can further promote the human parts prediction. Finally, fusing with the flipped images gives 0.6% gain.</p><p>Comparison with State-of-the-Arts We evaluate the performance of CE2P on the validation dataset of LIP and compare it to other state-of-the-art approaches. The results are reported in Tab. 2. First, we note that the mIoU of our CE2P significantly outperforms other approaches. The improvement over the state-of-the-art method validates the effectiveness of our CE2P for human parsing. When comparing with the current state-of-the-art approach JPPNet, our method exceeds by 1.73% in terms of mIoU. In particular, the performance on the small-size categories, i.e. 'socks'  and 'sunglasses', yields obviously improvement. Thanks to the high resolution embedding and edge perceiving module, the details and characteristic of small objects can be captured for further pixel-level classification. Besides, the JPP-Net achieves the performance of 51.37% by utilizing extra pose annotation with a complex network structure. Nevertheless, our CE2P obtains better performance with a simpler network structure and no need for extra annotation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Human Parsing</head><p>We provide abundant experimental results on two large datasets of multiple human parsing task, named CIHP and MHP v2.0. The results are exhibited in Tab. 4. Due to space limitation, we only show detailed results on the more challenging MHP v2.0 dataset. The detailed explanation will be given in the following paragraphs. Label Refinement A remarkable improvement attributes the success to the label refinement operation. As Tab. 4 shows, it exactly brings performance boosting, despite of the combination strategies. As we mentioned before, the results provided by Mask R-CNN are likely to ignore some partial area of marginal body parts, especially on complex images. However, CE2P may catch these parts from the localized human sub images. Therefore, the refinement operation can alleviate the under segmentation problem. For clarity, the following analyses are all based on refined results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Various Combination Strategies</head><p>To prove the effectiveness of the multi-branch fusion strategies, we perform experiments with various branch combinations. From Tab. 4, we can notice that the results produced by the double-branch model are better than that produced by the single-branch model, and the all-branch model, i.e. M-CE2P, catch the best performance on most of the metrics. Especially on more convincing human-centric metrics like AP p and P CP , M-CE2P shows out a significant performance boosting than all the other models. It proves that branches can make complements with each other. As mentioned in the previous section, the branch B g trained with the entire image lacks the ability to grab small-scale persons in a scene, such as the first image in <ref type="figure" target="#fig_2">Fig. 4</ref>. Hence, it only achieves the performance of 24.04% in terms of AP p 0.5 . Nevertheless, this shortcoming can be compensated by B l 1 and B l 2 to capture a more precise view of local context. On the other hand, the benefits from B g are still cannot be ignored. The global context that B g has makes a performance improvement of 2.08% in terms of AP p 0.5 than the result only utilizes B l 1 and B l 2 . Finally, the all-branch fused M-CE2P can reach the best performance under the AP p and P CP metrics.</p><p>Comparison with State-of-the-Arts Compared to other state-of-the-art methods, our M-CE2P model still maintains a large-margin leading edge on major metrics, as Tab. 3 and Tab. 4 illustrate. On validation set of MHP v2.0, our proposed model outperforms than <ref type="bibr" target="#b20">(Zhao et al. 2018)</ref> 9.64%, 9.40% in terms of AP p 0.5 and P CP 0.5 , respectively;Furthermore, on the test set of MHP v2.0, we outperform than <ref type="bibr" target="#b20">(Zhao et al. 2018)</ref> 8.20%, 9.57% and 0.47% in terms of AP p 0.5 , P CP 0.5 and mean AP p , respectively; On the test set of CIHP, our M-CE2P performs 63.77%, 45.31%, 50.94% in terms of mIoU, mean AP r and AP r 0.5 , respec- tively, which all outperform than . Above all, we investigate and combine several useful properties to improve the parsing results. Each module plays an important role and makes improvements for the final results. Benefiting from our parsing network, the prediction on the whole multi-person image already has a comparable result with baseline methods. Based on the accurate parsing results, our fusion strategy and label refinement can make further boost. Visualization Some visualized results with their failure cases are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Generally speaking, our M-CE2P can handle a rather complex scene with multiple persons, and produce a satisfactory result. However, there are also some failure cases produced by our M-CE2P. From the perspective of fusion strategy, it has following problems: 1) Under some circumstances, the B g brings too much negative confidence to parts belong to human far away from camera, so that the confidence provided by local parsing may be drastically reduced; 2) When acting sub-image fusion, the local information of tightly closed humans may be disturbed with each other. For example, in bottom line of <ref type="figure" target="#fig_2">Fig. 4</ref>, the man with black suit make the edge of woman's skirt strongly be mistaken as other semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in Challenge</head><p>With our CE2P framework, we achieved the 1st places of all three human parsing tracks in the 2nd Look into Person (LIP) Challenge. Tab. 3 shows a few of the results on the single human parsing lead-board. By integrating the results from three models with different input size, our CE2P achieved 57.9%. More importantly, our single model already attained the state-of-arts performance without any bells and whistles. Besides, we design a M-CE2P upon our CE2P for multiple human parsing with three branches to predict from global to local view. Benefiting from the M-CE2P model, we achieved high performance in all the multiple human parsing benchmarks without refinement process, i.e. CIHP and MHP v2.0, respectively. Specifically, we yielded 45.31% of Mean AP r and 33.34% of AP p 0.5 , which outperform the second place more than 3.81% 1.87%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we investigate several useful properties for human parsing, including feature resolution, global context information and edge details. We design a simple yet effective CE2P system, which consists of three key modules to incorporate the context and detailed information into a unified framework. Specifically, we use a high-resolution embedding module to capture details from shallow layer, a global context embedding module for multi-scale context information, and an edge perceiving module to constrain object contours. For multiple human parsing, we fuse three CE2P based model to generate global parsing prediction, and use a straight-forward way to produce the instance-level result with the guide of human mask. The experimental results demonstrate the superiority of the proposed CE2P.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework of our proposed CE2P. The overall architecture consists of three key modules: 1) high resolution embedding module 2) global context embedding module; 3) edge perceiving module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Some issues of baseline model on LIP dataset. (a) The left arm (leg) is wrongly predicted as right arm (leg). (b) The dress is separated into a upper-clothes and skirt. (c) The jumpsuits is parsed into a upper-clothes and pants. (d) The coat is mislabeled as upper-clothes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>A visualized example for Multiple Human Parsing task. The areas surrounded by white bounding boxes indicate the failure cases of predicted results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, a 1Ã—1 convolution are conducted to conv2, conv3 and conv4 to generate 2-channel score map for the semantic edge. And then, 1Ã—1 convolution is performed to obtain the fused edge map. Those intermediate features of edge branches, which can capture useful characteristics of object boundaries, are upsampled and concatenated with the features from highresolution. Finally, 1Ã—1 convolution is performed on the concatenated feature map to predict the pixel-level human parts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, our M-CE2P leverages three branches, denoted by B g , B l 1 , B l 2 , to make predictions from global to local views. The details of the three branches are introduced in the following. Global Parsing B g In spite of the CE2P is proposed for single human parsing, we find it shows considerable performance on multiple human images as well. Hence, we first apply it over the entire image for global parsing. For the branch of B g , we train a CE2P model with the entire images. Then, the output of this branch is leveraged as complementary to the following local parsing. The global parsing</figDesc><table><row><cell>Mask -RCNN</cell><cell>"_$</cell></row><row><cell></cell><cell>"_%</cell></row><row><cell></cell><cell>&amp;</cell></row><row><cell cols="2">Figure 2: The overall framework of CE2P for Multiple Hu-</cell></row><row><cell>man Parsing task.</cell><cell></cell></row><row><cell cols="2">branch can provide context information when there exist oc-</cell></row><row><cell cols="2">clusions among multiple persons. For instance, the same se-</cell></row><row><cell cols="2">mantic parts form different persons can be easy to tell apart,</cell></row><row><cell cols="2">and the spatial relationship among persons can be captured</cell></row><row><cell cols="2">to handle the circumstance of occlusion. However, it does</cell></row><row><cell cols="2">not concentrate on the relatively small human instances. As a</cell></row><row><cell cols="2">result, body parts belonging to small-scale person are likely</cell></row><row><cell>ignored by B</cell><cell></cell></row></table><note>g . Local Parsing with Predicted Instance Masks B l 1 To al- leviate the problem of global parsing B g , we consider lo- cating the persons as a preprocessing step to generate ac- curate parsing results. Towards this end, we propose a two- stage branch devoting to human-level local parsing. Specif- ically, we employ Mask R-CNN (He et al. 2017) to extract all the person patches in the input image, and resize them to fit the input size of CE2P. And then, all the human-level sub-images are fed into CE2P to training the model for local view. During inference stage, the sub-images with single hu- man instance of input images are extracted by Mask R-CNN, and further fed into the trained model to make parsing pre- dictions. The predicted confidence maps are resized to the original size by bilinear interpolation for the following pre- diction over the entire image. Finally, the confidence maps for each sub-images are padded with zeros to keep the same size as the confidence map from B g , and further fused to- gether by element-wise summation on foreground channels and minimization on background channel. Local Parsing with Ground-truth Instance Masks B l 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We use mean IoU to evaluate the global-level predictions, and use the following three metrics to evaluate the instance-level predictions. M ean AP r computes the area under the precision-recall curve with the limitation of a set of IoU threshold, and figure out the final averaging result,</figDesc><table><row><cell>Liang et al. 2018) dataset, and we further evaluate the</cell></row><row><cell>multiple human parsing on CIHP (Gong et al. 2018) and</cell></row><row><cell>MHP v2.0 (Li et al. 2017a) dataset.</cell></row><row><cell>LIP dataset: The LIP (Liang et al. 2017) dataset is used</cell></row><row><cell>in LIP challenge 2016, which is a large-scale dataset that</cell></row><row><cell>focuses on single human parsing. There are 50,462 images</cell></row><row><cell>with fine-grained annotations at pixel-level with 19 semantic</cell></row><row><cell>human part labels and one background label. Those images</cell></row><row><cell>are further divided into 30K/10K/10K for training, valida-</cell></row><row><cell>tion and testing, respectively.</cell></row><row><cell>CIHP dataset: CIHP (Gong et al. 2018) provides a dataset</cell></row><row><cell>with 38,280 diverse human images, in which contains</cell></row><row><cell>28,280 training, 5K validation and 5K test images. The</cell></row><row><cell>images have pixel-wise annotation on 20 categories and</cell></row><row><cell>instance-level identification.</cell></row><row><cell>MHP v2.0 dataset: The MHP v2.0 dataset is designed for</cell></row><row><cell>multi-human parsing in the wild including 25,403 images</cell></row><row><cell>with finer categories up to 58 semantic labels. The validation</cell></row><row><cell>set and test set have 5K images respectively. The rest 15,403</cell></row><row><cell>are provided as the training set.</cell></row><row><cell>Metrics:</cell></row></table><note>which is first introduced in (Hariharan et al. 2014); AP p (Li et al. 2017a) computes the pixel-level IoU of semantic part categories within a person, instead of global circumstance; P CP (Li et al. 2017a) elaborates how many body parts are</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison of CE2P in various module settings on the validation set of LIP. The results are obtained without left-right flipping except for the last row. 'B' means baseline model. 'G', 'H' and 'E' denote global context, high resolution and edge perceiving module, respectively. 63.24 70.14 33.75 29.45 66.15 24.10 51.79 45.70 70.75 21.52 13.56 20.36 73.04 58.24 60.88 49.70 48.06 36.50 36.40 47.97 B+H 86.69 64.65 71.53 33.76 33.30 66.50 25.11 51.52 46.10 71.46 23.62 14.77 21.44 74.08 60.04 62.41 50.36 50.74 36.95 37.19 49.11 B+G 86.61 63.77 71.01 34.90 30.73 67.93 30.89 54.14 46.11 72.21 23.38 13.16 20.16 73.50 59.51 62.12 50.70 50.48 38.63 38.84 49.44 B+E 86.87 63.47 71.56 35.60 31.35 67.06 27.80 52.28 47.85 72.21 23.89 15.60 20.57 74.37 61.04 63.36 53.30 52.70 39.72 40.16 50.04 B+G+H 87.22 65.34 72.13 36.18 31.97 68.86 31.02 55.81 47.35 73.23 26.91 12.28 20.58 74.49 62.95 65.18 56.31 55.59 43.49 43.80 51.54 B+G+H+E (CE2P) 87.41 64.62 72.07 38.36 32.20 68.92 32.15 55.61 48.75 73.54 27.24 13.84 22.69 74.91 64.00 65.87 59.66 58.02 45.70 45.63 52.56 CE2P (Flipping) 87.67 65.29 72.54 39.09 32.73 69.46 32.52 56.28 49.67 74.11 27.23 14.19 22.51 75.50 65.14 66.59 60.10 58.59 46.63 46.12 53.10</figDesc><table><row><cell>Method</cell><cell>bkg</cell><cell>hat</cell><cell>hair</cell><cell>glove</cell><cell>glasses</cell><cell>u-cloth</cell><cell>dress</cell><cell>coat</cell><cell>socks</cell><cell>pants</cell><cell>j-suits</cell><cell>scarf</cell><cell>skirt</cell><cell>face</cell><cell>l-arm</cell><cell>r-arm</cell><cell>l-leg</cell><cell>r-leg</cell><cell>l-shoe</cell><cell>r-shoe</cell><cell>mIoU</cell></row><row><cell cols="10">B 86.08 correctly predicted of a certain person, guided with pixel-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>level IoU.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Implement Details</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">We implement the proposed framework in PyTorch (Paszke</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">et al. 2017) based on (Huang et al. 2018), and adopt ResNet-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">101 (He et al. 2016) as the backbone network. The input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">size of the image is 473Ã—473 during training and testing. We</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">adopt the similar training strategies with Deeplab (Chen et</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">al. 2018b), i.e. "Poly" learning rate policy with base learning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">rate 0.007. We fine-tune the networks for approximately 150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>epochs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of performance on the validation set of LIP with state-of-arts methods.To investigate the effectiveness of each module, we report the performance under several variants of CE2P in Tab. 1. We begin the experiment with a baseline model without any proposed modules. For our baseline, the prediction is directly performed on the final feature map extracted from ResNet-101. The resolution of the final feature map is 1/16 to the input size. The results are shown in Tab. 1, and we can see the baseline model reaches 47.97% accuracy. Some failure examples are shown in</figDesc><table><row><cell>Method</cell><cell cols="3">pixel acc. mean acc. mIoU</cell></row><row><cell>DeepLab (VGG-16)</cell><cell>82.66</cell><cell>51.64</cell><cell>41.64</cell></row><row><cell>Attention</cell><cell>83.43</cell><cell>54.39</cell><cell>42.92</cell></row><row><cell>DeepLab (ResNet-101)</cell><cell>84.09</cell><cell>55.62</cell><cell>44.80</cell></row><row><cell cols="2">JPPNet (Liang et al. 2018) 86.39</cell><cell>62.32</cell><cell>51.37</cell></row><row><cell>CE2P</cell><cell>87.37</cell><cell>63.20</cell><cell>53.10</cell></row><row><cell>Single Human Parsing</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the performance on the test set of single and multiple human parsing datasets. Here the subscript m in X m means the mean value of X.</figDesc><table><row><cell>Method</cell><cell cols="3">pixel acc. mean acc. mIoU</cell></row><row><cell cols="2">Single human parsing (Track 1)</cell><cell></cell><cell></cell></row><row><cell>ours</cell><cell>88.92</cell><cell>67.78</cell><cell>57.90</cell></row><row><cell>ours (single model)</cell><cell>88.24</cell><cell>67.29</cell><cell>56.50</cell></row><row><cell>JD BUPT</cell><cell>87.42</cell><cell>65.86</cell><cell>54.44</cell></row><row><cell>AttEdgeNet</cell><cell>87.40</cell><cell>67.17</cell><cell>54.17</cell></row><row><cell>Method</cell><cell>mIoU</cell><cell>AP r m</cell><cell>AP r 0.5</cell></row><row><cell cols="3">Multiple human parsing on CIHP (Track 2)</cell><cell></cell></row><row><cell>ours</cell><cell>63.77</cell><cell>45.31</cell><cell>50.94</cell></row><row><cell>DMNet(2nd place)</cell><cell>61.51</cell><cell>41.50</cell><cell>46.12</cell></row><row><cell>PGN (Gong et al. 2018)</cell><cell>55.80</cell><cell>33.60</cell><cell>35.80</cell></row><row><cell>Method</cell><cell>P CP0.5</cell><cell>AP p 0.5</cell><cell>AP p m</cell></row><row><cell cols="3">Multiple human parsing on MHP v2.0 (Track 5)</cell><cell></cell></row><row><cell>ours</cell><cell>41.82</cell><cell>33.34</cell><cell>42.25</cell></row><row><cell>S-LAB(2nd place)</cell><cell>38.27</cell><cell>31.47</cell><cell>40.71</cell></row><row><cell>NAN (Zhao et al. 2018)</cell><cell>32.25</cell><cell>25.14</cell><cell>41.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on MHP v2.0 validation dataset Mean AP p P CP0.5 Mean P CP</figDesc><table><row><cell>Method</cell><cell cols="2">mIoU AP r 0.5</cell><cell cols="2">Mean AP r AP p 0.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NAN (Zhao et al. 2018)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>24.83</cell><cell>42.77</cell><cell>34.37</cell><cell>-</cell></row><row><cell>Bg</cell><cell>38.25</cell><cell>27.32</cell><cell>24.96</cell><cell>21.63</cell><cell>37.13</cell><cell>32.59</cell><cell>34.94</cell></row><row><cell>B l 1</cell><cell>40.18</cell><cell>30.82</cell><cell>27.95</cell><cell>24.98</cell><cell>38.72</cell><cell>36.42</cell><cell>37.54</cell></row><row><cell>B l 2</cell><cell>40.30</cell><cell>30.75</cell><cell>28.04</cell><cell>24.46</cell><cell>38.60</cell><cell>36.09</cell><cell>37.46</cell></row><row><cell>B l 1 + Bg</cell><cell>40.48</cell><cell>30.63</cell><cell>27.79</cell><cell>29.62</cell><cell>40.52</cell><cell>39.29</cell><cell>38.46</cell></row><row><cell>B l 2 + Bg</cell><cell>40.52</cell><cell>30.67</cell><cell>27.89</cell><cell>29.29</cell><cell>40.56</cell><cell>39.13</cell><cell>38.50</cell></row><row><cell>B l 1 + B l 2</cell><cell>41.05</cell><cell>31.63</cell><cell>28.82</cell><cell>28.71</cell><cell>40.37</cell><cell>39.32</cell><cell>38.90</cell></row><row><cell>B l 1 + B l 2 + Bg(M-CE2P)</cell><cell>41.11</cell><cell>31.50</cell><cell>28.60</cell><cell>30.92</cell><cell>41.29</cell><cell>40.58</cell><cell>39.32</cell></row><row><cell>Bg with refinement</cell><cell>38.25</cell><cell>29.70</cell><cell>26.94</cell><cell>24.04</cell><cell>38.21</cell><cell>35.09</cell><cell>36.36</cell></row><row><cell>B l 1 with refinement</cell><cell>40.18</cell><cell>33.69</cell><cell>30.34</cell><cell>28.20</cell><cell>40.03</cell><cell>39.56</cell><cell>39.21</cell></row><row><cell>B l 2 with refinement</cell><cell>40.30</cell><cell>33.66</cell><cell>30.41</cell><cell>27.43</cell><cell>39.84</cell><cell>39.09</cell><cell>39.11</cell></row><row><cell>B l 1 + Bg with refinement</cell><cell>40.48</cell><cell>33.39</cell><cell>30.08</cell><cell>32.84</cell><cell>41.90</cell><cell>42.32</cell><cell>40.19</cell></row><row><cell>B l 2 + Bg with refinement</cell><cell>40.52</cell><cell>33.54</cell><cell>30.22</cell><cell>32.62</cell><cell>41.88</cell><cell>42.14</cell><cell>40.19</cell></row><row><cell>B l 1 + B l 2 with refinement</cell><cell>41.05</cell><cell>34.58</cell><cell>31.24</cell><cell>32.39</cell><cell>41.75</cell><cell>42.64</cell><cell>40.66</cell></row><row><cell cols="2">B l 1 + B l 2 + Bg(M-CE2P) with refinement 41.11</cell><cell>34.40</cell><cell>30.97</cell><cell>34.47</cell><cell>42.70</cell><cell>43.77</cell><cell>41.06</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<idno type="arXiv">arXiv:1802.02611</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3408" to="3415" />
		</imprint>
	</monogr>
	<note>A deformable mixture parsing model with parselets</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00157</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn</note>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. [Ladicky, Torr, and Zisserman</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3578" to="3585" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename><forename type="middle">;</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03612</idno>
		<title level="m">Holistic, instance-level human parsing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<idno type="arXiv">arXiv:1705.07206</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3193" to="3202" />
		</imprint>
	</monogr>
	<note>Towards real world human parsing: Multiple-human parsing in the wild</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="127" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look into person: Joint body parsing &amp; pose estimation network and A new benchmark. TPAMI</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
	<note>Refinenet: Multi-path refinement networks for highresolution semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Magic-wall: Visualizing room decoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="429" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelhamer</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<editor>NIPS-W. [Ronneberger, Fischer, and Brox</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint object and part segmentation using deep learned potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simo-Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1573" to="1581" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3570" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
	</analytic>
	<monogr>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Understanding humans in crowded scenes: Deep nested adversarial learning and A new benchmark for multi-human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03287</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
