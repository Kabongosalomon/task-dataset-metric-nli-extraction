<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
							<email>riejohnson@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<email>tzhang@stat.rutgers.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">RJ Research Consulting Tarrytown</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b14">[15]</ref> are neural networks that can make use of the internal structure of data such as the 2D structure of image data through convolution layers, where each computation unit responds to a small region of input data (e.g., a small square of a large image). On text, CNN has been gaining attention, used in systems for tagging, entity search, sentence modeling, and so on <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref>, to make use of the 1D structure (word order) of text data. Since CNN was originally developed for image data, which is fixed-sized, low-dimensional and dense, without modification it cannot be applied to text documents, which are variable-sized, highdimensional and sparse if represented by sequences of one-hot vectors. In many of the CNN studies on text, therefore, words in sentences are first converted to low-dimensional word vectors. The word vectors are often obtained by some other method from an additional large corpus, which is typically done in a fashion similar to language modeling though there are many variations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Use of word vectors obtained this way is a form of semi-supervised learning and leaves us with the following questions. Q1. How effective is CNN on text in a purely supervised setting without the aid of unlabeled data? Q2. Can we use unlabeled data with CNN more effectively than using general word vector learning methods? Our recent study <ref type="bibr" target="#b10">[11]</ref> addressed Q1 on text categorization and showed that CNN without a word vector layer is not only feasible but also beneficial when not aided by unlabeled data. Here we address Q2 also on text categorization: building on [11], we propose a new semi-supervised framework that learns embeddings of small text regions (instead of words) from unlabeled data, for use in a supervised CNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>like phrases, or regions of size 20 like sentences), eliminating the extra layer for word vector conversion. This direct learning of region embedding was noted to have the merit of higher accuracy with a simpler system (no need to tune hyper-parameters for word vectors) than supervised word vector-based CNN in which word vectors are randomly initialized and trained as part of CNN training. Moreover, the performance of <ref type="bibr" target="#b10">[11]</ref>'s best CNN rivaled or exceeded the previous best results on the benchmark datasets.</p><p>Motivated by this finding, we seek effective use of unlabeled data for text categorization through direct learning of embeddings of text regions. Our new semi-supervised framework learns a region embedding from unlabeled data and uses it to produce additional input (additional to one-hot vectors) to supervised CNN, where a region embedding is trained with labeled data. Specifically, from unlabeled data, we learn tv-embeddings ('tv' stands for 'two-view'; defined later) of a text region through the task of predicting its surrounding context. According to our theoretical finding, a tv-embedding has desirable properties under ideal conditions on the relations between two views and the labels. While in reality the ideal conditions may not be perfectly met, we consider them as guidance in designing the tasks for tv-embedding learning.</p><p>We consider several types of tv-embedding learning task trained on unlabeled data; e.g., one task is to predict the presence of the concepts relevant to the intended task (e.g., 'desire to recommend the product') in the context, and we indirectly use labeled data to set up this task. Thus, we seek to learn tv-embeddings useful specifically for the task of interest. This is in contrast to the previous word vector/embedding learning methods, which typically produce a word embedding for general purposes so that all aspects (e.g., either syntactic or semantic) of words are captured. In a sense, the goal of our region embedding learning is to map text regions to high-level concepts relevant to the task. This cannot be done by word embedding learning since individual words in isolation are too primitive to correspond to high-level concepts. For example, "easy to use" conveys positive sentiment, but "use" in isolation does not. We show that our models with tv-embeddings outperform the previous best results on sentiment classification and topic classification. Moreover, a more direct comparison confirms that our region tv-embeddings provide more compact and effective representations of regions for the task of interest than what can be obtained by manipulation of a word embedding.</p><p>1.1 Preliminary: one-hot CNN for text categorization <ref type="bibr" target="#b10">[11]</ref> A CNN is a feed-forward network equipped with convolution layers interleaved with pooling layers. A convolution layer consists of computation units, each of which responds to a small region of input (e.g., a small square of an image), and the small regions collectively cover the entire data. A computation unit associated with the -th region of input x computes:</p><formula xml:id="formula_0">σ(W · r (x) + b) ,<label>(1)</label></formula><p>where r (x) ∈ R q is the input region vector that represents the -th region. Weight matrix W ∈ R m×q and bias vector b ∈ R m are shared by all the units in the same layer, and they are learned through training. In <ref type="bibr" target="#b10">[11]</ref>, input x is a document represented by one-hot vectors ( <ref type="figure">Figure 1)</ref>; therefore, we call <ref type="bibr" target="#b10">[11]</ref>'s CNN one-hot CNN; r (x) can be either a concatenation of one-hot vectors, a bag-ofword vector (bow), or a bag-of-n-gram vector: e.g., for a region "love it"</p><formula xml:id="formula_1">I it love I it love r (x) =[ 0 0 1 | 0 1 0 ] (concatenation)<label>(2)</label></formula><formula xml:id="formula_2">I it love r (x) =[ 0 1 1 ] (bow)<label>(3)</label></formula><p>The bow representation (3) loses word order within the region but is more robust to data sparsity, enables a large region size such as 20, and speeds up training by having fewer parameters. This is what we mainly use for embedding learning from unlabeled data. CNN with (2) is called seq-CNN and CNN with (3) bow-CNN. The region size and stride (distance between the region centers) are meta-parameters. Note that we used a tiny three-word vocabulary for the vector examples above to save space, but a vocabulary of typical applications could be much larger. σ in (1) is a componentwise non-linear function (e.g., applying σ(x) = max(x, 0) to each vector component). Thus, each computation unit generates an m-dimensional vector where m is the number of weight vectors (W's rows) or neurons. In other words, a convolution layer embodies an embedding of text regions, which produces an m-dim vector for each text region. In essence, a region embedding uses co-presence and absence of words in a region as input to produce predictive features, e.g., if presence of "easy  to use" with absence of "not" is a predictive indicator, it can be turned into a large feature value by having a negative weight on "not" (to penalize its presence) and positive weights on the other three words in one row of W. A more formal argument can be found in the Appendix. The m-dim vectors from all the text regions of each document are aggregated by the pooling layer, by either component-wise maximum (max-pooling) or average (average-pooling), and used by the top layer (a linear classifier) as features for classification. Here we focused on the convolution layer; for other details, <ref type="bibr" target="#b10">[11]</ref> should be consulted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Semi-supervised CNN with tv-embeddings for text categorization</head><p>It was shown in <ref type="bibr" target="#b10">[11]</ref> that one-hot CNN is effective on text categorization, where the essence is direct learning of an embedding of text regions aided by new options of input region vector representation. We go further along this line and propose a semi-supervised learning framework that learns an embedding of text regions from unlabeled data and then integrates the learned embedding in supervised training. The first step is to learn an embedding with the following property.</p><formula xml:id="formula_3">Definition 1 (tv-embedding). A function f 1 is a tv-embedding of X 1 w.r.t. X 2 if there exists a function g 1 such that P (X 2 |X 1 ) = g 1 (f 1 (X 1 ), X 2 ) for any (X 1 , X 2 ) ∈ X 1 × X 2 .</formula><p>A tv-embedding ('tv' stands for two-view) of a view (X 1 ), by definition, preserves everything required to predict another view (X 2 ), and it can be trained on unlabeled data. The motivation of tvembedding is our theoretical finding (formalized in the Appendix) that, essentially, a tv-embedded feature vector f 1 (X 1 ) is as useful as X 1 for the purpose of classification under ideal conditions. The conditions essentially state that there exists a set H of hidden concepts such that two views and labels of the classification task are related to each other only through the concepts in H. The concepts in H might be, for example, "pricey", "handy", "hard to use", and so on for sentiment classification of product reviews. While in reality the ideal conditions may not be completely met, we consider them as guidance and design tv-embedding learning accordingly.</p><p>Tv-embedding learning is related to two-view feature learning <ref type="bibr" target="#b1">[2]</ref> and ASO <ref type="bibr" target="#b0">[1]</ref>, which learn a linear embedding from unlabeled data through tasks such as predicting a word (or predicted labels) from the features associated with its surrounding words. These studies were, however, limited to a linear embedding. A related method in <ref type="bibr" target="#b5">[6]</ref> learns a word embedding so that left context and right context maximally correlate in terms of canonical correlation analysis. While we share with these studies the general idea of using the relations of two views, we focus on nonlinear learning of region embeddings useful for the task of interest, and the resulting methods are very different. An important difference of tv-embedding learning from co-training is that it does not involve label guessing, thus avoiding risk of label contamination. <ref type="bibr" target="#b7">[8]</ref> used a Stacked Denoising Auto-encoder to extract features invariant across domains for sentiment classification from unlabeled data. It is for fully-connected neural networks, which underperformed CNNs in <ref type="bibr" target="#b10">[11]</ref>. Now let B be the base CNN model for the task of interest, and assume that B has one convolution layer with region size p. Note, however, that the restriction of having only one convolution layer is merely for simplifying the description. We propose a semi-supervised framework with the following two steps. 1. Tv-embedding learning: Train a neural network U to predict the context from each region of size p so that U's convolution layer generates feature vectors for each text region of size p for use in the classifier in the top layer. It is this convolution layer, which embodies the tv-embedding, that we transfer to the supervised learning model in the next step. (Note that U differs from CNN in that each small region is associated with its own target/output.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Final supervised learning:</head><p>Integrate the learned tv-embedding (the convolution layer of U) into B, so that the tv-embedded regions (the output of U's convolution layer) are used as an additional input to B's convolution layer. Train this final model with labeled data. These two steps are described in more detail in the next two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning tv-embeddings from unlabeled data</head><p>We create a task on unlabeled data to predict the context (adjacent text regions) from each region of size p defined in B's convolution layer. To see the correspondence to the definition of tv-embeddings, it helps to consider a sub-task that assigns a label (e.g., positive/negative) to each text region (e.g., ", fun plot") instead of the ultimate task of categorizing the entire document. This is sensible because CNN makes predictions by building up from these small regions. In a document "good acting, fun plot :)" as in <ref type="figure" target="#fig_4">Figure 2</ref>, the clues for predicting a label of ", fun plot" are ", fun plot" itself (view-1: X 1 ) and its context "good acting" and ":)" (view-2: X 2 ). U is trained to predict X 2 from X 1 , i.e., to approximate P (X 2 |X 1 ) by g 1 (f 1 (X 1 ), X 2 )) as in Definition 1, and functions f 1 and g 1 are embodied by the convolution layer and the top layer, respectively.</p><p>Given a document x, for each text region indexed by , U's convolution layer computes:</p><formula xml:id="formula_4">u (x) = σ (U ) W (U ) · r (U ) (x) + b (U ) ,<label>(4)</label></formula><p>which is the same as (1) except for the superscript "(U)" to indicate that these entities belong to U.</p><p>The top layer (a linear model for classification) uses u (x) as features for prediction. W (U ) and b (U ) (and the top-layer parameters) are learned through training. The input region vector representation</p><formula xml:id="formula_5">r (U ) (x) can be either sequential, bow, or bag-of-n-gram, independent of r (x) in B.</formula><p>The goal here is to learn an embedding of text regions (X 1 ), shared with all the text regions at every location. Context (X 2 ) is used only in tv-embedding learning as prediction target (i.e., not transferred to the final model); thus, the representation of context should be determined to optimize the final outcome without worrying about the cost at prediction time. Our guidance is the conditions on the relationships between the two views mentioned above; ideally, the two views should be related to each other only through the relevant concepts. We consider the following two types of target/context representation.</p><p>Unsupervised target A straightforward vector encoding of context/target X 2 is bow vectors of the text regions on the left and right to X 1 . If we distinguish the left and right, the target vector is 2|V |-dimensional with vocabulary V , and if not, |V |-dimensional. One potential problem of this encoding is that adjacent regions often have syntactic relations (e.g., "the" is often followed by an adjective or a noun), which are typically irrelevant to the task (e.g., to identify positive/negative sentiment) and therefore undesirable. A simple remedy we found effective is vocabulary control of context to remove function words (or stop-words if available) from (and only from) the target vocabulary.</p><p>Partially-supervised target Another context representation that we consider is partially supervised in the sense that it uses labeled data. First, we train a CNN with the labeled data for the intended task and apply it to the unlabeled data. Then we discard the predictions and only retain the internal output of the convolution layer, which is an m-dimensional vector for each text region where m is the number of neurons. We use these m-dimensional vectors to represent the context. <ref type="bibr" target="#b10">[11]</ref> has shown, by examples, that each dimension of these vectors roughly represents concepts relevant to the task, e.g., 'desire to recommend the product', 'report of a faulty product', and so on. Therefore, an advantage of this representation is that there is no obvious noise between X 1 and X 2 since context X 2 is represented only by the concepts relevant to the task. A disadvantage is that it is only as good as the supervised CNN that produced it, which is not perfect and in particular, some relevant concepts would be missed if they did not appear in the labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Final supervised learning: integration of tv-embeddings into supervised CNN</head><p>We use the tv-embedding obtained from unlabeled data to produce additional input to B's convolution layer, by replacing σ (W · r (x) + b) (1) with:</p><formula xml:id="formula_6">σ (W · r (x) + V · u (x) + b) ,<label>(5)</label></formula><p>where u (x) is defined by (4), i.e., u (x) is the output of the tv-embedding applied to the -th region. We train this model with the labeled data of the task; that is, we update the weights W, V, bias b, and the top-layer parameters so that the designated loss function is minimized on the labeled training data. W (U ) and b (U ) can be either fixed or updated for fine-tuning, and in this work we fix them for simplicity.</p><p>Note that while (5) takes a tv-embedded region as input, <ref type="bibr" target="#b4">(5)</ref> itself is also an embedding of text regions; let us call it (and also <ref type="formula" target="#formula_0">(1)</ref>) a supervised embedding, as it is trained with labeled data, to distinguish it from tv-embeddings. That is, we use tv-embeddings to improve the supervised embedding. Note that (5) can be naturally extended to accommodate multiple tv-embeddings by</p><formula xml:id="formula_7">σ W · r (x) + k i=1 V (i) · u (i) (x) + b ,<label>(6)</label></formula><p>so that, for example, two types of tv-embedding (i.e., k = 2) obtained with the unsupervised target and the partially-supervised target can be used at once, which can lead to performance improvement as they complement each other, as shown later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our code and the experimental settings are available at riejohnson.com/cnn download.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>We used the three datasets used in <ref type="bibr" target="#b10">[11]</ref>: IMDB, Elec, and RCV1, as summarized in <ref type="table" target="#tab_1">Table  1</ref>. IMDB (movie reviews) <ref type="bibr" target="#b16">[17]</ref> comes with an unlabeled set. To facilitate comparison with previous studies, we used a union of this set and the training set as unlabeled data. Elec consists of Amazon reviews of electronics products. To use as unlabeled data, we chose 200K reviews from the same data source so that they are disjoint from the training and test sets, and that the reviewed products are disjoint from the test set. On the 55-way classification of the second-level topics on RCV1 (news), unlabeled data was chosen to be disjoint from the training and test sets. On the multi-label categorization of 103 topics on RCV1, since the official LYRL04 split for this task divides the entire corpus into a training set and a test set, we used the entire test set as unlabeled data (the transductive learning setting).  Implementation We used the one-layer CNN models found to be effective in <ref type="bibr" target="#b10">[11]</ref> as our base models B, namely, seq-CNN on IMDB/Elec and bow-CNN on RCV1. Tv-embedding training mini- <ref type="bibr" target="#b1">2</ref> where i goes through the regions, z represents the target regions, and p is the model output. The weights α i,j were set to balance the loss originating from the presence and absence of words (or concepts in case of the partially-supervised target) and to speed up training by eliminating some negative examples, similar to negative sampling of <ref type="bibr" target="#b18">[19]</ref>. To experiment with the unsupervised target, we set z to be bow vectors of adjacent regions on the left and right, while only retaining the 30K most frequent words with vocabulary control; on sentiment classification, function words were removed, and on topic classification, numbers and stop-words provided by <ref type="bibr" target="#b15">[16]</ref> were removed. Note that these words were removed from (and only from) the target vocabulary. To produce the partially-supervised target, we first trained the supervised CNN models with 1000 neurons and applied the trained convolution layer to unlabeled data to generate 1000-dimensional vectors for each region. The rest of implementation follows <ref type="bibr" target="#b10">[11]</ref>; i.e., supervised models minimized square loss with L 2 regularization and optional dropout <ref type="bibr" target="#b8">[9]</ref>; σ and σ (U ) were the rectifier; response normalization was performed; optimization was done by SGD.</p><formula xml:id="formula_8">mized weighted square loss i,j α i,j (z i [j] − p i [j])</formula><p>Model selection On all the tested methods, tuning of meta-parameters was done by testing the models on the held-out portion of the training data, and then the models were re-trained with the chosen meta-parameters using the entire training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance results</head><p>Overview After confirming the effectiveness of our new models in comparison with the supervised CNN, we report the performances of <ref type="bibr" target="#b12">[13]</ref>'s CNN, which relies on word vectors pre-trained with a very large corpus <ref type="table" target="#tab_5">(Table 3)</ref>. Besides comparing the performance of approaches as a whole, it is also of interest to compare the usefulness of what was learned from unlabeled data; therefore, we show how it performs if we integrate the word vectors into our base model one-hot CNNs ( <ref type="figure">Figure  3</ref>). In these experiments we also test word vectors trained by word2vec <ref type="bibr" target="#b18">[19]</ref> on our unlabeled data <ref type="figure" target="#fig_3">(Figure 4</ref>). We then compare our models with two standard semi-supervised methods, transductive SVM (TSVM) <ref type="bibr" target="#b9">[10]</ref> and co-training <ref type="table" target="#tab_5">(Table 3)</ref>, and with the previous best results in the literature <ref type="table" target="#tab_6">(Tables 4-6</ref>). In all comparisons, our models outperform the others. In particular, our region tvembeddings are shown to be more compact and effective than region embeddings obtained by simple manipulation of word embeddings, which supports our approach of using region embedding instead of word embedding. names in <ref type="table" target="#tab_5">Table 3</ref>   Our CNN with tv-embeddings We tested three types of tv-embedding as summarized in <ref type="table">Table  2</ref>. The first thing to note is that all of our CNNs ( <ref type="table" target="#tab_5">Table 3</ref>, row 6-12) outperform their supervised counterpart in row 4. This confirms the effectiveness of the framework we propose. In <ref type="table" target="#tab_5">Table 3</ref>, for meaningful comparison, all the CNNs are constrained to have exactly one convolution layer (except for <ref type="bibr" target="#b12">[13]</ref>'s CNN) with 1000 neurons. The best-performing supervised CNNs within these constraints (row 4) are: seq-CNN (region size 3) on IMDB and Elec and bow-CNN (region size 20) on RCV1 1 . They also served as our base models B (with region size parameterized on IMDB/Elec). More complex supervised CNNs from <ref type="bibr" target="#b10">[11]</ref> will be reviewed later. On sentiment classification (IMDB and Elec), the region size chosen by model selection for our models was 5, larger than 3 for the supervised CNN. This indicates that unlabeled data enabled effective use of larger regions which are more predictive but might suffer from data sparsity in supervised settings. 'unsup3-tv.' (rows 10-11) uses a bag-of-n-gram vector to initially represent each region, thus, retains word order partially within the region. When used individually, unsup3-tv. did not outperform the other tv-embeddings, which use bow instead (rows 6-9). But we found that it contributed to error reduction when combined with the others (not shown in the table). This implies that it learned from unlabeled data predictive information that the other two embeddings missed. The best performances (row 12) were obtained by using all the three types of tv-embeddings at once according to <ref type="bibr" target="#b5">(6)</ref>. By doing so, the error rates were improved by nearly 1.9% (IMDB) and 1.4% (Elec and RCV1) compared with the supervised CNN (row 4), as a result of the three tv-embeddings with different strengths complementing each other.   [13]'s CNN It was shown in <ref type="bibr" target="#b12">[13]</ref> that CNN that uses the Google News word vectors as input is competitive on a number of sentence classification tasks. These vectors (300-dimensional) were trained by the authors of word2vec <ref type="bibr" target="#b18">[19]</ref> on a very large Google News (GN) corpus (100 billion words; 500-5K times larger than our unlabeled data). <ref type="bibr" target="#b12">[13]</ref> argued that these vectors can be useful for various tasks, serving as 'universal feature extractors'. We tested <ref type="bibr" target="#b12">[13]</ref>'s CNN, which is equipped with three convolution layers with different region sizes (3, 4, and 5) and max-pooling, using the GN vectors as input. Although <ref type="bibr" target="#b12">[13]</ref> used only 100 neurons for each layer, we changed it to 400, 300, and 300 to match the other models, which use 1000 neurons. Our models clearly outperform these models <ref type="table" target="#tab_5">(Table 3</ref>, row 3) with relatively large differences.</p><p>Comparison of embeddings Besides comparing the performance of the approaches as a whole, it is also of interest to compare the usefulness of what was learned from unlabeled data. For this purpose, we experimented with integration of a word embedding into our base models using two methods; one takes the concatenation, and the other takes the average, of word vectors for the words in the region. These provide additional input to the supervised embedding of regions in place of u (x) in <ref type="bibr" target="#b4">(5)</ref>. That is, for comparison, we produce a region embedding from a word embedding to replace a region tv-embedding. We show the results with two types of word embeddings: the GN word embedding above <ref type="figure">(Figure 3)</ref>, and word embeddings that we trained with the word2vec software on our unlabeled data, i.e., the same data as used for tv-embedding learning and all others <ref type="figure" target="#fig_3">(Figure 4</ref>). Note that <ref type="figure" target="#fig_3">Figure 4</ref> plots error rates in relation to the dimensionality of the produced additional input; a smaller dimensionality has an advantage of faster training/prediction.</p><p>On the results, first, the region tv-embedding is more useful for these tasks than the tested word embeddings since the models with a tv-embedding clearly outperform all the models with a word embedding. Word vector concatenations of much higher dimensionality than those shown in the figure still underperformed 100-dim region tv-embedding. Second, since our region tv-embedding takes the form of σ(W · r (x) + b) with r (x) being a bow vector, the columns of W correspond to words, and therefore, W · r (x) is the sum of W's columns whose corresponding words are in the -th region. Based on that, one might wonder why we should not simply use the sum or average of word vectors obtained by an existing tool such as word2vec instead. The suboptimal performances of 'w: average' <ref type="figure" target="#fig_3">(Figure 4</ref>) tells us that this is a bad idea. We attribute it to the fact that region embeddings learn predictiveness of co-presence and absence of words in a region; a region embedding can be more expressive than averaging of word vectors. Thus, an effective and compact region embedding cannot be trivially obtained from a word embedding. In particular, effectiveness of the combination of three tv-embeddings ('r: 3 tv-embed.' in <ref type="figure" target="#fig_3">Figure 4</ref>) stands out.</p><p>Additionally, our mechanism of using information from unlabeled data is more effective than <ref type="bibr" target="#b12">[13]</ref>'s CNN since our CNNs with GN ( <ref type="figure">Figure 3</ref>) outperform <ref type="bibr" target="#b12">[13]</ref>'s CNNs with GN (    ( <ref type="table" target="#tab_5">Table 3</ref>, rows 1-2). Since co-training is a meta-learner, it can be used with CNN. Random split of vocabulary and split into the first and last half of each document were tested. To reduce the computational burden, we report the best (and unrealistic) co-training performances obtained by optimizing the meta-parameters including when to stop on the test data. Even with this unfair advantage to cotraining, co-training <ref type="table" target="#tab_5">(Table 3</ref>, row 5) clearly underperformed our models. The results demonstrate the difficulty of effectively using unlabeled data on these tasks, given that the size of the labeled data is relatively large.</p><p>Comparison with the previous best results We compare our models with the previous best results on IMDB <ref type="table" target="#tab_6">(Table 4</ref>). Our best model with three tv-embeddings outperforms the previous best results by nearly 0.9%. All of our models with a single tv-embed. <ref type="table" target="#tab_5">(Table 3</ref>, row 6-11) also perform better than the previous results. Since Elec is a relatively new dataset, we are not aware of any previous semi-supervised results. Our performance is better than <ref type="bibr" target="#b10">[11]</ref>'s best supervised CNN, which has a complex network architecture of three convolution-pooling pairs in parallel <ref type="table" target="#tab_7">(Table 5</ref>). To compare with the benchmark results in <ref type="bibr" target="#b15">[16]</ref>, we tested our model on the multi-label task with the LYRL04 split <ref type="bibr" target="#b15">[16]</ref> on RCV1, in which more than one out of 103 categories can be assigned to each document. Our model outperforms the best SVM of <ref type="bibr" target="#b15">[16]</ref> and the best supervised CNN of <ref type="bibr" target="#b10">[11]</ref>  <ref type="table" target="#tab_8">(Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposed a new semi-supervised CNN framework for text categorization that learns embeddings of text regions with unlabeled data and then labeled data. As discussed in Section 1.1, a region embedding is trained to learn the predictiveness of co-presence and absence of words in a region. In contrast, a word embedding is trained to only represent individual words in isolation. Thus, a region embedding can be more expressive than simple averaging of word vectors in spite of their seeming similarity. Our comparison of embeddings confirmed its advantage; our region tvembeddings, which are trained specifically for the task of interest, are more effective than the tested word embeddings. Using our new models, we were able to achieve higher performances than the previous studies on sentiment classification and topic classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Theory of tv-embedding</head><p>Suppose that we observe two views (X 1 , X 2 ) ∈ X 1 × X 2 of the input, and a target label Y ∈ Y of interest, where X 1 and X 2 are finite discrete sets.</p><p>Assumption 1. Assume that there exists a set of hidden states H such that X 1 , X 2 , and Y are conditionally independent given h in H, and that the rank of matrix [P (X 1 , X 2 )] is |H|.</p><p>Theorem 1. Consider a tv-embedding f 1 of X 1 w.r.t. X 2 . Under Assumption 1, there exists a function q 1 such that P (Y |X 1 ) = q 1 (f 1 (X 1 ), Y ). Further consider a tv-embedding f 2 of X 2 w.r.t. X 1 . Then, under Assumption 1, there exists a function q such that P (Y |X 1 , X 2 ) = q(f 1 (X 1 ), f 2 (X 2 ), Y ).</p><p>Proof. First, assume that X 1 contains d 1 elements, and X 2 contains d 2 elements, and |H| = k. The independence and rank condition in Assumption 1 implies the decomposition</p><formula xml:id="formula_9">P (X 2 |X 1 ) = h∈H P (X 2 |h)P (h|X 1 )</formula><p>is of rank k if we consider P (X 2 |X 1 ) as a d 2 × d 1 matrix (which we denote by A). Now we may also regard P (X 2 |h) as a d 2 × k matrix (which we denote by B), and P (h|X 1 ) as a k × d 1 matrix (which we denote by C). From the matrix equation</p><formula xml:id="formula_10">A = BC, we obtain C = (B B) −1 B A. Consider the k × d 2 matrix U = (B B) −1 B</formula><p>. Then we know that its elements correspond to a function of (h, X 2 ) ∈ H × X 2 . Therefore the relationship C = UA implies that there exists a function u(h, X 2 ) such that</p><formula xml:id="formula_11">∀h ∈ H : P (h|X 1 ) = X2∈X2 P (X 2 |X 1 )u(h, X 2 ).</formula><p>Using the definition of embedding in Definition 1, we obtain</p><formula xml:id="formula_12">P (h|X 1 ) = X2∈X2 g 1 (f 1 (X 1 ), X 2 )u(h, X 2 ).</formula><p>Define t 1 (a 1 , h) = X2 g 1 (a 1 , X 2 )u(h, X 2 ), then for any h ∈ H we have</p><formula xml:id="formula_13">P (h|X 1 ) = t 1 (f 1 (X 1 ), h).<label>(7)</label></formula><p>Similarly, there exists a function t 2 (a 2 , h) such that for any h ∈ H</p><formula xml:id="formula_14">P (h|X 2 ) = t 2 (f 2 (X 2 ), h).<label>(8)</label></formula><p>Observe that</p><formula xml:id="formula_15">P (Y |X 1 ) = h∈H P (Y, h|X 1 ) = h∈H P (h|X 1 )P (Y |h, X 1 ) = h∈H P (h|X 1 )P (Y |h) = h∈H t 1 (f 1 (X 1 ), h)P (Y |h)</formula><p>where the third equation has used the assumption that Y is independent of X 1 given h and the last equation has used <ref type="bibr" target="#b6">(7)</ref>. By defining q 1 (a 1 , Y ) = h∈H t 1 (a 1 , h)P (Y |h), we obtain P (Y |X 1 ) = q 1 (f 1 (X 1 ), Y ), as desired.</p><p>Further observe that</p><formula xml:id="formula_16">P (Y |X 1 , X 2 ) = h∈H P (Y, h|X 1 , X 2 ) = h∈H P (h|X 1 , X 2 )P (Y |h, X 1 , X 2 ) = h∈H P (h|X 1 , X 2 )P (Y |h),<label>(9)</label></formula><p>where the last equation has used the assumption that Y is independent of X 1 and X 2 given h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that</head><formula xml:id="formula_17">P (h|X 1 , X 2 ) = P (h, X 1 , X 2 ) P (X 1 , X 2 ) = P (h, X 1 , X 2 ) h ∈H P (h , X 1 , X 2 ) = P (h)P (X 1 |h)P (X 2 |h) h ∈H P (h )P (X 1 |h )P (X 2 |h ) = P (h, X 1 )P (h, X 2 )/P (h) h ∈H P (h , X 1 )P (h , X 2 )/P (h ) = P (h|X 1 )P (h|X 2 )/P (h) h ∈H P (h |X 1 )P (h |X 2 )/P (h ) = t 1 (f 1 (X 1 ), h)t 2 (f 2 (X 2 ), h)/P (h) h ∈H t 1 (f 1 (X 1 ), h )t 2 (f 2 (X 2 ), h )/P (h ) ,</formula><p>where the third equation has used the assumption that X 1 is independent of X 2 given h, and the last equation has used <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. The last equation means that P (h|X 1 , X 2 ) is a function of (f 1 (X 1 ), f 2 (X 2 ), h). That is, there exists a functiont such that P (h|X 1 , X 2 ) = t(f 1 (X 1 ), f 2 (X 2 ), h). From <ref type="bibr" target="#b8">(9)</ref>, this implies that</p><formula xml:id="formula_18">P (Y |X 1 , X 2 ) = h∈Ht (f 1 (X 1 ), f 2 (X 2 ), h)P (Y |h).</formula><p>Now the theorem follows by defining q(a 1 , a 2 , Y ) = h∈Ht (a 1 , a 2 , h)P (Y |h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Representation Power of Region Embedding</head><p>We provide some formal definitions and theoretical arguments to support the effectiveness of the type of region embedding experimented with in the main text.</p><p>A text region embedding is a function that maps a region of text (a sequence of two or more words) into a numerical vector. The particular form of region embedding we consider takes either sequential or bow representation of the text region as input. More precisely, consider a language with vocabulary V . Each word w in the language is taken from V , and can be represented as a |V | dimensional vector referred to as one-hot-vector representation. Each of the |V | vector components represents a vocabulary entry. The vector representation of w has value one for the component corresponding to the word, and value zeros elsewhere. A text region of size m is a sequence of m words (w 1 , w 2 , . . . , w m ), where each word w i ∈ V . It can be represented as a m|V | dimensional vector, which is a concatenation of vector representations of the words, as in <ref type="formula" target="#formula_1">(2)</ref> in Section 1.1 of the main text. Here we call this representation seq-representation. An alternative is the bow-representation as in <ref type="formula" target="#formula_2">(3)</ref> of the main text.</p><p>Let R m be the set of all possible text regions of size m in the seq-representation (or alternatively, bow-representation). We consider embeddings of a text region x ∈ R m in the form of</p><formula xml:id="formula_19">(Wx + b) + = max(0, Wx + b) .</formula><p>The embedding matrix W and bias vector b are learned by training, and the training objective depends on the task. In the following, this particular form of region embedding is referred to as RETEX (Region Embedding of TEXt), and the vectors produced by RETEX or the results of RETEX are referred to as RETEX vectors.</p><p>The goal of region embedding learning is to map high-level concepts (relevant to the task of interest) to low-dimensional vectors. As said in the main text, this cannot be done by word embedding learning since a word embedding embeds individual words in isolation (i.e., word-i is mapped to vector-i irrespective of its context), which are too primitive to correspond to high-level concepts. For example, "easy to use" conveys positive sentiment, but "use" in isolation does not. Through the analysis of the representation power of RETEX, we show that unlike word embeddings, RETEX can model high-level concepts by using co-presence and absence of words in the region, which is similar to the traditional use of m-grams but more efficient/robust. </p><formula xml:id="formula_20">= v (Wx + b) + .</formula><p>The proof essentially constructs the indicator functions of all the m-grams (text regions of size m) in R m and maps them to the corresponding function values. Thus, the representation power of RETEX is at least as good as m-grams, and more powerful than the sum of word embeddings in spite of the seeming similarity in form. However, it is well known that the traditional m-gram-based approaches, which assign one vector dimension per m-gram, can suffer from the data sparsity problem because an m-gram is useful only if it is seen in the training data. This is where RETEX can have clear advantages. We show below that it can map similar m-grams (similar w.r.t. the training objective) to similar lower-dimensional vectors, which helps learning the task of interest. It is also more expressive than the traditional m-gram-based approaches because it can map not only co-presence but also absence of words (which m-gram cannot express concisely) into a single dimension. These properties lead to robustness to data sparsity.</p><p>We first introduce a definition of a simple concept. The following proposition shows that RETEX can embed concepts that are unions of simple concepts into low-dimensional vectors. Proposition 3. If C ⊂ R m is the union of q simple concepts C 1 , . . . , C q , then there exists a function f (x) that is the linear function of q-dimensional RETEX vectors so that x ∈ C if and only if f (x) &gt; 0.</p><p>Proof. Let b ∈ R q , and let W have q rows, so that I(x ∈ C i ) = (W i,· x + b i ) + for each row i, as constructed in the proof of Proposition 2. Let v = [1, . . . , 1] ∈ R q . Then f (x) = v (Wx + b) + is a function of the desired property.</p><p>Note that q can be much smaller than the number of m-grams in concept C. Proposition 3 shows that RETEX has the ability to simultaneously make use of word similarity (via word groups) and the fact that words occur in the context, to reduce the embedding dimension. A word embedding can model word similarity but does not model context. m-gram-based approaches can model context but cannot model word similarity -which means a concept/context has to be expressed with a large number of individual m-grams, leading to the data sparsity problem. Thus, the representation power of RETEX exceeds that of single-word embedding and traditional m-gram-based approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :X 1 …Figure 2 :</head><label>112</label><figDesc>One-hot CNN example. Region size 2, stride 1. Tv-embedding learning by training to predict adjacent regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Region tv-embeddings vs. word2vec word embeddings. Trained on our unlabeled data. x-axis: dimensionality of the additional input to supervised region embedding. 'r:': region, 'w:': word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 2 .</head><label>2</label><figDesc>Consider R m of the seq-representation. A high level semantic concept C ⊂ R m is called simple if it can be defined as follows. Let V 1 , . . . , V m ⊂ V be m word groups (each word group may either represent a set of similar words or the absent of certain words), and s 1 , . . . , s m ⊂ {±1} be signs. Define C such that x ∈ C if and only if the i-th word in x either belongs toV i (if s i = 1) or ¬V i (if s i = −1).The next proposition illustrates the points above by stating that RETEX has the ability to represent a simple concept (defined above via the notion of similar words) by a single dimension. This is in contrast to the construction in the proof of Proposition 1, where one dimension could represent only one m-gram.Proposition 2. The indicator function of any simple concept C can be embedded into one dimension using RETEX. Proof. Consider a text region vector x ∈ R m in seq-representation that contains m of |V |dimensional segments, where the i-th segment represents the i-th position in the text region. Let the i-th segment of w be a vector of zeros except for those components in V i being s i . Let b = 1 − m i=1 (s i + 1)/2. Then it is not difficult to check that I(x ∈ C) = (w x + b) + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets. †The multi-label RCV1 is used only inTable 6.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :Table 3 :</head><label>23</label><figDesc>Tested tv-embeddings. Error rates (%). For comparison, all the CNN models were constrained to have 1000 neurons. The parentheses around the error rates indicate that co-training meta-parameters were tuned on test data.</figDesc><table><row><cell>IMDB</cell><cell>Elec RCV1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>, row 3). This is because in our model, one-hot vectors (the original features) compensate for potential information loss in the embedding learned from unlabeled data. This, as well as region-vs-word embedding, is a major difference between our model and<ref type="bibr" target="#b12">[13]</ref>'s model.Standard semi-supervised methods Many of the standard semi-supervised methods are not ap-</figDesc><table><row><cell>NB-LM 1-3grams [18]</cell><cell>8.13</cell><cell>-</cell></row><row><cell>[11]'s best CNN</cell><cell>7.67</cell><cell>-</cell></row><row><cell>Paragraph vectors [14]</cell><cell>7.46</cell><cell>Unlab.data</cell></row><row><cell cols="3">Ensemble of 3 models [18] 7.43 Ens.+unlab.</cell></row><row><cell>Our best</cell><cell>6.51</cell><cell>Unlab.data</cell></row></table><note>plicable to CNN as they require bow vectors as input. We tested TSVM with bag-of-{1,2,3}-gram vectors using SVMlight. TSVM underperformed the supervised SVM 2 on two of the three datasets</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>IMDB: previous error rates (%).</figDesc><table><row><cell>SVM 1-3grams [11]</cell><cell>8.71</cell><cell>-</cell></row><row><cell cols="2">dense NN 1-3grams [11] 8.48</cell><cell>-</cell></row><row><cell>NB-LM 1-3grams [11]</cell><cell>8.11</cell><cell>-</cell></row><row><cell>[11]'s best CNN</cell><cell>7.14</cell><cell>-</cell></row><row><cell>Our best</cell><cell cols="2">6.27 Unlab.data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Elec: previous error rates (%).</figDesc><table><row><cell>models</cell><cell cols="2">micro-F macro-F</cell><cell>extra resource</cell></row><row><cell>SVM [16]</cell><cell>81.6</cell><cell>60.7</cell><cell>-</cell></row><row><cell>bow-CNN [11]</cell><cell>84.0</cell><cell>64.8</cell><cell>-</cell></row><row><cell>bow-CNN w/ three tv-embed.</cell><cell>85.7</cell><cell>67.1</cell><cell>Unlabeled data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>RCV1 micro-and macro-averaged F on the multi-label task (103 topics) with the LYRL04 split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>First we show that for any (possibly nonlinear) real-valued function f (·) defined on R m , there exists a RETEX so that this function can be expressed in terms of a linear function of RETEX vectors. This property is often referred to as universal approximation in the literature (e.g., see https://en.wikipedia.org/wiki/Universal_approximation_theorem).Proposition 1. Consider a real-valued function f (·) defined on R m . There exists an embedding matrix W, bias vector b, and vector v such that f (x) = v (Wx + b) + for all x ∈ R m .Proof. Denote by W i,j the entry of W corresponding to the i-th row and j-th column. Assume each element in R m can be represented as a d dimensional vector with no more than m ones (and the remaining entries are zeros). Given a specific x</figDesc><table /><note>i ∈ R m , let S i be a set of indexes j ∈ {1, . . . , d} such that the j-th component of x i is one. We create a row W i,· in W such that W i,j = 2I(j ∈ Si ) − 1 for 1 ≤ j ≤ d, where I(·) is the set indicator function. Let b i = −|S i | + 1 where b i denotes the i-th component of b. It follows that W i,· x + b i = 1 if x = x i , and W i,· x + b i ≤ 0 otherwise. In this manner we create one row of W per every member of R m . Let v i = f (x i ). Then it follows that f (x)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The error rate on RCV1 in row 4 slightly differs from<ref type="bibr" target="#b10">[11]</ref> because here we did not use the stopword list.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that for feasibility, we only used the 30K most frequent n-grams in the TSVM experiments, thus, showing the SVM results also with 30K vocabulary for comparison, though on some datasets SVM performance can be improved by use of all the n-grams (e.g., 5 million n-grams on IMDB)<ref type="bibr" target="#b10">[11]</ref>. This is because the computational cost of TSVM (single-core) turned out to be high, taking several days even with 30K vocabulary.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-view feature generation model for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marchine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view learning of word embeddings via CCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Li dent. Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patric</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modeling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">León</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marchine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5335v5</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mensnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Rainov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">#tagspace: Semantic embeddings from hashtags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1822" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Product feature mining: Semantic clues versus syntactic constituents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="336" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular CRF for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
