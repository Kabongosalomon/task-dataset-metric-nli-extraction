<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF-SUPERVISED LEARNING FOR FEW-SHOT IMAGE CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Chen</surname></persName>
							<email>chen.cd@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
							<email>yuefeng.chenyf@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Mao</surname></persName>
							<email>maofeng.mf@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
							<email>hui.xueh@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SELF-SUPERVISED LEARNING FOR FEW-SHOT IMAGE CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Few-shot learning</term>
					<term>Self-supervised learning</term>
					<term>Metric learning</term>
					<term>Cross-domain</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot image classification aims to classify unseen classes with limited labelled samples. Recent works benefit from the metalearning process with episodic tasks and can fast adapt to class from training to testing. Due to the limited number of samples for each task, the initial embedding network for meta-learning becomes an essential component and can largely affect the performance in practice. To this end, most of the existing methods highly rely on the efficient embedding network. Due to the limited labelled data, the scale of embedding network is constrained under a supervised learning(SL) manner which becomes a bottleneck of the few-shot learning methods. In this paper, we proposed to train a more generalized embedding network with self-supervised learning (SSL) which can provide robust representation for downstream tasks by learning from the data itself. We evaluate our work by extensive comparisons with previous baseline methods on two few-shot classification datasets (i.e., MiniImageNet and CUB) and achieve better performance over baselines. Tests on four datasets in cross-domain few-shot learning classification show that the proposed method achieves state-of-theart results and further prove the robustness of the proposed model. Our code is available at https://github.com/phecy/SSL-FEW-SHOT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recent advances in deep learning techniques have made significant progress in many areas. The main reason for such success is the ability to train a deep model that can retain profound knowledge from large scale labelled dataset. This is somehow against human learning behaviour -one can easily classify objects from just a few examples with limited prior knowledge. How to computationally model such behaviour motivates the recent researches in few-shot learning, where the focus is on how to adapt the model to new data or tasks with a restricted number of instances.</p><p>One popular solution for few-shot classification is to apply a fine-tuning process on existing embedding network to adapt new classes. The main challenge here is that the fine-tuning could easily lead to overfitting, as only a few samples(1-shot or 5-shot) for each class are available. One recent proposed solution for few-shot classification is a meta-learning process, in which the dataset is divided into subsets for different meta tasks to learn how to adapt the model according to the task change. These methods highly rely on an effective pre-train embedding network.</p><p>Current methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> with good performance mostly apply a ResNet12 <ref type="bibr" target="#b4">[5]</ref> or a wide ResNet <ref type="bibr" target="#b5">[6]</ref> as the embedding network and surpass the methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> with deeper network. We argue that † Corresponding Author. the abandon of large network is mainly because all these methods are trained in a supervised way with limited labelled samples. In this paper, we propose to apply a much larger embedding network with self-supervised learning (SSL) to incorporate with episodic task based meta-learning. According to the evaluation presented in Section 4, the proposed method can significantly improve few-shot image classification performance over baseline methods in two common datasets. As a remark, under the the same experiment setting, the proposed method improves 1-shot and 5-shot tasks by nearly 3% and 4% on MiniImageNet, by nearly 9% and 3% on CUB. Moreover, the proposed method can gain the improvement of 15%, 13% and 15%, 8% in two tasks on MiniImageNet and CUB dataset by pretraining using more unlabeled data. We also observe that the proposed model can be robustly transferred to other datasets under a recently proposed cross-domain few-shot learning scenario <ref type="bibr" target="#b8">[9]</ref> and achieve the state-of-the-art result(69.69% vs. 68.14%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Few-shot learning as an active research topic has been extensively studied. In this paper, we will primarily review recent deep-learning based approaches that are more relevant to our work. A number of works aim to improve the robustness of the training process. Zhao et al. <ref type="bibr" target="#b9">[10]</ref> split the features to three orthogonal parts to improve the classification performance for few-short learning, allowing simultaneous feature selection and dense estimation. Chen et al. <ref type="bibr" target="#b7">[8]</ref> propose a Self-Jig algorithm to augment the input data in few-shot learning by synthesizing new images that are either labelled or unlabeled.</p><p>A popular strategy for few-shot learning is through metalearning (also called learning-to-learn) with multi-auxiliary tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref>. The key is how to robustly accelerate the learning progress of the network without suffering from over-fitting with limited training data. Finn et al. propose MAML <ref type="bibr" target="#b11">[12]</ref> to search the best initial weights through gradient descent for network training, making the fine-tuning easier. REPTILE <ref type="bibr" target="#b13">[14]</ref> simplifies the complex computation of MAML by incorporating an L2 loss, but still performs in high dimension space. To reduce the complexity, Rusu et al. propose a network called LEO <ref type="bibr" target="#b3">[4]</ref> to learn a low dimension latent embedding of the model. Another stream of meta-learning based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> attempt to learn a deep embedding model that can effectively project the input samples to a specific feature space. Then the samples can be classified by the nearest neighbour (NN) criterion using a pre-defined distance function. Deeper embedding backbone has also been tried. Chen et al. <ref type="bibr" target="#b7">[8]</ref> propose a data augmentation method to cope with the over-fitting issue with deeper backbone embedding network. In <ref type="bibr" target="#b15">[16]</ref>, Hariharan et al. indicate similar observation, as claimed in this paper, that with a deeper backbone(ResNet-50) as embedding is not only costly but also not effective and proposed a novel representation regularization techniques.  <ref type="figure">Fig. 1</ref>. The overall architecture of our approach. LEFT: Train embedding network by Self-Supervised learning. The pretext task is designed to maximize the mutual information between two views xa, x b generated from the same image x by data augmentation. Right: Meta-learning with an episodic task(3-way, 1-shot example). For each task, the training samples and query samples are encoded by the embedding network. Query sample embeddings are compared with the centroid of training sample embeddings and make a further prediction.</p><p>Self-supervised learning(SSL) aims to learn robust representations from the data itself without class labels. The main challenge here is how to design the pretext tasks that are complex enough to exploit high-level compact semantic visual representations that are useful for solving downstream tasks. This is consistent with the mission of the pre-trained embedding network in few-shot learning. The work of <ref type="bibr" target="#b16">[17]</ref> revisited some state-of-the-art methods based on various classification based pretext tasks(e.g., Rotation, Exemplar, RelPatchLoc, Jigsaw). Recently, several methods have been proposed to combine SSL with few-shot learning. Gidaris et al. <ref type="bibr" target="#b17">[18]</ref> proposed to apply a combination of supervised loss and self-supervised loss to pretrain the embedding network. A study about embedding network in few-shot learning <ref type="bibr" target="#b18">[19]</ref> indicates that self-supervised learning based embedding network can achieve similar results as supervised learning. All these methods indicate that with a simple network such as ResNet-12, ResNet-18 can outperform ResNet-50. Different from these work, the proposed method does not rely on any supervised learning during the pre-training phase and prove that large scale embedding network can be applied in few-shot learning and achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>Few-shot learning is a challenging problem as it has only limited data for training and needs to verify the performance on the data for unseen classes. An effective solution for few-shot learning classification problem is to apply a meta-learning scheme on top of a pretrained embedding network. Most of the current methods are mainly focusing on the second stage i.e., meta-learning stage. In this work, we follow this two stages paradigm but utilize self-supervised learning to train a large embedding network as our strong base in stage one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-supervised learning stage</head><p>Our goal is to learn representations that enhance the feature's generalization. In our approach, we use Augmented Multiscale Deep InfoMax (AMDIM) <ref type="bibr" target="#b19">[20]</ref> as our self-supervised model. The pretext task is designed to maximize the mutual information between features extracted from multiple views of a shared context.</p><p>The mutual information (MI) measures the shared information between two random variables X and Y which is defined as the Kullback-Leibler (KL) divergence between the joint and the product of the marginals.</p><formula xml:id="formula_0">I(X, Y ) = DKL (p(x, y)||p(x)p(y)) = p(x, y) log p(x|y) p(x)<label>(1)</label></formula><p>where P (x, y) is the joint distribution and P (x) and P (y) are the marginal distributions of X and Y . Estimating MI is challenging as we just have samples but not direct access to the underlying distribution. <ref type="bibr" target="#b20">[21]</ref> proved that we can maximize a lower bound on mutual information by minimizing the Noise Contrastive Estimation (NCE) loss based on negative sampling. The core is to maximize mutual information between global features and local features from two views (xa, x b ) of the same image. Specifically, maximize mutual information between fg(xa), f5(x b ) , fg(xa), f7(x b ) and f5(xa), f5(x b ) . Where fg is the global feature, f5 is encoder's 5 × 5 local feature map as well as f7 as the encoder's 7 × 7 feature map. For example, the NCE loss between fg(xa) and f5(x b ) is defined as below:</p><formula xml:id="formula_1">L ssl (fg(xa), f5(x b )) = − log exp{φ(fg(xa), f5(x b ))} x b ∈Nx∪x b exp{φ(fg(xa), f5( x b ))} (2)</formula><p>Nx are the negative samples of image x, φ is the distance metric function. At last, the overall loss between xa and x b is as follows:</p><formula xml:id="formula_2">L ssl (xa, x b ) = L ssl (fg(xa), f5(x b )) + L ssl (fg(xa), f7(x b )) + L ssl (f5(xa), f5(x b ))<label>(3)</label></formula><p>For more details, please refer to <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meta-learning stage</head><p>Given a pre-trained embedding network from stage one, metalearning is applied to further fine-tune the model with an episodic manner. A few-shot K-way image classification task can be illustrated as a K-way C-shot problem i.e., given C labelled samples for each unseen class, the model should fast adapt to them to classify novel classes. The entire training set can be presented by D <ref type="figure">= {(x1, y1)</ref>, . . . , (xN , yN )} where N is the total number of classes in D, x is the training sample with label y. For a specific K-way C-shot meta task T , V = {yi|i = 1, . . . , K} denotes the class labels randomly chosen from D. Training samples from these classes are randomly chosen to form a support set and a query set: (a) the support set for task T is denoted by S, which contains C × K samples (K-way C-shot); (b) the query set is Q where n is the number of samples selected for meta testing. In this paper, during the meta-learning stage, the proposed model is trained to learn an embedding function to map all input samples from same class to a mean vector c in a description space as a class descriptor for each class <ref type="bibr" target="#b14">[15]</ref>. For class k, it is represented by the centroid of embedding features of training samples and can be obtained as:</p><formula xml:id="formula_3">c k = 1 |S k | (x i ,y i )∈S f (xi),<label>(4)</label></formula><p>where f (xi) is the embedding function initialized by stage one, S k is the training samples labelled with class k.</p><p>As a metric learning based method, we employ a distance function d and produce a distribution over all classes given a query sample q from the query set Q:</p><formula xml:id="formula_4">p(y = k|q) = exp(−d(f (q), c k )) k exp(−d(f (q), c k ))<label>(5)</label></formula><p>In this paper, Euclidean distance is chosen as distance function d. As shown in Eq. 5, the distribution is based on a softmax over the distance between the embedding of the samples (in the query set) and the class descriptors. The loss in the meta-learning stage can then read:</p><formula xml:id="formula_5">Lmeta = d(f (q), c k ) + log k d(f (q), c k )<label>(6)</label></formula><p>In a short conclusion, the proposed method first applies an SSL way to pre-train a large scale embedding network in stage one, followed by a detailed fine-tuning in stage two with a meta-learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>In this section, we first introduce the dataset and training process used in our evaluation, then show quantitative comparisons against other baseline methods, finally we conduct a detailed study to validate the transferability of our approach under a cross-domain fewshot learning evaluation set-up proposed in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>MiniImageNet dataset <ref type="bibr" target="#b10">[11]</ref>, is a subset of ImageNet which is a standard benchmark to evaluate the performance of few-shot learning methods. It contains 60,000 images from 100 classes, and each class has 600 images. We follow the data split strategy in <ref type="bibr" target="#b21">[22]</ref> to sample  <ref type="bibr" target="#b22">[23]</ref>, is a dataset for fine-grained classification. It contains 200 classes of birds with 11788 images in total. For evaluation, we follow the split in <ref type="bibr" target="#b23">[24]</ref>. 200 species of birds are randomly split to 100 classes for training, 50 classes for validation, and 50 classes for the test.</p><p>For cross-domain few-shot learning, 4 datasets are proposed to test suggested by <ref type="bibr" target="#b8">[9]</ref>, i.e., 1) CropDiseases <ref type="bibr" target="#b24">[25]</ref>, a plant diseases dataset, 2) EuroSAT <ref type="bibr" target="#b25">[26]</ref>, a dataset for satellite images, 2) ISIC <ref type="bibr" target="#b26">[27]</ref> a medical skin image dataset, 4) ChestX <ref type="bibr" target="#b27">[28]</ref>, a dataset for X-ray chest images. The similarity comparing to MiniImageNet is decreasing across these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details</head><p>Several recent works show that a typical training process can include a pre-trained network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> or employ co-training <ref type="bibr" target="#b1">[2]</ref> for feature embedding. This can significantly improve the classification accuracy. In this paper, we adopt the AMDIM <ref type="bibr" target="#b19">[20]</ref> SSL training framework to pre-train the feature embedding network. AmdimNet(ndf=192, ndepth=8, nrkhs=1536) is used for all datasets and the embedding dimension is 1536. Adam is chosen as the optimizer with a learning rate of 0.0002. We use 128 × 128 as the input resolution of unlabelled images among these datasets for self-supervised training. During meta-learning stage, image size is down to 84×84 following previous methods. For MiniImageNet dataset, 3 embedding models are trained. Mini80-SSL is self-supervised trained from 48,000 images (80 classes training and validation ) without labels. Mini80-SL is supervised training using same AmdimNet by cross-entropy loss with labels. Image900-SSL is SSL trained from all images from Im-ageNet1K except MiniImageNet. For CUB dataset, CUB150-SSL is trained by SSL from 150 classes (training and validation). CUB150-SL is the supervised trained model. Image1K-SSL is SSL trained from all images from ImageNet1K without any label. For crossdomain test, Mini80-SSL is applied as embedding network across all tests in four datasets during training.  <ref type="table">Table 2</ref>. Few-shot classification accuracy results on CUB dataset <ref type="bibr" target="#b22">[23]</ref>. − indicates result without meta-learning. For each task, the best-performing method is highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Standard few-shot learning evaluation</head><p>For MiniImageNet and CUB, we evaluate our method in two common few-shot learning tasks i.e., 1-shot 5-way task and 5-shot 5-way task against baseline methods with different embedding networks including classical ones <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and recently proposed methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34]</ref>. For CUB dataset, we follow the work <ref type="bibr" target="#b28">[29]</ref> to evaluate the robustness of the proposed framework with 7 other alternatives on this fine-grained dataset.</p><p>As detailed in <ref type="table" target="#tab_0">Table 1</ref>, the proposed method outperforms all baselines in the tested tasks. In 1-shot 5-way test, our approach achieves 7.53% and 2.27% improvement over ProtoNet + <ref type="bibr" target="#b14">[15]</ref> and LEO <ref type="bibr" target="#b3">[4]</ref> respectively. The former is an amended variant of ProtoNet using pre-trained Resnet as embedding network and has the same meta-learning stage with the proposed method. In the experience for 5-Shot 5-Way, we observe a similar improvement. Furthermore, we observe that the performance of our proposed method significantly increases when receiving more images/classes as input for pre-train. With more unlabeled samples(Image900 SSL), the model can achieve average 16% improvement over baselines while applies the same amount of labelled data. <ref type="table">Table 2</ref> illustrates our experiment on CUB dataset. Our proposed method yields the highest accuracy from all trials. In the 1-shot 5way test, we have 71.85% gaining a margin of 20.54% increment to the classic ProtoNet <ref type="bibr" target="#b14">[15]</ref>. The improvement is more significant for the 5-shot 5-way test. Our proposed method results is 84.29% which introduces 2.39% improvement to DN4-Da <ref type="bibr" target="#b30">[31]</ref>. Comparing to Baseline++ <ref type="bibr" target="#b28">[29]</ref>, our method shows a significant improvement, i.e., 11.32% and 4.95% in both tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Cross-domain few-shot Learning</head><p>Follow the set-up in <ref type="bibr" target="#b8">[9]</ref>, the proposed method is tested on four datasets across three tasks i.e., 5-way 5-shot, 5-way 20-shot 5-way 50-shot. Only Mini80 SSL embedding network(training only with MiniImageNet 80 classes) is applied in this section. We adopt the same transductive learning set-up proposed in <ref type="bibr" target="#b8">[9]</ref> for few-shot learning. As suggested in <ref type="bibr" target="#b8">[9]</ref>, results across all tests on four datasets are averaged. As shown in Tab. 3, after averaging the test results, the proposed method has nearly 1.5% improvement over the state-ofthe-art results which further prove the robustness of the proposed model. Noted that the state-of-the-art method obtain less accuracy in ISIC dataset, which needs more investigation in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>ChestX ISIC  <ref type="table">Table 3</ref>. Cross-domain few-shot learning tests on four datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>As shown in the quantitative evaluation, the proposed method can significantly improve the performance in few-shot classification task by including a large scale embedding network. One concern that may be raised is that if the gain of improvements of the proposed network is simply due to the increment of the network's capacity. To prove the effectiveness of the proposed method, we train the embedding network with labelled data (Mini80-SL and CUB150-SL as detailed in Section 4.2). As shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table">Table 2</ref>, it performs even worse than the methods with simple 4 Conv blocks embedding networks as such big network under supervised learning with limited data can cause overfitting problem and cannot adjust to new unseen classes during testing. However, with SSL based pre-training a more generalized embedding network can be obtained and improve the results significantly. One may also concern about the effectiveness of the meta-learning fine-tuning in the second stage. To test this, the pre-train embedding network is directly applied to the task with the nearest neighbourhood(NN) classification. As shown in the test results on both dataset, meta-learning can effectively fine-tune the embedding network and achieve remarkable improvement. We also include more data without labels during SSL pretraining and observe a more significant improvement of the result. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the proposed method can gain the improvement of 15% and 13% in two test tasks. As detailed analyzed in <ref type="bibr" target="#b28">[29]</ref>, current few-shot learning methods can not efficiently transfer the domain of learning, i.e., the training domain can not have a huge gap with the testing set. In this paper, a transferability test is also conducted by pre-training the embedding network on ImageNet and applied on CUB dataset. As shown in <ref type="table">Table 2</ref>, the proposed method with ImageNet pre-trained embedding network can be efficiently transferred to CUB dataset and gain an improvement of 15%,8% in both test tasks. Tests on four extra datasets suggested by <ref type="bibr" target="#b8">[9]</ref> further prove the transferability of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we propose to utilize self-supervised learning to efficiently train a robust embedding network for few-shot image classification. The resulted embedding network is more generalized and more transferable comparing to other baselines. After fine-tuning by a meta-learning process, the performance of the proposed method can significantly outperform all baselines based on the quantitative results using two common few-shot classification datasets and a cross-domain few-shot learning set-up. The current framework can be extended in several ways in the future. For instance, one direction is to combine these two stages together and develop an end-to-end method for this task. Another direction is to investigate the effectiveness of the proposed method on another few-shot tasks such as few-shot detection, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENT</head><p>This work was supported by Alibaba Group.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Few-shot classification accuracy results on MiniImageNet.</figDesc><table><row><cell>Baselines</cell><cell>Embedding Net</cell><cell>1-Shot 5-Way</cell><cell>5-Shot 5-Way</cell></row><row><cell>MatchingNet [11]</cell><cell>4 Conv</cell><cell cols="2">43.56 ± 0.84% 55.31 ± 0.73%</cell></row><row><cell>MAML [12]</cell><cell>4 Conv</cell><cell cols="2">48.70 ± 1.84% 63.11 ± 0.92%</cell></row><row><cell>RelationNet [13]</cell><cell>4 Conv</cell><cell cols="2">50.44 ± 0.82% 65.32 ± 0.70%</cell></row><row><cell>REPTILE [14]</cell><cell>4 Conv</cell><cell cols="2">49.97 ± 0.32% 65.99 ± 0.58%</cell></row><row><cell>ProtoNet [15]</cell><cell>4 Conv</cell><cell cols="2">49.42 ± 0.78% 68.20 ± 0.66%</cell></row><row><cell>Baseline* [29]</cell><cell>4 Conv</cell><cell cols="2">41.08 ± 0.70% 54.50% ± 0.66</cell></row><row><cell>Spot&amp;learn [30]</cell><cell>4 Conv</cell><cell cols="2">51.03 ± 0.78% 67.96% ± 0.71</cell></row><row><cell>DN4 [31]</cell><cell>4 Conv</cell><cell cols="2">51.24 ± 0.74% 71.02% ± 0.64</cell></row><row><cell>SNAIL [32]</cell><cell>ResNet12</cell><cell cols="2">55.71 ± 0.99% 68.88 ± 0.92%</cell></row><row><cell>ProtoNet + [15]</cell><cell>ResNet12</cell><cell>56.50 ± 0.40%</cell><cell>74.2 ± 0.20%</cell></row><row><cell>MTL [33]</cell><cell>ResNet12</cell><cell>61.20 ± 1.8%</cell><cell>75.50 ± 0.8%</cell></row><row><cell>DN4 [31]</cell><cell>ResNet12</cell><cell cols="2">54.37 ± 0.36% 74.44 ± 0.29%</cell></row><row><cell>TADAM [2]</cell><cell>ResNet12</cell><cell>58.50%</cell><cell>76.70%</cell></row><row><cell>Qiao-WRN [3]</cell><cell>Wide-ResNet28</cell><cell cols="2">59.60 ± 0.41% 73.74 ± 0.19%</cell></row><row><cell>LEO [4]</cell><cell>Wide-ResNet28</cell><cell cols="2">61.76 ± 0.08% 77.59 ± 0.12%</cell></row><row><cell>Dis. k-shot [7]</cell><cell>ResNet34</cell><cell cols="2">56.30 ± 0.40% 73.90 ± 0.30%</cell></row><row><cell>Self-Jig(SVM) [8]</cell><cell>ResNet50</cell><cell cols="2">58.80 ± 1.36% 76.71 ± 0.72%</cell></row><row><cell>FEAT [34]</cell><cell>ResNet50</cell><cell>53.8%</cell><cell>76.0%</cell></row><row><cell>Ours Mini80 SL</cell><cell>AmdimNet</cell><cell cols="2">43.92 ± 0.19% 67.13 ± 0.16%</cell></row><row><cell>Ours Mini80 SSL −</cell><cell>AmdimNet</cell><cell cols="2">46.13 ± 0.17% 70.14 ± 0.15%</cell></row><row><cell>Ours Mini80 SSL</cell><cell>AmdimNet</cell><cell cols="2">64.03 ± 0.20% 81.15 ± 0.14%</cell></row><row><cell>Ours Image900 SSL</cell><cell>AmdimNet</cell><cell cols="2">76.82 ± 0.19% 90.98 ± 0.10%</cell></row><row><cell cols="3">− indicates result without meta-learning.</cell><cell></cell></row><row><cell cols="4">images of 64 classes for training, 16 classes for validation, 20 classes</cell></row><row><cell>for the test.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CUB-200-2011(CUB) dataset, proposed in</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Ours trans 28.50 ± 0.40% 33.79 ± 0.48% 38.78 ± 0.64% 44.15 ± 0.52% 55.63 ± 0.49% 62.76 ± 0.50% Cross [9] 26.09 ± 0.96% 31.01 ± 0.59% 36.79 ± 0.53% 49.68 ± 0.36% 61.09 ± 0.44% 67.20 ± 59% Ours trans 83.44 ± 0.61% 90.43 ± 0.52% 94.71 ± 0.47% 91.79 ± 0.48% 97.38 ± 0.65% 99.50 ± 0.63% Cross[9] 81.76 ± 0.48% 87.97 ± 0.42% 92.00 ± 0.56% 90.64 ± 0.54% 95.91 ± 0.72% 97.48 ± 0.56%</figDesc><table><row><cell>5-way 5-shot</cell><cell>5-way 20-shot</cell><cell>5-way 50-shot</cell><cell>5-way 5-shot</cell><cell>5-way 20-shot</cell><cell>5-way 50-shot</cell></row><row><cell></cell><cell>EuroSAT</cell><cell></cell><cell></cell><cell>CropDiseases</cell><cell></cell></row><row><cell>5-way 5-shot</cell><cell>5-way 20-shot</cell><cell>5-way 50-shot</cell><cell>5-way 5-shot</cell><cell>5-way 20-shot</cell><cell>5-way 50-shot</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to learn with conditional class dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farshid</forename><surname>Varno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="719" to="729" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7229" to="7238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Discriminative k-shot learning using probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub Bartlomiej</forename><surname>Swiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image block augmentation for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A broader study of cross-domain few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajana</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Msplit lbi: Realizing feature selection and dense estimation simultaneously in few-shot and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04360</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3630" to="3638" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reptile: a scalable metalearning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8059" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11539</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Few-shot learning with metric-agnostic conditional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artëm</forename><surname>Yankov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Courtney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">O</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hodas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04376</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using deep learning for image-based plant disease detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salathé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in plant science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1419</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadi</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Marchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spot and learn: A maximum-entropy patch sampler for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6251" to="6260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8808" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
