<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LaneRCNN: Distributed Representations for Graph-Centric Motion Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
							<email>wenyuan@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<email>rjliao@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LaneRCNN: Distributed Representations for Graph-Centric Motion Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Forecasting the future behaviors of dynamic actors is an important task in many robotics applications such as self-driving. It is extremely challenging as actors have latent intentions and their trajectories are governed by complex interactions between the other actors, themselves, and the maps. In this paper, we propose LaneRCNN, a graphcentric motion forecasting model. Importantly, relying on a specially designed graph encoder, we learn a local lane graph representation per actor (LaneRoI) to encode its past motions and the local map topology. We further develop an interaction module which permits efficient message passing among local graph representations within a shared global lane graph. Moreover, we parameterize the output trajectories based on lane graphs, a more amenable prediction parameterization. Our LaneRCNN captures the actor-toactor and the actor-to-map relations in a distributed and map-aware manner. We demonstrate the effectiveness of our approach on the large-scale Argoverse Motion Forecasting Benchmark. We achieve the 1st place on the leaderboard and significantly outperform previous best results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous vehicles need to navigate in dynamic environments in a safe and comfortable manner. This requires predicting the future motions of other agents to understand how the scene will evolve. However, depending on each agent's intention (e.g. turning, lane-changing), the agents' future motions can involve complicated maneuvers such as yielding, nudging, and acceleration. Even worse, those intentions are not known a priori by the ego-robot, and agents may also change their minds based on behaviors of nearby agents. Therefore, even with access to the ground-truth trajectory histories of the agents, forecasting their motions is very challenging and is an unsolved problem.</p><p>By leveraging deep learning, the motion forecasting community has been making steady progress. Most stateof-the-art models share a similar design principle: using a single feature vector to characterize all the informa-Actor Feature Attention Actor LaneRoI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Paradigm</head><p>Actor node Interaction node-to-node</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our LaneRCNN</head><p>Actor graph Interaction graph-to-graph <ref type="figure" target="#fig_0">Figure 1</ref>: Popular motion forecasting methods encode actor and its context information into a feature vector, and treat it as a node in an interaction graph. In contrast, we propose a graph-based representation LaneRoI per actor, which is structured and expressive. Based on it, we model interactions and forecast motion in a map topology aware manner.</p><p>tion related to an actor, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, left. They typically first encode for each actor its past motions and the surrounding map (or other context information) into a feature vector, which is computed either by feeding a 2D rasterization to a convolutional neural network (CNN) <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b7">8]</ref>, or directly using a recurrent neural network (RNN) <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref>. Next, they exchange the information among actors to model interactions, e.g., via a fully-connected graph neural network (GNN) <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref> or an attention mechanism <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b33">34]</ref>. Finally, they predict future motions per actor from its feature vector via a regression header <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b55">55]</ref>. Although such a paradigm has shown competitive results, it has three main shortcomings: 1) Representing the context information of large regions of space, such as fast moving actors traversing possibly a hundred meters within five seconds, with a single vector is difficult. 2) Building a fully-connected interaction graph among actors ignores important map structures. For example, an unprotected left turn vehicle should yield to oncoming traffic, while two spatially nearby vehicles driving on opposite lanes barely interact with each other. 3) The regression header does not explicitly leverage the lane information, which could provide a good inductive bias for accurate predictions. As a <ref type="figure">Figure 2</ref>: Overview of LaneRCNN. It first encodes each actor with our proposed LaneRoI representation, processes each LaneRoI with an encoder, and then models interactions among actors with a graph-based interactor. Finally, LaneRCNN predicts final positions of actors in a fully-convolutional manner, and then decodes full trajectories based on these positions. consequence, regression-based predictors sometimes forecast shooting-out-of-road trajectories, which are unrealistic.</p><p>In this paper, we propose a graph-centric motion forecasting model, i.e., LaneRCNN, to address the aforementioned issues. We represent an actor and its context in a distributed and map-aware manner by constructing an actor-specific graph, called Lane-graph Region-of-Interest (LaneRoI), along with node embeddings that encode the past motion and map semantics. In particular, we construct LaneRoI following the topology of lanes that are relevant to this actor, where nodes on this graph correspond to small spatial regions along these lanes, and edges represent the topological and spatial relations among regions. Compared to using a single vector to encode all the information of a large region, our LaneRoI naturally preserves the map structure and captures the more fine-grained information, as each node embedding only needs to represent the local context within a small region. To model interactions, we embed the LaneRoIs of all actors to a global lane graph and then propagate the information over this global graph. Since the LaneRoI's of interacting actors are highly relevant, those actors will share overlapping regions on the global graph, thus having more frequent communications during the information propagation compared to irrelevant actors. Importantly, this process neither requires any heuristics nor makes any oversimplified assumptions while learning interactions conditioned on maps. We then predict future motions on each LaneRoI in a fully-convolutional manner, such that small regions along lanes (nodes in LaneRoI) can serve as anchors and provide good priors . We demonstrate the effectiveness of our method on the large-scale Argoverse motion forecasting benchmark <ref type="bibr" target="#b9">[10]</ref>. We achieve the first rank on the challenging Argoverse competition leaderboard <ref type="bibr" target="#b0">[1]</ref>, significantly outperforming previous results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motion Forecasting: Traditional methods use handcrafted features and rules based on human knowledge to model interactions and constraints in motion forecasting <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b31">32]</ref>, which are sometimes oversimplified and not scalable. Recently, learning-based approaches employ the deep learning and significantly outperform traditional ones. Given the actors and the scene, a deep forecasting model first needs to design a format to encode the information. To do so, previous methods <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">37]</ref> often rasterize the trajectories of actors into a Birds-Eye-View (BEV) image, with different channels representing different observation timesteps, and then apply a CNN and RoI pooling <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b19">20]</ref> to extract actor features. Maps can be encoded similarly <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">49]</ref>. However, the square receptive fields of a CNN may not be efficient to encode actor movements <ref type="bibr" target="#b28">[29]</ref>, which are typically long curves. Moreover, the map rasterization may lose useful information like lane topologies. RNNs are an alternative way to encode actor kinematic information <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2]</ref> compactly and efficiently. Recently, VectorNet <ref type="bibr" target="#b15">[16]</ref> and LaneGCN <ref type="bibr" target="#b28">[29]</ref> generalized such compact encodings to map representations. VectorNet treats a map as a collection of polylines and uses a RNN to encode them, while LaneGCN builds a graph of lanes and conducts convolutions over the graph. Different from all these work, we encode both actors and maps in an unified graph representation, which is more structured and powerful.</p><p>Modeling interactions among actors is also critical for a multi-agent system. Pioneering learning-based work design a social-pooling mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref> to aggregate the information from nearby actors. However, such a pooling operation may potentially lose actor-specific information. To address this, attention-mechanism <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b48">48]</ref> or GNNbased methods <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref> build actor interaction graphs (usually fully-connected with all actors or k-nearest neighbors based), and perform attention or message passing to update actor features. Social convolutional pooling <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b47">47]</ref> has also been explored, which maintains the spatial distribution of actors. However, most of these work do not explicitly consider map structures, which largely affects interactions among actors in reality.</p><p>To generate each actor's predicted futures, many works sample multi-modal futures under a conditional variational auto-encoder (CVAE) framework <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b6">7]</ref>, or with a multi-head/mode regressor <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34]</ref>. Others output discrete sets of trajectory samples <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b8">9]</ref> or occupancy maps <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b42">42]</ref>. Recently, TNT <ref type="bibr" target="#b61">[61]</ref> concurrently and independently designs a similar output parameterization as ours where lanes are used as priors for the forecasting. Note that, in addition to the parameterization, we contribute a novel graph representation and a powerful architecture which significantly outperforms their results.</p><p>Graph Neural Networks: Relying on operators like graph convolution and message passing, graph neural networks (GNNs) and their variants <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31]</ref> generalize deep learning on regular graphs like grids to ones with irregular topologies. They have achieved great successes in learning useful graph representations for various tasks <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref>. We draw inspiration from the general concept "ego-graph" and propose LaneRoI, which is specially designed for lane graphs and captures both the local map topologies and the past motion information of an individual actor. Moreover, to capture interactions among actors, we further propose an interaction module which effectively communicates information among LaneRoI graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LaneRCNN</head><p>Our goal is to predict the future motions of all actors in a scene, given their past motions and an HD map. Different from existing work, we represent an actor and its context with a LaneRoI, an actor-specific graph which is more structured and expressive than the single feature vector used in the literature. Based on this representation, we design LaneRCNN, a graph-centric motion forecasting model that encodes context, models interactions between actors, and predicts future motions all in a map topology aware manner. An overview of our model is shown in <ref type="figure">Fig. 2</ref>.</p><p>In the following, we first introduce our problem formulation and notations in Sec. <ref type="bibr" target="#b2">3</ref>          </p><formula xml:id="formula_0">Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d</formula><formula xml:id="formula_1">Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d</formula><formula xml:id="formula_2">Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d</formula><formula xml:id="formula_3">+ d W l / 2 Y = " &gt; A A A B 6 H i c b V B N S w M x F H x b v 2 q t W r 1 6 C R b B U 8 l 6 0 a M g i M c K 9 g O 6 S 8 m m 2 T Y 0 m 1 2 S t 0 J Z + j e 8 e F D E X + T N f 2 O 2 7 U F b B w L D z H u 8 y U S Z k h Y p / f Y q W 9 s 7 u 3 v V / d p B / f D o u H F S 7 9 o 0 N 1 x 0 e K p S 0 4 + Y F U p q 0 U G J S v Q z I 1 g S K d G L p n e l 3 3 s W x s p U P + E s E 2 H C x l r G k j N 0 U h A k D C d R X N z P h 3 L Y a N I W X Y B s E n 9 F m r B C e 9 j 4 C k Y p z x O h k S t m 7 c C n G Y Y F M y i 5 E v N a k F u R M T 5 l Y z F w V L N E 2 L B Y Z J 6 T C 6 e M S</formula><formula xml:id="formula_4">+ d W l / 2 Y = " &gt; A A A B 6 H i c b V B N S w M x F H x b v 2 q t W r 1 6 C R b B U 8 l 6 0 a M g i M c K 9 g O 6 S 8 m m 2 T Y 0 m 1 2 S t 0 J Z + j e 8 e F D E X + T N f 2 O 2 7 U F b B w L D z H u 8 y U S Z k h Y p / f Y q W 9 s 7 u 3 v V / d p B / f D o u H F S 7 9 o 0 N 1 x 0 e K p S 0 4 + Y F U p q 0 U G J S v Q z I 1 g S K d G L p n e l 3 3 s W x s p U P + E s E 2 H C x l r G k j N 0 U h A k D C d R X N z P h 3 L Y a N I W X Y B s E n 9 F m r B C e 9 j 4 C k Y p z x O h k S t m 7 c C n G Y Y F M y i 5 E v N a k F u R M T 5 l Y z F w V L N E 2 L B Y Z J 6 T C 6 e M S</formula><formula xml:id="formula_5">f G + p Y r G 3 A T 5 P P O M n F l l S K J E 2 6 e Q z N X f G z m N j Z n G o Z 0 s M p p l r x D / 8 / o Z R l d B L l S a I V d s c S j K J M G E F A W Q o d C c o Z x a Q p k W N i t h Y 6 o p Q 1 t T z Z b g L X 9 5 l X Q v m p 7 b 9 O 7 d R u u 6 r K M K J 3 A K 5 + D B J b T g D t r Q A Q Y p P M M r v D m Z 8 + K 8 O x + L 0 Y p T 7 h z D H z i f P y / U k b w = &lt; / l a t e x i t &gt; &lt; l a</formula><formula xml:id="formula_6">Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d</formula><formula xml:id="formula_7">Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We denote the past motion of the i-th actor as a set of 2D points encoding the center locations over the past L timesteps, i.e., (</p><formula xml:id="formula_8">x −L i , y −L i ), · · · , (x −1 i , y −1 i ) ,</formula><p>with (x, y) the 2D coordinates in bird's eye view (BEV). Our goal is to forecast the future motions of all actors in the scene</p><formula xml:id="formula_9">(x 0 i , y 0 i ), · · · , (x T i , y T i )|i = 1, · · · , N ,</formula><p>where T is our prediction horizon and N is the number of actors.</p><p>In addition to the past kinematic information of the actors, maps also play an important role for motion forecasting since (i) actors usually follow lanes on the map, (ii) the map structure determines the right of way, which in turns affects the interactions among actors. As is common practice in self-driving, we assume an HD map is accessible, which contains lanes and associated semantic attributes, e.g., turning lane and lane controlled by traffic light. Each lane is composed of many consecutive lane segments i , which are short segments along the centerline of the lane. In addition, a lane segment i can have pairwise relationships with another segment j in the same lane or in another lane, such as i being a successor of j or a left neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LaneRoI Representation</head><p>Graph Representation: One straight-forward way to represent an actor and its context (map) information is by first rasterizing both its trajectory as well as the map to form a 2D BEV image, and then cropping the underlying representation centered in the actor's location in BEV <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b5">6]</ref>. However, rasterizations are prone to information loss such as connectivities among lanes. Furthermore, it is a rather inefficient representation since actor motions are expanded typically in the direction along the lanes, not across them. Inspired by <ref type="bibr" target="#b28">[29]</ref>, we instead use a graph representation for our LaneRoI to preserve the structure while being compact. For each actor i in the scene, we first retrieve all relevant lanes that this actor can possibly go to in the prediction horizon T as well as come from in the observed history horizon L. We then convert the lanes into a directed graph G i = {V, {E suc , E pre , E left , E right }} where each node v ∈ V represents a lane segment within those lanes and the lane topology is represented by different types of edges E r , encoding the following relationships: predecessor, successor, left and right neighbor. Two nodes are connected by an edge e ∈ E r if the corresponding lane segments i , j have a relation r, e.g., lane segment i is a successor of lane segment j . Hereafter, we will use the term node interchangeably with the term lane segment.</p><p>Graph Input Encoding: The graph G i only characterizes map structures around the i-th actor without much information about the actor. We therefore augment the graph with a set of node embeddings to construct our LaneRoI . Recall that each node k in G i is associated with a lane segment k . We design its embedding f k ∈ R C to capture the geometric and semantic information of k , as well as its relations with the actor. In particular, geometric features include the center location, the orientation and the curvature of k ; semantic features include binary features indicating if k is a turning lane, if it is currently controlled by a red light, etc.</p><p>To encode the actor information into f k , we note that the past motion of an actor can be identified as a set of 2D displacements, defining the movements between consecutive timesteps. Therefore, we also include the relative positions and orientations of these 2D displacements w.r.t. k into f k which encodes actor motions in a map-dependent manner. This is beneficial for understanding actor behaviors w.r.t. the map, e.g., a trajectory that steadily deviates from one lane and approaches the neighboring lane is highly likely a lane change. In practice, it is important to clamp the actor information, i.e., if k is more than 5 meters away from the actor we replace the actor motion embedding in f k with zeros. We hypothesize that such a restriction encourages the model to learn better representations via the message passing over the graph. To summarize, (G i , F i ) is the LaneRoI of the actor i, encoding the actor-specific information for motion forecasting, where F i ∈ R Mi×C is the collection of node embeddings f k and M i is the number of nodes in G i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">LaneRCNN Backbone</head><p>As LaneRoIs have irregular graph structures, we can not apply standard 2D convolutions to obtain feature representations. In the following, we first introduce the lane convolution and pooling operators <ref type="figure" target="#fig_9">(Fig. 4)</ref>, which serve similar purposes as their 2D counterparts while respecting the graph topology. Based on these operators, we then describe how our LaneRCNN updates features of each LaneRoI as well as handles interactions among all LaneRoIs (actors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lane Convolution Operator:</head><p>We briefly introduce the lane convolution which was originally proposed in <ref type="bibr" target="#b28">[29]</ref>   Given a LaneRoI (G i , F i ), a lane convolution updates features F i by aggregating features from its neighborhood (in the graph). Formally, we use E i (r) to denote the binary adjacency matrix for G i under the relation r, i.e., the (p, q) entry in this matrix is 1 if lane segments p and q have the relation r and 0 otherwise. We denote the n-hop connectivity under the relation r as the matrix bool (E i (r) · E i (r) · · · E i (r)) = bool (E n i (r)), where the operator bool sets any non-zero entries to one and otherwise keeps them as zero. The output node features are updated as follows,</p><formula xml:id="formula_10">F i ← Ψ F i W + r,n bool (E n i (r)) F i W n,r ,<label>(1)</label></formula><p>where both W and W n,r are learnable parameters, Ψ(·) is a non-linearity consisted of LayerNorm <ref type="bibr" target="#b2">[3]</ref> and ReLU <ref type="bibr" target="#b36">[36]</ref>, and the summation is over all possible relations r and hops n. In practice, we use n ∈ {1, 2, 4, 8, 16, 32}. Such a multi-hop mechanism mimics the dilated convolution <ref type="bibr" target="#b58">[58]</ref> and effectively enlarges the receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lane Pooling Operator:</head><p>We design a lane pooling operator which is a learnable pooling function. Given a LaneRoI (G i , F i ), recall G i actually corresponds to a number of lanes spanned in the 2D plane (scene). For an arbitrary 2D vector v in the plane, a lane pooling operator pools, or 'interpolates', the feature of v from F i . Note that v can be a lane segment in another graph G j (spatially close to G i ). Therefore, lane pooling helps communicate information back and forth between graphs, which we will explain in the interaction part. To generate the feature f v of vector v, we first retrieve its 'neighboring nodes' in G i , by checking if the center distance between a lane segment k in G i and vector v is smaller than a certain threshold. A naive pooling strategy is to simply take a mean of those k . However, this ignores the fact that relations between k and v can vary a lot depending on their relative pose: a lane segment that is perpendicular to v (conflicting) and the one that is aligned with v have very different semantics. Inspired by the generalized convolution on graphs/manifolds <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b28">29]</ref>, we use the relative pose and some non-linearities to learn a pooling function. In particular, we denote the set of surrounding nodes on G i as N , and the relative pose between v and k as ∆ vk which includes relative position and orientation. The pooled feature f v can then be written as,</p><formula xml:id="formula_11">f v = M b k∈N M a ([f k , ∆ vk ]) ,<label>(2)</label></formula><p>where [· · · ] means concatenation and M is a two-layer multi-layer perceptron (MLP).</p><p>LaneRoI Encoder: Equipped with operators introduced above, we now describe how LaneRCNN processes features for each LaneRoI. Given a scene, we first construct a LaneRoI per actor and encode its input information into node embeddings as described in Sec. 3.2. Then, for each LaneRoI, we apply four lane convolution layers and get the updated node embeddings F i . Essentially, a lane convolution layer propagates information from a node to its (multihop) connected nodes. Stacking more layers builds larger receptive fields and has a larger model capacity. However, we find deeper networks do not necessarily lead to better performances in practice, possibly due to the well-known difficulty of learning long-term dependencies. To address this, we introduce a graph shortcut mechanism on LaneRoI.</p><p>The graph shortcut layer can be applied after any layer of lane convolution: we aggregate F i output from the previous layer into a global embedding with the same dimension as node embeddings, and then add it to embeddings of all nodes in G i . Recall that the actor past motions are a number of 2D vectors, i.e., movements between consecutive timesteps. We use the lane pooling to extract features for these 2D vectors. A 1D CNN with downsampling is then applied to these features to build the final shortcut embedding. Intuitively, a lane convolution may suffer from the diminishing information flow during the message-passing, while such a shortcut can provide an auxiliary path to communicate among far-away nodes efficiently. We will show that the shortcut significantly boosts the performance in the ablation study.</p><p>LaneRoI Interactor: So far, our LaneRoI encoder provides good features for a given actor, but it lacks the ability to model interactions among different actors, which is extremely important for the motion forecasting in a multiagent system. We now describe how we handle actor interactions under LaneRoI representations. After processing all LaneRoIs with the LaneRoI encoder (shared weights), we build a global lane graph G containing all lanes in the scene. Its node embeddings are constructed by projecting all LaneRoIs to G itself. We then apply four lane convolution layers on G to perform message passing. Finally, we distribute the 'global node' embeddings back to each LaneRoI . Our design is motivated by the fact that actors have interactions since they share the same space-time region. Similarly, in our model, all LaneRoIs share the same global graph G where they communicate with each other following map structures.</p><p>In particular, suppose we have a set of LaneRoIs {(G i , F i )|i = 1, · · · , N } encoded from previous layers and a global lane graph G. For each node in G, we use a lane pooling to construct its embedding: retrieving its neighbors from all LaneRoIs as N , measured by center distance, and then applying Eq. 2. This ensures each global node has the information of all those actors that could interact with it. The distribute step is an inverse process: for each node in G i , find its neighbors, apply a lane pooling, and add the resulted embedding to original F i (serving as a skipconnection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Map-Relative Outputs Decoding</head><p>The future is innately multi-modal and an actor can take many different yet possible future motions. Fortunately, different modalities can be largely characterized by different goals of an actor. Here, a goal means a final position of an actor at the end of prediction horizon. Note that actors mostly follow lane structures and thus their goals are usually close to a lane segment . Therefore, our model can predict the final goals of an actor in a fully convolutional manner, based on its LaneRoI features. Namely, we apply a 2-layer MLP on each node feature f k , and output five values including the probability that k is the closest lane segment to final destination p( k = goal), as well as relative residues from k to the final destination x gt − x k , y gt − y k , sin(θ gt − θ k ), cos(θ gt − θ k ).</p><p>Based on results of previous steps, we select the top K 1 For each prediction, we use the position and the direction of the actor at t = 0 as well as those at the goal to interpolate a curve, using Bezier quadratic parameterization. We then unroll a constant acceleration kinematic model along this curve, and sample 2D points at each future timestep based on the curve and the kinematic information. These 2D points form a trajectory, which serves as an initial proposal of our final forecasting. Despite its simplicity, this parameterization gives us surprisingly good results.</p><p>Our final step is to refine those trajectory proposals using a learnable header. Similar to the shortcut layer introduced in Sec. 3.3, we use a lane pooling followed by a 1D CNN to pool features of this trajectory. Finally, we decode a pair of values per timestep, representing the residue from the trajectory proposal to the ground-truth future position at this timestep (encoded in Frenet coordinate of this trajectory proposal). We provide more detailed definitions of our parameterization and output space in the supplementary A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Learning</head><p>We train our model end-to-end with a loss containing the goal classification, the goal regression, and the trajectory refinements. Specifically, we use</p><formula xml:id="formula_12">L = L cls + αL reg + βL refine ,</formula><p>where α and β are hyparameters determining relative weights of different terms. As our model predicts the goal classification and regression results per node, we simply adopt a binary cross entropy loss for L cls with online hard example mining <ref type="bibr" target="#b46">[46]</ref> and a smooth-L1 loss for L reg , where the L reg is only evaluated on positive nodes, i.e. closest lane segments to the ground-truth final positions. The L refine is also a smooth-L1 loss with training labels generated on the fly: projecting ground-truth future trajectories to the predicted trajectory proposals, and use the Frenet coordinate values as our regression targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate the effectiveness of LaneRCNN on the large-scale Argoverse motion forecasting benchmark (Argoverse), which is publicly available and provides annotations of both actor motions and HD maps. In the following, we first explain our experimental setup and then compare our method against state-of-the-art on the leaderboard. We also conduct ablation studies on each module of Lan-eRCNN to validate our design choices. Finally, we present some qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset: Argoverse provides a large-scale dataset <ref type="bibr" target="#b9">[10]</ref> for the purpose of training, validating and testing models, where the task is to forecast 3 seconds future motions given 2 seconds past observations. This dataset consists of more than 30K real-world driving sequences collected in Miami and Pittsburgh. Those sequences are further split into train, validation, and test sets without any geographical overlapping. Each of them has 205942, 39472, and 78143 sequences respectively. In particular, each sequence contains the positions of all actors in a scene within the past 2 seconds history, annotated at 10Hz. It also specifies one interesting actor in this scene, with type 'agent', whose future 3 seconds motions are used for the evaluation. The train and validation splits additionally provide future locations of all actors within 3 second horizon labeled at 10Hz, while annotations for test sequences are withheld from the public and used for the leaderboard evaluation. Besides, HD map information can be retrieved for all sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics:</head><p>We follow the benchmark setting and use Miss-Rate (MR), Average Displacement Error (ADE) and Final Displacement Error (FDE), which are also widely used in the community. MR is defined as the ratio of data that none of the predictions has less than 2.0 meters L2 error at the final timestep. ADE is the averaged L2 errors of all future timesteps, while FDE only counts the final timestep. To evaluate the mutli-modal prediction, we also adopt the benchmark setting: predicting K=6 future trajectories per actor and evaluating the min K MR, min K ADE, min K FDE using the trajectory that is closest to the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details:</head><p>We train our model on the train set with the batch size of 64 and terminate at 30 epochs. We use Adam <ref type="bibr" target="#b22">[23]</ref> optimizer with the learning rate initialized at 0.01 and decayed by 10 at 20 epochs. To normalize the data, we translate and rotate the coordinate system of each sequence so that the origin is at current position (t = 0) of 'agent' actor and x-axis is aligned with its current direction. During training, we further apply a random rotation data augmentation within (− 2 3 π, 2 3 π). No other data processing is applied such as label smoothing. More implementation details are provided in the supplementary C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art</head><p>We compare our approach with top entries on Argoverse motion forecasting leaderboard <ref type="bibr" target="#b0">[1]</ref> as well as official baselines provided by the dataset <ref type="bibr" target="#b9">[10]</ref> as shown in <ref type="table" target="#tab_2">Table 1</ref>. We only submit our final model once to the leaderboard and achieve state-of-the-art performance. 2 This is a very challenging benchmark with around 100 participants at the time of our submission. Note that for the official ranking metric MR (K=6), previous leading methods are extremely close to each other, implying the difficulty of further improving the performance. Nevertheless, we significantly boost the number which verifies the effectiveness of our method. Among the competitors, both Jean <ref type="bibr" target="#b33">[34]</ref> and TNT <ref type="bibr" target="#b61">[61]</ref> use RNNs to encode actor kinematic states and lane polylines. They then build a fully-connected interaction graph among all actors and lanes, and use either the attention or GNNs to model interactions. As a result, they represent each actor with a single feature vector, which is less expressive than our LaneRoI representations. Moreover, the fully-connected interaction graph may also discard valuable map structure information. Note that TNT shares a similar output parameterization as ours, yet we perform better on all metrics. This further validates the advantages of our LaneRoI compared against traditional representations. Unfortunately, since Poly team does not publish their method, we can not compare with it qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Past labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future labels</head><p>Others' future Our predictions refers to average-pooling all node embeddings within a LaneRoI, and 'Center Pool' means we pool a feature from a LaneRoI using nodes that around the last observation of the actor and a lane pooling. As we can see, although these two approaches can possibly spread out information to every node in a LaneRoI (and thus build a shortcut), they barely improve the performance. On the contrary, ours achieve significant improvements. This is because we pool features along the past trajectory of the actor, which results in a larger and actor-motion-specific receptive field. Here, ×1 and ×2 refer to an encoder with 1 shortcut per 4 and 2 lane convolution layers respectively. This shows stacking more shortcuts provides some, but diminishing, benefits.</p><p>Ablations on LaneRoI Interactor: To verify the effectiveness of our map-aware interaction module, we compare against several model variants based on the fully-connected interaction graph among actors. Specifically, for each actor, we apply a LaneRoI encoder 3 to process node embeddings, and then pool an actor-specific feature vector from LaneRoI via either the global average pooling or our shortcut mechanism. These actor features are then fed into a transformerstyle <ref type="bibr" target="#b51">[51]</ref> attention module or a fully-connected GNN. Finally, we add the output actor features to nodes in their LaneRoI respectively and make predictions using our decoding module. As a result, these variants have the same pipeline as ours, with the only difference on how to communicate across actors. To make comparisons as fair as possible, both the attention and GNN have the same numbers of layers and channels as our LaneRoI Interactor. <ref type="bibr" target="#b3">4</ref> As shown in <ref type="table" target="#tab_4">Table 2</ref>, all interaction-based models outperform the one without considering interactions (row 1) as expected. In addition, our approach significantly improves the performance compared to both the attention and GNN. Interestingly, all fully-connected interaction graph based model reach similar performance, which might imply such backbones may saturate the performance (as also shown by leading methods on the leaderboard). We also show that naively using the average pooling to embed features from LaneRoIs to global graph does not achieves good performance because it ignores local structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative results</head><p>In <ref type="figure" target="#fig_10">Figure 5</ref>, we show some qualitative results on Argoverse validation set. We can see that our method generally follows the map very well and demonstrates good multimodalities. From left to right, we show 1) when an actor follows a curved lane, our model predicts two direction modes with different velocities; 2) when it is on a straight lane, our model covers the possibilities of lane changing; 3) when it's approaching an intersection, our model captures both the go-straight and the turn-right modes, especially with lower speeds for turning right, which are quite common in the real world; 4) when there is an actor blocking the path, we predict overtaking behaviors matching exactly with the groundtruth. Moreover, for the lane-following mode, we predict much slower speeds which are consistent with this scenario, showing the effectiveness of our interaction modeling. We provide more qualitative results in the supplementary E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose LaneRCNN, a graph-centric motion forecasting model. Relying on learnable graph operators, LaneRCNN builds a distributed lane-graph-based representation (LaneROI) per actor to encode its past motion and the local map topology. Moreover, we propose an interaction module which effectively captures the interactions among actors within the shared global lane graph. And lastly, we parameterize the output trajectory using lane graphs which helps improve the prediction. We demonstrate that LaneRCNN achieves state-of-the-art performance on the challenging Argoverse motion forecasting benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Map-Relative Output Decoding</head><p>Our output decoding can be divided into three steps: predicting the final goal of an actor based on node embeddings, propose an initial trajectory based on the goal and the initial pose, and refine the trajectory proposal with a learnable header. As the first step is straight-forward, we now explain how we perform the second and third steps in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Trajectory Proposal</head><p>Given a predicted final pose x T , y T , dx T , dy T and initial pose x 0 , y 0 , dx 0 , dy 0 of an actor, where (x, y) is the 2D location and (dx, dy) is the tangent vector, we fit a Bezier quadratic curve satisfying these boundary conditions, i.e., zero-th and first order derivative values. Specifically, the curve can be parameterized by</p><formula xml:id="formula_13">x(s) = a 0 s 2 + a 1 s + a 2 , y(s) = b 0 s 2 + b 1 s + b 2 , s.t. x(0) = x 0 , x(1) = x T , x (0) x (1) = dx 0 dx T , y(0) = y 0 , y(1) = y T , y (0) y (1) = dy 0 dy T .</formula><p>Here, s is the normalized distance. As a result, each predicted goal uniquely defines a 2D curve. Next, we unroll a velocity profile along this curve to get 2D waypoint proposals at every future timestamp. Assuming the actor is moving with a constant acceleration within the prediction horizon, we can compute the acceleration based on the initial velocity v and the traveled distance s (from (x 0 , y 0 ) to (x T , y T ) along the Bezier curve) using</p><formula xml:id="formula_14">a = 2 × (s − vT ) T 2 .</formula><p>Therefore, the future position of the actor at any timestamp t can be evaluated by querying the position along the curve at s(t) = vt + 1 2 at 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Trajectory Refinement</head><p>Simply using our trajectory proposals for motion forecasting will not be very accurate, as not all actors move with constant accelerations and follow Bezier curves, yet the proposals provide us good initial estimations which we can further refine. To do so, for each trajectory proposal, we construct its features using a shortcut layer on top our LaneRoI node embeddings. We then use a 2 layer MLP to decode a pair of values (s t , d t ) for each future timestamp, representing the actor position at time t in the Frenet Coordinate System <ref type="bibr" target="#b56">[56]</ref>. The Cartesian coordinates (x t , y t ) can be mapped from (s t , d t ) by first traversing along the Bezier curve distance s t (a.k.a. longitudinal), and then deviating perpendicularly from the curve distance d t (a.k.a. lateral).</p><p>The sign of d t indicates the deviation is either to-the-left or to-the-right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LaneRoI Construction</head><p>To construct a LaneRoI, we need to retrieve all relevant lanes of a given actor. In our experiments, we use a simple heuristic for this purpose. Given an HD map of a scene and an arbitrary actor in this scene, we first uniformly sample segments k with 1 meter length along each lane's centerline . Then, for the actor's location at each past timestamp, we find the nearest lane segment and collect them together into a set. This is a simplified version of lane association and achieve very high recall of the true ego-lane. Finally, we retrieve predecessing and successing lane segments (within a range D) of those segments in the set to capture lanefollowing behaviors, as well as left and right neighbors of those predecessing and successing lanes which are necessary to capture lane-changing behaviors. The range D equals to an expected length of future movement by integrating the current velocity within the prediction horizon, plus a buffer value, e.g., 20 meters. Therefore, D is dynamically changed based on actor velocity. This is motivated by the fact that high speed actors travel larger distances and thus should have larger LaneRoIs to capture their motions as well as interactions with other actors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture and Learning Details</head><p>Our LaneRCNN is constructed as follows: we first feed each input LaneRoI representation into an encoder, consists of 2 lane convolution layers and a shortcut layer, followed by another 2 lane convolution layers and a shortcut layer. We then use a lane pooling layer to build the node embeddings of the global graph (interactor), where the neighborhood threshold is set to 2 meters. Another four layers of lane convolution are applied on top of this global graph. Next, we distribute the global node embeddings to each LaneRoI by using a lane pooling layer and adding the pooled features to original LaneRoI embeddings (previous encoder outputs) as a skip-connection architecture. Another 4 layers of lane convolution and 2 layers of shortcut layers are applied afterwards. Finally, we use two layers of MLP to decode a classification score per node using its embeddings, and another two layer for regression branch as well. All layers except the final outputs have 64 channels. We use Layer Normalization <ref type="bibr" target="#b2">[3]</ref> and ReLU <ref type="bibr" target="#b36">[36]</ref> for normalization and non-linearity respectively.</p><p>During training, we apply online hard example mining <ref type="bibr" target="#b46">[46]</ref> for L cls (Eq. 3.5). Recall each node predict a binary classification score indicating whether this node is the closest lane segment to the final actor position. We use the closest lane segment to the ground-truth location as our positive  <ref type="table">Table 3</ref>: Ablation studies on lane segments sampling strategy and regression branch. We compare different sampling strategies including using the original segment labels (upsample 1x) or upsample the labels (upsample 2x), as well as uniformly sample segments along lanes (uniform 2/1m). For each, we also compare using the regression branch or not (only classification branch). Our final model is shaded in grey .</p><p>the ground-truth by more than 6 meters and the remaining nodes are 'don't care' which do not contribute any loss for L cls . Then, we randomly subsample one fourth of all negative examples and among them we use the hardest 100 negative examples for each data sample to compute our loss. The final L cls is the average of positive example loss plus the average of negative example loss. Finally, we add L reg and L ref ine with relative weights of α = 0.5 and β = 0.2 respectively to form our total loss L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>When constructing our LaneRoI, we define a lane segment to be a node in the graph. However, there are different ways to sample lane segments and we find such a choice largely affects the final performance. In <ref type="table">Table 3</ref> we show different strategies of sampling lane segments. The first four rows refer to upsampling the original lane segment labels provided in the dataset. <ref type="bibr" target="#b4">5</ref> Such a strategy provides segments with different lengths, e.g., shorter and denser segments where the geometry of the lanes changes more rapidly. The last four rows sample segments uniformly along lanes with a predefined length.</p><p>As we can observe from <ref type="table">Table 3</ref>, even though the 'upsample' strategy can result in similar average segment length as the 'uniform' strategy, it performs much worse in all metrics. This is possible because different lengths of segments introduce additional variance and harm the representation learning process. We can also conclude from the table that using denser sampling can achieve better results. Besides, we show the effectiveness of adding a regression branch for each node in addition to a classification branch, shown in 'Reg' column.</p><p>Our output parameterization explicitly leverages the lane information and thus ease the training. In <ref type="figure">Fig. 6</ref>, we validate <ref type="bibr" target="#b4">5</ref> Lanes are labeled in the format of polylines in Argoverse, thus points on those polylines naturally divide lanes into segments.</p><p>such an argument where we compare against a regressionbased variant of our model. In particular, we use the same backbone as ours, and then perform a shortcut layer on top of each LaneRoI to extract an actor-specific feature vector. We then build a multi-modal regression header which directly regresses future motions in the 2D Cartesian coordinates. <ref type="bibr" target="#b5">6</ref> We can see from <ref type="figure">Fig. 6</ref> that our model achieves decent performance when only small amounts of training data are available: with only 1% of training data, our method can achieve 20% of miss-rate. On the contrary, the regressionbased model requires much more data. This shows our method can exploit map structures as good priors and ease learning.</p><p>In <ref type="table">Table 4</ref>, we summarize ablations on different trajectory parameterizations. We can see a constant acceleration rollout slightly improves over constant speed assumption, and the Bezier curve significantly outperforms a straight line parameterization, indicating it is more realistic. In addition, adding a learnable header to refine the trajectory proposals (e.g., Bezier curve) can further boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Results</head><p>Lastly, we provide more visualization of our model outputs, as we believe the metric numbers can only reveal part of the story. We present various scenarios in <ref type="figure" target="#fig_11">Fig. 7</ref>, including turning, following curved roads, interacting with other actors and some abnormal behaviors. On the first two rows, we show turning behaviors under various map topologies. We can see our motion forecasting results nicely capture possible modes: turning into different directions or occupying different lanes after turning. We also do well even if the actor is not strictly following the centerlines. On the third row, we show predictions following curved roads, which are difficult scenarios for auto-regressive predictors <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b7">8]</ref>. On <ref type="figure">Figure 6</ref>: Model performance when using different amounts of data for training. Our output parameterization explicitly leverages lanes as priors for motion forecasting, and thus significantly ease the learning compared to directly regressing the future motions in the 2D plane.  <ref type="table">Table 4</ref>: Ablation studies on output parameterizations. We compare different ways of proposing curves (straight line v.s. Bezier quadratic curve), unrolling velocities (const as constant velocity and acc as constant acceleration), as well as using learnable refinement header or not. Our final model is shaded in grey .</p><p>the fourth row, we show that our model can predict breaking or overtaking behaviors when leading vehicles are blocking the ego-actor. Finally, we show in the fifth row that our model also works well when abnormal situations occur, e.g., driving out of roads. This is impressive as the model relies on lanes to make predictions, yet it shows capabilities to predict non-map-compliant behaviors. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt; l a t e x i t s h a 1 _</head><label>1</label><figDesc>b a s e 6 4 = " 2 P 4 M 1 9 J 4 B 8 I 9 5 1 k 5 h I u y a 5 p o e E I = " &gt; A A A B 7 X i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Q 2 Y s n X Z X c O O y g n 1 A O 5 R M m m l j M 8 m Q Z I R S + g 9 u X C j i 1 v 9 x 5 9 + Y a S u o 6 I E L h 3 P u 5 d 5 7 o p Q z b R D 6 c N b W N z a 3 t g s 7 x d 2 9 / Y P D 0 t F x W 8 t M E d o i k k v V j b C m n A n a M s x w 2 k 0 V x U n E a S e a X O V + 5 5 4 q z a S 4 N d O U h g k e C R Y z g o 2 V 2 n 3 K + W A y K J W R W 6 l 5 9 W o A k X t Z 9 w O U k 8 D z q 1 U f e i 5 a o A x W a A 5 K 7 / 2 h J F l C h S E c a 9 3 z U G r C G V a G E U 7 n x X 6 m a Y r J B I 9 o z 1 K B E 6 r D 2 e L a O T y 3 y h D G U t k S B i 7 U 7 x M z n G g 9 T S L b m W A z 1 r + 9 X P z L 6 2 U m r o U z J t L M U E G W i + K M Q y N h / j o c M k W J 4 V N L M F H M 3 g r J G C t M j A 2 o a E P 4 + h T + T 9 q + 6 y H X u 6 m U G / 4 q j g I 4 B W f g A n g g A A 1 w D Z q g B Q i 4 A w / g C T w 7 0 n l 0 X p z X Z e u a s 5 o 5 A T / g v H 0 C B o u P Y A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 P 4 M 1 9 J 4 B 8 I 9 5 1 k 5 h I u y a 5 p o e E I = " &gt; A A A B 7 X i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Q 2 Y s n X Z X c O O y g n 1 A O 5 R M m m l j M 8 m Q Z I R S + g 9 u X C j i 1 v 9 x 5 9 + Y a S u o 6 I E L h 3 P u 5 d 5 7 o p Q z b R D 6 c N b W N z a 3 t g s 7 x d 2 9 / Y P D 0 t F x W 8 t M E d o i k k v V j b C m n A n a M s x w 2 k 0 V x U n E a S e a X O V + 5 5 4 q z a S 4 N d O U h g k e C R Y z g o 2 V 2 n 3 K + W A y K J W R W 6 l 5 9 W o A k X t Z 9 w O U k 8 D z q 1 U f e i 5 a o A x W a A 5 K 7 / 2 h J F l C h S E c a 9 3 z U G r C G V a G E U 7 n x X 6 m a Y r J B I 9 o z 1 K B E 6 r D 2 e L a O T y 3 y h D G U t k S B i 7 U 7 x M z n G g 9 T S L b m W A z 1 r + 9 X P z L 6 2 U m r o U z J t L M U E G W i + K M Q y N h / j o c M k W J 4 V N L M F H M 3 g r J G C t M j A 2 o a E P 4 + h T + T 9 q + 6 y H X u 6 m U G / 4 q j g I 4 B W f g A n g g A A 1 w D Z q g B Q i 4 A w / g C T w 7 0 n l 0 X p z X Z e u a s 5 o 5 A T / g v H 0 C B o u P Y A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 P 4 M 1 9 J 4 B 8 I 9 5 1 k 5 h I u y a 5 p o e E I = " &gt; A A A B 7 X i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Q 2 Y s n X Z X c O O y g n 1 A O 5 R M m m l j M 8 m Q Z I R S + g 9 u X C j i 1 v 9 x 5 9 + Y a S u o 6 I E L h 3 P u 5 d 5 7 o p Q z b R D 6 c N b W N z a 3 t g s 7 x d 2 9 / Y P D 0 t F x W 8 t M E d o i k k v V j b C m n A n a M s x w 2 k 0 V x U n E a S e a X O V + 5 5 4 q z a S 4 N d O U h g k e C R Y z g o 2 V 2 n 3 K + W A y K J W R W 6 l 5 9 W o A k X t Z 9 w O U k 8 D z q 1 U f e i 5 a o A x W a A 5 K 7 / 2 h J F l C h S E c a 9 3 z U G r C G V a G E U 7 n x X 6 m a Y r J B I 9 o z 1 K B E 6 r D 2 e L a O T y 3 y h D G U t k S B i 7 U 7 x M z n G g 9 T S L b m W A z 1 r + 9 X P z L 6 2 U m r o U z J t L M U E G W i + K M Q y N h / j o c M k W J 4 V N L M F H M 3 g r J G C t M j A 2 o a E P 4 + h T + T 9 q + 6 y H X u 6 m U G / 4 q j g I 4 B W f g A n g g A A 1 w D Z q g B Q i 4 A w / g C T w 7 0 n l 0 X p z X Z e u a s 5 o 5 A T / g v H 0 C B o u P Y A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 P 4 M 1 9 J 4 B 8 I 9 5 1 k 5 h I u y a 5 p o e E I = " &gt; A A A B 7 X i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Q 2 Y s n X Z X c O O y g n 1 A O 5 R M m m l j M 8 m Q Z I R S + g 9 u X C j i 1 v 9 x 5 9 + Y a S u o 6 I E L h 3 P u 5 d 5 7 o p Q z b R D 6 c N b W N z a 3 t g s 7 x d 2 9 / Y P D 0 t F x W 8 t M E d o i k k v V j b C m n A n a M s x w 2 k 0 V x U n E a S e a X O V + 5 5 4 q z a S 4 N d O U h g k e C R Y z g o 2 V 2 n 3 K + W A y K J W R W 6 l 5 9 W o A k X t Z 9 w O U k 8 D z q 1 U f e i 5 a o A x W a A 5 K 7 / 2 h J F l C h S E c a 9 3 z U G r C G V a G E U 7 n x X 6 m a Y r J B I 9 o z 1 K B E 6 r D 2 e L a O T y 3 y h D G U t k S B i 7 U 7 x M z n G g 9 T S L b m W A z 1 r + 9 X P z L 6 2 U m r o U z J t L M U E G W i + K M Q y N h / j o c M k W J 4 V N L M F H M 3 g r J G C t M j A 2 o a E P 4 + h T + T 9 q + 6 y H X u 6 m U G / 4 q j g I 4 B W f g A n g g A A 1 w D Z q g B Q i 4 A w / g C T w 7 0 n l 0 X p z X Z e u a s 5 o 5 A T / g v H 0 C B o u P Y A = = &lt; / l a t e x i t &gt; f k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h P + 6 L r U f 2 d 3 t Z a l d q a Q Q v E K M X y w = " &gt; A A A B 2 X i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b n Q p u H F Z w b Z C O 5 R M 5 k 4 b m s k M y R 2 h D H 0 B F 2 5 E f C 9 3 v o 3 p z 0 J b D w Q + z k n I v S c u l L Q U B N 9 e b W d 3 b / + g f u g f N f z j k 9 N m o 2 f z 0 g j s i l z l 5 j n m F p X U 2 C V J C p 8 L g z y L F f b j 6 f 0 i 7 7 + g s T L X T z Q r M M r 4 W M t U C k 7 O 6 o y a r a A d L M W 2 I V x D C 9 Y a N b + G S S 7 K D D U J x a 0 d h E F B U c U N S a F w 7 g 9 L i w U X U z 7 G g U P N M 7 R R t R x z z i 6 d k 7 A 0 N + 5 o Y k v 3 9 4 u K Z 9 b O s t j d z D h N 7 G a 2 M P / L B i W l t 1 E l d V E S a r H 6 K C 0 V o 5 w t d m a J N C h I z R x w Y a S b l Y k J N 1 y Q a 8 Z 3 H Y S b G 2 9 D 7 7 o d B u 3 w M Y A 6 n M M F X E E I N 3 A H D 9 C B L g h I 4 B X e v Y n 3 5 n 2 s u q p 5 6 9 L O 4 I + 8 z x 8 4 x I o 4 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y 9 u 6 H A O e A p n C G g I V z y w 5 3 n j R R u g = " &gt; A A A B 3 3 i c b Z B L S w M x F I X v 1 F e t V a t b N 8 E i u C o z b n Q p u H F Z 0 T 6 g H U o m v d O G Z j J D c k c o Q 3 + C G x e K + K / c + W 9 M H w t t P R D 4 O C c h 9 5 4 o U 9 K S 7 3 9 7 p a 3 t n d 2 9 8 n 7 l o H p 4 d F w 7 q b Z t m h u B L Z G q 1 H Q j b l F J j S 2 S p L C b G e R J p L A T T e 7 m e e c Z j Z W p f q J p h m H C R 1 r G U n B y 1 m M 8 m A x q d b / h L 8 Q 2 I V h B H V Z q D m p f / W E q 8 g Q 1 C c W t 7 Q V + R m H B D U m h c F b p 5 x Y z L i Z 8 h D 2 H m i d o w 2 I x 6 o x d O G f I 4 t S 4 o 4 k t 3 N 8 v C p 5 Y O 0 0 i d z P h N L b r 2 d z 8 L + v l F N + E h d R Z T q j F 8 q M 4 V 4x S N t + b D a V B Q W r q g A s j 3 a x M j L n h g l w 7 F V d C s L 7 y J r S v G o H f C B 5 8 K M M Z n M M l B H A N t 3 A P T W i B g B G 8 w B u 8 e 8 p 7 9 T 6 W d Z W 8 V W + n 8 E f e 5 w 8 r H o x z &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q z 3 f z S z 3 a d S Z R Q a S 7 Y 3 L h k 3 J R R c = " &gt; A A A B 3 3 i c d Z D N S g M x F I X v 1 L 9 a q 1 a 3 b o J F c F U S i 3 a 6 E 9 y 4 r G h t o R 1 K J s 2 0 o Z n M k G S E M v Q R 3 L h Q x L d y 5 9 u Y / g g q e i B w O C c h 9 3 5 h K o W x G H 9 4 h b X 1 j c 2 t 4 n Z p p 7 y 7 t 1 8 5 K N + b J N O M t 1 k i E 9 0 N q e F S K N 6 2 w k r e T T W n c S h 5 J 5 x c z f v O A 9 d G J O r O T l M e x H S k R C Q Y t S 6 6 j Q a T Q a W K a 8 Q n 5 8 0 m w r V 6 o 3 F B s D M + 9 v 0 6 Q a S G F 6 r C S q 1 B 5 b 0 / T F g W c 2 W Z p M b 0 C E 5 t k F N t B Z N 8 V u p n h q e U T e i I 9 5 x V N O Y m y B e j z t C J S 4 Y o S r Q 7 y q J F + v 1 F T m N j p n H o b s b U j s 3 v b h 7 + 1 f U y G / l B L l S a W a 7 Y 8 q M o k 8 g m a L 4 3 G g r N m Z V T Z y j T w s 2 K 2 J h q y q y j U 3 I Q v j Z F / 5 v 7 s x p x r G 4 w F O E I j u E U C D T g E q 6 h B W 1 g M I J H e I Y X T 3 p P 3 u s S V 8 F b c T u E H / L e P g G i r 4 z I &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 5 1 q Z o p z v o z J r 8 k t u 1 v Q O a 8 2 s s o = " &gt; A AA B 6 n i c d V D J S g N B E K 2 J W 4 x b 1 K O X x i B 4 C t 0 J m s k t 4 M V j R L N A M o S e T k / S p G e h u 0 c I Q z 7 B i w d F v P p F 3 v w b O 4 u g o g 8 K H u 9 V U V X P T 6 T Q B u M P J 7 e 2 v r G 5 l d 8 u 7 O z u 7 R 8 U D 4 / a O k 4 V 4 y 0 W y 1 h 1 f a q 5 F B F v G W E k 7 y a K 0 9 C X v O N P r u Z + 5 5 4 r L e L o z k w T 7 o V 0 F I l A M G q s d B s M J o N i C Z e J S y 7 q d Y T L 1 V r t k m B L X O y 6 V Y J I G S 9 Q g h W a g + J 7 f x i z N O S R Y Z J q 3 S M 4 M V 5 G l R F M 8 l m h n 2 q e U D a h I 9 6 z N K I h 1 1 6 2 O H W G z q w y R E G s b E U G L d T v E x k N t Z 6 G v u 0 M q R n r 3 9 5 c / M v r p S Z w v U x E S W p 4 x J a L g l Q i E 6 P 5 3 2 g o F G d G T i 2 h T A l 7 K 2 J j q i g z N p 2 C D e H r U / Q / a V f K x G Z 1 g 0 u N y i q O P J z A K Z w D g R o 0 4 B q a 0 A I G I 3 i A J 3 h 2 p P P o v D i v y 9 a c s 5 o 5 h h 9 w 3 j 4 B v V S O C Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w h a R 3 Q z 6 i C + f t 9 o 7 X + 7 d L i k 4 C e w = " &gt; A A A B 6 n i c d V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 L T u J m s 0 t 4 M V j R P O A Z A m z k 9 l k y O z s M j M r h J B P 8 O J B E a 9 + k T f / x s l D U N G C h q K q m + 6 u M B V c G 8 / 7 c F Z W 1 9 Y 3 N n N b + e 2 d 3 b 3 9 w s F h U y e Z o q x B E 5 G o d k g 0 E 1 y y h u F G s H a q G I l D w V r h 6 G r m t + 6 Z 0 j y R d 2 a c s i A m A 8 k j T o m x 0 m 3 U G / U K R c / F P r 6 o V p H n l i u V S + x Z 4 n u + X 8 Y I u 9 4 c R V i i 3 i u 8 d / s J z W I m D R V E 6 w 7 2 U h N M i D K c C j b N d z P N U k J H Z M A 6 l k o S M x 1 M 5 q d O 0 a l V + i h K l C 1 p 0 F z 9 P j E h s d b j O L S d M T F D / d u b i X 9 5 n c x E f j D h M s 0 M k 3 S x K M o E M g m a / Y 3 6 X D F q x N g S Q h W 3 t y I 6 J I p Q Y 9 P J 2 x C + P k X / k 2 b J x T a r m / N i r b S M I w f H c A J n g K E C N b i G O j S A w g A e 4 A m e H e E 8 O i / O 6 6 J 1 x V n O H M E P O G + f v p S O D Q = = &lt; / l a t e x i t &gt; F i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h P + 6 L r U f 2 d 3 t Z a l d q a Q Q v E K M X y w = " &gt; A A A B 2 X i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b n Q p u H F Z w b Z C O 5 R M 5 k 4 b m s k M y R 2 h D H 0 B F 2 5 E f C 9 3 v o 3 p z 0 J b D w Q + z k n I v S c u l L Q U B N 9 e b W d 3 b / + g f u g f N f z j k 9 N m o 2 f z 0 g j s i l z l 5 j n m F p X U 2 C V J C p 8 L g z y L F f b j 6 f 0 i 7 7 + g s T L X T z Q r M M r 4 W M t U C k 7 O 6 o y a r a A d L M W 2 I V x D C 9 Y a N b + G S S 7 K D D U J x a 0 d h E F B U c U N S a F w 7 g 9 L i w U X U z 7 G g U P N M 7 R R t R x z z i 6 d k 7 A 0 N + 5 o Y k v 3 9 4 u K Z 9 b O s t j d z D h N 7 G a 2 M P / L B i W l t 1 E l d V E S a r H 6 K C 0 V o 5 w t d m a J N C h I z R x w Y a S b l Y k J N 1 y Q a 8 Z 3 H Y S b G 2 9 D 7 7 o d B u 3 w M Y A 6 n M M F X E E I N 3 A H D 9 C B L g h I 4 B X e v Y n 3 5 n 2 s u q p 5 6 9 L O 4 I + 8 z x 8 4 x I o 4 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O i I 3 J C 2 z 0 3 V y f n S v 6 7 Z B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>J w a 9 z S S h f p 7 o 2 C J t b M k c p N l R r v u l e J / 3 i D H + C Y s p M 5 y F J o v D 8 W 5 I p i S s g A y k k Z w V D N H G D f S Z S V 8 w g z j 6 G q q u R L 8 9 S 9 v k u 5 V y 6 c t / 5 F C F c 7 g H C 7 B h 2 u 4 h Q d o Q w c 4 Z P A C b / D u 5 d 6 r 9 7 G s q + K t e j u F P / A + f w D r e Z B o &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O i I 3 J C 2 z 0 3 V y f n S v 6 7 Z B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>J w a 9 z S S h f p 7 o 2 C J t b M k c p N l R r v u l e J / 3 i D H + C Y s p M 5 y F J o v D 8 W 5 I p i S s g A y k k Z w V D N H G D f S Z S V 8 w g z j 6 G q q u R L 8 9 S 9 v k u 5 V y 6 c t / 5 F C F c 7 g H C 7 B h 2 u 4 h Q d o Q w c 4 Z P A C b / D u 5 d 6 r 9 7 G s q + K t e j u F P / A + f w D r e Z B o &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e i q y E H L T a + b Q a f S 0 A O h n T P 3 7 l t Q = " &gt; A A A B 8 3 i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 J 4 0 Y O H g i A e K 9 h a a E L Z b D f t 0 s 0 m 7 L 4 I J f R v e P G g i F f / j D f / j Z s 2 B 2 0 d W B h m 3 u P N T p h K Y d B 1 v 5 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / U D 8 8 6 p o k 0 4 x 3 W C I T 3 Q u p 4 V I o 3 k G B k v d S z W k c S v 4 Y T m 4 K / / G J a y M S 9 Y D T l A c x H S k R C U b R S r 4 f U x y H U X 4 7 G 4 h B v e E 2 3 T n I K v F K 0 o A S 7 U H 9 y x 8 m L I u 5 Q i a p M X 3 P T T H I q U b B J J / V / M z w l L I J H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9 Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d 4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9 Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d 4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9 Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d 4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y J Y Q z 0 l y 1 C 3 k + w 1 K P j 7 F K Z d c o I w = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 M J F Q R C X F e w D m l A m 0 0 k 7 d D I J M z d C C f 0 N N y 4 U c e v P u P N v n L Z Z a O u B g c M 5 9 3 L P n D C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D Z J p h l v s U Q m u h t S w 6 V Q v I U C J e + m m t M 4 l L w T j m 9 n f u e J a y M S 9 Y i T l A c x H S o R C U b R S r 4 f U x y F U X 4 3 7 Y t + t e b W 3 T n I K v E K U o M C z X 7 1 y x 8 k L I u 5 Q i a p M T 3 P T T H I q U b B J J 9 W / M z w l L I x H f K e p Y r G 3 A T 5 P P O U n F l l Q K J E 2 6 e Q z N X f G z m N j Z n E o Z 2 c Z T T L 3 k z 8 z + t l G F 0 H u V B p h l y x x a E o k w Q T M i u A D I T m D O X E E s q 0 s F k J G 1 F N G d q a K r Y E b / n L q 6 R 9 U f f c u v d w W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E M T W s A g h W d 4 h T c n c 1 6 c d + d j M V p y i p 1 j + A P n 8 w c x F J H A &lt; / l a t e x i t &gt;G i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 nZ k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B De 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M= " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B De 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M= " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B De 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h P + 6 L r U f 2 d 3 t Z a l d q a Q Q v E K M X y w = " &gt; A A A B 2 X i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b n Q p u H F Z w b Z C O 5 R M 5 k 4 b m s k M y R 2 h D H 0 B F 2 5 E f C 9 3 v o 3 p z 0 J b D w Q + z k n I v S c u l L Q U B N 9 e b W d 3 b / + g f u g f N f z j k 9 N m o 2 f z 0 g j s i l z l 5 j n m F p X U 2 C V J C p 8 L g z y L F f b j 6 f 0 i 7 7 + g s T L X T z Q r M M r 4 W M t U C k 7 O 6 o y a r a A d L M W 2 I V x D C 9 Y a N b + G S S 7 K D D U J x a 0 d h E F B U c U N S a F w 7 g 9 L i w U X U z 7 G g U P N M 7 R R t R x z z i 6 d k 7 A 0 N + 5 o Y k v 3 9 4 u K Z 9 b O s t j d z D h N 7 G a 2 M P / L B i W l t 1 E l d V E S a r H 6 K C 0 V o 5 w t d m a J N C h I z R x w Y a S b l Y k J N 1 y Q a 8 Z 3 H Y S b G 2 9 D 7 7 o d B u 3 w M Y A 6 n M M F X E E I N 3 A H D 9 C B L g h I 4 B X e v Y n 3 5 n 2 s u q p 5 6 9 L O 4 I + 8 z x 8 4 x I o 4 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P a Q 3 B T Z D O G Q c 3 l 1 c Z 8 R N C M B U M V 0 = " &gt; A A A B 6 X i c b V B N S w M x F H x b v 2 q t W r 1 6 C R b B U 9 n 1 o k f B g x 4 r 2 A 9 o l / I 2 z b a h S X Z N s o W y 9 H d 4 8 a C I f 8 i b / 8 Z s 2 4 O 2 D g S G m f d 4 k 4 l S w Y 3 1 / W + v t L W 9 s 7 t X 3 q 8 c V A + P j m s n 1 b Z J M k 1 Z i y Y i 0 d 0 I D R N c s Z b l V r B u q h n K S L B O N L k r / M 6 U a c M T 9 W R n K Q s l j h S P O U X r p L A v 0 Y 4 p i v x + P u C D W t 1 v + A u Q T R K s S B 1 W a A 5 q X / 1 h Q j P J l K U C j e k F f m r D H L X l V L B 5 p Z 8 Z l i K d 4 I j 1 H F U o m Q n z R e g 5 u X D K k M S J d k 9 Z s l B / b + Q o j Z n J y E 0 W I c 2 6 V 4 j / e b 3 M x j d h z l W a W a b o 8 l C c C W I T U j R A h l w z a s X M E a S a u 6 y E j l E j t a 6 n i i s h W P / y J m l f N Q K / E T z 6 U I Y z O I d L C O A a b u E B m t A C C s / w A m / w 7 k 2 9 V + 9 j W V f J W / V 2 C n / g f f 4 A r 7 m Q 2 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P a Q 3 B T Z D O G Q c 3 l 1 c Z 8 R N C M B U M V 0 = " &gt; A A A B 6 X i c b V B N S w M x F H x b v 2 q t W r 1 6 C R b B U 9 n 1 o k f B g x 4 r 2 A 9 o l / I 2 z b a h S X Z N s o W y 9 H d 4 8 a C I f 8 i b / 8 Z s 2 4 O 2 D g S G m f d 4 k 4 l S w Y 3 1 / W + v t L W 9 s 7 t X 3 q 8 c V A + P j m s n 1 b Z J M k 1 Z i y Y i 0 d 0 I D R N c s Z b l V r B u q h n K S L B O N L k r / M 6 U a c M T 9 W R n K Q s l j h S P O U X r p L A v 0 Y 4 p i v x + P u C D W t 1 v + A u Q T R K s S B 1 W a A 5 q X / 1 h Q j P J l K U C j e k F f m r D H L X l V L B 5 p Z 8 Z l i K d 4 I j 1 H F U o m Q n z R e g 5 u X D K k M S J d k 9 Z s l B / b + Q o j Z n J y E 0 W I c 2 6 V 4 j / e b 3 M x j d h z l W a W a b o 8 l C c C W I T U j R A h l w z a s X M E a S a u 6 y E j l E j t a 6 n i i s h W P / y J m l f N Q K / E T z 6 U I Y z O I d L C O A a b u E B m t A C C s / w A m / w 7 k 2 9 V + 9 j W V f J W / V 2 C n / g f f 4 A r 7 m Q 2 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U K q Y / t m / Z n R O 8 J v 0 M K s 4 8 G k Q J L 0 = " &gt; A A A B 9 H i c b V C 7 S g N B F L 3 r M 8 Z X 1 N J m M A h W Y d d G C 4 u A h Z Y R z A O S J c x O Z p M h s 7 P r z N 1 A W P I d N h a K 2 P o x d v 6 N s 8 k W m n h g 4 H D O v d w z J 0 i k M O i 6 3 8 7 a + s b m 1 n Z p p 7 y 7 t 3 9 w W D k 6 b p k 4 1 Y w 3 W S x j 3 Q m o 4 V I o 3 k S B k n c S z W k U S N 4 O x r e 5 3 5 5 w b U S s H n G a c D + i Q y V C w S h a y e 9 F F E e M y u x u 1 h f 9 S t W t u X O Q V e I V p A o F G v 3 K V 2 8 Q s z T i C p m k x n Q 9 N 0 E / o x o F k 3 x W 7 q W G J 5 S N 6 Z B 3 L V U 0 4 s b P 5 q F n 5 N w q A x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k H t I s e 7 n 4 n 9 d N M b z 2 M 6 G S F L l i i 0 N h K g n G J G + A D I T m D O X U E s q 0 s F k J G 1 F N G d q e y r Y E b / n L q 6 R 1 W f P c m v f g V u s 3 R R 0 l O I U z u A A P r q A O 9 9 C A J j B 4 g m d 4 h T d n 4 r w 4 7 8 7 H Y n T N K X Z O 4 A + c z x / 4 y J I v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B D e 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B D e 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B D e 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B D e 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B D e 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A x G z k 9 E J P 0 q R q Q 7 7 J y k x I f + n N Q M = " &gt; A A A B 9 H i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 G w C r s i a G E R s N A y g n l A s o S 7 k 0 k y Z H Z 2 n Z k N h C X f Y W O h i K 0 f Y + f f O J t s o Y k H B g 7 n 3 M s 9 c 4 J Y c G 1 c 9 9 s p r K 1 v b G 4 V t 0 s 7 u 3 v 7 B + X D o 6 a O E k V Z g 0 Y i U u 0 A N R N c s o b h R r B 2 r B i G g W C t Y H y b + a 0 J U 5 p H 8 t F M Y + a H O J R 8 w C k a K / n d E M 2 I o k j v Z j 3 e K 1 f c q j s H W S V e T i q Q o 9 4 r f 3 X 7 E U 1 C J g 0 V q H X H c 2 P j p 6 g M p 4 L N S t 1 E s x j p G I e s Y 6 n E k G k / n Y e e k T O r 9 M k g U v Z J Q + b q 7 4 0 U Q 6 2 n Y W A n s 5 B 6 2 c v E / 7 x O Y g b X f s p l n B g m 6 e L Q I B H E R C R r g P S 5 Y t S I q S V I F b d Z C R 2 h Q m p s T y V b g r f 8 5 V X S v K h 6 b t V 7 u K z U b v I 6 i n A C p 3 A O H l x B D e 6 h D g 2 g 8 A T P 8 A p v z s R 5 c d 6 d j 8 V o w c l 3 j u E P n M 8 f + g i S M w = = &lt; / l a t e x i t &gt; The LaneRoI of the actor i is a collection of a graph G i (constructed following lane topology: nodes as lane segments and edges as segment connectivities) and node embeddings F i (encoding motions of the actor, as well as geometric and semantic properties of lane segments).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>An illustration for lane convolution and lane pooling operators, which have similar functionalities as their 2D counterparts while respecting the lane topology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on Argoverse validation set. Here we show (from left-to-right): 1) curved lanes 2) lane changing 3) intersection 4) overtaking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on Argoverse validation set. We show a various of scenarios including, turning (row 1-2), curved roads (row 3), breaking and overtaking (row 4), abnormal behaviors (row 5).14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.1. We then define our LaneRoI representations in Sec. 3.2. In Sec. 3.3, we explain how LaneRCNN processes features and models interactions via graph-based message-passing. Finally, we show our mapaware trajectory prediction and learning objectives in Sec. 3.4 and Sec. 3.5 respectively.</figDesc><table><row><cell>Raw Map Data</cell><cell>Lane Graph Representation</cell></row><row><cell></cell><cell>Geometric Encoding</cell></row><row><cell></cell><cell>Semantic Encoding</cell></row><row><cell></cell><cell>Actor Encoding</cell></row><row><cell></cell><cell>Relative poses</cell></row><row><cell></cell><cell>at observations</cell></row><row><cell></cell><cell>Predecessor, Left, etc`k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Argoverse Motion Forecasting Leaderboard. All metrics are lower the better and Miss-Rate (MR, K=6) is the official ranking metric.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="6">K=1 minADE minFDE MR minADE minFDE MR K=6</cell></row><row><cell></cell><cell>NN</cell><cell>3.45</cell><cell>7.88</cell><cell>87.0</cell><cell>1.71</cell><cell>3.28</cell><cell>53.7</cell></row><row><cell>Argoverse Baseline [10]</cell><cell>NN+map</cell><cell>3.65</cell><cell>8.12</cell><cell>94.0</cell><cell>2.08</cell><cell>4.02</cell><cell>58.0</cell></row><row><cell></cell><cell>LSTM+map</cell><cell>2.92</cell><cell>6.45</cell><cell>75.0</cell><cell>2.08</cell><cell>4.19</cell><cell>67.0</cell></row><row><cell></cell><cell>TNT (4th) [61]</cell><cell>1.78</cell><cell>3.91</cell><cell>59.7</cell><cell>0.94</cell><cell>1.54</cell><cell>13.3</cell></row><row><cell>Leaderboard [1]</cell><cell>Jean (3rd) [34] Poly (2nd) [1]</cell><cell>1.74 1.71</cell><cell>4.24 3.85</cell><cell>68.6 59.6</cell><cell>1.00 0.89</cell><cell>1.42 1.50</cell><cell>13.1 13.1</cell></row><row><cell></cell><cell>Ours-LaneRCNN (1st)</cell><cell>1.69</cell><cell>3.69</cell><cell>56.9</cell><cell>0.90</cell><cell>1.45</cell><cell>12.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablations on different modules of LaneRCNN. Metrics are reported on the validation set. In the upper half, we examine our LaneRoI Encoder, comparing per-actor 1D feature vector v.s. LaneRoI representations as well as different designs for the shortcut mechanism. In the lower half, we compare different strategies to model interactions, including a fully connected graph among actors with GNN / attention, as well as ours. Pooling refers to how we pool a 1D actor feature from each LaneRoI which are used by GNN / attention. Rows shaded in gray indicate the architecture used in our final model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>example. Negative examples are all nodes deviating from Sampling Avg Length ( k ) Reg min 1 ADE min 1 FDE min 6 ADE min 6 FDE min 6 MR</figDesc><table><row><cell>up 1x</cell><cell>2.1 m</cell><cell>1.52</cell><cell>3.32</cell><cell>0.94</cell><cell>1.69</cell><cell>24.0</cell></row><row><cell>up 1x</cell><cell>2.1 m</cell><cell>1.41</cell><cell>3.03</cell><cell>0.83</cell><cell>1.35</cell><cell>14.1</cell></row><row><cell>up 2x</cell><cell>1.1 m</cell><cell>1.43</cell><cell>3.09</cell><cell>0.85</cell><cell>1.39</cell><cell>13.4</cell></row><row><cell>up 2x</cell><cell>1.1 m</cell><cell>1.39</cell><cell>2.99</cell><cell>0.80</cell><cell>1.24</cell><cell>10.2</cell></row><row><cell>uniform</cell><cell>2 m</cell><cell>1.39</cell><cell>2.94</cell><cell>0.86</cell><cell>1.44</cell><cell>10.5</cell></row><row><cell>uniform</cell><cell>2 m</cell><cell>1.35</cell><cell>2.86</cell><cell>0.80</cell><cell>1.29</cell><cell>8.2</cell></row><row><cell>uniform</cell><cell>1 m</cell><cell>1.37</cell><cell>2.90</cell><cell>0.83</cell><cell>1.32</cell><cell>9.9</cell></row><row><cell>uniform</cell><cell>1 m</cell><cell>1.33</cell><cell>2.85</cell><cell>0.77</cell><cell>1.19</cell><cell>8.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Curve Velocity Learnable min 1 ADE min 6 ADE</figDesc><table><row><cell>line</cell><cell>const</cell><cell>1.53</cell><cell>1.04</cell></row><row><cell>line</cell><cell>acc</cell><cell>1.52</cell><cell>1.02</cell></row><row><cell>line</cell><cell>acc</cell><cell>1.41</cell><cell>0.86</cell></row><row><cell>Bezier</cell><cell>const</cell><cell>1.46</cell><cell>0.96</cell></row><row><cell>Bezier</cell><cell>acc</cell><cell>1.44</cell><cell>0.94</cell></row><row><cell>Bezier</cell><cell>acc</cell><cell>1.33</cell><cell>0.77</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">On Argoverse, we follow the official metric and use K=6. We also remove duplicate goals if two predictions are too close, where the lower confidence one is ignored.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Snapshot of the leaderboard at the submission time: Nov. 12, 2020.4.3. Ablation StudiesAblations on LaneRoI Encoder: We first show the ablation study on one of our main contributions, i.e., LaneRoI , in the upper half ofTable 2. The first row shows a representative of the traditional representations. Specifically, we first build embeddings for lane graph nodes using only the map information and 4 lane convolution layers. We then use a 1D CNN (U-net style) to extract a motion feature vector from actor kinematic states, concatenate it with every graph node embedding and make predictions. Conceptually, this is similar to TNT<ref type="bibr" target="#b61">[61]</ref> except that we modify the backbone network to make comparisons fair. On the second row, we show the result of our LaneRoI representations with again four lane convolution layers (no shortcuts). Hence, the only difference is whether the actor is encoded with a single motion vector shared by all nodes, or encoded in a distributed and structured manner as ours. As shown in the table, our LaneRoI achieves similar or better results on all metrics, exhibiting its advantages. Note that this row is not yet our best result in terms of using LaneRoI representations, as the actor information is only exposed to a small region during the input encoding (clamping at input node embeddings) and can not efficiently propagate to the full LaneRoI without the help of the shortcut, which we will show next.Subsequent rows inTable 2compare different design choices for the shortcut mechanism, in particular how we pool the global feature for each LaneRoI. 'Global Pool'</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We choose LaneRoI encoder rather than other encoder, e.g., CNN, for fair comparisons with ours.<ref type="bibr" target="#b3">4</ref> The GNN here is almost identical to our lane convolution used in Interactor except for removing the multi-hop as the graph is fully-connected.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">As building such a header is non-trivial, we borrow an open-sourced implementation in LaneGCN<ref type="bibr" target="#b28">[29]</ref> which has tuned on the same dataset and shown strong results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to sincerely thank Siva Manivasagam, Yun Chen, Bin Yang, Wei-Chiu Ma and Shenlong Wang for their valuable help on this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Argoverse motion forecasting competition</title>
		<ptr target="https://eval.ai/web/challenges/challenge-page/454/leaderboard/1279.2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Layer normalization. arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Ogale</surname></persName>
		</author>
		<title level="m">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. arXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spatially-aware graph neural networks for relational behavior forecasting from sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><surname>Gulino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implicit latent variable model for scene-consistent motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><surname>Gulino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2nd Conference on Robot Learning</title>
		<meeting>The 2nd Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified framework for multi-target tracking and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding collective activitiesof people from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jeff Schneider, and Nemanja Djuric. Multimodal trajectory predictions for autonomous driving using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How would surround vehicles move? a unified framework for maneuver classification and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional social pooling for vehicle trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiket</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Discrete residual flow for probabilistic pedestrian behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end contextual perception and prediction with interaction transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning lane graph representations for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pnpnet: End-to-end perception and prediction with tracking in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Forecasting interactive dynamics of pedestrians with fictitious play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei-Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><forename type="middle">El</forename><surname>Zoghby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Sandou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beauvois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Pita</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-head attention for multi-modal joint vehicle motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Covernet: Multimodal behavior prediction using trajectory sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><forename type="middle">Corina</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddy</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">R2p2: A reparameterized pushforward policy for diverse, precise generative path forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vernaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Precog: Prediction conditioned on goals in visual multi-agent settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pranaab Dhawan, and Raquel Urtasun. Perceive, predict, and plan: Safe motion planning through interpretable semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Car-net: Clairvoyant attentive recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdinand</forename><surname>Legros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv, 2020. 3</idno>
		<title level="m">Pip: Planninginformed trajectory prediction for autonomous driving</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Relational action forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Multiple futures prediction. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Yichuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Social attention: Modeling attention in human crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Vemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Muelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">V2vnet: Vehicle-to-vehicle communication for joint perception and prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivabalan</forename><surname>Manivasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urtasun</forename><surname>Raquel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Bin Yang, and Raquel Urtasun. Perceive, attend, and drive: Learning spatial attention for safe self-driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<idno>arXiv, 2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Optimal trajectory generation for dynamic street scenarios in a frenet frame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Kammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Who are you with and where are you going</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-end interpretable neural motion planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Bin Yang, and Raquel Urtasun. Dsdnet: Deep structured selfdriving network. arXiv, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">Target-driven trajectory prediction. arXiv</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-agent tensor fusion for contextual trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
