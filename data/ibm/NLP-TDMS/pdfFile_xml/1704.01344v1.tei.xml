<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an endto-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic image segmentation enjoys wide applications, such as video surveillance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">36]</ref> and autonomous driving <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5]</ref>. Recent advanced deep architectures, such as the residual network (ResNet) <ref type="bibr" target="#b12">[13]</ref> and Inception <ref type="bibr" target="#b33">[32]</ref>, significantly improve the accuracy of image segmentation by increasing the depth and number of parameters in deep models. For example, ResNet-101 is six times deeper than VGG-16 <ref type="bibr" target="#b29">[29]</ref> network, with the former outperforms the latter by 4 percent on the challenging PASCAL VOC 2012 image segmentation benchmark <ref type="bibr" target="#b7">[8]</ref>.</p><p>Although promising results can be achieved through the increase of model capacity, they come with a price of runtime complexity, which impedes the deployments of 72.5% 0.9% 1.1% 1.0% 0.8% 0.9%  and its ground truth label map (middle) from the Pascal VOC 2012 dataset. The difficulty level (e.g. recognizability) of pixels are visualized in the right image, where pixels are partitioned into three sets, including 'easy' (ES), 'moderate' (MS), and 'extremely hard' (HS) sets. (b) depicts two histograms. The left one plots the percentages of pixels in VOC validation set with respect to each object category. It can be observed that ES occupies at least 30% pixels of most objects. The right one reveals that 70% pixels in HS are located at object boundaries, which have large ambiguity.</p><p>Best viewed in color with 300% zoom.</p><p>existing deep models in many applications that demand real-time performance. For instance, the segmentation speeds of VGG, ResNet-101, and Inception-ResNet on a 300×500 image are 5.7, 7.1 and 9.0 frame per second (FPS), which are far away from real time. To address this issue, this work presents Deep Layer Cascade (LC), which not only substantially reduces the runtime of deep models, but also improves their segmentation accuracies. Many deep architectures, including VGG, ResNet, and Inception, can benefit from the above appealing properties by adapting their structures into LC. Layer Cascade inherits the advantage of the conventional model cascade (MC) <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b36">35]</ref>, which has multiple stages and usually trains one classifier in each stage. MC is capable of increasing both speed and accuracy for object detection, because the earlier stages (classifiers) reject most of the easy samples (detection windows) and the later stages can pay attention on a small number of difficult samples, thus reducing false alarms. Different from MC, LC is carefully devised for deep models in the task of image segmentation. It considers different layers in a deep network as different stages. In particular, most of the pixels in an image are recognizable by the lower stages and the higher stages, which typically possess far more parameters than the bottom layers, are learned to recognize a small set of challenging pixels. In this case, the runtime of deep models can be significantly reduced by LC. Moreover, unlike MC that learns the current stage by keeping all previous stages fixed, LC trains all stages jointly to boost performance.</p><p>Another important difference between LC and MC is the cascade strategy. In MC, the current stage propagates a sample to the next stage, if its classification score or probability (i.e. the response after softmax) is higher than a large threshold, such as 0.95, indicating that this sample is classified as positive by the current stage with 95% confidence. In other words, later stages refine the labels of samples that are considered highly positive in the previous stages, so as to reduce false alarms.</p><p>In contrast, LC 'rejects' samples with high scores in earlier stages, but those samples with low and moderate confidences are propagated forward. <ref type="figure" target="#fig_0">Figure 1</ref> takes the segmentation results of LC as an example to illustrate this cascade strategy. In (a), an image of 'cow' and 'background' and its ground truth label map from the VOC validation set (VOC val) are shown on the left and middle respectively. We partition all pixels in the validation set into three different sets, namely "easy", "moderate", and "extremely hard" sets. The easy set (ES) contains pixels that are correctly classified with larger than 95% confidence, while the extremely hard set (HS) comprises pixels that are misclassified with larger than 95% confidence. The moderate set (MS) covers pixels that have classification scores smaller than 0.95.</p><p>In a certain stage of LC, ES and HS are discarded and MS is propagated to the next stage, because of the following two reasons. First, as shown in the right histogram of <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, we observe that almost 70 percent 1 pixels in HS are located on the boundaries between objects, demonstrating that these pixels are extremely hard to be recognized because of large ambiguity. An example is given by the right image of <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Fitting HS during training may lead to over-fitting in the test stage. Second, the left histogram of <ref type="figure" target="#fig_0">Fig. 1</ref>(b) plots the percentages of pixels with respect to each object category in VOC val. For most of the categories, we <ref type="bibr" target="#b0">1</ref> We found that the other 30 percent pixels in HS have wrong annotations. Since our purpose is to improve speed and accuracy of deep models, we do not correct those wrong annotations to enable a fair comparison with previous works. found that at least 30 percent pixels belong to ES. As the background pixels are dominated (72.5%), rejecting ES and HS reduces more than 40 present pixels in earlier stages and thus significantly reduces computations of deep networks, while improves accuracy, by enabling deeper layers to focus on foreground objects.</p><p>This study makes three main contributions. (1) This is the first attempt to identify the segmentation difficulty of pixels for deep models. With this observation, a novel Deep Layer Cascade (LC) approach is proposed to significantly reduce computations of deep networks while improving their segmentation accuracies. (2) LC's properties can be easily applied to many recent advanced network structures. After applying LC on Inception-ResNet-v2 (IRNet) <ref type="bibr" target="#b33">[32]</ref>, its speed and accuracy are improved by 42.8% and 1.7%, respectively. (3) Connections between LC and previous models such as model cascade, deeply supervised network <ref type="bibr" target="#b17">[17]</ref>, and dropout <ref type="bibr" target="#b30">[30]</ref> are clearly presented. Extensive studies are conducted to demonstrate the superiority of LC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Image Segmentation. While early efforts focused on structural models with handcrafted features <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b39">38]</ref>, recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b41">40]</ref>. For instance, Long et al. <ref type="bibr" target="#b25">[25]</ref> transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>. Chen et al. <ref type="bibr" target="#b2">[3]</ref>, Zheng et al. <ref type="bibr" target="#b41">[40]</ref>, and Liu et al. <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> further showed that back-propagation and inference of Markov Random Field (MRF) can be incorporated into CNN. Though attaining high accuracy, these models generally have high computational costs, preventing them from deploying in real-time.</p><p>Another line of research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">27]</ref> alleviates this problem by using lightweight network architectures. For example, SegNet [1] adopted a convolutional encoder-decoder and removed unnecessary layers to reduce the number of parameters. ENet <ref type="bibr" target="#b27">[27]</ref> utilized a bottleneck module to reduce computation of convolutions. Although these networks are speeded up, they sacrificed high performances as presented in previous deep models. This work proposes Deep Layer Cascade (LC), which improves both speed and accuracy of existing deep networks. It achieves state-ofthe-art performances on both Pascal VOC and Cityscape datasets, and runs in real time. Deep Learning Cascade. Network cascades <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b24">24]</ref> have been studied to improve the performance in classification <ref type="bibr" target="#b26">[26]</ref>, detection <ref type="bibr" target="#b18">[18]</ref>, and pose estimation <ref type="bibr" target="#b34">[33]</ref>. For example, Deep Decision Network <ref type="bibr" target="#b26">[26]</ref> improved the image classification performance by dividing easy data from the The tables at the right show the structure of IRNet.</p><p>hard ones. The hard cases with high confusion will be propagated and handled by the subsequent expert networks.</p><p>Li et al. <ref type="bibr" target="#b18">[18]</ref> used CNN cascade for face detection, which rejects false detections quickly in early stages and carefully refines detections in later stages. DeepPose <ref type="bibr" target="#b34">[33]</ref> employed a divide-and-conquer strategy and designed a cascaded deep regression framework for human pose estimation. Different from previous network cascades that train each network separately, LC is jointly optimized to boost the segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Layer Cascade (LC)</head><p>Sec. 3.1 takes Inception-ResNet-v2 <ref type="bibr" target="#b33">[32]</ref> as an example to illustrate how one could turn a deep model into LC. The approach can be easily generalized to the other deep networks. Sec. 3.3 introduces the training algorithm of LC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Turning a Deep Model into LC</head><p>Network Overview. To illustrate the effectiveness of LC, we choose Inception-ResNet-v2 pre-trained on ImageNet dataset as a strong baseline, denoted as IRNet, which outperforms ResNet-101 by 1.2% on the Pascal VOC2012 validation set. Experiments demonstrate that LC is able to achieve 1.7% improvement on this competitive baseline. <ref type="figure">Figure 2</ref> (a) visualizes the architecture of IRNet, which has six different components, including 'Stem', 'IRNet-A/B/C', and 'Reduction-A/B'. Different components have different configurations of layers, such as convolution, pooling, and concatenation layer. The right column of <ref type="figure">Fig. 2</ref> shows the structures of 'Stem' and 'IRNet-A/B/C' respectively, including layer types, kernel sizes, and the number of channels (in bracket). The stride typical equals one unless otherwise stated. For example, 'Stem' employs an RGB image as an input and produces features of 384 channels. More specifically, the input image is forwarded to three convolutional layers with 3×3 kernels, and then the learned features are split into two streams, which have 3 and 5 convolutional layers respectively.</p><p>Similar network structure as IRNet has achieved great success in image recognition <ref type="bibr" target="#b33">[32]</ref>. However, two important modifications are necessary to adapt it to image segmentation. Firstly, to increase the resolution of prediction, we remove the pooling layer at the end of IRNet and enlarge the size of feature maps by decreasing the convolutional strides in 'Reduction-A/B' (from 2 to 1). In this case, we expand the size of network outputs (label maps) by 4×. We also replace convolutions in 'IRNet-B/C' by the dilated convolutions similar to <ref type="bibr" target="#b2">[3]</ref>. Secondly, as feature maps with high resolution consume a large amount of GPU memory in the learning process, they limit the size of minibatch (e.g. 8), making the batch normalization (BN) layers <ref type="bibr" target="#b14">[14]</ref> unstable (as which need to estimate sample mean and variance from the data in a mini-batch). We cope with this issue by simply fixing the values of all parameters in BNs. This strategy works well in practice.</p><p>From IRNet to LC (IRNet-LC). IRNet is turned into LC by dividing its different components as different stages. The number of stages is three, which is a common setting in previous cascade methods <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b34">33]</ref>. As shown in <ref type="figure">Fig. 2</ref> (b), components before 'Reduction-A' are considered as the first stage, components between 'Reduction-A' and '-B' are the second stage, and the remaining layers become the third stage. In <ref type="figure">Fig. 2</ref> (b), these three stages are distinguished in yellow, green, and blue respectively. For instance, stage-1 contains one 'Stem', five 'IRNet-A', and one 'Reduction-A'. In addition, we append two convolutional layers and a softmax loss at the end of each stage. In this case, the original IRNet with one loss function develops into multiple stages, where each stage has its own loss function. Now we introduce the information flows for three stages in IRNet-LC. In the first stage as shown in <ref type="figure">Fig. 2 (b)</ref>, given a 3×512×512 image I, stage-1 predicts a 21×64×64 segmentation label map L 1 , where each 21×1 column vector, denoted as L 1 i ∈ R 21×1 , indicates the probabilities (confidence scores) of the i-th pixel belonging to 21 object categories in VOC respectively. We have 21 j=1 L 1 ij = 1, which can be satisfied by using the softmax function. If the maximum score of the i-th pixel, 1 i = max(L 1 i ) and</p><formula xml:id="formula_0">1 i ∈ {L 1 ij |j = 1...21}, is larger than a threshold ρ ( 1 i ≥ ρ)</formula><p>, we accept its prediction and do not propagate it forward to stage-2. The value of ρ is usually larger than 0.95. As introduced in Sec. 1, those pixels in stage-1 that fulfil 1 ≥ 0.95 occupy nearly 40% region of an image, containing a lot of easy pixels and a small number of extremely hard pixels that have high confidence to be misclassified. Removing them from the network significantly reduces computations and improves accuracy, by enabling deeper layers to focus on foreground objects.</p><p>Stage-2 strictly follows the same procedure as above to determine which pixel is forwarded to stage-3. In other words, LC only introduces one hyper-parameter ρ to IRNet. In our implementation, the value of ρ is the same for both stage-1 and -2. Specifically, ρ represents how many easy and extremely hard pixels are rejected (discarded) in each stage. A larger value of ρ rejects a smaller number of pixels, whilst smaller ρ discards more pixels. To the extreme, when ρ = 1.0, no pixels are rejected. IRNet-LC becomes the original IRNet. When ρ = 0.9, 52% and 35% pixels are discarded in stage-1 and -2 respectively. However, if ρ becomes smaller, i.e. ρ &lt; 0.9, more 'moderate' pixels that locate on the important parts of objects are discarded, hindering the performance of the deep model. Experiments show that IRNet-LC is robust when ρ ∈ [0.9, 1.0]. For example, when ρ = 0.95, IRNet-LC obtains nearly realtime of 18 FPS compared to 9 FPS of IRNet, while outperforms it by 0.8% accuracy on VOC val. When ρ = 0.985, IRNet-LC improves IRNet by 1.7% with a speed of 15 FPS.</p><p>After propagating an image through all three stages, we directly combine the predicted label maps of these stages as the final prediction, because different stages predict different regions. For example, as shown in <ref type="figure">Fig. 2 (b)</ref>, stage-1 trusts the predictions in most of the 'background' (pixels with 1 i ≥ ρ) and propagates the other region forward. Pixels in this region are marked as 'unknown' because 1 i &lt; ρ. In stage-2, 'IRNet-B' and 'Reduction-B' only compute convolutions with respect to the forwarded region. It is learned to predict 'harder' region, such as 'person' and 'horse'. This process is repeated in stage-3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region Convolution</head><p>As presented above, stage-2 and -3 only calculate convolutions on those pixels that have been propagated forward. Specifically, (c) shows how to apply RC on a residual module, which can be represented as h(I) = I + conv(I), where feature h is attained by an identity mapping <ref type="bibr" target="#b12">[13]</ref> of I and a convolution over I. We replace the conventional convolution with a RC as introduced above, and the feature h (I) is the elementwise sum between I and the output of RC. This is equivalent to learn a masked residual representation, where values inside M are the outputs of RC and those outside M are copied from I. It works well because different stages in LC cope with different non-overlapping regions, and each stage only needs to learn features of regions it concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training IRNet-LC</head><p>The parameters of IRNet are initialized by pre-training in ImageNet. Since IRNet-LC has additional convolutional layers stacked before each loss function, their parameters are initialized by sampling from a normal distribution. Given a set of images and their per-pixel label maps, IRNet-LC is learned in two steps, where the first one aims at initial training and the second one employs cascade training. Initial Training. This step is similar to deeply supervised network (DSN) <ref type="bibr" target="#b17">[17]</ref>, which has multiple identical loss functions in different layers of the network. Its objective is to adapt IRNet pre-trained by classifying one thousand image categories in ImageNet to the task of image segmentation. It learns discriminative and robust features. In IRNet-LC, every stage is trained to minimize a pixelwise softmax loss function, measuring the discrepancies between the predicted label map and the ground truth label map of the entire image. These loss functions are jointly optimized by using back-propagation (BP) and stochastic gradient descent (SGD). Cascade Training. Once we finish the initial training, we fine-tune each stage of IRNet-LC by leveraging the cascade strategy of ρ as introduced in Sec. 3.1. Similar to the previous step, all stages are trained jointly, but different stages minimize their pixel-wise softmax losses with respect to different regions. More specific, the gradients in BP are only propagated to the region of interest in each stage, which is able to learn discriminative features corresponding to regions (pixels) in a specific difficulty-level. Intuitively, the current stage is fine-tuned on pixels that have low confidences in the previous stage, enabling 'harder' pixels to be captured by deeper layers to improve segmentation accuracy and reduce computation. Training Details. We fix a mini-batch size of 12 images, momentum 0.9 and weight decay of 0.0005 for both two steps. In the initial training, we set the initial learning rate to be 10 −4 and drop it by a factor of 10 after every 10 epochs. In the cascade training, we also set the initial learning rate to be 10 −4 and drop it by a factor of 10 after every 15 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relations with Previous Models</head><p>The relationships and differences between LC and MC have been discussed in Sec. 1. LC also relates to deeply supervised nets (DSN) <ref type="bibr" target="#b17">[17]</ref> and dropout <ref type="bibr" target="#b30">[30]</ref>. DSN. Similar to DSN, LC adds supervision to each stage. However, to enable adaptive processing of hard/easy regions, LC employs different supervisions for different stages. In contrast, the supervision used in each stage of DSN are kept the same. Specifically, the stage-wise supervision in LC is determined by the estimated difficulty of each pixel. In this way, each stage in LC is able to focus on regions with a similar difficulty level. Dropout. LC connects to dropout in the sense that both methods discard some regions in the feature maps, but they are essentially different. LC drops those pixels with high confidences and only propagates difficult pixels forward to succeeding stages. The easy and ambiguous regions are perpetually dropped in upper layers so as to reduce computations and the deeper layers focus more on 'hard' regions such as foreground objects. Dropout randomly zeros out pixels in each layer independently. It prevents over-fitting but slightly increases computations. In the experiment, LC is compared with dropout to identify that the performance gain mainly comes from the proposed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Settings. We evaluate our method on the PASCAL VOC 2012 (VOC12) <ref type="bibr" target="#b7">[8]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref> datasets. VOC12 dataset is a generic object segmentation benchmark with 21 classes. Following previous works, we also use the extra annotations provided by <ref type="bibr" target="#b11">[12]</ref>, which contains 10, 582 images for training, 1, 449 images for validation, and 1, 456 images for testing. Cityscapes dataset, on the other hand, focuses on street scenes segmentation and contains 19 categories. In our experiments, we only employ images with fine pixel-level annotations. There are 2975 training, 500 validation and 1525 testing images. This is consistent with existing studies <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b3">4]</ref>. We adopt mean intersection over union (mIoU) to evaluate the performance of different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>In this section, we investigate the effects of adjusting probability threshold in LC and demonstrate the merits of LC by comparing it to other counterparts. All performance are reported on the validation set of VOC12. Probability Thresholds. In each stage of LC, we employ a pixel-wise probability from softmax layer to represent the confidence of prediction. By choosing appropriate probability threshold ρ, LC can separate easy regions, moderate regions and extremely hard regions for adaptive processing. As discussed in Sec. 3.1, ρ controls how many easy and extremely hard pixels are discarded in each stage. <ref type="table" target="#tab_1">Table 1</ref> lists the processed pixel percentage in stage-1 &amp; -2 and the overall performance as ρ varies. If ρ = 1, LC will degenerate to DSN, which is slightly better than fully convolutional IRNet. When ρ decreases, more easy regions are classified in early stages while hard regions are progressively handled by later stages. It can be understood  as hard negative mining <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">28]</ref> which improves the performance. On the other hand, if the value of ρ is too small, the algorithm might become too optimistic, i.e. many hard regions are processed in early stages and early decisions are made. The performance will be harmed by overly early decisions when hard regions do not receive sufficient inference using deeper layers. As shown in <ref type="table" target="#tab_1">Table 1</ref>, when ρ = 0.985, i.e., LC processes around 52% regions in early stages and achieves the best performance. This value is used in all the following experiments. In practice, the value of ρ can be chosen empirically using a validation set. Effectiveness of Layer Cascade. To show the merits of LC, we compare it to some important counterparts as discussed in Sec. 3.4, including:</p><p>• IRNet <ref type="bibr" target="#b33">[32]</ref>: We use the model describe in Sec. 3.1 as baseline. To conduct a fair comparison, all the following methods are based on this backbone network.</p><p>• DSN <ref type="bibr" target="#b17">[17]</ref>: By setting ρ = 1, we make LC degenerate to a DSN, where each stage process all regions and has full supervision as the final target.</p><p>• DSN [17] + Dropout <ref type="bibr" target="#b30">[30]</ref>: To distinguish our method from dropout, LC is compared against DSN equipped with random label dropout in each stage. We keep the dropout ratio identical as that in LC.</p><p>• Model Cascade: MC has a similar network architecture to LC, but with different training strategy as discussed in Sec. 1. Specifically, MC divides the IRNet into three stages, and each stage is trained separately. When we train a certain stage, we fix the parameters of all previous stages. The same threshold as in LC is employed here, i.e., ρ = 0.985.</p><p>The results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. We have three observations here. Firstly, the improvement from deep supervision (DSN) is relatively limited, which only leads to 0.48 mIoU gain in comparison to the baseline IRNet. Since pre-training on ImageNet has been a common practice  <ref type="figure">Figure 5</ref>: (a) is the change of label distribution in stage-2 and -3.</p><p>(b) shows the percentage of pixels that are classified in different stages.</p><p>in semantic segmentation <ref type="bibr" target="#b25">[25]</ref>, which effectively prevents gradients exploding or vanishing, it renders the advantages of deeply supervision marginal. Secondly, random label dropout does not bring significant effect to the result. The result is expected because the dropout technique is designed to alleviate the hazard of overfitting given small training data size. However, semantic segmentation is a per-pixel labeling task and we have abundant training data to support the learning task. Thirdly, Model Cascade (MC) performs even worse than the baseline IRNet. It is because MC divides the IRNet into several independent sub-models.</p><p>But each sub-model is shallow and therefore weaken the overall modeling capacity. On the contrary, LC has the appealing properties of cascading and also keeping the intrinsic depth for the whole model. The capability of maintaining the model depth adaptively for hard regions makes our approach outstanding in the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stage-wise Analysis</head><p>In this section, we demonstrate how LC enables adaptive processing for different classes and visualize the regions handled by different regions. Stage-wise Label Distribution. First, we provide a label distribution analysis across different stages. Here we take the 20 classes (excluding "background") in VOC12 as an example. <ref type="figure">Fig. 5 (a)</ref> shows how the number of pixels changes with respect to each class in stage-2 and -3. For example, the upper histogram shows a ratio for each class, obtained by dividing its number of pixels in stage-2 by those in stage-1. Ratios larger than one indicates stage-2 focus more on the corresponding classes than stage-1 does. We find that all ratios have increased and belong to the range of 1 to 1.4. It is because stage-1 already handles the easy regions (i.e. "background") and leaves the hard regions (i.e. "foreground") to stage-2. Ratios of stage-3 can be obtained similarly in the bottom histogram. When comparing stage-3 to -2, we can see that stage-3 further focus on harder classes (e.g. "bicycle", "chair" and "dining table"). LC learns to process samples in a "difficulty-aware" manner. We also conduct a per-class analysis as illustrated in <ref type="figure">Fig. 5 (b)</ref>. Harder classes like "chair" and "table" have more pixels handled by deeper layers (stage-3).</p><p>Stage-wise Visualization. Here we visualize the output label maps of different stages for both VOC12 and Cityscapes, as shown in <ref type="figure" target="#fig_4">Fig. 4 and 6</ref>. The uncertain regions in different stages are also marked out. In VOC12, the easy regions like "background" and "human faces" are first labeled by stage-1 in LC. The remaining foreground and boundary regions are then progressively labeled by stage-2 and stage-3 in LC. Similarly, in Cityscapes, the easy regions like "road" and "building" are first labeled by stage-1. Other small objects and fine details like "pole" and "pedestrian" are handled by stage-2 and -3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance and Speed Analysis</head><p>Comparisons with DeepLab and SegNet. To highlight the trade-off between performance and speed, we compare the proposed LC model with two representative state-ofthe-art methods, DeepLab-v2 <ref type="bibr" target="#b3">[4]</ref> and SegNet <ref type="bibr" target="#b0">[1]</ref>. The performance are reported on VOC12 and summarized in <ref type="table" target="#tab_4">Table 3</ref>. The runtime speed is measured on a single Titan X GPU. To ensure a fair comparison, we evaluate DeepLab-v2 and SegNet without any pre-and post-processing, e.g., training with extra data, multi-scale fusion, or smoothing with conditional random fields (CRF). DeepLab-v2 achieves an acceptable mIoU of 70.42. Nonetheless, it uses an ultra-deep ResNet-101 model as the backbone network, its speed of inference is thus slow (7.1 FPS). On the contrary, SegNet is faster due to a smaller model size, however, its accuracy is greatly compromised. In particular, it increases its speed to 14.6 FPS through sacrificing of over 10 mIoU. The proposed LC alleviates the need of trading-off speed with a large drop in performance. The cascaded end-to-end trainable framework with region convolution allows it to achieve the best performance (73.91 mIoU) with an acceptable speed (14.7 FPS). Further Performance and Speed Trade-off. It is worth pointing out that the runtime of LC can be further reduced by decreasing ρ to allow more regions to be handled by early stages. The performance and speed trade-off is depicted in <ref type="figure" target="#fig_6">Fig. 7 (a)</ref> with the corresponding ρ values. It is observed that decreasing ρ slightly affects the accuracy, but it greatly reduces the computation time. Notably, when LC attains real-time inference at 23.6 FPS, it still exhibits competitive mIoU of 66.95, in comparison to mIoU of 70.42 yielded by at 7.1 FPS. We also include the per-stage runtime in <ref type="figure" target="#fig_6">Fig. 7 (b)</ref>. The increasing computation for higher performance mainly comes from later stages.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Benchmark</head><p>In this section, we show that LC can achieve state-of-theart performance on standard benchmarks like VOC12 <ref type="bibr" target="#b7">[8]</ref> and Cityscapes <ref type="bibr" target="#b4">[5]</ref> datasets. Following <ref type="bibr" target="#b3">[4]</ref>, atrous spatial pyramid pooling <ref type="bibr" target="#b3">[4]</ref>, three-scale testing and dense CRF <ref type="bibr" target="#b16">[16]</ref> are employed. VOC12. <ref type="table" target="#tab_5">Table 4</ref> lists the per-class and overall mean IoU on VOC12 test set. The approaches pre-trained on COCO <ref type="bibr" target="#b20">[20]</ref> are marked with † . LC achieves a mIoU of 80.3 and further improves the mIoU to 82.7 with pre-training on COCO, which is the best-performing method on VOC12 benchmark. By inspecting closer, we observe that LC wins 16 out of 20 foreground classes. For other 4 classes, LC also achieves competitive performance. Large gain is observed in some particular classes such as "bike", "chair", "plant", and "sofa". Based on our statistics in <ref type="figure">Fig. 5</ref>, we found that these few classes, in general, require a deeper stage to make decisions on hard regions. Cityscapes. Next, we evaluate LC on Cityscapes benchmark, with results summarized in <ref type="table" target="#tab_6">Table 5</ref>. "sub" denotes whether the method used subsampling images for training. LC also achieves promising performance with a mIoU of 71.1, which shows its great generalization ability to diverse objects and scenes. Lin et al. <ref type="bibr" target="#b19">[19]</ref>'s performance is slightly better than ours, however, LC still wins on 9 out of 19 classes. It is noticed that <ref type="bibr" target="#b19">[19]</ref> used a deeper backbonenetwork and explored richer contextual information. We believe that further performance gain can be achieved if LC is incorporated with these techniques. LC gains outstanding performance on the classes that are 'traditionally regarded' as hard classes, e.g., "fence", "pole", "sign", "truck", "bus" and "bike", which usually exhibit flexible shapes and finegrained details. The results suggest that the end-to-end cascading mechanism in LC is meaningful, especially in alleviating the burden of deeper layers on analyzing easy regions but focusing themselves on hard regions adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">More Comparisons between IRNet-LC and state-of-the-art Methods</head><p>In <ref type="table" target="#tab_7">Table 6</ref>, we compare the settings of different bestperforming methods on VOC12 <ref type="bibr" target="#b7">[8]</ref> test set, including CRF-RNN <ref type="bibr" target="#b41">[40]</ref>, DPN <ref type="bibr" target="#b22">[22]</ref> and DeepLab-v2 <ref type="bibr" target="#b3">[4]</ref>. These methods are summarized in terms of 'backbone network', 'number of parameters', 'pre-trained using MS COCO', 'multi-scale training/test', 'MRF/CRF', 'frame per second (FPS)', and 'mIOU'. Note that '-' indicates the corresponding information was not disclosed in previous paper.</p><p>IRNet-LC uses Inception-ResNet-v2(IRNet) <ref type="bibr" target="#b33">[32]</ref> as backbone network, which is smaller than ResNet-101 (35.5M vs. 44.5M). Following DeepLab-v2 <ref type="bibr" target="#b3">[4]</ref>, atrous spatial pyramid pooling is employed in IRNet-LC. As shown in <ref type="table" target="#tab_7">Table 6</ref>, IRNet-LC achieves the best performance even </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visual Quality Comparison</head><p>We inspect visual quality of obtained label maps on VOC12 validation set. <ref type="figure">Fig. 8</ref> demonstrates the comparisons of LC with DPN <ref type="bibr" target="#b22">[22]</ref> and DeepLab-v2 <ref type="bibr" target="#b3">[4]</ref>. We use the publicly released model to re-generate label maps of DeepLab-v2 while the results of DPN are downloaded from their project page. LC generally makes more accurate predictions. We also include more examples of LC label maps on Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref> in <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Deep layer cascade (LC) is proposed in this work to simultaneously improve the accuracy and speed of semantic image segmentation. It has three advantages over previous approaches. First, LC adopts a "difficulty-aware" learning paradigm, where earlier stages are trained to handle easy and confident regions and hard regions are progressively forwarded to later stages. Secondly, since each stage only processes part of the input, LC can accelerate both training and testing by the usage of region convolution. Thirdly, LC is an end-to-end trainable framework that jointly optimizes the feature learning for different regions, thus achieving state-of-the-art performance on both PASCAL VOC and Cityscapes datasets. LC is capable of running in real-time yet still yielding competitive accuracies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) shows an image of 'cow' and 'background' (left)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 Figure 2 :</head><label>32</label><figDesc>(a) depicts the Inception-ResNet-v2 (IRNet) for classification task. (b) is the architecture of Layer Cascade IRNet (IRNet-LC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) shows the conventional convolution that operates on an entire image. (b) is region convolution (RC) where filters only convolve irregular region of interest denoted as M . Values of the other region are set as zeros. (c) illustrates RC in a residual module. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 (</head><label>3</label><figDesc>b) illustrates this region convolution (RC) compared to the traditional convolution in (a), which is applied on an entire feature map. The filters in RC only convolves a region of interest, denoted as M , and ignores the other region, reducing computations a lot. The values of the other region are directly set as zeros. M can be implemented as a binary mask, where the pixels inside M equal one, otherwise zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of different stages' outputs in VOC12 dataset. Best viewed in color.cascade strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of different stages' outputs in Cityscapes dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>(a) shows the performance and speed trade-off in Layer Cascade (LC) by adjusting ρ. (b) is the time spent in each stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on probability thresholds ρ. ) 72.70 73.56 73.91 73.63 73.03 72.53 71.20 66.95</figDesc><table><row><cell>ρ</cell><cell>1</cell><cell cols="7">0.995 0.985 0.970 0.950 0.930 0.900 0.800</cell></row><row><cell>stage-1 (%)</cell><cell>0</cell><cell>15</cell><cell>23</cell><cell>30</cell><cell>35</cell><cell>35</cell><cell>44</cell><cell>56</cell></row><row><cell>stage-2 (%)</cell><cell>0</cell><cell>14</cell><cell>29</cell><cell>31</cell><cell>30</cell><cell>41</cell><cell>31</cell><cell>29</cell></row><row><cell>mIoU (%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with related methods.</figDesc><table><row><cell></cell><cell>mIoU(%)</cell></row><row><cell>IRNet [32]</cell><cell>72.22</cell></row><row><cell>DSN [17]</cell><cell>72.70</cell></row><row><cell>DSN [17] + Dropout [30]</cell><cell>72.63</cell></row><row><cell>Model Cascade (MC)</cell><cell>44.20</cell></row><row><cell>Layer Cascade (LC)</cell><cell>73.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>A comparison of performance and speed of Layer Cascade (LC) against existing methods.</figDesc><table><row><cell></cell><cell>mIoU</cell><cell>ms</cell><cell>FPS</cell></row><row><cell cols="3">DeepLab-v2 [4] 70.42 140.0</cell><cell>7.1</cell></row><row><cell>SegNet [1]</cell><cell>59.90</cell><cell>69.0</cell><cell>14.6</cell></row><row><cell>LC</cell><cell>73.91</cell><cell>65.1</cell><cell>14.7</cell></row><row><cell>LC (fast)</cell><cell>66.95</cell><cell>42.5</cell><cell>23.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Per-class results on VOC12 test set. Approaches pre-trained on COCO [20] are marked with † . areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mIoU FCN [25] 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 DeepLab [3] 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6 RNN [40] 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0 Adelaide [37] 91.9 48.1 93.4 69.3 75.5 94.2 87.5 92.8 36.7 86.9 65.2 89.1 90.2 86.5 87.2 64.6 90.1 59.7 85.5 72.7 79.1 RNN [4] 92.6 60.4 91.6 63.4 76.3 95.0 88.4 92.6 32.7 88.5 67.6 89.6 92.1 87.0 87.4 63.3 88.3 60.0 86.8 74.5 79.7 LC 94.1 63.0 91.2 67.9 79.5 93.4 90.0 93.8 37.4 83.7 65.9 90.7 86.1 88.8 87.5 68.5 86.9 64.3 85.6 72.2 80.3 LC † 85.5 66.7 94.5 67.2 84.0 96.1 89.8 93.5 47.2 90.4 71.5 88.9 91.7 89.2 89.1 70.4 89.4 70.7 84.2 79.6 82.7</figDesc><table><row><cell>† [40]</cell><cell>90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7</cell></row><row><cell>BoxSup  † [6]</cell><cell>89.8 38.0 89.2 68.9 68.0 89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7 85.2 83.5 58.6 84.9 55.8 81.2 70.7 75.2</cell></row><row><cell>DPN  † [22]</cell><cell>89.0 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 85.6 85.4 63.6 87.3 61.3 79.4 66.4 77.5</cell></row><row><cell>DeepLab-v2  †</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Per-class results on Cityscapes test set. "sub" denotes whether the method used subsampling images for training.</figDesc><table><row><cell></cell><cell>sub</cell><cell cols="3">road swalk build. wall fence pole tlight sign veg. terrain sky person rider car</cell><cell>truck bus train mbike bike mIoU</cell></row><row><cell>RNN [40]</cell><cell>2</cell><cell cols="3">96.3 73.9 88.2 47.6 41.3 35.2 49.5 59.7 90.6 66.1 93.5 70.4 34.7 90.1 39.2 57.5 55.4 43.9 54.6 62.5</cell></row><row><cell>DeepLab [3]</cell><cell>2</cell><cell cols="3">97.3 77.7 87.7 43.6 40.5 29.7 44.5 55.4 89.4 67.0 92.7 71.2 49.4 91.4 48.7 56.7 49.1 47.9 58.6 63.1</cell></row><row><cell>FCN [25]</cell><cell>no</cell><cell>97.4 78.4 89.2 34.9 44.2 47.4 60.1 65</cell><cell cols="2">91.4 69.3 93.9 77.1 51.4 92.6 35.3 48.6 46.5 51.6 66.8 65.3</cell></row><row><cell>DPN [22]</cell><cell>no</cell><cell cols="3">97.5 78.5 89.5 40.4 45.9 51.1 56.8 65.3 91.5 69.4 94.5 77.5 54.2 92.5 44.5 53.4 49.9 52.1 64.8 66.8</cell></row><row><cell cols="2">Dilation10 [39] no</cell><cell cols="2">97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 55</cell><cell>93.3 45.5 53.4 47.7 52.2 66</cell><cell>67.1</cell></row><row><cell cols="2">DeepLab-v2 [4] no</cell><cell cols="3">97.8 81.3 90.3 48.7 47.3 49.5 57.8 67.2 91.8 69.4 94.1 79.8 59.8 93.7 56.5 67.4 57.4 57.6 68.8 70.4</cell></row><row><cell>Adelaide [19]</cell><cell>no</cell><cell cols="3">98.0 82.6 90.6 44.0 50.7 51.1 65.0 71.7 92.0 72.0 94.1 81.5 61.1 94.3 61.1 65.1 53.8 61.6 70.6 71.6</cell></row><row><cell>LC</cell><cell>no</cell><cell cols="3">97.9 83.1 91.6 53.7 57.4 58.4 62.0 73.3 91.9 61.3 93.8 78.8 53.1 93.4 62.2 76.9 53.5 57.0 74.7 71.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparisons with state-of-the-art methods on VOC12 test set. '-' indicates the corresponding information was not disclosed in the previous papers. backbone network # params COCO multi-scale MRF/CRF FPS mIoU CRF-RNN<ref type="bibr" target="#b41">[40]</ref> without pre-training on MS COCO<ref type="bibr" target="#b20">[20]</ref>, demonstrating the effectiveness of the Layer Cascade framework. When all components of pre-and post-processing such as 'COCO', 'multiscale', and 'CRF' are removed, IRNet-LC still obtains comparable performance with respect to DeepLab-v2 (78.2% vs. 79.7%), but significantly outperforms DeepLab-v2 in terms of FPS (14.3 fps vs. 0.9 fps). In other words, IRNet-LC improves FPS of DeepLab-v2 by 15 times with merely 1.5% decrease in accuracy. Note that IRNet-LC outperforms state-of-the-art systems like CRF-RNN and DPN by 3.5% and 0.7% respectively, without employing any pre-and post-processing steps.</figDesc><table><row><cell></cell><cell>VGG [29]</cell><cell>134.4M</cell><cell>yes</cell><cell>-</cell><cell>yes</cell><cell>-</cell><cell>74.7</cell></row><row><cell>DPN [22]</cell><cell>VGG [29]</cell><cell>134.4M</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>-</cell><cell>77.5</cell></row><row><cell>DeepLab-v2 [4]</cell><cell>ResNet-101 [13]</cell><cell>44.5M</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>0.9</cell><cell>79.7</cell></row><row><cell>IRNet-LC</cell><cell>IRNet [32]</cell><cell>35.5M</cell><cell>no</cell><cell>no</cell><cell>no</cell><cell>14.3</cell><cell>78.2</cell></row><row><cell>IRNet-LC</cell><cell>IRNet [32]</cell><cell>35.5M</cell><cell>no</cell><cell>yes</cell><cell>no</cell><cell>7.7</cell><cell>79.5</cell></row><row><cell>IRNet-LC</cell><cell>IRNet [32]</cell><cell>35.5M</cell><cell>no</cell><cell>yes</cell><cell>yes</cell><cell>1.0</cell><cell>80.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) input image (b) ground truth (c) DPN (d) DeepLab-v2 (e) LC</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3361" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01640v2</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Visual quality comparison of different methods: (a) input image (b) ground truth</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>c) DPN [22] (d) DeepLab-v2 [4] and (e) LC</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Augmenting crfs with boltzmann machine shape priors for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013v2</idno>
		<imprint>
			<date type="published" when="2015-04-23" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning dynamic hierarchical models for anytime scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="650" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning markov random field for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07230</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fashion landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="229" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep decision network for multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural (a) input image (b) ground truth (c) LC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual quality of LC label maps: (a) input image (b) ground truth (white labels indicating ambiguous regions) and (c) LC. networks from overfitting</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>JMLR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Filter-based meanfield inference for random fields with higher-order terms and product label-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, pages I-511</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context driven scene parsing with attention to rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3294" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03240v2</idno>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
