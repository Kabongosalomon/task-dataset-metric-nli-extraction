<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCENE COMPLETENESS-AWARE LIDAR DEPTH COMPLETION FOR DRIVING SCENARIO</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SCENE COMPLETENESS-AWARE LIDAR DEPTH COMPLETION FOR DRIVING SCENARIO</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Sparse Depth Completion</term>
					<term>Driving Sce- nario</term>
					<term>Scene Completeness</term>
					<term>Sensor Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces Scene Completeness-Aware Depth Completion (SCADC) to complete raw lidar scans into dense depth maps with fine and complete scene structures. Recent sparse depth completion for lidars only focuses on the lower scenes and produces irregular estimations on the upper because existing datasets, such as KITTI, do not provide groundtruth for upper areas. These areas are considered less important since they are usually sky or trees of less scene understanding interest. However, we argue that in several driving scenarios such as large trucks or cars with loads, objects could extend to the upper parts of scenes. Thus depth maps with structured upper scene estimation are important for RGBD algorithms. SCADC adopts stereo images that produce disparities with better scene completeness but are generally less precise than lidars, to help sparse lidar depth completion. To our knowledge, we are the first to focus on scene completeness of sparse depth completion. We validate our SCADC on both depth estimate precision and scene-completeness on KITTI. Moreover, we experiment on less-explored outdoor RGBD semantic segmentation with scene completeness-aware D-input to validate our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Autonomous driving usually adopts lidars as the main depth acquisition sensors due to their high precision and practicability on outdoor depth sensing. However, lidar scans are limited to number of scanlines and spatial resolutions, and thus they are sparse when aligned with images. Recent research on lidar depth completion for autonomous driving tries to complete sparse lidar depth into a dense map <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> using KITTI Depth Completion Dataset <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, their depth map processing and evaluations always crop out the upper side of maps for two reasons. First, these upper side areas are usually sky or trees of low scene understanding interest. Second, lidars are active sensors with limited scanlines and smaller vertical field-of-view than cameras. Thus, most lidar scans do not span the whole image height and are concentrated on the lower parts of images. For <ref type="figure">Fig. 1</ref>. Comparison of depth from stereo matching network <ref type="bibr" target="#b12">[13]</ref>, depth completion network <ref type="bibr" target="#b3">[4]</ref>, and our SCADC. Our results leverage advantages of both stereo matching, which have more structured upper scenes, and lidars, which have more precise depth measurements. KITTI, topside 1/3 to 1/4 areas are unscanned by lidars. Also, KITTI's depth groundtruth is acquired by accumulating 3D point clouds with a 64-scanline lidar. Hence their groundtruth are also concentrated on the lower parts of images. Both of KITTI's quantitative and qualitative evaluations focus only on the lower parts.</p><p>Nevertheless, upper scenes are especially important under several autonomous driving scenarios, such as a huge truck beside or just in front occupies a large area of the upper scene when close enough. Traffic signs or lights are important road structures extending to the upper parts. Although more and more research focuses on multi-modal learning from images and depth, scene incompleteness issue is mostly ignored for the following reasons. First, depth completion is treated as a standalone task in previous works without validating their completed depth maps on other scene understanding tasks such as semantic segmentation. Second, not enough data of large objects extending to the upper scenes are collected, and thus the issue is generally omitted. However, autonomous driving needs to take care of all kinds of scenarios to prevent accidents and thus needs more attention to scene completeness issue.</p><p>In contrast, stereo matching produces disparity maps with much structured upper scenes because disparities are derived from images. However, stereo matching is known for less reliable depth measurements for far-range sensing and edge bleeding artifact <ref type="bibr" target="#b13">[14]</ref>, which produces distorted shapes.</p><p>In this work, we leverage the completeness of disparity maps to help sparse depth completion. To our knowledge, we are the first one focusing on the scene completeness issue arXiv:2003.06945v3 [cs.CV] 20 Feb 2021 of depth completion. Our Scene Completeness-Aware Depth Completion (SCADC) fuses depth estimations from a stereo matching network and a lidar depth completion network. We propose Attentional Point Confidence (APC) to regress confidence maps to fuse multi-modal information. Later, we use a stacked hourglass network to refine estimations stage by stage with groundtruth. Output examples are in <ref type="figure">Fig. 1</ref>. We use both quantitative and qualitative comparisons to validate that our SCADC combines the advantages of stereo cameras and lidars, producing both scene completeness-aware and precise depth maps.</p><p>Unlike previous works that treat depth completion as a standalone task, we further validate our scene completenessaware depth on semantic segmentation. We use SSMA <ref type="bibr" target="#b14">[15]</ref>, a high-performing outdoor RGB-D semantic segmentation framework, as the baseline to show that our recovered depth could help better scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse Depth</head><p>Completion. Recent work of sparse depth completion works on completing lidar depth using real-world data from KITTI Depth Completion Benchmark. Sparse-to-Dense <ref type="bibr" target="#b1">[2]</ref> stacks sparse depth maps and images to form a 4channel input to a ResNet-based depth completion network. SSDC <ref type="bibr" target="#b3">[4]</ref> uses ego-pose coherence with a photometric loss to regress depth. CSPN <ref type="bibr" target="#b15">[16]</ref> and Non-local <ref type="bibr" target="#b16">[17]</ref> adopt convolutional spatial propagation to enhance local information. Other studies, such as Deep-Lidar <ref type="bibr" target="#b2">[3]</ref> and PwP <ref type="bibr" target="#b17">[18]</ref> adopt an extra surface normal regression to help the depth estimation. However, these methods either crop out the upper scene of depth maps or produce random structures on the upper areas since KITTI only provides groundtruth for lower scenes. Besides, they work on depth completion alone without further vision applications using their depth maps. By contrast, we utilize scene completeness of disparity maps to help depth completion. In addition, we use our completed depth maps to help outdoor RGBD semantic segmentation, which further shows the values of depth completion.</p><p>Stereo Matching. Stereo matching is a fundamental problem in computer vision. Traditional SGM and variants <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> match left/right frame features and output sparse disparity estimations. Recent stereo matching methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> using neural networks could estimate dense disparity maps. They usually aggregate features into a cost volume and then apply 3D convolutions to regress disparities. The estimated dense disparities have more structured upper scenes than lidar completion since they are directly derived from image space with upper scene details. However, stereo matching usually suffers from edge bleeding that estimated disparities bleed out from object contours and form distorted areas <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>. Further, stereo matching-based methods are unreliable for long-range sensing or areas without textures, and in general they produces depth less precise than lidar measurements. Our SCADC combines advantages of preciseness of lidars at lower scenes and structured upper scenes of stereo matching to better utilize the two modalities.</p><p>RGBD Semantic Segmentation. There are much fewer works on RGBD outdoor semantic segmentation than indoors since high-quality outdoor depth maps are hard to get to match information from image and depth domains. SSMA <ref type="bibr" target="#b14">[15]</ref> combines two AdapNet++ <ref type="bibr" target="#b14">[15]</ref> branches and densely fuses information from images and depth encoders with a decoder to regress the depth map. We adopt SSMA and validate completed depth from our SCADC on outdoor semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODS</head><p>The whole network design of our SCADC is in <ref type="figure" target="#fig_0">Fig. 2</ref>. Our goal is to construct a network for sensor fusion, which takes advantage of depth from stereo matching with more structured upper scenes, and depth from lidar completion with higher precision, to generate both scene completeness-aware and precise depth maps.</p><p>PSMNet <ref type="bibr" target="#b12">[13]</ref> and SSDC <ref type="bibr" target="#b3">[4]</ref> are adopted as our base methods for stereo matching and lidar completion respectively. We use the estimated depth maps from two modalities, D stereo and D lidar , as inputs to our SCADC. SCADC consists of two parts, multi-modal fusion and regression with a stacked hourglass network.</p><p>At the multi-modal fusion stage, we utilize the early fusion strategy. Early fusion incorporates multi-modal information before an encoder stage and has the advantage of retaining finer local structures and neighborhood relationships. Opposed to early fusion, late fusion is usually adopted for multi-modal learning with modalities from different domains to capture higher-level semantics, such as fusing information of images and depth <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>. Our SCADC operates information fusion only on the depth domain, and thus early fusion of retaining local features and structures is more desirable.</p><p>We propose a novel confidence regression module, Attentional Point Confidence (APC), to estimate the pixel-level confidence of lidars, M lidar ∈ [0, 1] H×W , where H and W are height and width of inputs. APC decides for each pixel which modality is more probable to estimate more reliable depth. Previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> also use confidence maps for RGBD fusion without direct supervision on confidence regression. However, for stereo cameras/lidars fusion, since we have priors that depth from stereo matching is more structured on upper scenes and depth from lidar scans is more precise in general, using direct supervision on confidence regression could make the network generate better confidence maps of maintaining and combining both advantages from stereo cameras and lidars.</p><p>We create guiding confidence M g from the raw lidar scans. Lidar measurements are comparatively precise, so pixel positions at raw lidar points should have higher confi-   <ref type="bibr" target="#b4">[5]</ref>. is for point-wise product.</p><p>dence. We set their scores to 1. Next, the depths of neighboring pixels are generally similar, so we dilate confidence at each raw lidar point using Gaussian kernels to obtain M g . Kernel size and variance are based on point density of raw lidar scans. We find density along a scanline for KITTI is 44.6% at the center and 30.6% near the left/right side. Thus, we use a 3 × 3 kernel and choose a variance which makes confidence scores drop to half with 1-pixel distance from the center. M g provides better priors of confidence maps for learning. Note that (1) M g is not a hard constraint. The network could still generate confidence maps learned from all loss combinations. (2) Estimating a confidence map from one modality is enough for the probabilistic fusion of two modalities using the sum to 1 constraint. We choose to estimate lidars' since their scanned points are generally more precise, giving convenience to create M g .</p><p>Sparse data are intrinsically hard for CNN to extract effective features. In APC, We utilize Sparsity-Attentional Convolution (SAConv) <ref type="bibr" target="#b4">[5]</ref> to extract features from sparse lidar maps. SAConv attends on feature extraction of each nonzero point with an extra mask to keep track of visibility. After regressing M lidar , we calculate the confidence loss as L c = M lidar − M g 2 2 . Structures are illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. The confidence for stereo is M stereo = 1 − M lidar , and the fused depth is</p><formula xml:id="formula_0">D f = D stereo × M stereo + D lidar × M lidar .</formula><p>The second stage is depth regression. We use a stacked hourglass network <ref type="bibr" target="#b27">[28]</ref> with dense connections for regressing depth. Our stacked hourglass network consists of 3 cascaded encoder-decoder structures. It has the advantage of refining depth maps stage by stage when compared with mostly used single encoder-decoder of FCN-like structure in other depth completion works <ref type="bibr" target="#b3">[4]</ref> [2] [5] <ref type="bibr" target="#b25">[26]</ref>. The stacked hourglass produces 3 stage outputs (S1, S2, and S3). We further use skip connection and densely connect each corresponding layer of these hourglasses and feed the regressed depth to every subsequent stage to enhance information flow. Finer depth is regressed at later stages. At inference time, S3 is the final depth output. ReLU <ref type="bibr" target="#b28">[29]</ref> and batch normalization <ref type="bibr" target="#b29">[30]</ref> are adopted after each convolution in stacked hourglass and APC.</p><p>We use groundtruth, D gt , to directly supervise the regression and calculate loss terms for each stage output. The corresponding mean square error losses are computed as follows.</p><formula xml:id="formula_1">L i = D gt − Si 2 2 , ∀i ∈ [1, 3].<label>(1)</label></formula><p>The total loss is L 1 + L 2 + L 3 + L c . Note that D gt from KITTI Depth Completion does not contain points on the upper scenes. We find that using more stages or deeper network would cause overfitting on the lower parts and yield unstructured upper scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sparse Depth Completion</head><p>Dataset. We evaluate spare depth completion on KITTI Depth Completion Benchmark, which contains 42K stereo pairs and lidar scans as training data and 3.4K frames for validation. Following <ref type="bibr" target="#b3">[4]</ref>, we uniformly bottom crop the size to 352 × 1216. Inputs to our SCADC are generated by PSMNet <ref type="bibr" target="#b12">[13]</ref> and SSDC <ref type="bibr" target="#b3">[4]</ref>. We use their released code and best-pretrained weights on KITTI. Error Metrics. We follow error metrics from previous works. (1) RMSE: root mean square error; (2) Rel: mean absolute relative error; (3) δ i : percentage of predicted pixels where the relative error is within 1.25 i . Formally,</p><formula xml:id="formula_2">δ i = |{d : max(d d , d d ) &lt; 1.25 i }| |{d}| ,<label>(2)</label></formula><p>where |.| denotes the cardinality of a set.d and d are prediction and groundtruth. Most studies adopt i = 1, 2, 3.</p><p>Results. The quantitative comparison of depth error on KITTI Depth Completion val set is in <ref type="table">Table 1</ref>. Depth maps for comparison from PSMNet <ref type="bibr" target="#b12">[13]</ref> and SSDC <ref type="bibr" target="#b3">[4]</ref> are generated following their steps. Note that the numerical results only Results of other works are directly from KITTI website. ADNN <ref type="bibr" target="#b8">[9]</ref> shows null on the upper since there are no groundtruth points. We are the only that reconstructs upper scene structures.  <ref type="bibr" target="#b30">[31]</ref> 51.15 SGDepth <ref type="bibr" target="#b31">[32]</ref> 53.04 SSMA <ref type="bibr" target="#b14">[15]</ref> 54.76 SSMA + Our SCADC depth 61.57 evaluate depth estimations on the lower scenes. A qualitative comparison is shown in <ref type="figure" target="#fig_2">Fig.4</ref> Left. From both numerical and visual results, although PSMNet produces more structured upper scenes than SSDC, the depth estimation error is larger on the lower part. By contrast, while SSDC has a smaller numerical error, it creates irregular and unstructured depth estimations on the upper scenes. Our SCADC combines the advantages of both stereo matching and depth completion to generates both scene completeness-aware and precise depth estimations. We also compare with other depth completion methods on KITTI test set. The comparison is shown in <ref type="figure" target="#fig_2">Fig.4</ref> Right. Our SCADC is the only work that successfully reconstructs the upper scene structures among the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Outdoor RGBD Semantic Segmentation</head><p>Datasets. We next validate our SCADC on outdoor semantic segmentation. KITTI Semantic Segmentation dataset contains only images, and we match the corresponding lidar frames in KITTI Raw that contains all public raw data. We split the fetched data into 6:1 training and validation set. Although Cityscapes has more data for semantic segmentation, they only adopt stereo cameras as the depth acquisition. Evaluations. SSMA <ref type="bibr" target="#b14">[15]</ref> is a high-performing framework on outdoor RGBD semantic segmentation. We follow SSMA's setting and use their Cityscapes pretrains to perform fine-tuning on KITTI. Standard mean intersection over union (mIoU) is adopted as the metrics. Two other RGBD outdoor semantic segmentation methods SDNet <ref type="bibr" target="#b30">[31]</ref> and SGDepth <ref type="bibr" target="#b31">[32]</ref> are included for comparison. The quantitative and qualitative results are in <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_3">Fig. 5</ref>. From the results, our scene completeness-aware and precise depth could further help performance improvements. In the visual results, finer structures such as road signs and traffic poles that extend to the upper scenes could be clearly segmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>Our SCADC combines the advantages of scene completeness from stereo matching to help lidar depth completion, obtaining both scene complete and precise depth maps. Our APC module predicts confidences of lidars with a guiding supervision. With APC, information fusion from the two modalities are successfully performed. We show that our depth maps have good upper scene control for practical scenarios that objects of interest extend to the upper scenes. Furthermore, we validate that our depth maps help outdoor RGBD scene understanding, showing more values of depth completion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Network pipeline of our SCADC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Structure of APC module and Sparisty Attentional Convolution (SAConv)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(Left) Qualitative results of stereo matching (PSMNet [13]), SSDC (direct lidar completion [4]), and our SCADC on KITTI Depth Completion validation set. We show driving scenarios of large trucks beside and cars with loads. Vehicle structures extend to upper scenes. SSDC fails to regress upper structures. Shape distortion of PSMNet could be seen in highlights (a) Bicycle contour. (b) Bridge structure bleeds into the background and creates irregular estimations. (Right) Comparison on KITTI Depth Completion test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Semantic Segmentation results of SSMA with depth from our SCADC on KITTI Semantic Segmentation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Evaluation on KITTI Depth Completion val set. Comparison on KITTI Semantic Segmentation dataset. Our depth could enhance SSMA performance.</figDesc><table><row><cell>Methods</cell><cell>RMSE</cell><cell>Rel</cell><cell>δ1</cell><cell>δ2</cell><cell>δ3</cell></row><row><cell>PSMNet</cell><cell>2.4107</cell><cell>0.1296</cell><cell>98.6</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell>SSDC</cell><cell>1.0438</cell><cell>0.0191</cell><cell>99.3</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell>SCADC</cell><cell>1.0096</cell><cell>0.0226</cell><cell>99.5</cell><cell>99.9</cell><cell>100.0</cell></row><row><cell></cell><cell cols="2">Methods</cell><cell></cell><cell>mIoU</cell><cell></cell></row><row><cell></cell><cell>SDNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilherme</forename><surname>Venturelli Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3288" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep rgb-d canonical correlation analysis for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5332" to="5342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth completion with deep geometry and context guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Uk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Sunghoon Im, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Plug-and-play: Improve depth estimation via sparse data propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth coefficients for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfei</forename><surname>Saif Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Convolutional Compressed Sensing for LiDAR Depth Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10023" to="10032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00761</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5410" to="5418" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stereo vision-based depth of field rendering on a mobile device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23009</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Issue: Deep Learning for Robotic Vision</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungdon</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kuei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depthnormal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">chine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="328" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A real-time lowpower stereo vision engine using semi-global matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Eberli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Systems (ICCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterative semi-global matching for robust driver assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="465" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-to-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Computer vision: A modern approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fawzi</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
	<note>in European conference on computer vision (ECCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sdnet: Semantically guided depth estimation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kretz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="288" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Termöhlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
