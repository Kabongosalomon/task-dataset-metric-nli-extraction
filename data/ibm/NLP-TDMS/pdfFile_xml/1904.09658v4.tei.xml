<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Face Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
							<email>shiyichu@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
							<email>jain@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Face Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations. We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When humans are asked to describe a face image, they not only give the description of the facial attributes, but also the confidence associated with them. For example, if the eyes are blurred in the image, a person will keep the eye size as an uncertain information and focus on other features. Furthermore, if the image is completely corrupted and no attributes can be discerned, the subject may respond that he/her cannot identify this face. This kind of uncertainty (or confidence) estimation is common and important in human decision making.</p><p>On the other hand, the representations used in state-ofthe-art face recognition systems are generally confidenceagnostic. These methods depend on an embedding model (e.g. Deep Neural Networks) to give a deterministic point representation for each face image in the latent feature space <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4]</ref>. A point in the latent space represents the model's estimation of the facial features in the given image. If the error in the estimation is somehow bounded, the distance between two points can effectively measure the semantic similarity between the corresponding face images. But given a low-quality input, where the expected facial features are ambiguous or absent in the image, a large shift in the embedded points is inevitable, leading to false recognition <ref type="figure" target="#fig_0">(Figure 1a</ref>). Given that face recognition systems have already achieved high recognition accuracies on relatively constrained face recognition benchmarks, e.g. LFW <ref type="bibr" target="#b9">[10]</ref> and YTF <ref type="bibr" target="#b37">[38]</ref>, where most facial attributes can be clearly observed, recent face recognition challenges have moved on to more unconstrained scenarios, including surveillance videos <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref> (See <ref type="figure">Figure 2</ref>). In these tasks, any type and degree of variation could exist in the face image, where most of the desired facial features learned by the representation model could be absent. Given this lack of information, it is unlikely to find a feature set that could always match these faces accurately. Hence state-of-the-art face recognition systems which obtained over 99% accuracy on LFW have suffered from a large performance drop on IARPA Janus benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>To address the above problems, we propose Probabilistic Face Embeddings (PFEs), which give a distributional estimation instead of a point estimation in the latent space for each input face image <ref type="figure" target="#fig_0">(Figure 1b</ref>). The mean of the distribution can be interpreted as the most likely latent feature values while the span of the distribution represents the un-(a) IJB-A <ref type="bibr" target="#b18">[19]</ref> (b) IJB-S <ref type="bibr" target="#b12">[13]</ref>  <ref type="figure">Figure 2</ref>: Example images from IJB-A and IJB-S. The first columns show still images, followed by video frames of the respective subjects in the next three columns. These benchmarks present a more unconstrained recognition scenario where there is a large variability in the image quality. certainty of these estimations. PFE can address the unconstrained face recognition problem in a two-fold way: <ref type="bibr" target="#b0">(1)</ref> During matching (face comparison), PFE penalizes uncertain features (dimensions) and pays more attention to more confident features. <ref type="bibr" target="#b1">(2)</ref> For low quality inputs, the confidence estimated by PFE can be used to reject the input or actively ask for human assistance to avoid false recognition. Besides, a natural solution can be derived to aggregate the PFE representations of a set of face images into a new distribution with lower uncertainty to increase the recognition performance. The implementation of PFE is open-sourced 1 . The contributions of the paper can be summarized as below:</p><p>1. An uncertainty-aware probabilistic face embedding (PFE) which represents face images as distributions instead of points. 2. A probabilistic framework that can be naturally derived for face matching and feature fusion using PFE. 3. A simple method that converts existing deterministic embeddings into PFEs without additional training data. 4. Comprehensive experiments showing that the proposed PFE can improve face recognition performance of deterministic embeddings and can effectively filter out lowquality inputs to enhance the robustness of face recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Uncertainty Learning in DNNs To improve the robustness and interpretability of discriminant Deep Neural Networks (DNNs), deep uncertainty learning is getting more attention <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref>. There are two main types of uncertainty: model uncertainty and data uncertainty. Model uncertainty refers to the uncertainty of model parameters given the training data and can be reduced by collecting additional training data <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5]</ref>. Data uncertainty accounts for the uncertainty in output whose primary source is the inherent noise in input data and hence cannot be eliminated with more training data <ref type="bibr" target="#b15">[16]</ref>. The uncertainty studied in our work can be categorized as data uncertainty. Although techniques have been developed for estimating data uncertainty in different tasks, including classification and regression <ref type="bibr" target="#b15">[16]</ref>, they are not suitable for our task since our target space is not well-defined by given labels 2 . Variational Autoencoders <ref type="bibr" target="#b17">[18]</ref> can also be regarded as a method for estimating data uncertainty, but it mainly serves a generation purpose. Specific to face recognition, some studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47]</ref> have leveraged the model uncertainty for analysis and learning of face representations, but to our knowledge, ours is the first work that utilizes data uncertainty 3 for recognition tasks.</p><p>Probabilistic Face Representation Modeling faces as probabilistic distributions is not a new idea. In the field of face template/video matching, there exists abundant literature on modeling the faces as probabilistic distributions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b0">1]</ref>, subspace <ref type="bibr" target="#b2">[3]</ref> or manifolds <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> in the feature space. However, the input for such methods is a set of face images rather than a single face image, and they use a between-distribution similarity or distance measure, e.g. KL-divergence, for comparison, which does not penalize the uncertainty. Meanwhile, some studies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref> have attempted to build a fuzzy model of a given face using the features of face parts. In comparison, the proposed PFE represents each single face image as a distribution in the latent space encoded by DNNs and we use an uncertainty-aware log likelihood score to compare the distributions.</p><p>Quality-aware Pooling In contrast to the methods above, recent work on face template/video matching aims to leverage the saliency of deep CNN embeddings by aggregating the deep features of all faces into a single compact vector <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref>. In these methods, a separate module learns to predict the quality of each face in the image set, which is then normalized for a weighted pooling of feature vectors. We show that a solution can be naturally derived under our framework, which not only gives a probabilistic explanation for quality-aware pooling methods, but also leads to a more general solution where an image set can also be modeled as a PFE representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Limitations of Deterministic Embeddings</head><p>In this section, we explain the problems of deterministic face embeddings from both theoretical and empirical views. Let X denote the image space and Z denote the latent feature space of D dimensions. An ideal latent space Z should only encode identity-salient features and be disentangled from identity-irrelevant features. As such, each identity should have a unique intrinsic code z ∈ Z that best represents this person and each face image x ∈ X is an observation sampled from p(x|z). The process of training face embeddings can be viewed as a joint process of searching for such a latent space Z and learning the inverse mapping p(z|x). For deterministic embeddings, the inverse mapping is a Dirac delta function p(z|x) = δ(z − f (x)), where f is the embedding function. Clearly, for any space Z, given the possibility of noises in x, it is unrealistic to recover the exact z and the embedded point of a low-quality input would inevitably shift away from its intrinsic z (no matter how much training data we have). The question is whether this shift could be bounded such that we still have smaller intra-class distances compared to inter-class distances. However, this is unrealistic for fully unconstrained face recognition and we conduct an experiment to illustrate this. Let us start with a simple example: given a pair of identical images, a deterministic embedding will always map them to the same point and therefore the distance between them will always be 0, even if these images do not contain a face. This implies that "a pair of images being similar or even the same does not necessarily mean the probability of their belonging to the same person is high".</p><p>To demonstrate this, we conduct an experiment by manually degrading the high-quality images and visualizing their similarity scores. We randomly select a high-quality image of each subject from the LFW dataset <ref type="bibr" target="#b9">[10]</ref> and manually insert Gaussian blur, occlusion, and random Gaussian noise to the faces. In particular, we linearly increase the size of Gaussian kernel, occlusion ratio and the standard deviation of the noise to control the degradation degree. At each degradation level, we extract the feature vectors with a 64-layer CNN 4 , which is comparable to state-of-the-art face recognition systems. The features are normalized to a hyper-spherical embedding space. Then, two types of cosine similarities are reported: (1) similarity between pairs of original image and its respective degraded image, and (2) similarity between degraded images of different identities. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, for all the three types of degradation, the genuine similarity scores decrease to 0 while the 4 trained on Ms-Celeb-1M <ref type="bibr" target="#b7">[8]</ref> with AM-Softmax <ref type="bibr" target="#b33">[34]</ref> (a) Low-similarity Genuine Pairs (b) High-similarity Impostor Pairs impostor similarity scores converge to 1.0! These indicate two types of errors that can be expected in a fully unconstrained scenario even when the model is very confident (very high/low similarity scores):</p><p>(1) false accept of impostor low-quality pairs and (2) false reject of genuine cross-quality pairs.</p><p>To confirm this, we test the model on the IJB-A dataset by finding impostor/genuine image pairs with the highest/lowest scores, respectively. The situation is exactly as we hypothesized (See <ref type="figure" target="#fig_2">Figure 4</ref>). We call this Feature Ambiguity Dilemma which is observed when the deterministic embeddings are forced to estimate the features of ambiguous faces. The experiment also implies that there exist a dark space where the ambiguous inputs are mapped to and the distance metric is distorted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Probabilistic Face Embeddings</head><p>To address the aforementioned problem caused by data uncertainty, we propose to encode the uncertainty into the face representation and take it into account during matching. Specifically, instead of building a model that gives a point estimation in the latent space, we estimate a distribution p(z|x) in the latent space to represent the potential appearance of a person's face <ref type="bibr" target="#b4">5</ref> . In particular, we use a multivariate Gaussian distribution:</p><formula xml:id="formula_0">p(z|x i ) = N (z; µ i , σ 2 i I) (1)</formula><p>where µ i and σ i are both a D-dimensional vector predicted by the network from the i th input image x i . Here we only consider a diagonal covariance matrix to reduce the complexity of the face representation. This representation should have the following properties:</p><p>1. The center µ should encode the most likely facial features of the input image. 2. The uncertainty σ should encode the model's confidence along each feature dimension. In addition, we wish to use a single network to predict the distribution. Considering that new approaches for training face embeddings are still being developed, we aim to develop a method that could convert existing deterministic face embedding networks to PFEs in an easy manner. In the followings, we first show how to compare and fuse the PFE representations to demonstrate their strength and then propose our method for learning PFEs.</p><p>Note 1. Because of the space limit, we provide the proofs of all the propositions below in Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Matching with PFEs</head><p>Given the PFE representations of a pair of images (x i , x j ), we can directly measure the "likelihood" of them belonging to the same person (sharing the same latent code):</p><formula xml:id="formula_1">p(z i = z j ), where z i ∼ p(z|x i ) and z j ∼ p(z|x j ). Specifi- cally, p(z i = z j ) = p(z i |x i )p(z j |x j )δ(z i − z j )dz i dz j . (2)</formula><p>In practice, we would like to use the log likelihood instead, whose solution is given by:</p><formula xml:id="formula_2">s(x i , x j ) = log p(z i = z j ) = − 1 2 D l=1 ( (µ (l) i − µ (l) j ) 2 σ 2(l) i + σ 2(l) j + log(σ 2(l) i + σ 2(l) j )) − const,<label>(3)</label></formula><p>where const = D 2 log 2π, µ (l) i refers to the l th dimension of µ i and similarly for σ (l) i . Note that this symmetric measure can be viewed as the expectation of likelihood of one input's latent code conditioned on the other, that is</p><formula xml:id="formula_3">s(x i , x j ) = log p(z|x i )p(z|x j )dz = log E z∼p(z|xi) [p(z|x j )] = log E z∼p(z|xj ) [p(z|x i )].</formula><p>(4) <ref type="bibr" target="#b4">5</ref> following the notations in Section 3. As such, we call it mutual likelihood score (MLS). Different from KL-divergence, this score is unbounded and cannot be seen as a distance metric. It can be shown that the squared Euclidean distance is equivalent to a special case of MLS when all the uncertainties are assumed to be the same:</p><formula xml:id="formula_4">Property 1. If σ (l)</formula><p>i is a fixed number for all data x i and dimensions l, MLS is equivalent to a scaled and shifted negative squared Euclidean distance.</p><p>Further, when the uncertainties are allowed to be different, we note that MLS has some interesting properties that make it different from a distance metric:</p><p>1. Attention mechanism: the first term in the bracket in Equation <ref type="formula" target="#formula_2">(3)</ref> can be seen as a weighted distance which assigns larger weights to less uncertain dimensions. 2. Penalty mechanism: the second term in the bracket in Equation <ref type="formula" target="#formula_2">(3)</ref> can be seen as a penalty term which penalizes dimensions that have high uncertainties. 3. If either input x i or x j has large uncertainties, MLS will be low (because of penalty) irrespective of the distance between their mean. 4. Only if both inputs have small uncertainties and their means are close to each other, MLS could be very high. The last two properties imply that PFE could solve the feature ambiguity dilemma if the network can effectively estimate σ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fusion with PFEs</head><p>In many cases we have a template (set) of face images, for which we need to build a compact representation for matching. With PFEs, a conjugate formula can be derived for representation fusion ( <ref type="figure" target="#fig_3">Figure 5</ref>). Let {x 1 , x 2 , . . . , x n } be a series of observations (face images) from the same identity and p(z|x 1 , x 2 , . . . , x n ) be the posterior distribution after the n th observation. Then, assuming all the observations are conditionally independent (given the latent code z). It can be shown that:</p><formula xml:id="formula_5">p(z|x 1 , x 2 , . . . , x n+1 ) = α p(z|x n+1 ) p(z) p(z|x 1 , x 2 , . . . , x n ),<label>(5)</label></formula><p>where α is a normalization factor. To simplify the notations, let us only consider a one-dimensional case below; the solution can be easily extended to the multivariate case. If p(z) is assumed to be a noninformative prior, i.e. p(z) is a Gaussian distribution whose variance approaches ∞, the posterior distribution in Equation <ref type="formula" target="#formula_5">(5)</ref> is a new Gaussian distribution with lower uncertainty (See Section A). Further, given a set of face images {x 1 , x 2 , . . . , x n }, the parameters of the fused representation can be directly given by:</p><formula xml:id="formula_6">µ n = n i=1σ 2 n σ 2 i µ i ,<label>(6)</label></formula><formula xml:id="formula_7">1 σ 2 n = n i=1 1 σ 2 i .<label>(7)</label></formula><p>In practice, because the conditional independence assumption is usually not true, e.g. video frames include a large amount of redundancy, Equation <ref type="formula" target="#formula_7">( 7)</ref> will be biased by the number of images in the set. Therefore, we take dimension-wise minimum to obtain the new uncertainty.</p><p>Relationship to Quality-aware Pooling If we consider a case where all the dimensions share the same uncertainty σ i for i th input and let the quality value q i = 1 σ 2 i be the output of the network. Then Equation <ref type="formula" target="#formula_6">(6)</ref> can be written aŝ</p><formula xml:id="formula_8">µ n = n i=1 q i µ i n j q j .<label>(8)</label></formula><p>If we do not use the uncertainty after fusion, the algorithm will be the same as recent quality-aware aggregation methods for set-to-set face recognition <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b40">41</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning</head><p>Note that any deterministic embedding f , if properly optimized, can indeed satisfy the properties of PFEs: (1) the embedding space is a disentangled identity-salient latent space and (2) f (x) represents the most likely features of the given input in the latent space. As such, in this work we consider a stage-wise training strategy: given a pre-trained embedding model f , we fix its parameters, take µ(x) = f (x), and optimize an additional uncertainty module to estimate σ(x). When the uncertainty module is trained on the same dataset of the embedding model, this stage-wise training strategy allows us to have a more fair comparison between PFE and the original embedding f (x) than an end-to-end learning strategy.</p><p>The uncertainty module is a network with two fullyconnected layers which shares the same input as of the bottleneck layer <ref type="bibr" target="#b5">6</ref> . The optimization criteria is to maximize the mutual likelihood score of all genuine pairs (x i , x j ). Formally, the loss function to minimize is</p><formula xml:id="formula_9">L = 1 |P| (i,j)∈P −s(x i , x j )<label>(9)</label></formula><p>where P is the set of all genuine pairs and s is defined in Equation <ref type="formula" target="#formula_2">(3)</ref>. In practice, the loss function is optimized within each mini-batch. Intuitively, this loss function can be understood as an alternative to maximizing p(z|x): if the latent distributions of all possible genuine pairs have a large overlap, the latent target z should have a large likelihood p(z|x) for any corresponding x. Notice that because µ(x) is fixed, the optimization wouldn't lead to the collapse of all the µ(x) to a single point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first test the proposed PFE method on standard face recognition protocols to compare with deterministic embeddings. Then we conduct qualitative analysis to gain more insight into how PFE behaves. Due to the space limit, we provide the implementation details in Section B.</p><p>To comprehensively evaluate the efficacy of PFEs, we conduct the experiments on 7 benchmarks, including the well known LFW <ref type="bibr" target="#b9">[10]</ref>, YTF <ref type="bibr" target="#b37">[38]</ref>, MegaFace <ref type="bibr" target="#b13">[14]</ref> and four other more unconstrained benchmarks: CFP [29] contains 7, 000 frontal/profile face photos of 500 subjects. We only test on the frontal-profile (FP) protocol, which includes 7, 000 pairs of frontal-profile faces. IJB-A <ref type="bibr" target="#b18">[19]</ref> is a template-based benchmark, containing 25, 813 faces images of 500 subjects. Each template includes a set of still photos or video frames. Compared with previous benchmarks, the faces in IJB-A have larger variations and present a more unconstrained scenario. IJB-C <ref type="bibr" target="#b23">[24]</ref> is an extension of IJB-A with 140, 740 faces images of 3, 531 subjects. The verification protocol of IJB-C includes more impostor pairs so we can compute True Accept Rates (TAR) at lower False Accept Rates (FAR). IJB-S [13] is a surveillance video benchmark containing 350 surveillance videos spanning 30 hours in total, 5, 656 enrollment images, and 202 enrollment videos of 202 subjects. Many faces in this dataset are of extreme pose or lowquality, making it one of the most challenging face recognition benchmarks (See <ref type="figure">Figure 2</ref> for example images).</p><p>We use the CASIA-WebFace <ref type="bibr" target="#b43">[44]</ref> and a cleaned version 7 of MS-Celeb-1M <ref type="bibr" target="#b7">[8]</ref> as training data, from which we remove the subjects that are also included in the test datasets 8 .  <ref type="table">Table 2</ref>: Results of our models (last three rows) trained on MS-Celeb-1M and state-of-the-art methods on LFW, YTF and MegaFace. The MegaFace verification rates are computed at FAR=0.0001%. "-" indicates that the author did report the performance on the corresponding protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on Different Base Embeddings</head><p>Since our method works by converting existing deterministic embeddings, we want to evaluate how it works with different base embeddings, i.e. face representations trained with different loss functions. In particular, we implement the following state-of-the-art loss functions: Soft-max+Center Loss <ref type="bibr" target="#b35">[36]</ref>, Triplet Loss <ref type="bibr" target="#b27">[28]</ref>, A-Softmax <ref type="bibr" target="#b20">[21]</ref> and AM-Softmax <ref type="bibr" target="#b33">[34]</ref>  <ref type="bibr" target="#b8">9</ref> . To be aligned with previous work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>, we train a 64-layer residual network <ref type="bibr" target="#b20">[21]</ref> with each of these loss functions on the CASIA-WebFace dataset as base models. All the features are 2-normalized to a hyper-spherical embedding space. Then we train the uncertainty module for each base model on the CASIA-WebFace again for 3, 000 steps. We evaluate the performance on four benchmarks: LFW <ref type="bibr" target="#b9">[10]</ref>, YTF <ref type="bibr" target="#b37">[38]</ref>, CFP-FP <ref type="bibr" target="#b28">[29]</ref> and IJB-A <ref type="bibr" target="#b18">[19]</ref>, which present different challenges in face recognition. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. The PFE improves over the original representation in all cases, indicating the proposed method is robust with different embeddings and testing scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-Of-The-Art</head><p>To compare with state-of-the-art face recognition methods, we use a different base model, which is a 64-layer <ref type="bibr" target="#b8">9</ref> We also tried implementing ArcFace <ref type="bibr" target="#b3">[4]</ref> but it does not converge well in our case. So we did not use it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Data IJB-A (TAR@FAR) CFP-FP 0.1% 1.0%   network trained with AM-Softmax on the MS-Celeb-1M dataset. Then we fix the parameters and train the uncertainty module on the same dataset for 12, 000 steps. In the following experiments, we compare 3 methods:</p><p>• Baseline only uses the original features of the 64-layer deterministic embedding along with cosine similarity for matching. Average pooling is used in case of template/video benchmarks. • PFE fuse uses the uncertainty estimation σ in PFE and Equation <ref type="formula" target="#formula_6">(6)</ref> to aggregate the features of templates but uses cosine similarity for matching. If the uncertainty module could estimate the feature uncertainty effectively, fusion with σ should be able to outperform average pooling by assigning larger weights to confident features. • PFE fuse+match uses σ both for fusion and matching (with mutual likelihood scores). Templates/videos are fused based on Equation <ref type="formula" target="#formula_6">(6)</ref> and Equation <ref type="formula" target="#formula_7">(7)</ref>.</p><p>In <ref type="table">Table 2</ref> we show the results on three relatively easier benchmarks: LFW, YTF and MegaFace. Although the accuracy on LFW and YTF are nearly saturated, the proposed PFE still improves the performance of the original representation. Note that MegaFace is a biased dataset: because all the probes are high-quality images from FaceScrub, the positive pairs in MegaFace are both high-quality images while the negative pairs only contain at most one low-quality image <ref type="bibr" target="#b9">10</ref> . Therefore, neither of the two types of error caused <ref type="bibr" target="#b9">10</ref> The negative pairs of MegaFace in the verification protocol only in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>Surveillance-to-Single Surveillance-to-Booking Surveillance-to-Surveillance Rank-1 Rank-5 Rank-10 1% 10% Rank-  <ref type="table">Table 5</ref>: Performance comparison on three protocols of IJB-S. The performance is reported in terms of rank retrieval (closed-set) and TPIR@FPIR (open-set) instead of the media-normalized version <ref type="bibr" target="#b12">[13]</ref>. The numbers "1%" and "10%" in the second row refer to the FPIR. by the feature ambiguity dilemma (Section 3) will show up in MegaFace and it naturally favors deterministic embeddings. However, the PFE still maintains the performance in this case. We also note that such a bias, namely the target gallery images being of higher quality than the rest of gallery, would not exist in real world applications.</p><p>In <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref> we show the results on three more challenging datasets: CFP, IJB-A and IJB-C. The images in these datasets present larger variations in pose, occlustion, etc, and facial features could be more ambiguous. As such, we can see that PFE achieves a more significant improvement on these three benchmarks. In particular on IJB-C at FAR= 0.001%, PFE reduces the error rate by 64%. In addition, simply fusing the original features with the learned uncertainty (PFE fuse ) also helps the performance.</p><p>In <ref type="table">Table 5</ref> we report the results on three protocols of the latest benchmark, IJB-S. Again, PFE is able to improve the performance in most cases. Notice that the gallery templates in the "Surveillance-to-still" and "Surveillance-tobooking" all include high-quality frontal mugshots, which clude those between probes and distractors. present little feature ambiguity. Therefore, we only see a slight performance gap in these two protocols. But in the most challenging "surveillance-to-surveillance" protocol, larger improvement can be achieved by using uncertainty for matching. Besides, PFE fuse+match improves the performance significantly on all the open-set protocols, which indicates that MLS has more impact on the absolute pairwise score than the relative ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Analysis</head><p>Why and when does PFE improve performance? We first repeat the same experiments in Section 3 using the PFE representation and MLS. The same network is used as the base model here. As one can see in <ref type="figure" target="#fig_4">Figure 6</ref>, although the scores of low-quality impostor pairs are still increasing, they converge to a point that is lower than the majority of genuine scores. Similarly, the scores of cross-quality genuine pairs converge to a point that is higher than the majority of impostor scores. This means the two types of errors discussed in Section 3 could be solved by PFE. This is further confirmed by the IJB-A results in <ref type="figure">Figure 7</ref>. <ref type="figure" target="#fig_5">Figure 8</ref> shows the distribution of estimated uncertainty on LFW, IJB-A and IJB-S. As one can see, the "variance" of uncertainty increases in the following order: LFW &lt; IJB-A &lt; IJB-S. Comparing with the performance in Section 5.2, we can see that PFE tends to achieve larger performance improvement on datasets with more diverse image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What does DNN see and not see?</head><p>To answer this question, we train a decoder network on the original embedding, then apply it to PFE by sampling z from the estimated distribution p(z|x) of given x. For a high-quality image <ref type="figure" target="#fig_6">(Figure 9</ref> Row 1), the reconstructed images tend to be very consistent without much variation, implying the model is very certain about the facial features in this images. In contrast, for a lower-quality input <ref type="figure" target="#fig_6">(Figure 9</ref> Row 2), larger variation can be observed from the reconstructed images. In particular, attributes that can be clearly discerned from the image (e.g. thick eye-brow) are still consistent while attributes cannot (e.g. eye shape) be discerned have larger variation. As for a mis-detected image <ref type="figure" target="#fig_6">(Figure 9</ref> Row 3), significant variation can be observed in the reconstructed images: the model does not see any salient feature in the given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Risk-controlled Face Recognition</head><p>In many scenarios, we may expect a higher performance than our system is able to achieve or we may want to make sure the system's performance can be controlled when facing complex application scenarios. Therefore, we would expect the model to reject input images if it is not confident. A common solution for this is to filter the images with a quality assessment tool. We show that PFE provides a natural solution for this task. We take all the images from LFW and IJB-A datasets for image-level face verification (We do not follow the original protocols here). The system is allowed to "filter out" a proportion of all images to maintain a better performance. We then report the TAR@FAR= 0.001% against the "Filter Out Rate". We consider two criteria for filtering: (1) the detection score of MTCNN <ref type="bibr" target="#b36">[37]</ref> and (2) a confidence value predicted by our uncertainty module. Here the confidence for i th sample is defined as the inverse of har-  monic mean of σ i across all dimensions. For fairness, both methods use the original deterministic embedding representations and cosine similarity for matching. To avoid saturated results, we use the model trained on CASIA-WebFace with AM-Softmax. The results are shown in <ref type="figure" target="#fig_0">Figure 11</ref>. As one can see, the predicted confidence value is a better indicator of the potential recognition accuracy of the input image. This is an expected result since PFE is trained under supervision for the particular model while an external quality estimator is unaware of the kind of features used for matching by the model. Example images with high/low confidence/quality scores are shown in <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have proposed probabilistic face embeddings (PFEs), which represent face images as distributions in the latent space. Probabilistic solutions were derived to compare and aggregate the PFE of face images. Unlike deterministic embeddings, PFEs do not suffer from the feature ambiguity dilemma for unconstrained face recognition. Quantitative and qualitative analysis on different settings showed that PFEs can effectively improve the face recognition performance by converting deterministic embeddings to PFEs. We have also shown that the uncertainty in PFEs is a good indicator for the "discriminative"quality of face images. In the future work we will explore how to learn PFEs in an end-to-end manner and how to address the data dependency within face templates.</p><formula xml:id="formula_10">D l=1 ( (µ (l) i − µ (l) j ) 2 σ 2(l) i + σ 2(l) j + log(σ 2(l) i + σ 2(l) j )) − D 2 log 2π.<label>(11)</label></formula><p>Note that directly solving the integral will lead to the same solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Property 1</head><p>Let us consider the case that σ 2(l) i equals to a constant c &gt; 0 for any image x i and dimension l. Thus the mutual likelihood score between a pair (x i , x j ) becomes:</p><formula xml:id="formula_11">s(x i , x j ) = − 1 2 D l=1 ( (µ (l) i − µ (l) j ) 2 2c + log(2c)) − D 2 log 2π = − c 1 µ i − µ j 2 − c 2 ,<label>(12)</label></formula><p>where c 1 = 1 4c and c 2 = D 2 log(4πc) are both constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Representation Fusion</head><p>We first prove Equation (5) in the main paper. Assuming all the observations x 1 , x 2 . . . x n+1 are conditionally independent given the latent code z. The posterior distribution is:</p><formula xml:id="formula_12">p(z|x 1 , x 2 , . . . , x n+1 ) = p(x 1 , x 2 , . . . , x n+1 |z)p(z) p(x 1 , x 2 , . . . , x n+1 ) = p(x 1 , x 2 , . . . , x n |z)p(x n+1 |z)p(z) p(x 1 , x 2 , . . . , x n+1 ) = p(x 1 , x 2 , . . . , x n )p(x n+1 ) p(x 1 , x 2 , . . . , x n+1 ) p(x 1 , x 2 , . . . , x n |z)p(x n+1 |z)p(z) p(x 1 , x 2 , . . . , x n )p(x n+1 ) = α p(x 1 , x 2 , . . . , x n , z)p(x n+1 , z) p(x 1 , x 2 , . . . , x n )p(x n+1 )p(z) = α p(z|x n+1 ) p(z) p(z|x 1 , x 2 , . . . , x n ),<label>(13)</label></formula><p>where α is a normalization constant. In this case, α = p(x1,x2,...,xn)p(xn+1) p(x1,x2,...,xn+1)</p><p>. Without loss of generality, let us consider a onedimensional case for the followings. The solution can be easily extended to a multivariate case since all feature dimensions are assumed to be independent. It can be shown that the posterior distribution in Equation <ref type="formula" target="#formula_2">(13)</ref> is a Gaussian distribution through induction. Let us assume p(z|x 1 , x 2 , . . . , x n ) is a Gaussian distribution withμ n and σ 2 n as mean and variance, respectively. Note that the initial case p(z|x 1 ) is guaranteed to be a Gaussian distribution. Let µ 0 and σ 2 0 denote the parameters of the noninformative prior of z. Then, if we take log on both side of Equation (13), we have:</p><formula xml:id="formula_13">log p(z|x 1 , x 2 , . . . , x n+1 ) = log p(z|x n+1 ) + log p(z|x 1 , x 2 , . . . , x n ) − log p(z) + const = − (z − µ n+1 ) 2 2σ 2 n+1 − (z −μ n ) 2 2σ 2 n + (z − µ 0 ) 2 2σ 2 0 + const = − (z −μ n+1 ) 2 2σ 2 n+1 + const.<label>(14)</label></formula><p>where "const" refers to the terms irrelevant to z and</p><formula xml:id="formula_14">µ n+1 =σ 2 n+1 ( µ n+1 σ 2 n+1 +μ n σ 2 n − µ 0 σ 2 0 ),<label>(15)</label></formula><formula xml:id="formula_15">1 σ 2 n+1 = 1 σ 2 n+1 + 1 σ 2 n − 1 σ 2 0 .<label>(16)</label></formula><p>Considering σ 0 → ∞, we havê</p><formula xml:id="formula_16">µ n+1 =σ 2 n µ n+1 + σ 2 n+1μn σ 2 n+1 +σ 2 n ,<label>(17)</label></formula><formula xml:id="formula_17">σ 2 n+1 = σ 2 n+1σ 2 n σ 2 n+1 +σ 2 n .<label>(18)</label></formula><p>The result means the posterior distribution is a new Gaussian distribution with a smaller variance. Further, we can directly give the solution of fusing n samples:</p><formula xml:id="formula_18">log p(z|x 1 , x 2 , . . . , x n ) = log[αp(z|x 1 ) n i=2 p(z|x i ) p(z) ] = (n − 1)log p(z) − n i=1 log p(z|x i ) + const = (n − 1) (z − µ 0 ) 2 2σ 2 0 − n i=1 (z − µ i ) 2 2σ 2 i + const = − (z −μ n ) 2 2σ 2 n + const.<label>(19)</label></formula><p>where α = n i=1 p(xi) p(x1,x2,...,xn) and</p><formula xml:id="formula_19">µ n = n i=1σ 2 n σ 2 i µ i − (n − 1)σ 2 n σ 2 0 µ 0 ,<label>(20)</label></formula><formula xml:id="formula_20">1 σ 2 n = n i=1 1 σ 2 i − (n − 1) 1 σ 2 0 .<label>(21)</label></formula><p>Considering σ 0 → ∞, we havê</p><formula xml:id="formula_21">µ n = n i=1σ 2 n σ 2 i µ i ,<label>(22)</label></formula><formula xml:id="formula_22">σ 2 n = 1 n i=1 1 σ 2 i .<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>All the models in the paper are implemented using Tensorflow r1.9. Two and Four GeForce GTX 1080 Ti GPUs are used for training base models on CASIA-Webface <ref type="bibr" target="#b43">[44]</ref> and MS-Celeb-1M <ref type="bibr" target="#b7">[8]</ref>, respectively. The uncertainty modules are trained using one GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Data Preprocessing</head><p>All the face images are first passed through MTCNN face detector <ref type="bibr" target="#b36">[37]</ref> to detect 5 facial landmarks (two eyes, nose and two mouth corners). Then, similarity transformation is used to normalize the face images based on the five landmarks. After transformation, the images are resized to 112 × 96. Before passing into networks, each pixel in the RGB image is normalized by subtracting 127.5 and dividing by 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Base Models</head><p>The base models for CASIA-Webface <ref type="bibr" target="#b43">[44]</ref> are trained for 28, 000 steps using a SGD optimizer with a momentum of 0.9. The learning rate starts at 0.1, and is decreased to 0.01 and 0.001 after 16, 000 and 24, 000 steps, respectively. For the base model trained on Ms-Celeb-1M <ref type="bibr" target="#b7">[8]</ref>, we train the network for 140, 000 steps using the same optimizer settings. The learning rate starts at 0.1, and is decreased to 0.01 and 0.001 after 80, 000 and 120, 000 steps, respectively. The batch size, feature dimension and weight decay are set to 256, 512 and 0.0005, respectively, for both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Uncertainty Module</head><p>Architecture The uncertainty module for all models are two-layer perceptrons with the same architecture: FC-BN-ReLU-FC-BN-exp, where FC refers to fully connected layers, BN refers to batch normalization layers <ref type="bibr" target="#b11">[12]</ref> and exp function ensures the outputs σ 2 are all positive values <ref type="bibr" target="#b15">[16]</ref>. The first FC shares the same input with the bottleneck layer, i.e. the output feature map of the last convolutional layer. The output of both FC layers are Ddimensional vectors, where D is the dimensionality of the latent space. In addition, we constrain the last BN layer to share the same γ and β across all dimensions, which we found to help stabilizing the training.</p><p>Training For the models trained on CASIA-WebFace <ref type="bibr" target="#b43">[44]</ref>, we train the uncertainty module for   3, 000 steps using a SGD optimizer with a momentum of 0.9. The learning rate starts at 0.001, and is decreased to 0.0001 after 2, 000 steps. For the model trained on MS-Celeb-1M <ref type="bibr" target="#b7">[8]</ref>, we train the uncertainty module for 12, 000 steps. The learning rate starts at 0.001, and is decreased to 0.0001 after 8, 000 steps. The batch size for both cases are 256. For each mini-batch, we randomly select 4 images from 64 different subjects to compose the positive pairs (384 pairs in all). The weight decay is set to 0.0005 in all cases. A Subset of the training data was separated as the validation set for choosing these hyper-parameters during development phase.</p><p>Inference Speed Feature extraction (passing through the whole network) using one GPU takes 1.5ms per image. Note that given the small size of the uncertainty module, it has little impact on the feature extraction time. Matching images using cosine similarity and mutual likelihood score takes 4ns and 15ns , respectively. Both are neglectable in comparison with feature extraction time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on Different Architectures</head><p>Throughout the main paper, we conducted the experiments using a 64-layer CNN network <ref type="bibr" target="#b20">[21]</ref>. Here, we evaluate the proposed method on two different network architectures for face recognition: CASIA-Net <ref type="bibr" target="#b43">[44]</ref> and 29-layer Light-CNN <ref type="bibr" target="#b38">[39]</ref>. Notice that both networks require differ-ent image shapes from our preprocessed ones. Thus we pad our images with zero values and resize them into the target size. Since the main purpose of the experiment is to evaluate the efficacy of the uncertainty module rather than comparing with the original results of these networks, the difference in the preprocessing should not affect a fair comparison. Besides, the original CASIA-Net does not converge for A-Softmax and AM-Softmax, so we add an bottleneck layer to output the embedding representation after the average pooling layer. Then we conduct the experiments by comparing probabilistic embeddings with base deterministic embeddings, similar to Section 5.1 in the main paper. The results are shown in <ref type="table" target="#tab_6">Table 6</ref> and <ref type="table" target="#tab_7">Table 7</ref>. Without tuning the architecture of the uncertainty module nor the hyper-parameters, PFE still improve the performance in most cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Difference between deterministic face embeddings and probabilistic face embeddings (PFEs). Deterministic embeddings represent every face as a point in the latent space without regards to its feature ambiguity. Probabilistic face embedding (PFE) gives a distributional estimation of features in the latent space instead. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of feature ambiguity dilemma. The plots show the cosine similarity on LFW dataset with different degrees of degradation. Blue lines show the similarity between original images and their respective degraded versions. Red lines show the similarity between impostor pairs of degraded images. The shading indicates the standard deviation. With larger degrees of degradation, the model becomes more confident (very high/low scores) in a wrong way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example genuine pairs from IJB-A dataset estimated with the lowest similarity scores and impostor pairs with the highest similarity scores (among all possible pairs) by a 64-layer CNN model. The genuine pairs mostly consist of one high-quality and one low-quality image while the impostor pairs are all low-quality images. Note that these pairs are not templates in the verification protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Fusion with PFEs. (a) Illustration of the fusion process as a directed graphical model. (b) Given the Gaussian representations of faces (from the same identity), the fusion process outputs a new Gaussian distribution in the latent space with a more precise mean and lower uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Repeated experiments on feature ambiguity dilemma with the proposed PFE. The same model inFigure 3is used as the base model and is converted to a PFE by training an uncertainty module. No additional training data nor data augmentation is used for training.(a) Low-score Genuine Pairs (b) High-score Impostor PairsFigure 7: Example genuine pairs from IJB-A dataset estimated with the lowest mutual likelihood scores and impostor pairs with the highest scores by the PFE version of the same 64-layer CNN model in Section 3. In comparison toFigure 4, most images here are high-quality ones with clear features, which can mislead the model to be confident in a wrong way. Note that these pairs are not templates in the verification protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Distribution of estimated uncertainty on different datasets. Here, "Uncertainty" refers to the harmonic mean of σ across all feature dimensions. Note that the estimated uncertainty is proportional to the complexity of the datasets. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Visualization results on a high-quality, a low-quality and a misdetected image from IJB-A. For each input, 5 images are reconstructed by a pre-trained decoder using the mean and 4 randomly sampled z vectors from the estimated distribution p(z|x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Example images from LFW and IJB-A that are estimated with the highest (H) confidence/quality scores and the lowest (L) scores by our method and MTCNN face detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of verification performance on LFW and IJB-A (not the original protocol) by filtering a proportion of images using different quality criteria.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Original 98.93 94.74 93.84 78.16 Softmax + Center Loss [36] PFE 99.27 95.42 94.51 80.83 Original 97.65 93.36 89.76 60.82 Triplet [28] PFE 98.45 93.96 90.04 61.00 Original 99.15 94.80 92.41 78.54 Results of models trained on CASIA-WebFace. "Original" refers to the deterministic embeddings. The better performance among each base model are shown in bold numbers. "PFE" uses mutual likelihood score for matching. IJB-A results are verification rates at FAR=0.1%.</figDesc><table><row><cell>A-Softmax [21]</cell><cell>PFE</cell><cell cols="4">99.32 94.94 93.37 82.58</cell></row><row><cell>AM-Softmax [34]</cell><cell>Original PFE</cell><cell cols="4">99.28 95.64 94.77 84.69 99.55 95.92 95.06 87.58</cell></row><row><cell>Method</cell><cell cols="3">Training Data LFW YTF</cell><cell cols="2">MF1 Rank1 Veri. MF1</cell></row><row><cell>DeepFace+ [32]</cell><cell>4M</cell><cell cols="2">97.35 91.4</cell><cell>-</cell><cell>-</cell></row><row><cell>FaceNet [28]</cell><cell>200M</cell><cell cols="2">99.63 95.1</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepID2+ [31]</cell><cell>300K</cell><cell cols="2">99.47 93.2</cell><cell>-</cell><cell>-</cell></row><row><cell>CenterFace [36]</cell><cell>0.7M</cell><cell cols="4">99.28 94.9 65.23 76.52</cell></row><row><cell>SphereFace [21]</cell><cell>0.5M</cell><cell cols="4">99.42 95.0 75.77 89.14</cell></row><row><cell>ArcFace [4]</cell><cell>5.8M</cell><cell cols="4">99.83 98.02 81.03 96.98</cell></row><row><cell>CosFace [35]</cell><cell>5M</cell><cell cols="4">99.73 97.6 77.11 89.88</cell></row><row><cell>L2-Face [26]</cell><cell>3.7M</cell><cell cols="2">99.78 96.08</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>4.4M</cell><cell cols="4">99.70 97.18 79.43 92.93</cell></row><row><cell>PFE fuse</cell><cell>4.4M</cell><cell>-</cell><cell>97.32</cell><cell>-</cell><cell>-</cell></row><row><cell>PFE fuse+match</cell><cell>4.4M</cell><cell cols="4">99.82 97.36 78.95 92.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="6">Results of our models (last three rows) trained on MS-Celeb-1M</cell></row><row><cell cols="6">and state-of-the-art methods on CFP (frontal-profile protocol) and IJB-A.</cell></row><row><cell>Method</cell><cell>Training Data</cell><cell cols="3">IJB-C (TAR@FAR) 0.001% 0.01% 0.1%</cell><cell>1%</cell></row><row><cell>Yin et al. [45]</cell><cell>0.5M</cell><cell>-</cell><cell>-</cell><cell>69.3</cell><cell>83.8</cell></row><row><cell>Cao et al. [2]</cell><cell>3.3M</cell><cell>74.7</cell><cell>84.0</cell><cell>91.0</cell><cell>96.0</cell></row><row><cell>Multicolumn [41]</cell><cell>3.3M</cell><cell>77.1</cell><cell>86.2</cell><cell>92.7</cell><cell>96.8</cell></row><row><cell>DCN [40]</cell><cell>3.3M</cell><cell>-</cell><cell>88.5</cell><cell>94.7</cell><cell>98.3</cell></row><row><cell>Baseline</cell><cell>4.4M</cell><cell>70.10</cell><cell cols="3">85.37 93.61 96.91</cell></row><row><cell>PFE fuse</cell><cell>4.4M</cell><cell>83.14</cell><cell cols="3">92.38 95.47 97.36</cell></row><row><cell>PFE fuse+match</cell><cell>4.4M</cell><cell cols="4">89.64 93.25 95.49 97.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Results of our models (last three rows) trained on MS-Celeb-1M and state-of-the-art methods on IJB-C.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>0M 50.82 61.16 64.95 16.44 24.19 53.04 62.67 66.35 27.40 29.70 10.05 17.55 21.06 0.11 0.68 Baseline 4.4M 50.00 59.07 62.70 7.22 19.05 47.54 56.14 61.08 14.75 22.99 9.40 17.52 23.04 0.06 0.71 PFE fuse 4.4M 53.44 61.40 65.05 10.53 22.87 55.45 63.17 66.38 16.70 26.20 8.18 14.52 19.31 0.09 0.63 PFE fuse+match 4.4M 50.16 58.33 62.28 31.88 35.33 53.60 61.75 64.97 35.99 39.82 9.20 20.82 27.34 0.84 2.83</figDesc><table><row><cell></cell><cell>1 Rank-5 Rank-10 1%</cell><cell>10% Rank-1 Rank-5 Rank-10 1% 10%</cell></row><row><cell>C-FAN [7]</cell><cell>5.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Base Model Representation LFW YTF CFP-FP IJB-A Original 97.70 92.56 91.13 63.93 Softmax + Center Loss [36] PFE 97.89 93.10 91.36 64.33 Original 96.98 90.72 85.69 54.47 Triplet [28] PFE 97.10 91.22 85.10 51.35 Original 97.12 92.38 89.31 54.48 A-Softmax [21] PFE 97.92 91.78 89.96 58.09 Original 98.32 93.50 90.24 71.28 AM-Softmax [34] PFE 98.63 94.00 90.50 75.92</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results of CASIA-Net models trained on CASIA-WebFace. "Original" refers to the deterministic embeddings. The better performance among each base model are shown in bold numbers. "PFE" uses mutual likelihood score for matching. IJB-A results are verification rates at FAR=0.1%.</figDesc><table><row><cell>Base Model</cell><cell cols="2">Representation LFW YTF CFP-FP IJB-A</cell></row><row><cell>Softmax +</cell><cell>Original</cell><cell>97.77 92.34 90.96 60.42</cell></row><row><cell>Center Loss [36]</cell><cell>PFE</cell><cell>98.28 93.24 92.29 62.41</cell></row><row><cell>Triplet [28]</cell><cell>Original PFE</cell><cell>97.48 92.46 90.01 52.34 98.15 93.62 90.54 56.81</cell></row><row><cell>A-Softmax [21]</cell><cell>Original PFE</cell><cell>98.07 92.72 89.34 63.21 98.47 93.44 90.54 71.96</cell></row><row><cell>AM-Softmax [34]</cell><cell>Original PFE</cell><cell>98.68 93.78 90.59 76.50 98.95 94.34 91.26 80.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results of Light-CNN models trained on CASIA-WebFace. "Original" refers to the deterministic embeddings. The better performance among each base model are shown in bold numbers. "PFE" uses mutual likelihood score for matching. IJB-A results are verification rates at FAR=0.1%.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/seasonSH/ Probabilistic-Face-Embeddings</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although we are given the identity labels, they cannot directly serve as target vectors in the latent feature space.<ref type="bibr" target="#b2">3</ref> Some in the literature have also used the terminology "data uncertainty" for a different purpose<ref type="bibr" target="#b41">[42]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Bottleneck layer refers to the layer which outputs the original face embedding.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/inlmouse/MS-Celeb-1M_ WashList. 8 84 and 4, 182 subjects were removed from CASIA-WebFace and MS-Celeb-1M, respectively.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Mutual Likelihood Score</head><p>Here we prove Equation (3) in the main paper. For simplicity, we do not need to directly solve the integral. Instead, let us consider an alternative vector ∆z = z i − z j , where z i ∼ p(z|x i ), z j ∼ p(z|x j ) and (x i , x j ) are the pair of images we need to compare. Then, p(z i = z j ) , i.e. Equation (2) in the main paper, is equivalent to the density value of p(∆z = 0).</p><p>The l th component (dimension) of ∆z, ∆z (l) , is the subtraction of two Gaussian variables, which means:</p><p>Therefore, the mutual likelihood score is given by: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition with image sets using manifold density divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ognjen</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In FG</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition based on image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.10433</idno>
		<title level="m">On the capacity of face representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video face recognition: Component-wise feature aggregation network (c-fan)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Modelling uncertainty in representation of facial features for face recognition. In Face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajit</forename><surname>Ps Hiremath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Danti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prabhakar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Log-euclidean metric learning on symmetric positive definite manifold with application to image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">IJB-S : IARPA Janus Surveillance Video Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleb</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07590</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Brendan F Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic elastic matching for pose variant face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A practical bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otto</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niggel</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">L2-constrained softmax loss for discriminative face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Triplet probabilistic embedding for face verification and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face recognition from long-term observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Comparator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multicolumn networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data uncertainty in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cybernetics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangjie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00611</idno>
		<title level="m">Towards interpretable face recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-task convolutional neural network for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Kaleem Razzaq Malik, and Abdullahi Mohamud Sharif. Face recognition with bayesian convolutional networks for robust surveillance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umara</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubeen</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tehseen</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghufran</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Latif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
