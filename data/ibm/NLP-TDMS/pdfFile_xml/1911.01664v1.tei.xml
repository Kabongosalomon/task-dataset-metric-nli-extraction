<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Context Network for Scene Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
							<email>wangyuhang.casia@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Business Growth BU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
							<email>baoyongjun@jd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Business Growth BU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<email>jinhuitang@njust.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Nanjing University of Science and Technology 4 University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Context Network for Scene Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works attempt to improve scene parsing performance by exploring different levels of contexts, and typically train a well-designed convolutional network to exploit useful contexts across all pixels equally. However, in this paper, we find that the context demands are varying from different pixels or regions in each image. Based on this observation, we propose an Adaptive Context Network (AC-Net) to capture the pixel-aware contexts by a competitive fusion of global context and local context according to different per-pixel demands. Specifically, when given a pixel, the global context demand is measured by the similarity between the global feature and its local feature, whose reverse value can be used to measure the local context demand. We model the two demand measurements by the proposed global context module and local context module, respectively, to generate adaptive contextual features. Furthermore, we import multiple such modules to build several adaptive context blocks in different levels of network to obtain a coarse-to-fine result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene parsing is a fundamental image understanding task which aims to perform per-pixel categorizations for a given scene image. Most recent approaches for scene parsing are based on Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b23">[24]</ref>. However, there are two limitations in FCN framworks. First, the consecutive subsampling operations like pooling and convolution striding lead to a significant decrease of the initial image resolution and make the loss of spatial details for scene parsing. Second, due to the limited receptive field <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> or local context features, the per-pixel dense classification is often ambiguous. In the end, FCNs result in the problems of rough object boundaries, ignorance of small objects, and misclassification of big objects and stuff.</p><p>Throughout various FCN-based improvements to overcome the above limitations, effective strategies to utilize different levels of contexts (i.e., local context and global context) are the main directions. Specifically, some methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b8">9]</ref> adopt "U-net" architectures, which ex-ploit multi-scale local contexts from middle-layers, to complement more visual details. Some methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">35]</ref> employ dilated convolution layers to capture a wider context with a larger receptive field while maintaining the resolution. Besides, the image-level features obtained by global average pooling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b40">40]</ref> are proposed as a global context to clarify local confusion. However, these FCN-based variants adopt per-pixel unified processing and overlook different per-pixel demands on different levels of contexts. That is, the local context from middle layers is essential for the class prediction of those pixels on edges or small objects, while the global context exploring the image-level representation is benefit to categorize large objects or stuff regions, especially for the case when the target region exceeds the receptive field of the network. We can also observe the necessity of pixel-sensitive context modeling from the results comparison shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in which the local and global contexts achieve different improvements on different objects or stuff. Therefore, how to effectively capture such pixel-or region-aware contexts in an end-to-end training framework is an open but valuable research topic for comprehensively accurate scene parsing.</p><p>In this paper, we propose an Adaptive Context Network (ACNet) to capture the pixel-aware contexts for image scene parsing. Different from previous methods which fuse different-level contexts for each pixel equally, AC-Net generates different per-pixel contexts, i.e., the contextbased features are functions of the input data and also vary from different pixels. Such an adaptive context generation is achieved by a competitive fusion mechanism of the global context from image-level feature and local context from the middle-layer feature according to per-pixel different demands. In other words, with the more attention paid on global context for a certain pixel, the less attention is paid on local context, and vice versa.</p><p>Usually, the global average pooled feature has a semantic guidance for large objects and stuff, but it lacks spatial information which makes it different from the features of details. Hence, we can match the global pooled feature with the feature of each pixel, obtaining the possibility of the pixel to be an element of large objects or spatial details. It can be further used as a pixel-aware context guidance to adaptively fuse the global features (global context) and lowlevel features (local context). Motivated by this intuition, we propose a global context module to adaptively capture global context. By measuring the similarity between the global feature and per-pixel feature, we can obtain the pixelaware demanding extent, called as global gated coefficient. The larger gated coefficient indicates that more global context and the less local context could be fused to the pixel. Then we multiply the global feature with the pixel-aware global gated coefficient before adding it to the pixel feature, with which some mislabeling and inconsistent results can be further corrected.</p><p>We also propose a local context module to compensate spatial details according to the local context demands. Specifically, we find that the pixels with features dissimilar to global feature, trend to be detail parts of images and need more local context to obtain precise results. Hence, we regard the reverse value of the global gated coefficient as local gated coefficient and multiply it with the low-level feature to generate a local gated feature. It emphasizes pixel-aware local context to spatial details and avoids some noises to the pixels belonging to big objects. Furthermore, we reuse multiple local gated features, which is similar to a recurrent learning process and complements more detail information.</p><p>We jointly employ a global context module and a local context module as an adaptive context block, and import such blocks into different levels of network. The architecture of our proposed ACNet is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Finally, comprehensive experimental analyses on Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref>, ADE20k <ref type="bibr" target="#b42">[42]</ref>, PASCAL Context <ref type="bibr" target="#b25">[26]</ref>, and COCO stuff <ref type="bibr" target="#b0">[1]</ref> dataset demonstrate the effectiveness of ACNet.</p><p>The main contributions of this paper are as follows:</p><p>• We propose an Adaptive Context Network (ACNet) to improve contextual information fusion according to the context demands of different pixels.</p><p>• A novel mechanism is proposed to measure global context demand. Global pooled feature can be adaptively fused to the pixels which need large context, thus reducing misclassification for large objects or stuff.</p><p>• We improve local context fusion according to the local context demand and reuse local feature progressively, thus improving segmentation results on small objects and edges.</p><p>• ACNet achieves new state-of-the-art performance on various scene parsing datasets. In particular, our AC-Net achieves a Mean IoU score of 82.3% on Cityscapes testing set without using coarse data, and 45.90% on ADE20K validation set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Global context embedding. Global context embedding have been proven its effectiveness to improve the categorization of some large semantic regions. ParseNet <ref type="bibr" target="#b22">[23]</ref> employs the global average pooled feature to augment the features at each location. PSPNet <ref type="bibr" target="#b40">[40]</ref> applies the global average pooling in their Spatial Pyramid Pooling module to collect global context. The work <ref type="bibr" target="#b14">[15]</ref> captures global contexts by a gloabl context network based on scene similarities. BiSeNet <ref type="bibr" target="#b33">[33]</ref> adds the global pooling on the top of the encoder structure to capture the global context. EncNet <ref type="bibr" target="#b37">[37]</ref> employs an encoding layer to capture global context and selectively highlight the class-dependent featuremaps.</p><p>Local context embedding. U-net based methods often adopt local context from low-and middle-level visual features to generate sharp boundaries or small details for highresolution prediction. RefineNet <ref type="bibr" target="#b21">[22]</ref> utilizes an encoderdecoder framework and refines low-resolution segmentation with fine-gained low-level feature. ExFuse <ref type="bibr" target="#b39">[39]</ref> assigns auxiliary supervisions directly to the early stages of the encoder network for improving low-level context. Deeplabv3+ <ref type="bibr" target="#b3">[4]</ref> adds a simple decoder module to capture local context, refining the segmentation results. Attention and gating mechanisms. Attention mechanisms have been widely used to improve the performance of segmentation task. PAN <ref type="bibr" target="#b16">[17]</ref> uses a global pooling to generate global attention, which can select the channel maps effectively. LRR <ref type="bibr" target="#b9">[10]</ref> generates a multiplicative gating to refine segment boundaries reconstructed from lower-resolution score maps. Ding et al. <ref type="bibr" target="#b6">[7]</ref> proposes a scheme of RNN-based gating mechanism to selectively aggregate multi-scale score maps, which can achieve an optimal multi-scale aggregation. The works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b13">14]</ref> adopt self-attention mechanism to model the relationship of features. Different from these works,we introduce a data-driven gating mechanism to capture global context and local context according to pixel-aware context demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adaptive Context Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Contextual information is effective for scene parsing task, most of current methods fuse the different context to each pixel equally, ignoring the different demands of pixelaware contexts. In this work, we propose a novel Adaptive Context Network (ACNet) to weigh the global and local context complemented to each pixel by a competitive fusion mechanism.</p><p>The overall architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, which adopts pretrained dilated ResNet <ref type="bibr" target="#b12">[13]</ref>, as the backbone network, and multiple adaptive context blocks to progressively generate high-resolution segmentation map. In the backbone network, we remove the downsampling operations and employ dilated convolutions in the last ResNet blocks, thus obtaining dense feature with output size 1/16 of the input image. It could achieve the balance between retaining spatial details and computation cost <ref type="bibr" target="#b3">[4]</ref>. In upsampling process, three adaptive context blocks are employed with three different resolutions. Each adaptive context block consists of a global context module, an upsampling module and a local context module, where the global context module selectively captures global context from the high-level features and the local context module selectively captures local context from the low-level features.</p><p>In the following subsections, we will elaborate the designing details of the global context module, the local con-  text module and their aggregation within an adaptive context block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global Context Module</head><p>Global context can provide global semantic guidance for overall scene images, thus rectifying misclassification and inconsistent parsing results. However, the benefit from global context is different for large objects and spatial details. it is necessary to treat each pixel differently when exploring the global context, that is to say, some pixels need more global context for categorization, while others may do not. Based on the intuition that the global pooled feature prefers to the large objects and stuff and lacks spatial information, we can match the global feature with the feature in each pixel, obtaining its possibility to be as an element of large objects or spatial details. Then we can exploit it to adaptively fuse the global context. To this end, we propose a Global Context Module (GCM) as follows.</p><p>Given an input feature map A ∈ R C×H×W , we use a global average pooling following a convolution layer to generate a global feature p ∈ R C×1×1 . In order to obtain the pixel-aware demand for global context (global gated coefficient), we first measure the feature similarity by calculating the euclidean distance D ∈ R H×W between global feature p and the features a i ∈ A for each pixel i, d i ∈ D is denoted as:</p><formula xml:id="formula_0">d i = a i − p 2 (1) where a i ∈ A , i ∈ [1, 2, ..., H × W ] is i th location in A.</formula><p>Noted that the smaller d i indicates that the feature at i th locations is closer to the global feature. Then we generate a global gated coefficient W g ∈ R H×W which is smoothed by an exponential function, w g i ∈ W g is denoted as:  where k is set to min H×W i=1 (d i ) for limiting the range of w g i ∈ (0, 1]. And δ is a hyperparameter, which controls the amplitude of the difference between high response and low response.</p><formula xml:id="formula_1">w g i = exp(− d i − k δ )<label>(2)</label></formula><p>Finally, we multiply the global feature p by w g i and a scale parameter α, and then perform an element-wise sum operation with the features A to obtain the final output C ∈ R C×H×W , c i ∈ C is denoted as:</p><formula xml:id="formula_2">c i = αw g i p + a i<label>(3)</label></formula><p>where α is a learned factor and initialized as 1. Here, we adopt sum operation instead of concatenation for saving memory. The details of global context module is shown in <ref type="figure">Figure.</ref>3 <ref type="bibr" target="#b0">(1)</ref>. It can be inferred from the above formulation that the feature C at different position obtain different global context according to global gated coefficient W g .</p><p>With this design, GCM could selectively enhance semantic consistency and reduce the misclassification and inconsistent predictions for large objects or stuff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local Context Module</head><p>Local context contributes to refine object boundaries and details. However, many methods fuse local context to all pixels without considering the different demand for local context. To solve this problem, we propose a Local Context Module (LCM) to selectively fuse local context for better refined segmentation.</p><p>As mentioned in Section 3.2, the global gated coefficient with high response indicates the pixels belong to large objects and stuff while low response indicates the pixels belong to spatial details. Based on this observation, we could obtain the local gated coeffcient by reversing value of global gated coefficient, where the global gated coefficient have been upsampled, formulated as:</p><formula xml:id="formula_3">W l = 1 − up(W g )<label>(4)</label></formula><p>where up(·) denotes a bilinear interpolation operation. In this way, the local gated coefficient indicates the more possibility the pixels belong to spatial details, the more local contexts are required, and vice versa. Then we obtain pixel-aware local context (gated local features) by multiply the local feature B ∈ R C×H×W from middle-layer features with the local gated coefficient and a scale parameter β. Finally, we concat the feature with the upsampled feature E ∈ R C×H×W to generate a refined feature F ∈ R C×H×W , f i ∈ F is denoted as:</p><formula xml:id="formula_4">f i = cat(βw l i b i , e i )<label>(5)</label></formula><p>where cat(·) denotes a concatenation operation, and β is a learned factor and initialized as 1. We adopt concatenation operation to combine the gated local feature and high-level feature, and a convolution layer is employed to fuse them. The details of local context module is shown in <ref type="figure">Figure.</ref>3 <ref type="bibr" target="#b1">(2)</ref>. With this design, we can selectively aggregate the local context according to the context demand of each pixel.</p><p>In addition, we find that it is useful to introduce gated local context directly multiple times. Specifically, we reuse gated local features by a concatenation operation followed by a convolution layer for three times. Such a recurrent learning process complements more spatial details for each position, and achieve a coarse-to-fine performance improvement. Noted that, it haven't been discussed in previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b3">4]</ref>. And we also verify the effectiveness of this process in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adaptive Context Block</head><p>Based on GCM and LCM, we further design an Adaptive Context Block to selectively capture global and local contextual information simultaneously.</p><p>Adaptive context block is built upon a cascaded architecture, the high-layer features are first fed into a global context module to selectively fuse global context to each pixel. Then passed sequentially through a bilinear upsampling layer and a local context module for learning a restoration of refined features. In order to obtain resolution corresponding to low-level feature, we also enlarge spatial resolution of the global gated coefficient by a bilinear upsampling operation before feeding into the local context module. Following <ref type="bibr" target="#b3">[4]</ref>, we apply a convolution layer on the low-level features to reduce the number of channels, thus refining the low-level features.</p><p>In the adaptive context block, we introduce a competitive fusion mechanism to capture global and local context according to their correlation of gated coefficient, thus suitable context can be adaptively fused to each pixels for better feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed method are evaluated on Cityscapes <ref type="bibr" target="#b4">[5]</ref>, ADE20K <ref type="bibr" target="#b42">[42]</ref>, PASCAL Context <ref type="bibr" target="#b25">[26]</ref>, COCO Stuff <ref type="bibr" target="#b0">[1]</ref>. Experimental results demonstrate that ACNet achieves new state-of-the-art performance on these datasets. In the next subsections, we first introduce the datasets and implementation details, then we make detail comparisons to evaluate our approaches on Cityscapes dataset. Finally, we present our results compared with state-of-the-art methods on ADE20K, PASCAL Context, COCO Stuff dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Cityscapes The dataset is a well-known road scene dateset collected for scene parsing, which has 2,979 images for training, 500 images for validation and 1,525 images for testing. Each image has a high resolution of 2048 × 1024 pixels with 19 semantic classes. Noted that no coarse data is employed in our experiments. ADE20K The ADE20K dataset is a vary challenge scene understanding dataset, which contains 150 classes (35 stuff classes and 115 discrete object classes). The dataset is divided into 20, 210/2, 000/3, 352 images for training, validation and testing. PASCAL Context The dataset is widely used for scene parsing, which contains 4,998 images for training and 5,105 images for testing. Following previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">37]</ref>, we evaluate the method on 60 categories ( 59 classes and one background category ). COCO Stuff The dataset has 171 categories including 80 objects and 91 stuff annotated to each pixel. Following previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>, we adopt 9,000 images for training and 1,000 images for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We employ a dilated pretrained ResNet architecture as our backbone network, where the dilated rates in the last ResNet block is set to <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b1">2)</ref>. Following <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b40">40]</ref>, we apply a 3 × 3 convolution layer with BN, ReLU on the outputs of the last ResNet block to reduce the number of channels to 512 before feeding into the first adaptive context block. In addition, we adopt the outputs of ResNet block-1 and ResNet block-2 as the low-level features, which provide local context for the first two adaptive context blocks. And we only adopt a global context module in last adaptive context block. In the first two adaptive context block,we employ a 3 × 3 convolution layer on the low-level featues before feeding it into local context module. The other convolution layers in the first two adaptive context block are composed of a 3 × 3 convolution operation with 448 and 256 kernels respectively followed by BN and ReLU. Pytorch is used to implement our method.</p><p>During training phase, we employ a poly learning rate policy where the initial learning rate is multiplied by (1 − iter total iter ) 0.9 after each iteration, and enable synchronized batch normalization <ref type="bibr" target="#b37">[37]</ref>. The base learning rate is set to 0.005 for Cityscapes and ADE20K, 0.001 for PASCAL Context and COCO stuff. Momentum and weight decay coefficients are set to 0.9 and 0.0001 respectively. Following <ref type="bibr" target="#b40">[40]</ref>, auxiliary loss is adopted when we adopt the bockbone ResNet101. In addition, we apply random cropping and random left-right flipping during training phase, and the randomly scaling for data augmentation is not employed if not mentioned on Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Cityscape dataset</head><p>Global Context Module: Firts of all, we design a global context module to adaptively aggregate global context according to pixel-aware demands. Specifically, we follow <ref type="bibr" target="#b1">[2]</ref> and build two dilated networks (ResNet-50) which yield the final feature maps with the 1/8 and 1/16 size of the original image. Next, the global context are added on the top of the networks with two different settings, which are GC and GCM respectively (GC denotes that we directly sum the global features to each pixel equally, GCM represents the global context module).</p><p>Experimental results are shown in <ref type="table">Table.</ref> 1, we can see that GCM (global context module) achieves better performance than GC in both two settings, especially for the output 1/8 size of the original image. It shows the effectiveness of GCM and also indicates that the improvement will be more obvious if the higher-resolution global gated coefficient is produced in GCM based on the dilated FCN. In addition, we also provide a discussion about δ, which controls the amplitude of the difference between high response and low response of the global gated coefficient (mentioned in Sec.3.1). When we set δ to 5, the gloabl context module  context improves the performance from 72.45% to 73.48%. When we adopt the local gated coefficient to selectively fuse local context for each pixel once, the performance is further improved to 74.03%. Reusing local gated feature brings continuous improvements of the performance from 74.03% to 74.67%. Adaptive Context Block: We further build an adaptive context block and cascade it for three times to obtain high resolution predictions. Results are listed in <ref type="table" target="#tab_2">Table 3</ref>. When we employ three adaptive context blocks (ACB#3), the performance is improved to 76.53%, which verifies the effectiveness of our method. In addition, we visualize the global gated coefficients in three adaptive context blocks with different resolutions as shown in <ref type="figure">Figure 4</ref>. The images are from the validation set of Cityscapes. We can find that the pixels with large global gated coefficient perfer to dominant stuff and large objects, such as the "road" in the first row and "car" in the last two rows. These stuff and objects are improved in our method. In addition, the pixels with small global gated coefficient perfer to small objects and edges, such as the "traffic sign" Ground Truth ACNet Image Ground Truth ACNet Image   <ref type="bibr" target="#b40">[40]</ref> 78  <ref type="bibr" target="#b13">[14]</ref> 81  <ref type="table">Table 4</ref>. Category-wise comparison with state-of-the-art methods on Cityscapes testing set. and "pole","person", etc. These spatial details are also be refined in our results. A similar trend is also spotted in other images. Some improvement strategies: we follow the common procedure of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> to further improve the performance of ACNet: (1) A deeper and powerful network ResNet-101. (2) MG: Different dilated rates <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16)</ref> in the last ResNet block. (3) DA: We transform the input images with random scales (from 0.5 to 2.2) during training phase. (4) OHEM: The online hard example mining is also adopted. (5) MS: we apply the multi-scale inputs with scales {0.5 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25} as well as their mirrors for inference. Experimental results are shown in <ref type="table" target="#tab_2">Table 3</ref>, when employing a deeper backbone (ResNet101), ACNet obtains 77.42% in terms of mean IoU. Then multi-grid dilated convolutional improves the performance by 1.08%. Data augmentation with multi-scale input (DA) brings another 1.59% improvement. OHEM increases the performance to 80.89%. Finally, using multi-scale testing, we attains the best performance of 82.00% on the validation set. Compared with state-of-art methods: We also compare our method with state-of-the-art methods on Cityscapes test set. Specifically, we fine tune our best model of ACNet with only fine annotated trainval data, and submit our test results to the official evaluation server. For each method, we report the accuracy for each class and the average class accuracy, which are reported in the original paper. Results are shown in <ref type="table">Table.</ref> 4. We can see that our ACNet achieve a new state-of-the-art performance of 82.3% % 1 on the test set. With the same backbone ResNet-101, our model outperforms DANet <ref type="bibr" target="#b15">[16]</ref>. Moreover, ACNet also surpasses DenseASPP <ref type="bibr" target="#b32">[32]</ref> , which uses more powerful pretrained models, and is heigher than Deeplabv3+ <ref type="bibr" target="#b3">[4]</ref> (82.1%), which uses extra the coarse annotations in training phase.</p><formula xml:id="formula_5">.4 - - - - - - - - - - - - - - - - - - - BiSeNet [33] 78.9 - - - - - - - - - - - - - - - - - - - PSANet [41] 80.1 - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_6">.4 - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on ADE20K dataset</head><p>In this subsection, we conduct experiments on the ADE20K dataset to validate the effectiveness of our method. Following previous works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>, data augmentation with multi-scale input and multi-scale testing are used. We evalute ACNet by pixel-wise accuracy (PixelAcc) and mean of class-wise intersection over union (mIoU). Quantitative results are shown in <ref type="table">Table.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Final score(%)</p><p>PSPNet269 (1st in place 2016) 55.38 PSANet-101 <ref type="bibr" target="#b41">[41]</ref> 55.46 CASIA IVA JD (1st in place 2017) 55.47 EncNet-101 <ref type="bibr" target="#b37">[37]</ref> 55.67 ACNet-101 55.84 <ref type="table">Table 6</ref>. Results of semantic segmentation on ADE20K testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Method mIoU (%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-101</head><p>Ding et al. <ref type="bibr" target="#b6">[7]</ref> 51.6 EncNet <ref type="bibr" target="#b37">[37]</ref> 51.7 SGR <ref type="bibr" target="#b18">[19]</ref> 52.5 DANet <ref type="bibr" target="#b15">[16]</ref> 52.6 ACNet 54.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res-152</head><p>RefineNet <ref type="bibr" target="#b21">[22]</ref> 47.3 MSCI <ref type="bibr" target="#b20">[21]</ref> 50.3 Xception-71 Tian et al. <ref type="bibr" target="#b28">[28]</ref> 52.5 ploying a deeper backbone ResNet101, ACNet achieves a new state-of-the-art performance of 45.90%/81.96%, which outperforms the previous state-of-the-art methods. In addition, we also fine tune our best model of ACNet-101 with trainval data, and submit our test results on the test set. The with single model of ACNet-101 gets final score as 55.84%. Among the approaches, most of methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b13">14]</ref> attemp to explore the global information by aggregation variant and relationship of the feature on the the top of the backbones. While our method focuses on capturing the pixel-aware contexts from high and low-level features and achieves better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Method mIoU(%)</head><p>Res-101</p><p>RefineNet <ref type="bibr" target="#b21">[22]</ref> 33.6 Ding et al. <ref type="bibr" target="#b6">[7]</ref> 35.7 DSSPN <ref type="bibr" target="#b19">[20]</ref> 38.9 SGR <ref type="bibr" target="#b18">[19]</ref> 39.1 DANet <ref type="bibr" target="#b15">[16]</ref> 39.7 ACNet 40.1 <ref type="table">Table 8</ref>. Segmentation results on COCO Stuff testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on PASCAL Context Dataset</head><p>We also carry out experiments on the PASCAL Context dataset to further demonstrate the effectiveness of ACNet. We employ the ACNet-101 network with the same training strategy on ADE20K and compare our model with previous state-of-the-art methods. The results are reported in <ref type="table" target="#tab_7">Table 7</ref>. ACNet obtains a Mean IoU of 54.1%, which surpasses previous published methods. Among the approaches, the recent methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">28]</ref> use more powerful network(e.g. ResNet-152 and Xception-71) as encoder network and fuse high-and low-level feature in decoder network, our method outperforms them by a relatively large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results on COCO stuff Dataset</head><p>Finally, we demonstrate the effectiveness of ACNet on the COCO stuff dataset. The ACNet-101 network is also employed. The COCO stuff results are reported in <ref type="table">Table 8</ref>. ACNet achieves performance of 40.1% Mean IoU, which also outperforms other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel network of ACNet to capture pixel-aware adaptive contexts for scene parsing, in which a global context module and a local context module are carefully designed and jointly employed as an adaptive context block to obtain a competitive fusion of the both contexts for each position. Our work is motivated by the observation that the global context from high-level features helps the categorization of some large semantic confused regions, while the local context from lower-level visual features helps to generate sharp boundaries or clear details. Extensive experiments demonstrate the outstanding performance of ACNet compared with other state-of-the-art methods. We believe such an adaptive context block can also be extended to other vision applications including object detection, pose estimation, and fine-grained recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The performance improvements over the basic FCN (a. Dilated FCN) on Cityscapes val set with the help of global context (b. Dilated FCN+Global context) and local context (c. Dilated FCN+Local context). Specially, pixel-wise enhanced representation by the global average pooling feature are employed as the global context, and a concatenated representation with low-level features as the local context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of Adaptive Context Network. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The details of Adaptive Context Block including (1) Global Context Module and (2) Local Context Module. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Example results of ACNet on Cityscapes validation set. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation experiments of Global Context Module on Cityscapes validation set, δ denotes the amplitude of difference distribution of the global gated coefficient.</figDesc><table><row><cell>Image</cell><cell>FCN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiments of Adaptive Context Block on Cityscapes validation set, #n denotes the number of Adaptive Context Block, MG denotes multi-grid dilated convolution, DA denotes data augmentation with multi-scale input during training phase, MS denotes multi-scale testing.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>DenseASPP [32] 80.6 98.7 87.1 93.4 60.7 62.7 65.6 74.6 78.5 93.6 72.5 95.4 86.2 71.9 96.0 78.0 90.3 80.7 69.7 76.8 CCNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>5.With ResNet50, the dilated FCN obtains 37.32%/77.78% in terms of mIoU and PixelAcc. When adopting our method, the performance is improved by 5.69%/3.23%. When em-</figDesc><table><row><cell cols="2">Backbone Method</cell><cell cols="2">mIoU (%) PixAcc%</cell></row><row><cell></cell><cell>Dilated FCN</cell><cell>37.32</cell><cell>77.78</cell></row><row><cell></cell><cell>EncNet[37]</cell><cell>41.11</cell><cell>79.73</cell></row><row><cell>Res-50</cell><cell>GCU[18]</cell><cell>42.60</cell><cell>79.51</cell></row><row><cell></cell><cell>PSPNet[40]</cell><cell>42.78</cell><cell>80.76</cell></row><row><cell></cell><cell>PASNet[41]</cell><cell>42.98</cell><cell>80.92</cell></row><row><cell></cell><cell>ACNet</cell><cell>43.01</cell><cell>81.01</cell></row><row><cell></cell><cell>UperNet[31]</cell><cell>42.66</cell><cell>81.01</cell></row><row><cell></cell><cell>PSPNet[40]</cell><cell>43.29</cell><cell>81.39</cell></row><row><cell></cell><cell>DSSPN[20]</cell><cell>43.68</cell><cell>81.13</cell></row><row><cell>Res-101</cell><cell>PASNet[41]</cell><cell>43.77</cell><cell>81.51</cell></row><row><cell></cell><cell>SGR [19]</cell><cell>44.32</cell><cell>81.43</cell></row><row><cell></cell><cell>EncNet[37]</cell><cell>44.65</cell><cell>81.19</cell></row><row><cell></cell><cell>GCU[18]</cell><cell>44.81</cell><cell>81.19</cell></row><row><cell></cell><cell>ACNet</cell><cell>45.90</cell><cell>81.96</cell></row><row><cell cols="4">Table 5. Results of semantic segmentation on ADE20K validation</cell></row><row><cell>set.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Segmentation results on PASCAL Context testing set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.cityscapes-dataset.com/anonymo us-results/?id=205de9c8857dbbe1ab61e7f5cc08be5e a290ece3c5125e3135d081139720d8f3</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Magnenat Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8885" to="8894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04943</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic multiscale filters for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7519" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ccnet: Crisscross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scene parsing with global context embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2631" to="2639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Fu Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Haijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Yongjun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhiwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanqing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of thr IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>thr IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxue</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9245" to="9255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1858" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamicstructured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale context intertwining for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Scene segmentation with dag-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1480" to="1493" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;19)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
