<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-25">25 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-25">25 Mar 2021</date>
						</imprint>
					</monogr>
					<note>An Image is Worth 16x16 Words, What is a Video Worth?</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Leading methods in the domain of action recognition try to distill information from both the spatial and temporal dimensions of an input video. Methods that reach State of the Art (SotA) accuracy, usually make use of 3D convolution layers as a way to abstract the temporal information from video frames. The use of such convolutions requires sampling short clips from the input video, where each clip is a collection of closely sampled frames. Since each short clip covers a small fraction of an input video, multiple clips are sampled at inference in order to cover the whole temporal length of the video. This leads to increased computational load and is impractical for real-world applications. We address the computational bottleneck by significantly reducing the number of frames required for inference. Our approach relies on a temporal transformer that applies global attention over video frames, and thus better exploits the salient information in each frame. Therefore our approach is very input efficient, and can achieve SotA results (on Kinetics dataset) with a fraction of the data (frames per video), computation and latency. Specifically on Kinetics-400, we reach 78.8 top-1 accuracy with ×30 less frames per video, and ×40 faster inference than the current leading method. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The stellar growth in video content urges the need for more efficient video recognition. Increased camera coverage and constantly growing network bandwidth for video streaming are making online recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref> essential in varied domains such as robotics, security and human-computer interaction. Additional applications like large-scale video retrieval benefit directly from faster recognition <ref type="bibr" target="#b0">[1]</ref>, as well as from efficient utilization of video frames transcoding.</p><p>In action recognition the task is to classify a video by extracting relevant information from its individual frames. The success of Convolutional Neural Networks (CNN) over images has been utilized for action recognition via 3D convolutions, extracting both spatial and temporal information <ref type="bibr" target="#b0">1</ref> Code is available at: https://github.com/Alibaba-MIIL/STAM <ref type="figure">Figure 1</ref>. Kinetics-400 top-1 Accuracy vs Runtime, measured over Nvidia V100 GPU and presented in log-scale. Markers sizes are proportional to the number of frames used per video by leading methods. Our method provides dominating trade-off for those three properties. out of consecutive frames. Since 3D convolutions are computationally expensive, the common practice is to apply those on a predefined number of short video clips, each composed of densely sampled frames, and average the predictions over these clips. Since the clips should cover the entire video for accurate predictions, a large fraction of the video frames is used by such methods, leading to computational bottlenecks of frames processing and transcoding. Recent methods addressed the processing bottleneck from different angles: more efficient per-frame architectures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref> and 3D modules <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4]</ref>, clip sampling <ref type="bibr" target="#b37">[38]</ref> and two-stream networks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30]</ref>. While the trade-off between accuracy and efficiency is continuously improving, for many real-time applications the required runtime is lower by orders of magnitude than the ones offered by current state-of-the-art methods.</p><p>In this work, we take a different path towards efficient action recognition. We train classifiers to learn spatiotemporal representations from small numbers of uniformly sampled video frames via an end-to-end attention mechanism. Our approach is motivated by human action recog- <ref type="bibr">Figure 2</ref>. Frame attention 16 frames uniformly sampled from a 10 second input video depicting 'beekeeping'. These frames are used as input to our model. Each frame's border displays the attention weight of that frame corresponding to the classification token (in heatmap range). We see that more attention is given to frames in which the action can clearly be identified. nition that is shown to maintain a similar accuracy when a small numbers of frames is viewed instead of the entire video <ref type="bibr" target="#b27">[28]</ref>. Conversely, multiple clips inference with 3D convolutions is often involved with redundant computations as consecutive video frames tend to be similar. In addition, its scope is limited by design to short actions, while realworld applications often span over larger intervals.</p><p>Inspired by recent breakthroughs in sequence modeling in the field of Natural language Processing (NLP), we propose a natural extension of Visual Transformers (ViT) <ref type="bibr" target="#b8">[9]</ref> to videos. We view a video as analog to a text paragraph to be classified efficiently. To that end, we sample sentences (images) uniformly from it and divide those to words (patches). In NLP, the Transformer model <ref type="bibr" target="#b34">[35]</ref> has proven superior to other sequence modeling techniques such as RNNs. The Transformer builds on a multi-head self attention layer, that learns global attention over the elements in the sequence. Similarly, our approach relies entirely on transformers, for both the spatial and the temporal dimensions.</p><p>We introduce an action classification model composed exclusively of self-attention layers operating in the spatial and temporal directions. We name our model STAM (Space Time Attention Model). The input sequence, in our case, is the sequence of image patches extracted from the individual frames and linearly projected onto a patch embedding space. First a spatial and then a temporal Transformer encoder models are applied on top of this embedding sequence to extract a video level representation or attention weighting of the frames. By leveraging this attention mechanism, we claim that the video sequence can be temporally subsampled by a larger factor than has previously been achieved, without degradation of the classification accuracy. <ref type="figure">Figure 1</ref> demonstrates the trade-off between the accuracy and runtime of top action recognition methods. While previous models are either accurate or efficient, models trained with our method offer a good combination of both. For example, STAM-32F achieves comparable accuracy to X3D-XL while being ×40 faster.</p><p>The motivation behind the proposed method is that applying global self-attention over a sequence of input frames is the key to reduce the number of required frames, by allowing information from individual frames to be propagated globally across the entire sequence. 3D convolutions, on the other hand, extract information locally over a small temporal (and spatial) scale, and therefore require frames to be sampled on a lower scale (i.e. dense sampling). In the NLP domain, where transformers are mostly being used, there is no issue of temporal continuity (and sampling density) since the words in a sentence are not temporally continuous as frames in the video are. Hence, our approach is a unique advantage of using Transformers on video data.</p><p>Subsampling the input video substantially reduces the computational load during training and inference, and furthermore, has the additional benefit of lowering the cost of retrieving input data. Indeed, in several applications there is a cost associated with retrieving input data from storage, or across a communications network. In such bandwidth limited applications most action recognition methods are prohibitively expensive for deployment, and methods like ours that rely on significantly less input data to operate, posses a clear advantage. To give an idea of such a scenario, suppose that there is a cost incurred for every access to a video frame located in storage. For typical methods, 30 * 16 = 480 frames are accessed in order to perform inference on a video. Compared to our method which requires 16 frames for the same video, the cost reduction we achieve is 30-fold, in addition to the reduction in run-time.</p><p>An additional advantage of STAM is that it is an endto-end trainable model. This is both simpler to implement (requiring the same sampling strategy and model for train and inference) and has fewer hyperparameters. Methods employing multi-clip averaging during test-time cannot be considered end-to-end trainable, since they add an additional temporal aggregation (averaging) layer during inference. Since it is used only at inference time, this additional layer is an ad-hoc component, whose effects are not taken into account during training. (See <ref type="figure" target="#fig_3">Figure 4)</ref> Our contributions can be summarized as follows:</p><p>• We propose a novel method for video action recognition that is entirely based on transformers for representation of spatio-temporal visual data. It is very simple, end-to-end trainable, and able to capture video information using only 3% of the data processed by leading efficient methods.</p><p>• Our method matches state-of-the-art accuracy while being more efficient by orders of magnitude. Specifically, on Kinetics-400 benchmark it achieves 78.8 top1-accuracy with ×40 faster inference time, or alternatively improving the efficient ECO <ref type="bibr" target="#b40">[41]</ref> accuracy by +8% while being twice as fast. This makes it a leading solution for latency-sensitive video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Transformers and Self Attention</head><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention layers are the building blocks of an encoder-decoder architecture called Transformer <ref type="bibr" target="#b34">[35]</ref>.</p><p>The Transformer architecture has become the dominant model in the field of NLP, outperforming previous methods on tasks, such as language translation, and text generation ( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b34">[35]</ref>). Attempts to introduce Transformers to the computer vision domain <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> use only attention based layers instead of the commonly used convolutional layers, and produce state-of-the-art results on image classification benchmarks such as ImageNet. Other methods ( <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>) combine convolutional networks with transformers for object detection. Other methods that apply self-attention in vision tasks include <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref>. These methods apply the Transformer model on the image pixel level, and therefore have to resort to approximations (either downsampling the image, or applying local attention instead of global). More similar to our approach, <ref type="bibr" target="#b14">[15]</ref> applies a Transformer model in the domain of action recognition. However, their model is a hybrid of a 3D convolutional model and a Transfomer model that acts on the CNN's output feature vectors. Since it relies on 3D convolutions as part of the network, it has the same disadvantages of convolutional models (requiring dense sampling and a large number of frames), while our method is fully based on the Transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Action Recognition</head><p>Action recognition method typically operate by applying layers of 3D or 2D convolutions on spatio-temporal data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>X3D <ref type="bibr" target="#b10">[11]</ref> use network search and hyperparameter optimisation to find the network and sampling parameters (network depth, width, input spatial resolution, temporal resolution), while SlowFast <ref type="bibr" target="#b11">[12]</ref> train two networks operating on different temporal resolutions. R3D <ref type="bibr" target="#b32">[33]</ref> decomposes the 3D convolution operator into two separate convolutions operating on the temporal and spatial dimensions. Although these works improve classification efficiency by modifying the network structure, they still require densely sampled frames as input. We overcome this limitation by removing the dependence of 3D convolutions, and modeling the temporal information via a self-attention sequence model.</p><p>Several other works apply 2D convolutional networks on individual frames <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and capture the temporal dependence by shifting feature maps in the temporal direction. In these methods information is propagated in a local neighborhood of frames, while in our case the global selfattention allows interaction across the whole spatial temporal dimensions.</p><p>Another line of research is focused on reducing the computation cost of existing action recognition methods. These works introduce techniques to improve network efficiency by adaptive resolution sampling <ref type="bibr" target="#b20">[21]</ref>, importance clip sampling <ref type="bibr" target="#b16">[17]</ref> or reducing redundant computation using linear approximations of feature maps <ref type="bibr" target="#b21">[22]</ref>. However, these works still rely on multi-clip testing for inference, and thus suffer from the same type of inefficiency which our method proposes to solve. We tackle the problem of computation by reducing the required input frames sampled from the video.</p><p>Additional works focus on action recognition with subsampled data. Mauthner et al. <ref type="bibr" target="#b19">[20]</ref> suggested a method that uses a single-frame representation for short video sequences based on appearance and on motion information. Other methods proposed encoding techniques for video representations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref>. Similarly to the methods that use video clips, they processed samples separately and fused the results over time. ECO <ref type="bibr" target="#b40">[41]</ref> sampled frames uniformly and applied a long-term spatio-temporal architecture over those. They learned per-frame representations with 2D CNNs and fed them into a 3D CNN afterwards. Our motivation and input data is similar, and the use of spatial and temporal encoders offers a significant improvement over their method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As shown by the recent work on Visual Transformers (ViT) <ref type="bibr" target="#b8">[9]</ref>, self-attention models provide powerful representations for images, by viewing an image as a sequence of words, where each word embedding is equivalent to a linear projection of an image patch.</p><p>In this section, We design a convolution-free model that is fully based on self-attention blocks for the spatiotemporal domain. By that, we offer an extension of ViT that captures temporal representations. While the proposed method is focused on action recognition, we note that it can be easily modified for additional video understanding tasks.</p><p>While the attention mechanism can be extended to temporal dependencies in several ways, our design is modular, making its implementation simpler and intuitive. Most importantly, our design naturally leverages the advantages of attention mechanisms compared to convolution operators when it comes to better utilization of temporal information. Our goal is to provide a model that can utilize sparsely subsampled temporal data for accurate predictions. Such model need to be able to capture long-term dependencies as well. While 2D convolutions filters are tailor-made for the structure of images, utilizing local connections and providing desired properties for object recognition and detection <ref type="bibr" target="#b15">[16]</ref>, the same properties might negatively affect the processing of subsampled temporal data. While a series of 3D-convolutions can learn long-term interactions due to increased receptive field, they are biased towards local ones. In order to verify this, we conducted an experiment: we fed leading methods that are based on 3D convolutions with the same subsampled data as in our method. The results are presented in <ref type="table">Table 5</ref>. The performance of both methods degraded significantly, the error of X3D increased by 23% and SlowFast error by 50%.</p><p>Transformers offer advantages over their convolutional counterparts regarding modeling long-term dependencies. While a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer <ref type="bibr" target="#b5">[6]</ref>, it also has the ability of directly model longdistance interactions <ref type="bibr" target="#b25">[26]</ref>.</p><p>We propose a combined spatial and temporal transformer (STAM) which takes a sequence of frames sampled from the video as input, and outputs a video level classification prediction. As illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>, we process the sampled frames with a spatial transformer following the method of <ref type="bibr" target="#b8">[9]</ref>, and aggregate the resulting frame embeddings with a temporal transformer. In this way we separate the spatial attention (on each frame) from the temporal attention applied to the sequence of frame embedding vectors. This separation between the spatial and temporal attention components has several advantages. First, this reduces the computation by breaking down the input sequence into two shorter sequences. In the first stage each patch is compared to N other patches within a frames. The second stage compares each frame embedding vector to F other vectors, resulting in less overall computation than comparing each patch to N F other patches.</p><p>The second advantage stems from the understanding that temporal information is better exploited on a higher (more abstract) level of the network. In many works the where 2D and 3D convolutions are used in the same network, the 3D components are only used on the top layers. Using the same reasoning, we apply the temporal attention on frame embeddings rather than on individual patches, since frame level representations provide more sense of what's going on in a video compared to individual patches.</p><p>Input embeddings. The input to the spatio-temporal transformer is X ∈ R H×W ×3×F consists of F RGB frames of size H ×W sampled from the original video. Each frame in this input block is first divided into non-overlapping patches. For a frame of size H ×W , we have N = HW/P 2 patches of size P × P .</p><p>These patches are flattened into vectors and linearly projected into an embedding vector: </p><p>where input vector x (p,t) ∈ R 3P 2 , and embedding vector z (p,t) ∈ R D are related by a learnable positional embedding vector e pos (p,t) , and matrix E. The indices p, and t are the patch and frame index, respecitvely with p = 1, . . . , N , and t = 1, . . . , F . In order to use the Transformer model for classification, a learnable classification token is added in the first position in the embedding sequence z (0) (0,0) ∈ R D . As will be shown, this classification token will be used to encode the information from each frame and propagate it temporally across the sequence of frames. For this reason we include a separate classification token for each frame in the sequence z Multi-head Self-Attention block (MSA). STAM consists of L MSA blocks. At each block ∈ {1, . . . , L}, and head a ∈ {1, . . . , A}, each patch representation is transformed into query, key, and value vectors. The representation produced by the previous block z −1 (p,t) is used as input. Where LN represents a LayerNorm <ref type="bibr" target="#b1">[2]</ref>. The dimension of each attention head is given by D h = D/A. The attention weights are computed by a dot product comparison between queries and keys. The self-attention weights α α α ( ,a) (p,t) ∈ R N F +F for patch (p, t) are given by:</p><formula xml:id="formula_1">q ( ,a) (p,t) = W ( ,a) Q LN z ( −1) (p,t) ∈ R D h (2) k ( ,a) (p,t) = W ( ,a) K LN z ( −1) (p,t) ∈ R D h (3) v ( ,a) (p,t) = W ( ,a) V LN z ( −1) (p,t) ∈ R D h<label>(4)</label></formula><formula xml:id="formula_2">α α α ( ,a) (p,t) = SM   q ( ,a) (p,t) √ D h · k ( ,a) (0,t) k ( ,a) (p ,t ) p =1,...,N t =1,...,F  <label>(5)</label></formula><p>where SM() denotes the softmax activation function, and k ( ,a) (0,t) is the key value associated with the class token of frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial attention</head><p>Applying attention over all the patches of the sequence is computationally expensive, therefore, an alternative configuration is required in order to make the spatio-temporal attention computationally tractable. A reduction in computation can be achieved by disentangling the spatial and temporal dimensions. For the spatial attention, we apply attention between patches of the same frame:  These outputs from attention heads are concatenated and passed through a 2 Multi-Layer Perceptron (MLP) layers with GeLU <ref type="bibr" target="#b12">[13]</ref> activations:</p><formula xml:id="formula_3">α α α ( ,a)space (p,t) = SM   q ( ,a) (p,t) √ D h · k ( ,a) (0,t) , k</formula><formula xml:id="formula_4">z ( ) (p,t) = W O     s ( ,1) (p,t) . . . s ( ,A) (p,t)     + z ( −1) (p,t) (8) z ( ) (p,t) = MLP LN z ( ) (p,t) + z ( ) (p,t) .<label>(9)</label></formula><p>The MSA and MLP layers are operating as residual operators thanks to added skip-connections After passing through the spatial Transformer layers, the class embedding from each frame is used to produce an embedding vector f t . This frame embedding will be fed into the temporal attention.</p><formula xml:id="formula_5">f t = LN z (Lspace) (0,t) t=1,...,F ∈ R D .<label>(10)</label></formula><p>where L space is the number of layers of the spatial Transformer.</p><p>Temporal attention. The spatial attention provides a powerfull representation for each individual frame by applying attention between patches in the same image. However, in order to capture the temporal information across the frame sequence, a temporal attention mechanism is required. The effect of temporal modeling can be seen in table 2. The spatial attention backbone provides a good representation of the videos, however the additional temporal attention provides a significant improvement over it. In our model, the temporal attention layers are applied on the representations produced by the spatial attention layers. For the temporal blocks of our model, we use the frame embedding vectors from eqn. 10, stacked into a matrix X time ∈ R F ×D as the input sequence. As before, we add a trainable classification token at index t = 0. This input sequence is projected into query/key/value vectors q</p><formula xml:id="formula_6">( ,a) t , k ( ,a) t , v ( ,a) t</formula><p>. The temporal attention is then computed only over the frame index.</p><formula xml:id="formula_7">α α α ( ,a)time t = SM   q ( ,a) t √ D h · k ( ,a) 0 k ( ,a) t t =1,...,F   .<label>(11)</label></formula><p>Next, we apply the same equations of the attention block eqn. <ref type="bibr" target="#b6">(7)</ref> through <ref type="formula" target="#formula_4">(9)</ref>, with a single axis describing the frame indices instead of the double (p, t) index which was used in those equations. The embedding vector for a video sequence is given by applying the layer norm on the classification embedding from the top layer:</p><formula xml:id="formula_8">y = LN z (Ltime) (0) ∈ R D .<label>(12)</label></formula><p>where L time is the number of temporal attention layers. An additional single layer MLP is applied as the classifier, outputing a vector of dimension equal to the number of classes. The added cost of the temporal encoder layers over the spatial layers is negligible since they operate on an input sequence of length F , which is an order of magnitude smaller than the number of patches N (in our experiments usually F = 16, while N = 196). In this architecture, the total complexity is O(F N 2 + F 2 ). If the attention operation were applied over all spatio-temporal patches, the complexity would be in the order of O((F N ) 2 ), which is prohibitively large.</p><p>An alternative possible configuration for the spatiotemporal attention would be to apply the temporal attention (between patches at the same spatial position, but from different frames), after the spatial attention within each block. We found that this variation produces slightly worse results, and higher computational cost.</p><p>Using an analogy to NLP, we consider each frame to be a sentence (in which patches play the role of word tokens), and each video is a paragraph of sentences. Following this analogy, it makes sense to apply a transformer separately on the sentences, extracting a representation vector per sentence, and then an additional Transformer on these vectors to predict a class (e.g. sentiment) from the entire paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation Details STAM consists of two parts: the spatial attention, and the temporal attention. In our experiments we closely follow the ViT B model proposed by <ref type="bibr" target="#b34">[35]</ref> as the spatial Transformer. This model is the lighter verion of the ViT family of models and contains 12 MSA layers, each with 12 self-attention heads. We use the imagenet-21K pretraining provided by <ref type="bibr" target="#b34">[35]</ref> (unless specified otherwise). For the Temporal Transformer we use an even smaller version of the Transformer with 6-layers and 8-head self-attention (L space = 12, L time = 6). The temporal layers are trained from scratch, and initialized randomly.</p><p>We sample frames uniformly across the video. For training we resize the smaller dimension of each frame to a value ∈ [256, 320], and take a random crop of size 224×224 from the same location for all frames of the same video. We also apply random flip augmentation, Cutout with factor 0.5, and auto-augment with Imagenet policy on all frames.</p><p>For inference we resize each frame so that the smaller dimension is 256, and take a crop of size 224 × 224 from the center. We use the same uniform frame sampling for training and inference.</p><p>Training For Kinetics-400 we train our model on 8V100 GPUs for 30 epochs with learning rate warm-up, and a cosine learning rate schedule. Compared to X3D and Slow-Fast, both trained with 128 GPUs for 256 epochs, our training is much faster and requires less epochs (∼ 30).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kinetics:</head><p>We compare our method to others on Kinetics-400 dataset <ref type="bibr" target="#b39">[40]</ref>. <ref type="table">Table 1</ref> shows our method achieves 77.8% top-1 accuracy using 16 sampled frames per video (at 270 GFLOPS). Compared to X3D L, which achieves similar top-1 accuracy (77.5%) using 30 clips for inference (at 744 GFLOPS), this is an 0.3 improvement in top-1 accuracy, using only 36% of the computation required by X3D L.</p><p>The reduction in run-time is even more significant. Compared to X3D L we observe a reduction in inference time from 2.27 to 0.05 hrs for the entire validation set. We also calculate the video per second runtime by performing inference on a single batch of clips. We find that our method (with 16 frames) is able to outperform X3D L by a factor of 43. This substantial improvement in runtime is partly due to the fewer input frames required by our method, and in part due to the improved efficiency of the ViT B model compared to the more complex X3D architecture. Runtime is a more tangible metric for efficiency, and therefore we focus on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Experiments</head><p>This section provides ablation studies on Kinetics-400 comparing accuracy and computational complexity.</p><p>First, we compare models with only spatial attention to a model that has temporal attention as well. In table 2 we compare these two types of models and verify the positive effect of the temporal attention. We compare two variants of our method. The first is our full method using ViT B backbone as the spatial attention model with a temporal transformer, and the second uses the same backbone but replace the temporal transformer with an average of the frame embeddings. The table shows that the temporal transformer provides a gain of 2.7 over the naive approach. Additionally, we repeat this experiment using a different backbone and see the consistency of the result across different backbones. We use a TResNet-M <ref type="bibr" target="#b26">[27]</ref> backbone, and again observe that the temporal transformer significantly improves accuracy.</p><p>In <ref type="table">table 3</ref>, we show the effect of replacing the ViT model with a CNN model. We compare 3 different CNN variants (TResNet-M, TResNet-L <ref type="bibr" target="#b26">[27]</ref>, ResNet50) trained together with the temporal transformer. This experiment shows that the highest accuracy is achieved using the full spatial, and temporal transformer model. Although replacing the spatial transformer with a CNN model is possible, and achieves reasonable accuracy, it is less powerful than the combination of the spatial and temporal attention, which applies attention over all the patches of the frame sequence. <ref type="table">In table 4</ref> we compare different sequence lengths as input to STAM. We sample 8, 16, 32, and 64 frames and compare the results. The clear trend is an increase in accuracy along with the increase in sequence length. For an increase of 16 additional frames (from 16 frames to 32 frames) we see an improvement of 1% to the accuracy. Switching the number of input frames from 32 to 64, results in a smaller gain of only 0.4. Using a larger number of frames doesn't improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of frames.</head><p>The use of 4 frames to classify a video of length 10 seconds suggests the use of our method for longer range actions. For instance, if we use the same sampling rate for a 1 minute video, we would require 24 frames for inference.</p><p>Runtime comparison. In table 5 we evaluate the accuracy of two leading methods, with a reduced number of input frames. We use a single 16 frame input clip sampled uniformly, and so use these methods with the a similar running time to our method. The table clearly shows that by reducing the number of input frames, these methods suffer a large degradation in accuracy. This show that methods that rely on 3D convolutions require frames to be sampled at a higher rate than the one we use, and cannot be made to improve their runtime by sampling sparse input frames. <ref type="figure">Figure 1</ref> plots different action recognition methods on the accuracy vs. runtime (VPS) scale. We compare methods that are both designed for efficiency (ECO <ref type="bibr" target="#b40">[41]</ref>), and methods that apply heavier models for increased accuracy (X3D <ref type="bibr" target="#b10">[11]</ref>, SlowFast <ref type="bibr" target="#b11">[12]</ref>). We see that our method is on par with the accuracy of the slower methods and improves over their runtime by a significant margin. STAM's runtime is comparable to that of ECO, at 20 VPS, yet significantly outperforms that method in terms of accuracy by 8%. Since ECO employs a similar sampling strategy to ours (sampling individual frames across the video), we conclude that the temporal transformer is better at capturing the temporal information from separate frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we have presented a method for efficient video action recognition that is entirely based on transformers. Inspired by NLP, we model a video as a paragraph and uniformly select frames that are modeled as sentences. This modeling allows us to utilize transformers to capture complex spatio-temporal dependencies between distinct frames, leading to accurate predictions based on a small fraction of the video data. The accuracy of our models' predictions is comparable to state-of-the-art methods while being faster by orders of magnitude, making it a good candidate for latency-sensitive applications like real-time, such as recognition and video retrieval. In addition, our method is simple, end-to-end and generic, thus, it can be used for further video understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Datasests</head><p>We evaluate our method on 2 additional datasets: UCF101, Charades.</p><p>UCF101 UCF101 <ref type="bibr" target="#b30">[31]</ref> is an older, yet still popular action recognition dataset. Our results on this dataset are presented in table 6. The results suggest that our model achieves a good trade-off between runtime and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Top- Charades Charades <ref type="bibr" target="#b28">[29]</ref> is a dataset with longer range interactions, and multiple labels per video. <ref type="table" target="#tab_3">Table 7</ref> presents our results on this dataset. Although our model doesn't reach SotA accuracy, it shows promise as an efficient model, requiring less input frames from each video, and less FLOPS.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Our proposed Transformer Network for video</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t) = Ex (p,t) + e pos (p,t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Training and inference commonly used by other methods. In the common approach multiple clips are sampled from each video, i.e., dense sampling of frames. Furthermore, training and inference have a different structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 6 )</head><label>6</label><figDesc>The attention vector entries are used as coefficients in a weighted sum over the values for each attention head: s ( ,a) (p,t) = α ( ,a) (p,t),(0,t) v ( ,a) (0,t) + N p =1 α ( ,a) (p,t),(p ,t) v ( ,a) (p ,t) . (7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .Table 3 .Table 4 .Table 5 .</head><label>12345</label><figDesc>Model comparison on Kinetics400. Time measurements were done on Nvidia V100 GPUs with mixed precision. The Runtime [hrs] measures inference on Kinetics-400 validation set (using 8 GPUs), while the videos per second (VPS) measurement was done on a single GPU. Results of various methods are as reported in the relevant publications. The proposed STAM is an order of magnitude faster while providing SOTA accuracy. Temporal Transformer vs. Mean. Comparing the Temporal Transformer representation vs. mean of frame embeddings. Using different backbones with the Temporal Transformer. TResNet and ViT models use imagenet-21K pretraining, while ResNet50 is used with imagenet-1K pretraining. Number of frames used for prediction Comparing different number of frames sampled uniformly from the video as input, using the ViT+Temporal model. Evaluating X3D and SlowFast with one clip (16 frames). Applying the same sampling strategy that we use in Video Transformer on other methods. (In parentheses -accuracy at 30 views per video).</figDesc><table><row><cell>Model</cell><cell>Top-1 Accuracy [%]</cell><cell cols="2">Flops × views [G]</cell><cell cols="2">Param [M]</cell><cell>Runtime [hrs]</cell><cell>Runtime [VPS]</cell></row><row><cell>Oct-I3D + NL [5]</cell><cell>75.7</cell><cell></cell><cell>28.9 × 30</cell><cell cols="2">33.6</cell><cell>-</cell><cell>-</cell></row><row><cell>SlowFast 8 × 8, R50</cell><cell>77.0</cell><cell></cell><cell>65.7 × 30</cell><cell cols="2">34.4</cell><cell>2.75</cell><cell>1.4</cell></row><row><cell>X3D-M</cell><cell>76.0</cell><cell></cell><cell>6.2 × 30</cell><cell cols="2">3.8</cell><cell>1.47</cell><cell>1.3</cell></row><row><cell>X3D-L</cell><cell>77.5</cell><cell></cell><cell>24.8 × 30</cell><cell cols="2">6.1</cell><cell>2.06</cell><cell>0.46</cell></row><row><cell>X3D-XL</cell><cell>79.1</cell><cell></cell><cell>48.4 × 30</cell><cell cols="2">11.0</cell><cell>-</cell><cell>-</cell></row><row><cell>TSM R50</cell><cell>74.7</cell><cell></cell><cell>65 × 10</cell><cell cols="2">24.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Nonlocal R101</cell><cell>77.7</cell><cell></cell><cell>359 × 30</cell><cell cols="2">54.3</cell><cell>-</cell><cell>-</cell></row><row><cell>STAM (16 Frames)</cell><cell>77.8</cell><cell></cell><cell>270 × 1</cell><cell></cell><cell>96</cell><cell>0.05</cell><cell>20.0</cell></row><row><cell>STAM (64 Frames)</cell><cell>79.2</cell><cell></cell><cell>1040 × 1</cell><cell></cell><cell>96</cell><cell>0.21</cell><cell>4.8</cell></row><row><cell cols="2">Backbone+Temporal</cell><cell></cell><cell cols="4">Top-1 Accuracy [%]</cell><cell>Flops [G]</cell></row><row><cell cols="2">ViT+Temporal-Transformer</cell><cell></cell><cell></cell><cell cols="2">77.8</cell><cell>270</cell></row><row><cell cols="4">TResNet-M+Temporal-Transformer</cell><cell cols="2">75.7</cell><cell>93</cell></row><row><cell cols="2">ViT+Mean</cell><cell></cell><cell></cell><cell cols="2">75.1</cell><cell>265</cell></row><row><cell cols="2">TResNet-M+Mean</cell><cell></cell><cell></cell><cell cols="2">71.9</cell><cell>88</cell></row><row><cell cols="3">Model (num. of frames)</cell><cell cols="3">Top-1 Accuracy [%]</cell><cell>Flops [G]</cell></row><row><cell cols="3">TResNet-M+Temporal (16)</cell><cell cols="2">75.7</cell><cell></cell><cell>93</cell></row><row><cell cols="3">TResNet-L+Temporal (8)</cell><cell cols="2">75.9</cell><cell></cell><cell>77</cell></row><row><cell cols="3">ResNet50+Temporal (16)</cell><cell cols="2">72.5</cell><cell></cell><cell>70</cell></row><row><cell cols="2">ViT-B+Temporal (16)</cell><cell></cell><cell cols="2">77.8</cell><cell></cell><cell>270</cell></row><row><cell></cell><cell>Number of frames</cell><cell cols="3">Top-1 Accuracy [%]</cell><cell cols="2">Flops [G]</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell>74.1</cell><cell></cell><cell>67</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell>76.8</cell><cell></cell><cell>135</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell>77.8</cell><cell></cell><cell>270</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell>78.8</cell><cell></cell><cell>540</cell></row><row><cell></cell><cell>64</cell><cell></cell><cell>79.2</cell><cell></cell><cell cols="2">1080</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell cols="3">Top-1 Accuracy [%]</cell></row><row><cell></cell><cell>X3D L</cell><cell></cell><cell cols="2">72.25 (77.5)</cell><cell></cell></row><row><cell></cell><cell cols="2">SlowFast 8x8 R50</cell><cell cols="2">65.5 (77.0)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 6 .</head><label>6</label><figDesc>Results on UCF-101 dataset. Results of various methods are as reported in the relevant publications. We compare methods that use only RGB frames as input (without Optical Flow), and are pretrained on Kinetics-400 or Imagenet</figDesc><table><row><cell></cell><cell>1 Accuracy</cell><cell>Runtime</cell></row><row><cell></cell><cell>[%]</cell><cell>[VPS]</cell></row><row><cell>ECO [41]</cell><cell>93.6</cell><cell>20.8</cell></row><row><cell>R(2+1)D-34</cell><cell>96.8</cell><cell>N/A</cell></row><row><cell>I3D</cell><cell>95.6</cell><cell>N/A</cell></row><row><cell>S3D</cell><cell>96.8</cell><cell>N/A</cell></row><row><cell>FASTER32 [39]</cell><cell>96.9</cell><cell>2.8</cell></row><row><cell>STAM-32</cell><cell>97.0</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Results on Charades dataset. Results of various methods are as reported in the relevant publications. Including methods that are pretrained on Kinetics-400 or Imagenet.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale video retrieval using image queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1406" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3435" to="3444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu (</forename><surname>Richard) Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<title level="m">Gaussian error linear units (gelus). arXiv: Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Late temporal modeling in 3d cnn architectures with bert for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Esat Kalfaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aydin Alatan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="731" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kauderer-Abrams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01450</idno>
		<title level="m">Quantifying translation-invariance in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssnet: scale selection network for online 3d action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8349" to="8358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Action recognition from a small number of frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ar-net: Adaptive frame resolution for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Va-red 2 : Video adaptive redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Bowen Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camilo</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Fosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07887</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep quantization: Encoding convolutional activations with deep generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6759" to="6768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2199</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ijaz Ul Haq, and Sung Wook Baik. Efficient activity recognition using lightweight cnn and ds-gru network for surveillance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Palade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">107102</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic sampling networks for efficient action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Dong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7970" to="7983" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster recurrent networks for efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13098" to="13105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Natsev, and Mustafa Suleyman. The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Tim Green, Trevor Back, Paul</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
