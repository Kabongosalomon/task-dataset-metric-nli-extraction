<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synchronous Bidirectional Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
							<email>long.zhou@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<email>jjzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<email>cqzong@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Synchronous Bidirectional Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing approaches to neural machine translation (NMT) generate the target language sequence token by token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional neural machine translation (SB-NMT) that predicts its outputs using left-to-right and rightto-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-toleft (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese-English, WMT14 English-German, and WMT18 Russian-English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49 and 1.04 BLEU points respectively, and obtains the state-of-the-art performance on Chinese-English and English-German translation tasks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation has significantly improved the quality of machine translation in recent years <ref type="bibr" target="#b25">(Sutskever et al., 2014;</ref>  2015; <ref type="bibr" target="#b34">Zhang and Zong, 2015;</ref><ref type="bibr" target="#b7">Gehring et al., 2017;</ref><ref type="bibr" target="#b28">Vaswani et al., 2017)</ref>. Recent approaches to sequence to sequence learning typically leverage recurrence <ref type="bibr" target="#b25">(Sutskever et al., 2014)</ref>, convolution <ref type="bibr" target="#b7">(Gehring et al., 2017)</ref>, or attention <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> as basic building blocks. Typically, NMT adopts the encoder-decoder architecture and generates the target translation from left to right. Despite their remarkable success, NMT models suffer from several weaknesses <ref type="bibr" target="#b10">(Koehn and Knowles, 2017)</ref>. One of the most prominent issues is the problem of unbalanced outputs in which the translation prefixes are better predicted than the suffixes . We analyze translation accuracy of the first and last 4 tokens for left-to-right (L2R) and right-toleft (R2L) directions respectively. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the statistical results show that L2R performs better in the first 4 tokens, whereas R2L translates better in term of the last 4 tokens. This problem is mainly caused by the left-to-right unidirectional decoding, which conditions each output word on previously generated outputs only, but leaving the future information from target-side contexts unexploited during translation. The future context is commonly used in reading and writing in human cognitive process , and it is crucial to avoid under-translation <ref type="bibr" target="#b27">(Tu et al., 2016;</ref><ref type="bibr" target="#b16">Mi et al., 2016)</ref>.</p><p>To alleviate the problems, existing studies usually used independent bidirectional decoders for SBAtt: <ref type="figure">Figure 1</ref>: Illustration of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token l2r and R2L means right-to-left decoding indicated by the start token r2l . SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y 3 does not only rely on y 1 and y 2 , but also depends on y n and y n−1 of R2L.</p><p>NMT <ref type="bibr" target="#b20">Sennrich et al., 2016a)</ref>. Most of them trained two NMT models with left-to-right and right-to-left directions respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently,  presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than the conventional NMT framework beacuse they require two NMT models or decoders. Furthermore, the L2R and R2L decoders are independent from each other , or only the forward decoder can utilize information from the backward decoder . It is therefore a promising direction to design a synchronous bidirectional decoding algorithm in which L2R and R2L generations can interact with each other. Accordingly, we propose in this paper a novel framework (SB-NMT) that utilizes a single decoder to bidirectionally generate target sentences simultaneously and interactively. As shown in <ref type="figure">Figure 1</ref>, two special labels ( l2r and r2l ) at the beginning of the target sentence guide translating from left to right or right to left, and the decoder in each direction can utilize the previously generated symbols of bidirectional decoding when generating the next token. Taking L2R decoding as an example, at each moment, the generation of the target word (e.g., y 3 ) does not only rely on previously generated outputs (y 1 and y 2 ) of L2R decoding, but also depends on previously predicted tokens (y n and y n−1 ) of R2L decoding. Compared to the previous related NMT models, our method has the following advantages: 1) We use a single model (one encoder and one decoder) to achieve the decoding with left-to-right and right-to-left generation, which can be processed in parallel. 2) Via the synchronous bidirectional attention model (SBAtt, §3.2), our proposed model is an end-to-end joint framework and can optimize bidirectional decoding simultaneously. 3) Compared to two-phase decoding scheme in previous work, our decoder is faster and more compact using one beam-search algorithm.</p><p>Specifically, we make the following contributions in this paper:</p><p>• We propose a synchronous bidirectional NMT model that adopts one decoder to generate outputs with left-to-right and right-toleft directions simultaneously and interactively. To the best of our knowledge, this is the first work to investigate the effectiveness of a single NMT model with synchronous bidirectional decoding.</p><p>• Extensive experiments on NIST Chinese-English, WMT14 English-German and WMT18 Russian-English translation tasks demonstrate that our SB-NMT model obtains significant improvements over the strong Transformer model by 3.92, 1.49 and 1.04 BLEU points respectively. In particular, our approach separately establishes the stateof-the-art BLEU score of 51.11 and 29.21 on Chinese-English and English-German translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this paper, we build our model based on the powerful Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> with an encoder-decoder framework, where the encoder network first transforms an input sequence of symbols x = (x 1 , x 2 , ..., x n ) to a sequence of continues representations z = (z 1 , z 2 , ..., z n ), from which the decoder generates an output sequence y = (y 1 , y 2 , ..., y m ) one element at a time. Particularly, relying entirely on the multi-head attention mechanism, the Transformer with beam search algorithm achieves the state-of-the-art results for machine translation.</p><p>Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions. It op-  erates on queries Q, keys K, and values V . For multi-head intra-attention of encoder or decoder, all of Q, K, V are the output hidden state matrices of the previous layer. For multi-head interattention of the decoder, Q are the hidden states of the previous decoder layer, and K-V pairs come from the output (z 1 , z 2 , ..., z n ) of the encoder. Formally, multi-head attention first obtains h different representations of (Q i , K i , V i ). Specifically, for each attention head i, we project the hidden state matrix into distinct query, key and value representations</p><formula xml:id="formula_0">Q i =QW Q i , K i =KW K i , V i =V W V</formula><p>i respectively. Then we perform scaled dot-product attention for each representation, concatenate the results, and project the concatenation with a feed-forward layer.</p><formula xml:id="formula_1">MultiHead(Q, K, V ) = Concat i (head i )W O head i = Attention(QW Q i , KW K i , V W V i ) (1) where W Q i , W K i , W V i and W O are parameter pro- jection matrices .</formula><p>Scaled Dot-Product Attention can be described as mapping a query and a set of key-value pairs to an output. Specifically, we can then multiply query Q i by key K i to obtain an attention weight matrix, which is then multiplied by value V i for each token to obtain the self-attention token representation. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, scaled dotproduct attention operates on a query Q, a key K, and a value V as:</p><formula xml:id="formula_2">Attention(Q, K, V ) = Softmax( QK T √ d k )V (2)</formula><p>where d k is the dimension of the key. For the sake of brevity, we refer the reader to <ref type="bibr" target="#b28">Vaswani et al. (2017)</ref> for more details. Standard Beam Search Given the trained model and input sentence x, we usually employ beam search or greedy search (beam size = 1) to find the best translation y = argmax y P (y|x). Beam size N is used to control the search space by extending only the top-N hypotheses in the current stack. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the blocks represent the four best token expansions of the previous states, and these token expansions are sorted topto-bottom from most-probable to least-probable. We define a complete hypothesis as a hypothesis which outputs EOS, where EOS is a special target token indicating the end of sentence. With the above settings, the translation y is generated token by token from left to right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In this section, we will introduce the approach of synchronous bidirectional NMT. Our goal is to design a synchronous bidirectional beam search algorithm ( §3.1) which generates tokens with both L2R and R2L decoding simultaneously and interactively using a single model. The central module is the synchronous bidirectional attention (SBAtt, see §3.2). By using SBAtt, the two decoding directions in one beam-search process can help and interact with each other, and can make full use of the target-side history and future information during translation. Then, we apply our proposed SBAtt to replace the multi-head intra-attention in the decoder part of Transformer model ( §3.3), and the model is trained end-to-end by maximum likelihood using stochastic gradient descent ( §3.4).   <ref type="figure">Figure 4</ref> illustrates the synchronous bidirectional beam-search process with beam size 4. With two special start tokens which are optimized during the training process, we let half of the beam to keep decoding from left to right guided by the label l2r , and allow the other half beam to decode from right to left indicated by the label r2l . More importantly, via the proposed SBAtt ( §3.2) model, L2R (R2L) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by R2L (L2R) decoding. Note that (1) at each time step, we choose best items of the half beam from L2R decoding and best items of the half beam from R2L decoding to continue expanding simultaneously; (2) L2R and R2L beams should be thought of as parallel, with SBAtt computed between items of 1-best L2R and R2L, items of 2-best L2R and R2L, and so on 2 ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synchronous Bidirectional Beam Search</head><p>(3) the black blocks denote the ongoing expansion of the hypotheses and decoding terminates when the end-of-sentence flag EOS is predicted; (4) in our decoding algorithm, the complete hypotheses will not participate in subsequent SBAtt, and the L2R hypothesis attended by R2L decoding may change at different time steps, while the ongoing partial hypotheses in both directions of SBAtt always share the same length; (5) finally, we output the translation result with highest probability from all complete hypotheses. Intuitively, our model is able to choose from L2R output or R2L output as final hypothesis according to their model probabilities, and if a R2L hypothesis wins, we reverse the tokens before presenting it.</p><formula xml:id="formula_3">Matmul Scale Mask Softmax Matmul Matmul Scale Mask Softmax Matmul K V Q K V Q K K V V Fusion H H Synchronous Bidirectional Dot-Product Attention</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synchronous Bidirectional Attention</head><p>Instead of multi-head intra-attention which prevents future information flow in the decoder to preserve the auto-regressive property, we propose a synchronous bidirectional attention (SBAtt) mechanism.</p><p>With the two key modules of synchronous bidirectional dot-product attention ( §3.2.1) and synchronous bidirectional multi-head attention ( §3.2.2), SBAtt is capable of capturing and combining the information generated by L2R and R2L decoding.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Synchronous Bidirectional Dot-Product Attention</head><formula xml:id="formula_4">[ − → Q ; ← − Q ]), keys ([ − → K ; ← − K ]) and values ([ − → V ; ← − V ]) which</formula><formula xml:id="formula_5">− → H history = Attention( − → Q , − → K , − → V ) − → H f uture = Attention( − → Q , ← − K , ← − V ) − → H = Fusion( − → H history , − → H f uture )<label>(3)</label></formula><p>where − → H history is obtained by using conventional scaled dot-product attention as introduced in Equation 2, and its purpose is to take advantage of previously generated tokens, namely history information. We calculate − → H f uture using forward query ( − → Q ) and backward key-value pairs (</p><formula xml:id="formula_6">← − K , ← − V )</formula><p>, which attempts at making use of future information from R2L decoding as effectively as possible in order to help predict the current token in L2R decoding. The role of Fusion(·) (green block in <ref type="figure" target="#fig_4">Figure 5</ref>) is to combine − → H history and − → H f uture by using linear interpolation, nonlinear interpolation or gate mechanism.</p><p>Linear Interpolation − → H history and − → H f uture have different importance to prediction of current word. Linear interpolation of − → H history and − → H f uture produces an overall hidden state:</p><formula xml:id="formula_7">− → H = − → H history + λ * − → H f uture<label>(4)</label></formula><p>where λ is a hyper-parameter decided by the performance on development set. 3 Nonlinear Interpolation − → H is equal to − → H history in the conventional attention mechanism, and − → H f uture means the attention information between current hidden state and generated hidden states of the other decoding. In order to distinguish two different information sources, we present a nonlinear interpolation by adding an activation function to the backward hidden states:</p><formula xml:id="formula_8">− → H = − → H history + λ * AF ( − → H f uture )<label>(5)</label></formula><p>where AF denotes activation function, such as tanh or relu. Gate Mechanism We also propose a gate mechanism to dynamically control the amount of information flow from the forward and backward contexts. Specially, we apply a feed-forward gating layer upon − → H history as well as − → H f uture to enrich the non-linear expressiveness of our model:</p><formula xml:id="formula_9">r t , z t = σ(W g [ − → H history ; − → H f uture ]) − → H = r t − → H history + z t − → H f uture (6)</formula><p>where denotes element-wise multiplication. Via this gating layer, it is able to control how much past information can be preserved from previous context and how much reversed information can be captured from backward hidden states. Similar to the calculation of forward hidden states − → H i , the backward hidden states ← − H i can be computed as follows.</p><formula xml:id="formula_10">← − H history = Attention( ← − Q , ← − K , ← − V ) ← − H f uture = Attention( ← − Q , − → K , − → V ) ← − H = Fusion( ← − H history , ← − H f uture )<label>(7)</label></formula><p>where Fusion(·) is the same as introduced in Equation 4-6. Note that − → H and ← − H can be calculated in parallel. We refer to the whole procedure formulated in Equation 3 and Equation 7 as SBDPA(·).</p><formula xml:id="formula_11">[ − → H ; ← − H ] = SBDPA([ ← − Q ; − → Q ], [ ← − K ; − → K ], [ ← − V ; − → V ])<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Synchronous Bidirectional Multi-Head Attention</head><p>Multi-head attention consists of h attention heads, each of which learns a distinct attention function to attend to all of the tokens in the sequence, where mask is used for preventing leftward information flow in decoder. Compared to the multi-head attention, our inputs are the concatenation of forward and backward hidden states. We extend standard multi-headed attention by letting each head attend to both forward and backward hidden states, combined via SBDPA(·).</p><formula xml:id="formula_12">MultiHead([ ← − Q ; − → Q ], [ ← − K ; − → K ], [ ← − V ; − → V ]) = Concat([ − → H 1 ; ← − H 1 ], ..., [ − → H h ; ← − H h ])W O<label>(9)</label></formula><p>and</p><formula xml:id="formula_13">[ − → H i ; ← − H i ]</formula><p>can be computed as follows, which is the biggest difference from conventional multihead attention. where W Q i , W K i , W V i and W O are parameter projection matrices, which are the same as standard multi-head attention introduced in Equation 1.</p><formula xml:id="formula_14">[ − → H i ; ← − H i ] = SBDPA([ ← − Q ; − → Q ]W Q i , [ ← − K ; − → K ]W K i , [ ← − V ; − → V ]W V i )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integrating Synchronous Bidirectional Attention into NMT</head><p>We apply our synchronous bidirectional attention to replace the multi-head intra-attention in the decoder, as illustrated in <ref type="figure" target="#fig_6">Figure 6</ref>. The neural encoder of our model is identical to that of the standard Transformer model. From the source tokens, learned embeddings are generated which are then modified by an additive positional encoding. The encoded word embeddings are then used as input to the encoder which consists of N blocks each containing two layers: (1) a multi-head attention layer (MHAtt), and (2) a position-wise feedforward layer (FFN).</p><p>The bidirectional decoder of our model is extended from the standard Transformer decoder. For each layer in the bidirectional decoder, the lowest sub-layer is our proposed synchronous bidirectional attention network, and it also uses residual connections around each of the sublayers, followed by layer normalization. s l d = LayerNorm(s l−1 + SBAtt(s l−1 , s l−1 , s l−1 )) (11) where l denotes layer depth, subscript d means the decoder-informed intra-attention representation. SBAtt is our proposed synchronous bidirectional attention, and s l−1 is equal to</p><formula xml:id="formula_15">[ − → s l−1 ; ← − s l−1 ]</formula><p>containing forward and backward hidden states.</p><p>In addition, the decoder stacks another two sublayers to seek translation-relevant source semantics to bridge the gap between the source and target language:</p><formula xml:id="formula_16">s l e = LayerNorm(s l d + MHAtt(s l d , h N , h N )) s l = LayerNorm(s l e + FFN(s l e ))<label>(12)</label></formula><p>where MHAtt denotes the multi-head attention introduced in Equation 1, and we use e to denote the encoder-informed inter-attention representation. h N is the source top layer hidden state, and FFN means feed-forward networks.</p><p>Finally, we use a linear transformation and softmax activation to compute the probability of the next tokens based on s N = [ − → s N ; ← − s N ], namely the final hidden states of forward and backward decoding.</p><formula xml:id="formula_17">p( − → y j | − → y &lt;j , ← − y &lt;j , x, θ) = Softmax( − → s N W ) p( ← − y j | ← − y &lt;j , − → y &lt;j , x, θ) = Softmax( ← − s N W )<label>(13)</label></formula><p>where θ is shared weight for L2R and R2L decoding and W is the weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>We design a simple yet effective strategy to enable synchronous bidirectional translation within a decoder. We separately add the special labels ( l2r and r2l ) at the beginning of target sentence ( − → y and ← − y ) to guide translating from left to right or right to left. Given a set of training examples {x (z) , y (z) } Z z=1 , the training algorithm aims to find the model parameters that maximize the likelihood of the training data:</p><formula xml:id="formula_18">J(θ) = 1 Z Z z=1 M j=1 {log p( − → y (z) j | − → y (z) &lt;j , ← − y (z) &lt;j , x (z) , θ) + log p( ← − y (z) j | ← − y (z) &lt;j , − → y (z) &lt;j , x (z) , θ)}<label>(14)</label></formula><p>Similar to asynchronous bidirectional decoding  and bidirectional language models in BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>, the proposed SB-NMT model also faces the same training problem that the bidirectional decoding would allow the words (the second half of the decoding sequence) to indirectly "see themselves" from the other decoding direction. To ensure consistency between model training and testing, we construct pseudo references ← − y p ( − → y p ) for gold − → y g ( ← − y g ).</p><p>More specifically, we first train a L2R model using (x, − → y g ) and a R2L model using (x, ← − y g ). Then we use the two models to translate source sentences x into pseudo target sentences − → y p and ← − y p respectively. Finally, we get two triples (x, − → y p , ← − y g ) and (x, − → y g , ← − y p ) as our training data.</p><p>Once the proposed model is trained, we employ the bidirectional beam search algorithm to predict the target sequence, as illustrated in <ref type="figure">Figure 4</ref>. Compared to previous work that usually adopt a two-phase scheme to translate input sentences <ref type="bibr" target="#b19">Sennrich et al., 2017;</ref>, our decoding approach is more compact and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed model on three translation datasets with different size, including NIST Chinese-English, WMT14 English-German and WMT18 Russian-English translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>For Chinese-English, our training data includes about 2.0 million sentence pairs extracted from the LDC corpus. <ref type="bibr">4</ref> We use NIST 2002 (MT02) Chinese-English dataset as the validation set, NIST 2003-2006 (MT03-06) as our test sets. We use BPE <ref type="bibr" target="#b21">(Sennrich et al., 2016b)</ref> to encode Chinese and English respectively. We learn 30K merge operations and limit the source and target vocabularies to the most frequent 30K tokens.</p><p>For English-German translation, the training set consists of about 4.5 million bilingual sentence pairs from WMT 2014. <ref type="bibr">5</ref> We use newstest2013 4 The corpora includes LDC2000T50, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17 and LDC2004T07. Following previous work, we also using case-insensitive tokenized BLEU to evaluate Chinese-English which have been segmented by Stanford word segmentation and Moses Tokenizer respectively. 5 http://www.statmt.org/wmt14/translation-task.html. All preprocessed dataset and vocab can be directly download in  as the validation set and newstest2014 as the test set. Sentences are encoded using BPE, which has a shared vocabulary of about 37000 tokens. To evaluate the models, we compute the BLEU metric <ref type="bibr" target="#b18">(Papineni et al., 2002)</ref> on tokenized, true-case output. 6 For Russian-English translation, we use the following resources from the WMT parallel data 7 : ParaCrawl corpus, Common Crawl corpus, News Commentary v13 and Yandex Corpus. We do not use Wiki Headlines and UN Parallel Corpus V1.0. The training corpus consists of 14M sentence pairs. We emply the Moses Tokenizer 8 for precocessing. For subword segmentation, we use 50000 joint BPE operations and choose the most frequent 52000 tokens as vocabularies. We use newstest2017 as the development set and the newtest2018 as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setting</head><p>We build the described models by modifying the tensor2tensor 9 toolkit for training and evaluating. For our bidirectional Transformer model, we employ the Adam optimizer with β 1 =0.9, β 2 =0.998, and =10 −9 . We use the same warmup and decay strategy for learning rate as <ref type="bibr" target="#b28">Vaswani et al. (2017)</ref>, with 16,000 warmup steps. During training, we employ label smoothing of value ls =0.1. For evaluation, we use beam search with a beam size of k=4 (For SB-NMT, we use two L2R and R2L hypotheses respectively.) and length penalty α=0.6. Additionally, we use 6 encoder and decoder layers, hidden size d model =1024, 16 attention-heads, 4096 feed forward inner-layer dimensions, and P dropout =0.1. Our settings are close to trans-tensor2tensor website https://drive.google.com/ open?id=0B_bZck-ksdkpM25jRUN2X2UxMm8. <ref type="bibr">6</ref> This procedure is used in the literature to which we compare <ref type="bibr" target="#b7">Gehring et al., 2017;</ref><ref type="bibr" target="#b28">Vaswani et al., 2017</ref>  former_big setting as defined in <ref type="bibr" target="#b28">Vaswani et al. (2017)</ref>. We employ three Titan Xp GPUs to train English-German and Russian-English translation, and one GPU for Chinese-English translation pairs. In addition, we use a single model obtained by averaging the last 20 checkpoints for English-German and Russian-English and do not perform checkpoint averaging for Chinese-English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare the proposed model against the following state-of-the-art SMT and NMT systems 10 :</p><p>• Moses: an open source phrase-based SMT system with default configuration and a 4gram language model trained on the target portion of training data.</p><p>• RNMT <ref type="bibr" target="#b15">(Luong et al., 2015)</ref>: it is a state-ofthe-art RNN-based NMT system with default setting.</p><p>• Transformer: it has obtained the state-ofthe-art performance on machine translation, which predicts target sentence from left to right relying on self-attention <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>.</p><p>• Transformer (R2L): it is a variant of Transformer that generates translation in a right-toleft direction.</p><p>• Rerank-NMT: Via exploring the agreement on left-to-right and right-to-left NMT models, <ref type="bibr" target="#b20">Sennrich et al., 2016a)</ref> first run beam search for forward and reverse models independently to obtain two k-best lists, and then re-score the union of two kbest lists (k=10 in our experiments) using the joint model (adding logprobs) to find the best candidate.</p><p>• ABD-NMT: it is an asynchronous bidirectional decoding for NMT, which equipped the conventional attentional encoder-decoder NMT model with a backward decoder . ABD-NMT adopts a two-phrase decoding scheme: (1) use backward decoder to generate reverse sequence states;</p><p>(2) perform beam search on the forward decoder to find the best translation based on encoder hidden states and backward sequence states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Chinese-English Translation</head><p>Effect of Fusion Mechanism We first investigate the impact of different fusion mechanisms with different λs on the development set. As shown in <ref type="table" target="#tab_6">Table 2</ref>, we find that linear interpolation is sensitive to parameters λ. Nonlinear interpolation, which is more robust than linear interpolation, achieves the best performance when we use tanh with λ=0.1. Compared to gate mechanism, nonlinear interpolation is much simpler and needs less parameters. Therefore, we will use nonlinear interpolation with tanh and λ=0.1 for all experiments thereafter. Translation Quality <ref type="table" target="#tab_8">Table 3</ref> shows translation performance for Chinese-English. Specifically, the proposed model significantly outperforms Moses, RNMT, Transformer, Transformer (R2L), <ref type="bibr">8.54,</ref><ref type="bibr">3.92,</ref><ref type="bibr">4.90,</ref><ref type="bibr">2.91,</ref><ref type="bibr">2</ref>.82 BLEU points, respectively. Compared to Transformer and Transformer (R2L), our model exhibits much better performance. These results confirm our hypothesis that the two directions are mutually beneficial in bidirectional decoding. Furthermore, compared Model TEST GNMT ‡  24.61 Conv ‡ <ref type="bibr" target="#b7">(Gehring et al., 2017)</ref> 25.16 AttIsAll ‡ <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>    to Rerank-NMT in which two decoders are relatively independent and ABD-NMT where only the forward decoder can rely on a backward decoder, our proposed model achieves substantial improvements over them on all test sets, which indicates that joint modeling and optimizing with left-toright and right-to-left decoding behaves better in leveraging bidirectional decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on English-German Translation</head><p>We further demonstrate the effectiveness of our model in WMT14 English-German translation tasks, and we also display the performances of some competitive models including GNMT , Conv <ref type="bibr" target="#b7">(Gehring et al., 2017)</ref>, and At-tIsAll <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>. As shown in Table 4, our model also significantly outperforms others and gets an improvement of 1.49 BLEU points than a strong Transformer model. Moreover, our SB-NMT model establishes a state-ofthe-art BLEU score of 29.21 on the WMT14 English-German translation task. <ref type="bibr">11</ref> The BLEU scores for Transformer model are our reproduced results. Similar to footnote 7 in , our performance is slightly lower than those reported in <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>. Additionally, we only use 3 GPUs for English-German, whereas most papers employ 8 GPUs for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Param Speed Train Test   <ref type="table" target="#tab_11">Table 5</ref> shows the results of large-scale WMT18 Russian-English translation, and our approach still significantly outperforms the state-of-the-art Transformer model in development and test sets by 1.10 and 1.04 BLEU points respectively. Note that the BLEU score gains of English-German and Russian-English are not as significant as that on Chinese-English. The underlying reasons, which have also been mentioned in <ref type="bibr" target="#b23">Shen et al. (2016)</ref> and , are that (1) the Chinese-English datasets contain four reference translations for each source sentence while the English-German and Russian-English datasets only have single reference;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results on Russian-English Translation</head><p>(2) English is more distantly related to Chinese than German and Russian, leading to the predominant improvements for Chinese-English translation when leveraging bidirectional decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis</head><p>We conduct analyses on Chinese-English translation, to better understand our model from different perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters and Speeds</head><p>In contrast to the standard Transformer, our model does not increase any parameters except for a hyper-parameter λ, as shown in <ref type="table" target="#tab_13">Table 6</ref>. Rerank-NMT needs to train two sets of NMT models, so its parameters are doubled. The parameters of ABD-NMT are 333.8M since it has two decoders containing a backward decoder and a forward decoder. Hence, our model is more compact because it only has a single encoder-decoder NMT model.</p><p>We also show the training and testing speed of our model and baselines in  ing steps per second, which is faster than Rerank-NMT and ABD-NMT. When it comes to decoding procedure, the decoding speed of our model is 17.87 sentences per second with batch size 50, which is two or three times faster than Rerank-NMT and ABD-NMT.</p><p>Effect of Unbalanced Outputs According to <ref type="table" target="#tab_1">Table 1</ref>, L2R usually does well on predicting the left-side tokens of target sequences, while R2L usually performs well on the right-side tokens. Our central idea is combine the advantage of leftto-right and right-to-left modes. To test our hypothesis, we further analyze the translation accuracy of Rerank-NMT, ABD-NMT, and our model, as shown in <ref type="figure" target="#fig_7">Figure 7</ref>. Rerank-NMT and ABD-NMT can alleviate the unbalanced output problem, but fail to improve prefix and suffix accuracies at the same time. The experimental results demonstrate that our model can balance the outputs, and gets the best translation accuracy for both the first 4 words and the last 4 words. Note that our model chooses from L2R output or R2L output as final results according to their model probabilities, and the left-to-right decoding contributes 58.6% on test set.</p><p>Effect of Varying Beam Size We observe that beam search decoding only improves translation quality for narrow beams and degrades translation quality when exposed to a larger search space for L2R and R2L decoding as illustrated in <ref type="figure">Figure 8</ref>. Additionally, the gap between greedy search and beam search is significant and can be up to about 1-2 BLEU points. <ref type="bibr" target="#b10">Koehn and Knowles (2017)</ref> also demonstrate these phenomena in eight translation directions.</p><p>As for our SB-NMT model, we investigate the effect of different beam sizes k, as shown by the red line of <ref type="figure">Figure 8</ref>. Compared to conventional beam search, where worse translations are found beyond an optimal beam size setting (e.g., in the range of 4-32), the translation quality of our proposed model remains stable as beam size becomes larger. We attribute this to the ability of the combined objective to model both history and future translation information.</p><p>Effect of Long Sentences A well-known flaw of NMT models is the inability to properly translate long sentences. We follow <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> to group sentences of similar lengths together and compute a BLEU score per group (left picture). <ref type="figure">Figure 9</ref> shows the BLEU score and the averaged length of translations for each group (right picture). Transformer and Transformer (R2L) perform very well on short source sentences, but degrade on long source sentences. Our model can alleviate this problem by taking advantage of both history and future information. In fact, incorporating synchronous bidirectional attention boosts translation performance on all source sentence groups.</p><p>Comparison to Data-Enhanced NMT In the training setup, we have obtained pseudo L2R and R2L references ( − → y p and ← − y p ) by using L2R and R2L models respectively. Here, we first compare our proposed model with NMT enhanced by pseudo data, and further explore the data utilization of SB-NMT by using combined data strategy <ref type="bibr">[0,10) [10,20) [20,30) [30,40) [40,50) [50,60) [60,70)</ref> Length of Source Sentence [0,10) <ref type="bibr">[10,20) [20,30) [30,40) [40,50) [50,60) [60,70)</ref> Length of Source Sentence  (six triples data, that is, ( − → y g , ← − y p ), (reversed ← − y p , ← − y g ), ( − → y p , ← − y g ), ( − → y g , reversed − → y p ), ( − → y p , ← − y p ), and (reversed ← − y p , reversed − → y p )). As shown in <ref type="table" target="#tab_18">Table 7</ref>, we find that data-enhanced Transformer outperforms the original Transformer, but still behaves worse than our proposed model. Furthermore, by making full use of training data, our model (six triple data) significantly improves the translation quality by 1.03 BLEU points than the original set (two triples data). Subjective Evaluation We follow <ref type="bibr" target="#b27">Tu et al. (2016)</ref> to conduct a subjective evaluation to validate the benefit of the synchronous bidirectional decoder, as shown in <ref type="table" target="#tab_19">Table 8</ref>. Four human evaluators are asked to evaluate the translations of 100 source sentences, which are randomly sampled from the test sets without knowing which system the translation is selected from. These 100 source sentences have 2712 words. We evaluate over-or under-translation based on the number of source words which are dropped or repeated in translation 13 , though we use subword <ref type="bibr">(Sennrich 13</ref> For our SB-NMT model, 2 source words are overtranslated and 147 source words are under-translated. Additionally, it is interesting to combine with better scoring meth-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Over</p><formula xml:id="formula_19">-Trans Under-Trans Ratio ∆ Ratio ∆ L2R 0.07% - 7.85% - R2L</formula><p>0.14% -7.81% -Ours 0.07% -0.00% 5.42% -30.6%  <ref type="table" target="#tab_19">Table 8</ref>. Case Study <ref type="table" target="#tab_20">Table 9</ref> gives three examples to show the translations of different models, in order to better understand how our model outperforms others. We find that Transformer produces translations with good prefixes (red line or dotted line), while Transformer (R2L) generates translations with better suffixes (blue line or ::::: wave :::: line). Therefore, they are often unable to translate the whole sentence precisely. In contrast, the proposed approach can make full use of bidirectional decoding and remedy the errors in these cases. ods and stopping criteria  to strengthen the baseline and our model in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>捷克 总统 哈维 卸任 :: 新 ::::: 总统 ::: 仍 ::: 未 ::::: 确定 Reference czech president havel steps down while new president still not chosen L2R czech president leaves office R2L ::: the :::::::: outgoing ::::::::: president :: of ::: the :::::: czech ::::::: republic :: is :::: still ::::::::: uncertain Ours czech president havel leaves office , ::: new ::::::::: president ::: yet :: to ::: be ::::::::::: determined Source 他们 正在 研制 一 种 超大型 的 :::: 叫做 ::::: 炸弹 ::: 之 :: 母 ::: 。 Reference they are developing a kind of superhuge bomb called the mother of bombs . L2R they are developing a super , big , mother , called the bomb . R2L</p><p>they are working on a much larger mother :::::: called ::: the ::::::: mother :: of : a :::::: bomb : . Ours they are developing a super-large scale , ::::: called ::: the ::::::: mother :: of ::: the :::::: bomb : . sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our research is built upon a sequence-to-sequence model <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, but it is also related to future modeling and bidirectional decoding. We discuss these topics in the following. Future Modeling Standard neural sequence decoders generate target sentences from left to right, and it has been proven to be important to establish the direct information flow between current predicting word and previous generated words <ref type="bibr" target="#b38">(Zhou et al., 2017b;</ref><ref type="bibr" target="#b28">Vaswani et al., 2017)</ref>. However, current methods still fail to estimate some desired information in the future. To address this problem, reinforcement learning methods have been applied to predict future properties <ref type="bibr" target="#b11">(Li et al., 2017;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2017;</ref><ref type="bibr" target="#b8">He et al., 2017)</ref>. <ref type="bibr" target="#b12">Li et al. (2018)</ref> presented a target foresight based attention which uses the POS tag as the partial information of a target foresight word to improve alignment and translation. Inspired by the human cognitive behaviors,  proposed a deliberation network, which leverages the global information by observing both back and forward information in sequence decoding through a deliberation process. <ref type="bibr" target="#b36">Zheng et al. (2018)</ref> introduced two additional recurrent layers to model translated past contents and untranslated future contents. The most relevant models in future modeling are twin networks <ref type="bibr" target="#b22">(Serdyuk et al., 2018)</ref>, which encourage the hidden state of the forward network to be close to that of the backward network used to predict the same token. However, they still used two decoders and the backward network contributes nothing during inference. Along the direction of future modeling, we introduce a single synchronous bidirectional decoder, where forward decoding can be used as future information for backward decoding, and vice versa.</p><p>Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation <ref type="bibr" target="#b29">(Watanabe and Sumita, 2002;</ref><ref type="bibr" target="#b6">Finch and Sumita, 2009;</ref><ref type="bibr" target="#b33">Zhang et al., 2013)</ref>. To address the issue of unbalanced outputs,  proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results <ref type="bibr" target="#b20">(Sennrich et al., 2016a;</ref><ref type="bibr" target="#b9">Hoang et al., 2017;</ref><ref type="bibr" target="#b26">Tan et al., 2017;</ref><ref type="bibr" target="#b19">Sennrich et al., 2017;</ref><ref type="bibr" target="#b4">Deng et al., 2018)</ref>. Recently,  proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoderdecoder framework by introducing a backward decoder. Additionally, both <ref type="bibr" target="#b17">Niehues et al. (2016)</ref> and <ref type="bibr" target="#b37">Zhou et al. (2017a)</ref> combined the strengths of NMT and SMT, which can also be used to combine the advantages of bidirectional translation texts . Compared to previous methods, our method has the following advantages: (1) We use a single model to achieve the goal of synchronous left-to-right and right-to-left decoding.</p><p>(2) Our model can leverage and combine the two decoding directions in every layer of the Transformer decoder, which can run in parallel. (3) By using synchronous bidirectional attention, our model is an end-to-end joint framework and can optimize L2R and R2L decoding simultaneously. (4) Compared to two-phase decoding schemes in previous work, our decoder is more compact and faster.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(left) Scaled Dot-Product Attention. (right) Multi-Head Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the standard beam search algorithm with beam size 4. The black blocks denote the ongoing expansion of the hypotheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Synchronous bidirectional attention model based on scaled dot-product attention. It operates on forward (L2R) and backward (R2L) queries Q, keys K, values V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>shows our particular attention "Synchronous Bidirectional Dot-Product Attention (SBDPA)". The input consists of queries (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The new Transformer architecture with the proposed synchronous bidirectional multi-head attention network, namely SBAtt. The input of decoder is concatenation of forward (L2R) sequence and backward (R2L) sequence. Note that all bidirectional information flow in decoder runs in parallel and only interacts in synchronous bidirectional attention layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Translation accuracy of the first and last 4 tokens for Transformer, Transformer (R2L), Rerank-NMT, ABD-NMT and our proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Translation accuracy of the first 4 tokens and last 4 tokens in NIST Chinese-English translation tasks. L2R denotes left-to-right decoding and R2L means right-to-left decoding for conventional NMT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Experiment results on the development set using different fusion mechanism with different λs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Moses 37.85 37.47 41.20 36.41 36.03 37.78 -9.41 RNMT 42.43 42.43 44.56 41.94 40.95 42.47 -4.72 Transformer 48.12 47.63 48.32 47.51 45.31 47.19 -Transformer (R2L) 47.81 46.79 47.01 46.50 44.13 46.11 -1.08 Rerank-NMT 49.18 48.23 48.91 48.73 46.51 48.10 +0.91</figDesc><table><row><cell>Model</cell><cell>DEV MT03 MT04 M05 MT06 AVE</cell><cell>∆</cell></row><row><cell>ABD-NMT</cell><cell cols="2">48.28 49.47 48.01 48.19 47.09 48.19 +1.00</cell></row><row><cell>Our Model</cell><cell cols="2">50.99 51.87 51.50 51.23 49.83 51.11 +3.92</cell></row></table><note>).7 http://www.statmt.org/wmt18/translation-task.html.8 https://github.com/moses-smt/mosesdecoder/blob/mast- er/scripts/tokenizer/tokenizer.perl.9 https://github.com/tensorflow/tensor2tensor.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of translation quality for Chinese-English translation tasks using case-insensitive BLEU scores. All results of our model are significantly better than Transformer and Transformer (R2L) (p &lt; 0.01).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Results of WMT14 English-German transla-</cell></row><row><cell cols="2">tion using case-sensitive BLEU. Results with  ‡ mark</cell></row><row><cell cols="2">are taken from the corresponding papers.</cell></row><row><cell>Model</cell><cell>DEV TEST</cell></row><row><cell>Transformer</cell><cell>35.28 31.02</cell></row><row><cell cols="2">Transformer (R2L) 35.22 30.57</cell></row><row><cell>Our Model</cell><cell>36.38 32.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Results of WMT18 Russian-English transla- tion using case-insensitive tokenized BLEU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Statistics of parameters, training and testing speeds. Train denotes the number of global training steps processed per second at the same batch-size sentences; Test indicates the amount of translated sentences in one second.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 .</head><label>6</label><figDesc>During training, our model performs approximately 1.26 train-</figDesc><table><row><cell></cell><cell>45</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transformer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transformer(R2L)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rerank-NMT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ABD-NMT</cell></row><row><cell>Translation Accuracy</cell><cell>35 40</cell><cell>40.21 35.67 38.98 38.36 40.89</cell><cell>35.10</cell><cell>39.47 38.91 38.11 40.08 Our Model</cell></row><row><cell></cell><cell>30</cell><cell>The first 4 tokens</cell><cell cols="2">The last 4 tokens</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 7 :</head><label>7</label><figDesc>Chinese-English BLEU scores of standard Transformer enhanced with pseudo data, and our SB-NMT model with combined data strategy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 8 :</head><label>8</label><figDesc>Subjective evaluation on over-translation and under-translation for Chinese-English. Ratio denotes the percentage of source words which are over-or under-translated, ∆ indicates relative improvement.</figDesc><table><row><cell>et al., 2016b) in training and inference. Trans-</cell></row><row><cell>former and Transformer (R2L) suffer from serious</cell></row><row><cell>under-translation problems with 7.85% and 7.81%</cell></row><row><cell>errors. Our proposed model alleviates the under-</cell></row><row><cell>translation problems by exploiting the combina-</cell></row><row><cell>tion of left-to-right and right-to-left decoding di-</cell></row><row><cell>rections, reducing 30.6% of under-translation er-</cell></row><row><cell>rors. It should be emphasized that the proposed</cell></row><row><cell>model is especially effective for alleviating under-</cell></row><row><cell>translation problem, which is a more serious trans-</cell></row><row><cell>lation problem for Transformer systems as seen in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9 :</head><label>9</label><figDesc>Chinese-English translation examples of Transformer decoding in left-to-right and right-to-left way, and our proposed models. L2R performs well in the first half sentence, whereas R2L translates well in :: the :::::: second :::: half</figDesc><table><row><cell>:::::::</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also did experiments that all of L2R hypotheses attend to the 1-best R2L hypothesis, and all the R2L hypotheses attend to the 1-bset L2R hypothesis. The results of the two schemes are similar. For the sake of simplicity, we employed the previous scheme.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that we can also set λ to be a vector and learn λ during training with standard back-propagation, and we remain it as future exploration.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">For fair comparison, Rerank-NMT and ABD-NMT are based on strong Transformer models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">For greedy search in SB-NMT, it has one item L2R decoding and one item R2L decoding. In other words, its beam size is equal to 2 compared to conventional beam search decoding.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers as well as the Action Editor, George Foster, for insightful comments and suggestions. The research work has been funded by the Natural Science Foundation of China under Grant No. 61673380. This work is also supported by grants from NVIDIA NVAIL program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we propose a synchronous bidirectional NMT model that performs bidirectional decoding simultaneously and interactively. The bidirectional decoder, which can take full advantage of both history and future information provided by bidirectional decoding states, predicts its outputs using left-to-right and right-to-left directions at the same time. To the best of our knowledge, this is the first attempt to integrate synchronous bidirectional attention into a single NMT model. Extensive experiments demonstrate the effectiveness of our proposed model. Particularly, our model respectively establishes state-of-the-art BLEU scores of 51.11 and 29.21 on NIST Chinese-English and WMT14 English-German translation tasks. In future work, we plan to apply this framework to other tasks, such as sequence labeling, abstractive summarization and image captioning. Additionally, it is interesting to reduce the training cost by adding noise in the target sentence and using fine-tune technology.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th</title>
		<meeting>the 56th</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Alibaba&apos;s neural machine translation systems for wmt18</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanbo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenglan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guchun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="368" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1124" to="1132" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoding with value networks for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards decoding as continuous optimisation in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="156" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to decode for future success</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06549</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Target foresight based attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1380" to="1390" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Agreement on targetbidirectional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="411" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparable study on model averaging, ensembling and reranking in nmt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Baskaran Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1096</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pre-translation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The university of edinburgh&apos;s neural mt systems for wmt17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4739</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="389" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2323</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Twin networks: Matching the future for sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Chris Pal, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1159</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th</title>
		<meeting>the 54th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xmu neural machine translation systems for wmt 17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="400" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional decoding for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<editor>Greg Corrado, Macduff Hughes, and Jeffrey Dean</editor>
		<meeting><address><addrLine>Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1784" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Breaking the beam search curse: A study of (re-)scoring methods and stopping criteria for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3054" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beyond left-to-right: Multiple decomposition structures for smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep neural networks in machine translation: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.1109/MIS.2015.69</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="25" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asynchronous bidirectional decoding for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongji</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling past and future for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural system combination for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2060</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="378" to="384" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Look-ahead attention for generation in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="211" to="223" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
