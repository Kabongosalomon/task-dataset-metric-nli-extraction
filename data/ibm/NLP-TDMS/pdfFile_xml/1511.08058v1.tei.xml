<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Xi&apos;an 710119</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The discrimination and simplicity of features are very important for effective and efficient pedestrian detection. However, most state-of-the-art methods are unable to achieve good tradeoff between accuracy and efficiency. Inspired by some simple inherent attributes of pedestrians (i.e., appearance constancy and shape symmetry), we propose two new types of non-neighboring features (NNF): side-inner difference features (SIDF) and symmetrical similarity features (SSF). SIDF can characterize the difference between the background and pedestrian and the difference between the pedestrian contour and its inner part. SSF can capture the symmetrical similarity of pedestrian shape. However, it's difficult for neighboring features to have such above characterization abilities. Finally, we propose to combine both non-neighboring and neighboring features for pedestrian detection. It's found that nonneighboring features can further decrease the average miss rate by 4.44%. Experimental results on INRIA and Caltech pedestrian datasets demonstrate the effectiveness and efficiency of the proposed method. Compared to the state-ofthe-art methods without using CNN, our method achieves the best detection performance on Caltech, outperforming the second best method (i.e., Checkboards) by 1.63%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection is a premise in many computer vision tasks including gait recognition, behavior analysis, action recognition, and camera-based driver assistance. Generally speaking, the performance of pedestrian detection is determined by the performance of feature extraction and classification. This paper focuses on feature extraction.</p><p>There are three manners for feature extraction: (1) completely Hand-Crafted (HC) features, <ref type="bibr" target="#b0">(2)</ref> Hand-Crafted candidate features followed by Learning Algorithms (HCLA), and (3) Deep Leaning (DL) based features. Due to simplicity and robustness, it is much more possible for HCLA to achieve good tradeoff between efficiency and accuracy. So this paper concentrates on HCLA.</p><p>Usually, the input of HCLA for pedestrian detection is CIE-LUV color channels, gradient histogram channels, gradient magnitude channel, and so on. Once the channels are specified, the question remained is how to generate candidate features from the channels. Most of the state-of-the-art methods generate the candidate features by using local (e.g., local mean features) or neighboring features (e.g., haar features). In fact, some inherent attributes of pedestrians can also be used for feature design. Inspired by appearance constancy and shape symmetry of pedestrians, we design two types of non-neighboring features for pedestrian detection: side-inner difference features (SIDF) and symmetrical similarity features (SSF). The contributions of the paper are as follows:</p><p>1) Appearance constancy and shape symmetry can be seen as the inherent attributes of pedestrians. Inspired by these attributes, we propose side-inner difference features (SIDF) and symmetrical similarity features (SSF), respectively. Compared to some state-of-the-art features, our features are oriented non-neighboring features. SIDF can characterize the difference between the background and pedestrian and the difference between the pedestrian contour and its inner part. SSF can capture the symmetrical similarity of pedestrian shape. However, it's difficult for neighboring features to have such above characterization abilities.</p><p>2) We propose to employ non-neighboring and neighboring features for pedestrian detection. Among all the selected features, about 70% are neighboring features and 30% of them are non-neighboring ones. So the non-neighboring features are complementary to the neighboring ones.</p><p>3) Compared to the state-of-the-art methods without using CNN, we achieve the best detection performance (i.e., <ref type="bibr" target="#b14">16</ref>.84% miss rate on Caltech). Meanwhile, our methods achieve the best performance tradeoff between detection efficiency and log-average miss rate only by common CPU. Moreover, SIDF and SSF may also be combined with CNN features to further boost the detection performance.</p><p>The rest of the paper is organized as follows. We review related work in Section 2. The proposed method is given in Section 3. Experimental results are provided in Section 4. We then conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Pedestrian detection methods can be divided into three families <ref type="bibr" target="#b2">[4]</ref>: DPM (Deformable Part Detectors) variants <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20]</ref>, deep networks <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b21">23]</ref> and decision forests <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b25">27]</ref>. Our method can be categorized into the family of decision forests. Specifically, the process of this kind of methods is as follows: 1) a set of channel images are generated from an input image; 2) then, features are extracted from patches of the channels; and 3) finally, the features are fed into a decision forest learned via AdaBoost <ref type="bibr" target="#b30">[32]</ref>. Feature extraction is a very important step.</p><p>Integral Channel Features (ICF) <ref type="bibr" target="#b8">[10]</ref> is one of the most successful feature extraction method four years after Histograms of Oriented Gradients (HOG) <ref type="bibr" target="#b5">[7]</ref> was proposed. In ICF, Dollar et al. <ref type="bibr" target="#b8">[10]</ref> proposed to combine three types of channels: LUV color channels, normalized gradient magnitude, and histogram of oriented gradients (6 channels). First-order and higher-order features are then generated from the channel images <ref type="bibr" target="#b8">[10]</ref> . Soft cascade <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b28">30]</ref> is then used for learning discriminative features <ref type="bibr" target="#b8">[10]</ref>. Note that ICF is also known as ChnFtrs.</p><p>Aggregated Channel Features (ACF) <ref type="bibr" target="#b6">[8]</ref>, SquaresChn-Ftrs <ref type="bibr" target="#b1">[3]</ref>, InformedHaar <ref type="bibr" target="#b29">[31]</ref>, Locally Decorrelated Channel Features (LDCF) <ref type="bibr" target="#b17">[19]</ref>, and Checkboards <ref type="bibr" target="#b30">[32]</ref> employ the same channel images as ICF. In ACF, the pixel sum of each block in each channel is computed and then the resulting lower resolution channels are smoothed <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9]</ref>. SquaresChn-Ftrs <ref type="bibr" target="#b1">[3]</ref> is simpler than ICF because only the local sum of squares in each channel image is used as features. In-formedHaar <ref type="bibr" target="#b29">[31]</ref> is specifically designed for pedestrian detection where a pool of rectangular templates is tailored to the statistical model of the up-right human body across the channels. By using the technique of Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b13">[15]</ref>, the LDCF features are decorrelated so that they are suited for orthogonal decision trees <ref type="bibr" target="#b17">[19]</ref>. The decorrelation can be achieved by convolution with a filter bank learned by LDA. Checkboards <ref type="bibr" target="#b30">[32]</ref> generalizes ICF by using filter banks to compute features from channel images. Six types of filters are considered: InformedFilters, CheckerboardsFilters, RandomFilters, SquaresChntrs filters, LDCF8 filters, and PcaForeground filters. SpatialPooling+ <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22]</ref> does not take channel images as input. Instead, it applies the operator of spatial pooling (e.g., max-pooling) on covariance descriptor and Local Binary Pattern (LBP).</p><p>According to <ref type="bibr" target="#b2">[4]</ref> and our experimental results, the performance of the above methods can be summarized as follows: On the Caltech pedestrian dataset <ref type="bibr">[1,</ref><ref type="bibr" target="#b9">11]</ref>, the miss rates of the above methods are ICF &gt; ACF &gt; SquaresChnFtrs &gt; InformedHaar &gt; LDCF &gt; SpatialPool-ing+ &gt; Checkboards. Loosely speaking, the detection speeds of these methods are SpatialPooling+ &lt; ICF &lt; SquaresChnFtrs &lt; Checkboards &lt; InformedHaar &lt; LDCF &lt; ACF. It can be concluded that no method can simultaneously obtain the best log-average miss rate and detection speed. That is, these methods are unable to achieve satisfying tradeoff between accuracy and efficiency.</p><p>Recently, the methods based on CNN have achieved very good performance <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b27">29]</ref>. For example, Tian et al. <ref type="bibr" target="#b23">[25]</ref> proposed DeepParts to improve the detection performance by handling occlusion with an extensive part pool. Though the methods based CNN can achieve the best performance, it needs the relatively expensive device (i.e., GPU). On the other hand, the simple feature design can also be complementary to CNN. For example, by combining the simple local features (e.g, ACF <ref type="bibr" target="#b6">[8]</ref>, Checkboards <ref type="bibr" target="#b30">[32]</ref>, and LDCF <ref type="bibr" target="#b17">[19]</ref>) and very deep CNN features (e.g., VGG <ref type="bibr" target="#b22">[24]</ref> and AlexNet <ref type="bibr" target="#b15">[17]</ref>) , Cai et al. <ref type="bibr" target="#b4">[6]</ref> could decrease the miss rate from 18.9% to 11.70%. So in this paper, we focus the feature design in the traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Appearance constancy and shape symmetry</head><p>Most state-of-the-art features for pedestrian detection are designed to describe the local image region. Thus, they don't make full use of the inherent attributes of pedestrians. In fact, some inherent attributes of pedestrians can be used to further improve detection performance. For example, Zhang et al. <ref type="bibr" target="#b29">[31]</ref> incorporate the common sense that pedestrians usually appear up-right into the design of simple and efficient haar-like features. In this paper, we incorporate the appearance constancy and shape symmetry into the feature design. First of all, we give the explanations of appearance constancy and shape symmetry. <ref type="figure">Fig. 1</ref> gives some examples of the cropped pedestrians.</p><p>1) Appearance Constancy. The appearances of pedestrians are usually contrast to the surrounding background. Meanwhile, pedestrians can be seen as three main different parts (i.e., head, upper body, and legs). The appearances inside these parts are usually constancy. For example, the woman wears the sky-blue coat and black pants in <ref type="figure">Fig. 1(a)</ref>. We call this inherent attribute of pedestrians appearance constancy. Thus, the regions located inside the pedestrians (e.g., patches B in Figs. 1(a) and (b)) are contrast to that located in the background (e.g., patches A in Figs. 1(a) and (b). Note that patches A and B lie in the same horizontal. Patches B are called the inner patches, and patches A are called the side patches.</p><p>2) Shape Symmetry. As stated in <ref type="bibr" target="#b29">[31]</ref>, pedestrians usually appear up-right. Thus, the pedestrian shape is loosely symmetrical in the horizontal direction. For example, the symmetrical region (patches A and A ) in the Figs. 1(c) and  <ref type="figure">Figure 1</ref>. Some examples of the cropped pedestrians.</p><formula xml:id="formula_0">B A C C (a) B A C C (b) C C B A (c) C D B A E E (d) Figure 2. Average values of channel images . (a) Inversed L (Lu- minance) channel. (b) U channel. (c) Inversed V channel. (d) G channel.</formula><p>(d) have the similar characteristic. This inherent attribute is called shape symmetry. Inspired by the above appearance constancy and shape symmetry, we can design two types of non-neighboring features. It can be explained by the average appearance of pedestrians in channel images such as L, U, V, and G. <ref type="figure">Fig. 2</ref> gives the average values of above four channel images. Due to the appearance constancy, the pixel values of pedestrians in L, U, and V channel images are similar in the same horizontal, which are different from that of the two-side regions. Meanwhile, the pixel values of the inner part of pedestrians in G channel image are constantly small, and the pixel values of pedestrian contour in G channel image are relatively large. Thus, the large difference in G channel image can be characterized by not only the neighboring feature formed by patches A and B but also the non-neighboring feature formed by patches C and D in <ref type="figure">Fig. 2(d)</ref>. Though there is little difference between the inner part and contour in V channel ( <ref type="figure">Fig. 2(c)</ref>), the difference between the inner part and its two side background can be characterized by the nonneighboring feature formed by patches A and B. Due to shape symmetry, the symmetrical regions of pedestrians in the horizontal direction have the similar characteristic. For example, the symmetrical patches E and E in <ref type="figure">Fig. 2(d</ref>  The discrimination and usefulness of non-neighboring features are graphically supported by <ref type="figure" target="#fig_2">Fig. 3</ref>. In <ref type="figure" target="#fig_2">Fig. 3</ref>, there are two objects (classes) to be classified. We call the object in <ref type="figure" target="#fig_2">Fig. 3</ref>(a) Object 1 and the object in <ref type="figure" target="#fig_2">Fig. 3</ref></p><formula xml:id="formula_1">(d) Ob- ject 2.</formula><p>There is a line in the middle of Object 1 whereas the inner part of Object 2 is flat. In both Figs. 3(b) and (e), two neighboring dashed rectangles form a feature. We can see that this neighboring feature is unable to distinguish between Object 1 and Object 2 because the values of neighboring features in Object 1 (i.e., <ref type="figure" target="#fig_2">Fig. 3</ref>(b)) and Object 2 (i.e., <ref type="figure" target="#fig_2">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Side-inner difference features inspired by appearance constancy</head><p>Inspired by appearance constancy, we design the nonneighboring difference features in the same horizontal. We call this oriented non-neighboring difference features Side-Inner Difference Features (SIDF). <ref type="figure" target="#fig_5">Fig. 4</ref> gives some possible forms of SIDF. <ref type="figure" target="#fig_5">Fig. 4</ref>(a) shows that the distance d(A, B) of non-neighboring patches A and B in SIDF can be different. Theoretically, the distance can be arbitrary. But it results that the number of all possible SIDF is very large. Because a pedestrian is horizontally symmetrical in a loose sense, we restrict the location l(B) of B in the interval of the locations l(A) and l(A ) where A is the horizontal mir-   varies its size. It's good enough for letting A and B have the different width but the same height. Figs. 5(c) and (d) also give an example of the different widths of patches A and B. <ref type="figure" target="#fig_5">Fig. 4(d)</ref> shows SIDF with varying aspect ratio. The size of a patch (e.g., patch A in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>) is allowed to change in a reasonable range. In this paper, the variation of a patch is limited to a maximum square. In other words, the sizes of both patches A and B are allowed to be not larger than that of the maximum square. The green squares in <ref type="figure">Fig. 6</ref> are maximum squares and patches have to be inside them. A typical maximum square is of size 8 × 8 cells (1 cell=2 × 2 pixels).</p><formula xml:id="formula_2">ror of A. That is, l(B) ∈ [l(A), l(A )]. As demonstrated in</formula><formula xml:id="formula_3">B A A (a) B A A (b) B A A (c) B A A (d)</formula><p>Suppose that the side-inner difference feature f (A, B) consists of two patches A and B (see <ref type="figure" target="#fig_4">Fig. 5(a)</ref>). The number of pixels of A and B are denoted by N A and N B . Let S A and S B be the pixel sum in A and B, respectively. Then the side-inner difference feature f (A, B) can be calculated by</p><formula xml:id="formula_4">f (A, B) = S A N A − S B N B ,<label>(1)</label></formula><p>where N A and N B are used for normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Symmetrical similarity features inspired by shape symmetry</head><p>As stated in Section 3.1, the shape of pedestrian is symmetrical. Thus, patches A and A in <ref type="figure" target="#fig_4">Fig. 5</ref> have the similar characteristic. The symmetrical similarity features f (A, A ) of patches A and A can be calculated by the following equation:</p><formula xml:id="formula_5">f (A, A ) = |f A − f A |,<label>(2)</label></formula><p>where f A and f A represent the features of patches A and A (e.g., histogram features and local mean features). For the computational efficiency, we just use the local mean features to represent the patches. Namely, f A = S A /N A and f A = S A /N A . Then, Eq. (2) can be written as the following:</p><formula xml:id="formula_6">f (A, A ) = | S A N A − S A N A |.<label>(3)</label></formula><p>However, due to the changes of the pedestrian posture, the pedestrian symmetry is relatively loose. It results that Eq. (3) is very sensitive to the pedestrian deformation.</p><p>To eliminate the above influence caused by pedestrian deformation, we replace the mean features of patches by the max-pooling features <ref type="bibr" target="#b26">[28]</ref>. In <ref type="figure" target="#fig_7">Fig. 7</ref>, two symmetrical patches A and A are represented by three different color sub-patches, respectively. For examples, patch A consists of three sub-patches A 1 , A 2 , and A 3 . The sub-subpatches are randomly generated inside the patch A. The size and aspect ratio of them can arbitrary, whereas the area of them should be larger than half of patch A. Then, the feature value of patch A is set as the maximum of mean values of three sub-patches. It can be expressed as:  Note that the maximum is replaced by minimum in L and V channel images. Then, the symmetrical symmetry features f (A, A ) of patches A and A is calculated by the following equation:</p><formula xml:id="formula_7">f M (A) = max i=1,2,3 S i N i .<label>(4)</label></formula><formula xml:id="formula_8">f (A, A ) = |f M (A − f M (A ))|.<label>(5)</label></formula><p>The size of the symmetrical patches A and A is allowed to change in a reasonable range, which varies from 6 × 6 cells to 12×12 cells. As the symmetry in pedestrians mainly exists in L, U, V, and G channel images, we only use the above channel images to generate SSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Neighboring features</head><p>In fact, both non-neighboring and neighboring features are crucial for pedestrian detection. In this section, we propose to form the pool of neighboring features by using local mean features (see <ref type="figure" target="#fig_9">Fig. 8</ref>(a)) and neighboring difference features (see <ref type="figure" target="#fig_9">Fig. 8</ref>(b)) with enough freedom in size, aspect ratio, patch direction, and partition location. The left portion of <ref type="figure" target="#fig_9">Fig. 8(a)</ref> shows that the size of a feature is allowed to vary in a large extent. Patch direction is either vertical or horizontal. The patch direction in the middle of <ref type="figure" target="#fig_9">Fig. 8(a)</ref> and the left portion of <ref type="figure" target="#fig_9">Fig. 8(b)</ref> is vertical whereas the direction in the right portion of <ref type="figure" target="#fig_9">Fig. 8</ref>(a) and the right portion of <ref type="figure" target="#fig_9">Fig. 8(b)</ref> is horizontal.</p><p>Partition location is illustrated in <ref type="figure" target="#fig_9">Fig. 8(b)</ref> which is defined as the location where two neighboring patches intersect. Assigning freedom in partition location strengthens representative and discriminative ability of the features.</p><p>To avoid the large number of features, we specify a maximum square. The sizes of local mean features and neighboring difference features are allowed to be not larger than the size of the maximum square. The green squares in <ref type="figure" target="#fig_9">Fig.  8</ref> are maximum squares. As stated in Section 3.2, a typical size of the maximum square is 8×8 cells.</p><p>The neighboring features illustrated in <ref type="figure" target="#fig_9">Fig. 8</ref> are suitable to be computed with integral image. Hence the feature extraction process is very efficient. Note that neighboring difference features can be calculated using the same formula (i.e., Eq. (1)) of non-neighboring features.</p><p>In our method, both the neighboring (i.e., local mean features and neighboring difference features) and nonneighboring features (i.e., SIDF and SSF) are used as input of decision forests and AdaBoost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The public Caltech pedestrian dataset <ref type="bibr">[1,</ref><ref type="bibr" target="#b9">11]</ref> and IN-RIA dataset <ref type="bibr" target="#b5">[7]</ref> are employed for evaluation. In the INRIA dataset, there are 1237 pedestrian images used for training and 288 pedestrian images used for evaluation.</p><p>The Caltech pedestrian dataset is more challenging than the INRIA dataset and hence has become a benchmark. It consists of approximately 10 hours of 640×480 30Hz video [1]. The 10 hours data consists of 11 videos with the first 6 videos are used for training and the last 5 videos for testing. The standard positive training data is formed by sampling one image out of each 30 sequential frames. To enlarge the number of training samples, we sample a frame from every two or ten frames instead of every 30 frames. The resulting training sets are called Caltech 2x and Caltech 10x <ref type="bibr" target="#b30">[32]</ref>. Whenever Caltech 2x training set or Caltech 10x training set is used, the testing dataset is the same. The testing dataset consists of 4024 frames among which there are 1014 positive images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-comparison using the Caltech 2x training data</head><p>Before comparing with the state-of-the-art methods, experimental results on Caltech 2x dataset are reported to show how the proposed method works and the importance of each component of the proposed method. Note that the Caltech 2x training set instead of Caltech10x training set is used.</p><p>The experimental setup is as follows. Classical 10 channel images (i.e., HOG+LUV) are used for generating features. The final classifier consists of 4096 level-2 decision trees. The classifier is learned by five rounds, where the numbers of trees in subsequent rounds are 32, 128, 512, 2048, and 4096, respectively. Each tree is built by randomly sampling 1/32 of features from the large pool of features. 5000 hard negatives are added after each round and the cumulative negatives are limited to 15000. The stride of sliding windows is 4 pixels. The model size is 64×128, which consists of 2048 cells (1 cell=2x2 pixels). As the pedestrian is generally taller than 50 pixels, each testing image is upsampled by one octave.</p><p>In NNNF (a.k.a., NNF+NF), both Non-Neighboring Features (NNF) and Neighboring Features (NF) are em-    <ref type="figure" target="#fig_10">Fig. 9</ref>. It is seen that the performance of NNNF is systematically better than that of NF, meaning that incorporating NNF is useful for improving detection performance. Meanwhile, one can observe that NNNF-No is inferior to NNNF. NNNF employs channel-specific normalization in NF and SIDF whereas NNNF-No does not perform normalization. So it is concluded that pedestrian detection benefits from the proposed channel-specific normalization.</p><formula xml:id="formula_9">Channel L U V G 6 Oriented gradients Method x−µ σ x x µ G</formula><p>The above observation can also be seen from <ref type="table" target="#tab_1">Table 2</ref> where the log-average miss rates are given. The miss rates of NNNF (i.e., NNF+NF), NNNF-No, and NF are 23.06%, 24.34%, and 27.50%, respectively. The miss rate of NNF+NF is 4.44% smaller than that of NF. So it is said that non-neighboring features contribute significantly for improving detection performance. Specifically, NF+SIDF and NF+SSF outperform NF by 1.83% and 2.30%, respectively. NNNF outperforms NNNF-No by 1.28%. Though the contribution of channel-specific normalization is not as significant as non-neighboring features, it is steadily helpful for improving detection performance. Totally, 12288 features are selected, which consist of 3690 non-neighboring features and 8598 neighboring features. Among non-neighboring features, there are 2297 side-inner difference features (SIDF) and 1393 symmetrical similarity features (SSF). That is, the proportions of SIDF, SSF, and NF are approximately 18.69%, 11.34% and 69.97% (see <ref type="figure">Fig. 10</ref>). We can conclude that nonneighboring features are complementary to neighboring features. Several representative forms of non-neighboring (SIDF and SSF) and neighboring features (NF) are also shown in Figs. 10.</p><p>In <ref type="figure">Fig. 11</ref>, the representative non-neighboring features are also visualized on pedestrian images. The first two images show the side-inner difference features, and the last two images show the symmetrical similarity features.</p><p>In fact, SIDF features can be categorized into the following three types: 1) A SIDF feature is called Contour-Inner SIDF (CI-SIDF) feature if one of its patch is located on the pedestrian contour and the other is located inside the pedestrian; 2) A SIDF feature is called Background-Pedestrian SIDF (BP-SIDF) feature if one of its patch is on the background and the other patch is inside or on the contour of a pedestrian; and 3) A SIDF feature different from CI-SIDF and BP-SIDF features is called Other SIDF (O-SIDF) feature. To know the proportions of the three types of SIDF features, a ternary model ( <ref type="figure" target="#fig_12">Fig. 12(a)</ref>), consisting of background, contour body, and inner body, is created according to average appearance (e.g., <ref type="figure">Fig. 2(d)</ref>) of pedestrians. All the 2297 selected SIDF features are classified to CI-SIDF, BP-SIDF, and O-SIDF by computing the intersection of a SIDF feature and the ternary model. The results given in <ref type="figure" target="#fig_12">Fig. 12(b)</ref> indicate that the proportions of CI-SIDF, BP-SIDF, and O-SIDF are 42.66%, 50.11%, and 7.23%, respectively. <ref type="figure" target="#fig_12">Fig. 12</ref>(b) tells that SIDF features not only capture the difference the contour of a pedestrian and its inner part but also utilize the difference between the background and a pedestrian. Background can be regarded as context of a pedestrian image and hence context has been proved to be effective in object detection and recognition. It is difficult for neighboring features to utilize the context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-art methods on Caltech dataset</head><p>The proposed NNNF method can adopt different levels (depths) decision trees. In this section, NNNF-L2 stands for the NNNF method where level-2 trees are utilized. The Caltech 2x training data is used for NNNF-L2. All parameters in NNNF-L2 are the same as those in Section 4.1. In NNNF-L4, level-4 trees are employed. The Caltech 10x training data is used for NNNF-L4. The resulting classifier is composed of 4096 level-4 decision trees and each tree is built by randomly sampling 1/2 of features from the feature pool. The decision trees are obtained after five rounds. In each round, 20000 hard negatives are added and the cumulative negatives are limited to 50000. Other parameters are the same as those in Section 4.1. <ref type="figure" target="#fig_2">Fig. 13</ref> compares NNNF-L2 and NNNF-L4 with the state-of-the-art methods. The curves of ACF-Caltech <ref type="bibr" target="#b9">[11]</ref> are obtained when they are trained on the Caltech training   set. The models of VJ <ref type="bibr" target="#b25">[27]</ref>, HOG <ref type="bibr" target="#b5">[7]</ref>, LatSvm-V2 <ref type="bibr" target="#b11">[13]</ref>, and Roerei <ref type="bibr" target="#b1">[3]</ref> are trained on the INRIA dataset. The curves of other methods are obtained when the training set is Caltech 10x. They all utilize the Caltech testing set for evaluation.</p><p>The following observations can be seen from <ref type="figure" target="#fig_2">Fig. 13</ref>. Even the small Caltech 2x training dataset is used, the proposed NNNF-L2 is better than LDCF <ref type="bibr" target="#b17">[19]</ref> whose models are trained from the large Caltech10x dataset. Specifically, the log-average miss rate of NNNF-2 is 23.06%.</p><p>It can also be seen from <ref type="figure" target="#fig_2">Fig. 13</ref> that the proposed NNNF-L4 is superior to all other methods. The log-average miss rate of NNNF-L4 is as small as 16.84% whereas the logaverage miss rate of TA-CNN <ref type="bibr" target="#b24">[26]</ref> and Checkboards <ref type="bibr" target="#b30">[32]</ref> are 20.86% and 18.47%, respectively. Though the proposed non-neighboring and neighboring features are much simpler than those in CNN and Checkboards, they result in better detection results.</p><p>According to whether using CNN or not, <ref type="figure" target="#fig_5">Fig. 14</ref>   CNN. CompACT-Deep <ref type="bibr" target="#b4">[6]</ref> achieves the lowest miss rate (i.e., 11.70%) by combination of some local channel features (e.g., ACF <ref type="bibr" target="#b9">[11]</ref>, Checkboards <ref type="bibr" target="#b30">[32]</ref>, and LDCF <ref type="bibr" target="#b17">[19]</ref>) and deep features (e.g., VGG <ref type="bibr" target="#b22">[24]</ref>). Though CompACT-Deep <ref type="bibr" target="#b4">[6]</ref> has a better performance than NNNF-L4, the improvement of CompACT-Deep are based on very deep CNN model (i.e., VGG). When only using the above local features and small CNN, CompACT can only achieve 18.9%, which is inferior to NNNF-L4. It means that NNNF-L4 are much more effective than the local features used in CompACT. Moveover, Our non-neighboring features are complementary to the features of CompACT. So the nonneighboring features can be combined with CompACT to boost the performance of pedestrian detection.</p><p>The log-average miss rates and frames per second of the methods without CNN are visualized in <ref type="figure" target="#fig_4">Fig. 15</ref>. It is desirable if miss rate is as small as possible and FPS is as large as possible. So <ref type="figure" target="#fig_4">Fig. 15</ref> implies that the proposed NNNF-L4 achieves the best tradeoff between miss rate and FPS. The log-average rate of NNNF-L4 is superior to that of Checkboards, and it is also 2.28 times faster than Checkboards. Note that the detection speed is measured on a computer with an Intel i7 CPU and a 640×480 image with the height of a pedestrian not less than 50 pixels. GPU is not used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art methods on the INRIA dataset</head><p>Experiments are also conducted on the INRIA dataset. Because pedestrian height in both the training and testing sets are larger than 100 pixels, we train a model with 64×128 pixels. The model consists of 2048 level-3 decision trees. Other parameters are the same as those in Section 4.1.</p><p>Experimental results are shown in <ref type="figure">Fig. 16</ref>. It can be observed that NNNF achieves the best performance (logaverage miss rate is 12.25%). The miss rate of NNNF is 7.71%, 5.03%, and 2.18% lower than that of LatSvm-V2 <ref type="bibr" target="#b11">[13]</ref>, ACF <ref type="bibr" target="#b6">[8]</ref>, and InformedHaar <ref type="bibr" target="#b29">[31]</ref>. NNNF outperforms LDCF <ref type="bibr" target="#b17">[19]</ref> by 1.54%. The advantage of NNNF over LDCF <ref type="bibr" target="#b17">[19]</ref> and Roerei <ref type="bibr" target="#b1">[3]</ref> is more remarkable for the complex Caltech dataset than for the simple INRIA dataset.</p><p>The comparison of detection speed and miss rate of different methods is given in <ref type="figure" target="#fig_7">Fig. 17</ref>. The image to be detected has 640×480 pixels and the height of a pedestrian   <ref type="figure">Figure 16</ref>. Comparison with state-of-the-art methods on the IN-RIA dataset.  is not less than 100 pixels. One can see that NNNF outperforms in terms of log-average miss rate all the methods except Spatialpool. Though slightly lower than the miss rate of Spatialpool, NNNF is 55.86 times faster than SpatialPool <ref type="bibr" target="#b20">[22]</ref>. Therefore, our method is able to get the best tradeoff between miss rate and detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented an effective and efficient pedestrian detection method. The main contribution lies in the proposed two types of non-neighboring features (NNF): side-inner difference features (SIDF) and symmetrical similarity features (SSF) which were found to be complementary to the proposed neighboring features (NF). SIDF features characterize not only the difference between contour of a pedestrian and its inner part but also the difference of the background and pedestrian. SSF can capture the symmetrical similarity of pedestrian shape. Though the forms of the proposed NNF and NF features are very simple, combining them results in the best tradeoff between miss rate and frames per second.</p><p>It is noted that the proposed non-neighboring features (i.e., NNF), the neighboring features (e.g., Checkboards), and CNN features (e.g., VGG) are complementary. In the future work, we will focus on combing NNF with them and investigating whether or not this combination is able to improve the performance of pedestrian detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) describe the similar edge characteristic, while patches C and C inFig. 2(c)are both bright.Figs. 2(a)and (b) also support the above two conclusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Demonstration of the dismicrination and usefulness of non-neighboring features. (a) and (d) show Object 1 and Object 2, respectively. In (b) and (e), neighboring features are extracted. In (c) and (f), non-neighboring features are extracted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(e)) are equal. Now we use in Figs. 3(c) and (f) two non-neighboring patches to form a feature. Because the blue dashed patch in Fig. 3(c) contains a line whereas the blue dashed patch in Fig. 3(f) contains nothing, the nonneighboring features in Object 1 (i.e.,Fig. 3(c)) and Object 2 (i.e.,Fig. 3(f)) have different values. The two objects can be correctly classified according to the different values. This demonstrates the dismicrination and usefulness of nonneighboring features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 ,</head><label>5</label><figDesc>l(B) is randomly sampled from [l(A), l(A )] in our experiments. Both Figs. 4(b) and (c) show varying sizes of patches. But in Fig. 4(b) both two non-neighboring patches equally vary with size (scale) whereas in Fig. 4(c) only one patch A B (a) Varying distance between two patches. (b) Varying size of two patches. (c) Varying size of one patch with the other fixed. (d) Varying aspect ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Some possible forms of side-inner difference features (SIDF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>The patch B is randomly located between the patch A and its horizontal mirror A . The locations of patch B in (a) and (b) are different. But they are both among A and its mirror A . (c) and (d) show that the width of patch B can be changed. The size of patch A is allowed to change inside the maximum square indicated by green squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Examples of symmetrical similarity features. (a) is an example of SSF located in the pedestrian. (b) shows a specific form of SSF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Some possible forms of neighboring features. The green squares are called maximum squares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Self-comparison: ROC curves of NF, and NNNF-No, and NNNF on the Caltech dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Among all the selected features, about 30% are nonneighboring features and 70% are neighboring features. Some representative non-neighboring and neighboring features also shown. Several selected non-neighboring features. The first two features are SIDF, and the last two features are SSF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>CI-SIDF, BP-SIDF, and O-SIDF features. (a) The ternary model of pedestrians. (b) The portions of SIDF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Miss rate of the state-of-the-art methods. The methods with blue bars are based on CNN. The methods with red bars are not using CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Log-average miss rate (MR) versus frames per second (FPS) on the Caltech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 .</head><label>17</label><figDesc>Log-average miss rate (MR) versus frames per second (FPS) on the INRIA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Channel-Specific Normalization</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Log-average Miss Rates ployed. In the NNF, there are two types of non-neighboring features: SIDF and SSF. NF+SIDF or NF+SSF mean that the neighboring features are combined with only one type of non-neighboring features (i.e., SIDF or SSF). In SIDF and NF, the channel-specific normalization can be used inTable 1. InTable 1, x is a feature in a detection window. µ and σ are the mean and variance of the features in the detection window. µ G is the mean of G channel. Because U and V channels are relatively stable to variations in illumination, we do not perform normalization. We denote NNNF-No the method which is the same as NNNF except that no normalization is conducted in SIDF and NF.The ROC curves of NF, NNNF-No and NNNF are shown in</figDesc><table><row><cell>Method</cell><cell>MR</cell><cell>∆ MR</cell></row><row><cell>NF</cell><cell>27.50%</cell><cell>N/A</cell></row><row><cell cols="3">NF+SIDF 25.67% +1.83%</cell></row><row><cell>NF+SSF</cell><cell cols="2">25.20% +2.30%</cell></row><row><cell cols="3">NNNF-No 24.34% +3.16%</cell></row><row><cell>NNNF</cell><cell cols="2">23.06% +4.44%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>divides the state-of-the-art methods into two classes. The methods with red bars do not use CNN. NNNF-L4 achives the best detection performance, outperforming Checkboards [32] by 1.63%. The methods with blue bars are based on .49fps/44.20%] ACF [14.10fps/53.90%] Crosstalk [0.16fps/21.90%] LatSvm−L2 [0.63fps/34.60%] InformedHaar [0.12fps/29.20%] SpatialPooling [3.62fps/24.80%] LDCF [0.12fps/21.90%] SpatialPooling+ [0.50fps/18.47%] Checkboards [1.62fps/23.06%] NNNF−L2 [1.14fps/16.84%] NNNF−L4</figDesc><table><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>log−average miss rate</cell><cell>20 30 40 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[9</cell></row><row><cell></cell><cell>1/16 10</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>1/1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">frames per second</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boosting decision trees-pruning underachieving features early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl Conf. Machine Learning</title>
		<meeting>Intl Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fastest feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The fastest pedestrian detector in the west</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">Integral channel features. Proc. British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascade object detection with deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multi-pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl Conf. Computer Vision</title>
		<meeting>IEEE Intl Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Robust real-time face detection. Int&apos;l J. Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Convolutional channel features. Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple-instance pruning for learning efficient cascade detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Informed haarlike features improve pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
