<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Slender Object Detection: Diagnoses and Improvements</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<addrLine>3 Megvii</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutao</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<addrLine>3 Megvii</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Slender Object Detection: Diagnoses and Improvements</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we are concerned with the detection of a particular type of objects with extreme aspect ratios, namely slender objects. In real-world scenarios, slender objects are actually very common and crucial to the objective of a detection system. However, this type of objects has been largely overlooked by previous object detection algorithms. Upon our investigation, for a classical object detection method, a drastic drop of 18.9% mAP on COCO is observed, if solely evaluated on slender objects. Therefore, we systematically study the problem of slender object detection in this work. Accordingly, an analytical framework with carefully designed benchmark and evaluation protocols is established, in which different algorithms and modules can be inspected and compared. Our study reveals that effective slender object detection can be achieved with none of (1) anchor-based localization; (2) specially designed box representations. Instead, the critical aspect of improving slender object detection is feature adaptation. It identifies and extends the insights of existing methods that are previously underexploited. Furthermore, we propose a feature adaption strategy that achieves clear and consistent improvements over current representative object detection methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a fundamental task in computer vision that draws considerable research attention from the community, object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> has made substantial progress in recent years. As the needs of real-world applications in a wide variety of scenarios arise <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b26">26]</ref>, the significance of research regarding a particular topic elevates. The works on improving specific aspects <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">39]</ref> of object detection, such as detecting dense objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> and small objects <ref type="bibr" target="#b40">[40]</ref>, boost the practical value of object detection and consequently inspire further advances <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>While a large portion of the problems have been well investigated and numerous new ideas have been proposed, grand challenges remained in object detection. <ref type="bibr" target="#b18">[19]</ref> propose * Authors contribute equally the focal loss to tackle dense object detection and prompt it to become a common practice for classification loss in object detection. Object scale has been widely considered in model design, as various detection paradigms <ref type="bibr" target="#b5">[6]</ref>, augmentation schemes <ref type="bibr" target="#b40">[40]</ref>, and modules <ref type="bibr" target="#b0">[1]</ref> are proposed to improve small object detection. Such insightful works propel object detection methods to transfer from academic research to a wide variety of real-world applications <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b16">17]</ref>. Despite such progress in object detection, one significant problem has not been formally explored in previous works. This work is dedicated to studying the problem of slender object detection. From the perspective of academic research, the distinctive properties of slender objects pose special challenges, which give rise to research topics of scientific value. From the perspective of application, once slender objects can be well-handled, the practical utility of object detection systems will become higher.</p><p>Inspired by previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>, which provide in-depth ablations, analyses, and insights regarding object detection algorithms, we start with diagnosing and analyzing existing methods for object detection. Specifically, we address issues in slenderness definition, feasible evaluation protocols, and data bias neutralization. A unified analytical framework is established, plus a standard detection pipeline, to dissect and compare different approaches in a clear and fair manner. We make it convenient to identify the key factors of previous methods, effective choices for model design, and potential directions for improvement. Key findings related to effective slender object detection, which is proven practicable without anchor-based localization nor specialized box representations, are presented in Sec. <ref type="bibr" target="#b2">3</ref>.</p><p>Beyond diagnoses and analyses, we further propose strategies to boost the detection of slender objects. In par-  ticular, a generalized feature adaption module, called selfadaption, is introduced. In addition, we show potential trade-off measures between the detection of slender and regular objects. According to the quantitative experiments (see Sec. 4), the proposed feature adaptation has proven effective for slender objects while also working well for regular objects (see Tab. 3).</p><p>In summary, the main contributions of this paper are as follows:</p><p>• We are the first to formally investigate the problem of slender object detection, which is important but largely overlooked by previous works. Issues preventing from in-depth study in slender object detection are addressed, including definition, metrics, and evaluation bias against slender objects.</p><p>• We construct an analytical framework for rigorously diagnosing different object detection methods. With this framework, a series of key insights and valuable findings, which may inspire other researchers in the field of object detection, is derived.</p><p>• We identify the feature adaption module as a key factor for the improvement of slender object detection. A generalized feature adaption module, called selfadaption, is devised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary Assessment</head><p>In this section, we will provide an overview of slender object detection and conduct a preliminary assessment on existing methods. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, slender objects in images can be roughly categorized into three groups: Distinct slender objects are those that are intrinsically slender in shape, such as ski, forks, and bats. Regular objects may also appear slender in images because of occlusion and truncation (top right in <ref type="figure" target="#fig_1">Fig. 1</ref>). In addition, some thin plates in the real world may appear slender from certain viewing angles, e.g., books and tables. Different categories of  objects exhibit different characteristics but may also share some properties in common. We analyze typical errors by previous methods for these different categories, and accordingly draw unified conclusions regarding slender objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Definition of Slenderness</head><p>For targeted evaluation and analyses, we need to estimate the slenderness of objects. In the context of object detection where bounding boxes are the common representation of objects, slenderness can be approximately computed from the width w b and height h b of axis-aligned bounding boxes as r b = w b /h b . This formula is specifiable for both the ground truth and detection results, thus being fully applicable to existing evaluation protocols, e.g., mean average precision (mAP) and mean average recall (mAR). However, the deviation of r b is obviously inaccurate for oriented slender objects as illustrated in <ref type="figure" target="#fig_3">Fig. 2</ref>. It would mistake oriented slender objects as regular objects and in consequence underestimate the gap between regular and slender objects. The more accurate approach is to find a rotated box which covers the object with the minimal area (green box in <ref type="figure" target="#fig_3">Fig. 2</ref> top), and compute the slenderness s as:</p><formula xml:id="formula_0">s = min(w, h)/ max(w, h).<label>(1)</label></formula><p>w and h are the width and height of the minimum-area rectangle. For the convenience of comparison, we refer to objects with s &lt; 1/5, 1/5 &lt; s &lt; 1/3, s &gt; 1/3 as extra slender (XS), slender (S), and regular (R), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation Metrics</head><p>In addition to the overall mAP that is the common metric for object detection, evaluation over particular aspects, e.g., areas and object class, is desired to diagnosis errors. An example is the evaluation over scales of COCO <ref type="bibr" target="#b19">[20]</ref> competition. Meanwhile, the situation is different in respect of object slenderness. Although we can always evaluate the overall mAP, particular mAP over slenderness is undefined.</p><p>Given ground truth objects G, detections D with confidence scores S produced by an algorithm, mAP is the mean over categories w.r.t average precision over all recalls. Formally, the precision and recall of a category k is defined as a fraction of true positives over predicted positives:</p><formula xml:id="formula_1">P (t, k) = ||D(t, k) ∩ G(k)|| ||D(t, k)|| , R (t, k) = ||D(t, k) ∩ G(k)|| ||G(t, k)|| ,<label>(2)</label></formula><p>where t is the confidence threshold that filters out detections with lower confidence scores. As shown in <ref type="figure" target="#fig_3">Fig. 2</ref> bottom, the current practice of object detection is to represent objects as bounding boxes that leave slenderness unknown. Thus, precision for a particular category is undefined. In contrast, the slenderness category of ground truth can be assigned to matched true positives, making recall feasi-  ble for evaluating slender object detection:</p><formula xml:id="formula_2">AR k = 1 0 R(t, k)dt, mAR = 1 K k AR k .<label>(3)</label></formula><p>In this paper, we use mAR shown above as the metric for particularly benchamrking slender object detection. Analogically to mAP, the amount of detections for each image is limited to 100 for evaluation. We also report overall mAP on datasets to compare with existing literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Data Bias Neutralization</head><p>As mentioned above, we rely on precise boundaries of objects to estimate their slenderness, which is not feasible with conventional axis-aligned bounding box annotations in object detection. The COCO dataset <ref type="bibr" target="#b19">[20]</ref>, one of the most popular datasets in recent research of object detection, provides pixel-level segmentation labels. It is a large-scale dataset collected for object detection and related tasks, e.g., keypoint detection and panoptic segmentation.</p><p>However, COCO is biased regarding slender objects and not sufficient for evaluating slender object detection by itself. The data distribution of COCO is visualized in <ref type="figure" target="#fig_4">Fig. 3</ref>, where more than 85% of objects are regular. The dominant proportion in the dataset implicitly forces the current evaluation to favor regular objects over slender objects. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, the overall mAR in COCO is close to that of regular objects. Such a bias against slender objects can be mitigated by extending the validation set of COCO.</p><p>We incorporate slender objects from another dataset, Objects365 <ref type="bibr" target="#b27">[27]</ref>, to complement COCO. Objects365 is a dataset aiming at object detection in the wild, containing 38k validation images sharing similar characteristics with COCO. In contrast to COCO which provides detailed boundaries of objects, Objects365 annotates objects with axis-aligned bounding boxes. We use a top-performing instance segmentation model by <ref type="bibr" target="#b1">[2]</ref> with a ResNeXt152 <ref type="bibr" target="#b32">[32]</ref> backbone to generate polygon borders of objects. Given ground truth bounding boxes during inference, the produced masks are accurate for slenderness estimation. The procedure and examples of polygon generation are shown in Appendix B in the supplementary material. According to the slenderness estimated from generated borders, we select images containing extra slender objects in Objects365 to mix with the COCO validation set, creating COCO + .As shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, the number of slender objects in COCO + is 8 times more than COCO, thus mitigating the bias against slender objects. Experimental validation shown in <ref type="figure" target="#fig_5">Fig. 4</ref> verifies that COCO + is fairly balanced since the overall mAR is closer to the average of mAR on extra slender objects and mAR of regular objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Error Analysis</head><p>Using the evaluation protocols and data, we assess the problem by observing the evaluation on a representative method <ref type="bibr" target="#b18">[19]</ref>. Models we implemented in this paper are built upon ResNet <ref type="bibr" target="#b8">[9]</ref> backbones with FPN <ref type="bibr" target="#b17">[18]</ref> and trained on the COCO training set with a 1x schedule. To make the baseline for experiments, we also provide evaluation results on COCO validation set.</p><p>The evaluation results are shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. It is noteworthy that detection mAR is inversely proportional to object slenderness, with a gap of 19.3% between XS and R objects. This correlation is consistent with different data sets, IoU thresholds, and object areas. It verifies that the detection of slender objects is more challenging. In consideration of the notable overlap between slender and small objects, we separately evaluate objects with different areas. As shown in the last row of <ref type="figure" target="#fig_5">Fig. 4</ref>, mAR on slender objects are consistently worse than regular objects with a large gap, regardless of the area of objects. The gap is surprisingly more notable for large objects, due to the increase of challenges in estimating object sizes.</p><p>Counterintuitively, accurately classifying slender objects is more difficult than locating them. As shown in <ref type="figure" target="#fig_5">Fig 4</ref>, evaluation on lower IoU threshold (&lt; 0.7), which tolerate inaccurate localization results more, bears more significant performance difference. We assume the cause is that bounding boxes of slender objects usually contains more background (see <ref type="figure" target="#fig_3">Fig. 2</ref> for an example), and validate it in latter experiments.</p><p>An intuitive alleviation of the problems on slender object detection is to increase the sample rate of slender objects in the dataset during training. Its validation is shown in Tab 2 with a classical baseline RetinaNet <ref type="bibr" target="#b18">[19]</ref>. It demonstrates the change of sampling rate in the training data as a trade-off between the effectiveness on regular and slender objects. Accompanying the increase of slender and extra slender mAR, regular mAR drops. What we concern more is that, when the sample rates of slender objects continue to increase, the drop of overall performance is also escalated. Therefore, besides data sampling, we conduct further investigation on models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Dissection</head><p>So far, we have addressed issues regarding slender object detection that prevent rigorous investigation of the problem. An evaluation with the conducted protocol is performed on several representative detection methods and shown in Tab. 3. As we can observe from the table, the model advantages over object slenderness fluctuates, making the problem beyond the scope of direct comparison of existing  <ref type="figure">Figure 5</ref>: Illustration of the decomposed stages of object detection. Note that a given stage can be performed more than once to form the actual pipeline in certain detectors. methods. For example, FCOS <ref type="bibr" target="#b28">[28]</ref> achieves better detection performance for regular (R) objects, but is limited in detecting slender objects in comparison with RetinaNet. The similar pattern exists between RetinaNet and FasterRCNN.</p><p>This problem is concealed by the evaluation bias against slender objects described in Sec. 2.4 and revealed by the proposed particular evaluation. However, new approaches in object detection are usually brought in with multiple alternations. Their ablation experiments are conducted in divert environments, e.g., backbones <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>, training protocols <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, thus discouraging direct comparison from recognizing true insights for effective slender object detection. In this section, we first unify main-stream detectors into standard stages, and then combine the proposed evaluation protocols to conduct experiments with controlled variables to reveal the impact of different modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Standard Object Detection Stages</head><p>Basically, the task of object detection is composed of two subtasks, localization and classification. A typical detection method localizes and classifies object regions from rich feature maps extracted by neural networks in pyramid resolutions. Some of existing methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref> directly perform localization and classification on extracted features, and another groups of methods apply feature adaption, e.g. ROI Pooling <ref type="bibr" target="#b23">[24]</ref>, according to coarsely localized object proposals. They are also referred to as one-stage and twostage methods in some literature, respectively. For dense detection, post-processing such as redundancy removal is required for most detectors, after which a set of final object detection is formed.</p><p>Deriving from the existing methods, four standard stages of object detection can be defined as follows.</p><p>1. Feature Extraction (FE) extracts features from the input image to form a high dimensional representation. As deep CNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">32]</ref> and their variants <ref type="bibr" target="#b17">[18]</ref> significantly improve the capability of detectors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>, experimental comparison is usually conducted on the same backbones.</p><p>2. Intermediate Prediction (IP) localizes and/or classifies object regions. They can be assembled together  <ref type="bibr" target="#b18">[19]</ref> or a verification problem <ref type="bibr" target="#b14">[15]</ref>.</p><p>3. Feature Adaptation (FA) adapts feature maps using intermediate localization results or directly from features for refined prediction. It usually exploits coarse estimation of object regions, i.e. proposals <ref type="bibr" target="#b5">[6]</ref>, as regions of interest to concentrate on objects for refining classification and localization. Note both FA and IP can be utilized multiple times <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr" target="#b3">4</ref>. Detection Formation (DF) forms the final results by removing redundant predictions, filtering lowconfidence objects, etc. A common detection formation for object detection is non-maximum suppression (NMS) and its successors. Recently, <ref type="bibr" target="#b2">[3]</ref> propose endto-end prediction of object localization and classes, resulting in simplified detection formation.</p><p>In addition to these stages that are required for both training and inference, label assignment and loss function identify the criterion for the training procedure. Loss Function (LF) acts as the optimization target during the training of detectors. It consists of the loss function for classification, where focal loss is dominant, and the loss function for localization, where smooth l1 loss and gIoU loss are preferred choices. Label Assignment (LA) fills the gap between the optimization target and network outputs. It assigns labels to prediction results, making the model directly trainable. Label assignment is still under active investigation as it is related to localization and classification representation of objects. By standardizing stages that identifies a detector, we eliminate undesired variance and conduct rigorous experiments in next sections and believe it will provide a guidance of fair ablation study for the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Component Inspection</head><p>Under the analytical framework, we recall typical errors of slender object detection revealed in Sec. 2.4 to inspire our inspection. One of the major errors of slender object detection is related to distinct slender objects introduced <ref type="figure">Figure 6</ref>: Pipelines of a 9-point representation method <ref type="bibr" target="#b36">[36]</ref>. The components are dissected in Sec. 3 to reveal critical aspects for slender object detection. Dotted boxes and arrows indicate the components only used for training.  <ref type="figure" target="#fig_1">Fig. 1a</ref>. Vertical and horizontal slender objects can be improperly assigned by the IoU matching between the bounding box and pre-defined anchors during training. In consideration of literate arguing the role of anchors <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b37">37]</ref>, we introduce the first conjecture to verify: Anchor-based localization is critical for slender object detection. On the other hand, we also demonstrate the goal of object detection, representing objects in bounding boxes, contradicts with the nature of slender objects. We conjecture works in novel box representations <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b14">15]</ref> may alleviate this problem and verify it using our analytical framework.</p><formula xml:id="formula_3">ℒ ℱℰ ℒℱ Smooth L1 ℱ NMS ℐ {Δx1, Δy1, Δx2, Δy2} ℱ DCN ℐ {Δx1, Δy1, Δx2, Δy2} ℒ ℒℱ Smooth L1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The controvertible role of anchors</head><p>Anchors are once regarded central to many detection systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref> and enable detectors to detect multiple objects at the same location. However, anchor-based relies on IoU matching between anchors and ground truth boxes for LA, which is shown sub-optimal for slender objects. This drawback may be partly alleviated by specially designed anchors <ref type="bibr" target="#b21">[22]</ref> (see Appendix C for experiments regarding rotated anchors) or bypassed by anchorfree detectors. Anchor-free detectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> are alternative approaches that directly regresses boxes from pixel locations instead of anchors. This family of detection methods achieve notable success but their properties remains unclear compared with that of anchor-based.</p><p>We choose a competitive anchor-free detector FCOS <ref type="bibr" target="#b28">[28]</ref> and a classical anchor-based detector Reti-naNet <ref type="bibr" target="#b18">[19]</ref> to reveal the difference between the bifurcated paradigms in essence. We refer readers not familiar with these two methods to Appendix D for an illustration of their architecture.</p><p>We show the evaluation of RetinaNet and FCOS in Tab. 3 and Tab. 4. Although FCOS shows advantages in the detection of regular objects and thus keeps ahead in overall performance, the advantage is not consistent for slender objects. Scrutinizing the evolution from an anchor-based detector to anchor-free detectors like FCOS, we design multiple intermediate variants of which each keeps only one variable to uncover the substantial aspects. Excluding implementation details unified by our framework, differences to be addressed exist in LF, LA, and localization paradigm in IP. More formally, an anchor-free detector (A) assigns locations inside object boxes as positive, (B) regresses from pixels rather than from anchors, (C) uses IoU loss as localization target, and specially FCOS (D) adopts centerness score to re-weight loss and confidence at different positions. Discretely implementing these variants, we show the performance change in Tab. 4.</p><p>We can draw clear conclusion from the experiments. The removal of anchor-based assignment brings such significant performance gap between the two paradigms that all other components are introduced to fulfill the rift. We also notice the impact of other attempts remain fairly trivial until LA is improved. It leads to the conclusion that the essential role of anchors is in label assignment.</p><p>In comparison with the difference in LA that bifurcates the anchor-free detector from anchor-based RetinaNet with 6% mAP, the effect of anchors for regression is minor. Other interesting phenomenon includes that the improvement of IoU loss is more significant than expected <ref type="bibr" target="#b25">[25]</ref>, especially for slender objects. We hypothesis the cause is two fold: (1) The anchor-free LA guarantees the predicted bounding box overlaps with the ground truth, while anchor- <ref type="table">Table 6</ref>: Validation experiments of self-adaptation. and × indicate models with and without self-adaptation, respectively. The improvements are consistent on stronger models. COCO mAP is evaluated on the validate set, which is usually slightly lower than the dev-test set. (2) Inaccurate predictions around box borders are less punished by IoU loss and thus are less likely to affect predictions at center area during NMS. These properties are enhanced by FCOS using centerness to achieve finally better performance than anchor-based baseline. It is also possible to utilize this property in slender object detection as a trade-off strategy. The details are provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The impact of box representation</head><p>In addition to anchors, we suppose box representations <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b3">4]</ref> that are closely related to object slenderness would help this problem. Works in this direction modify the regression targets and form bounding boxes from other representations during DF. Among these methods, Rep-Points <ref type="bibr" target="#b36">[36]</ref> stands out due to its promising improvements and novel box representation. We re-implement it inside our analytical framework to conduct fair experiments for mining valuable components in terms of slender objects.</p><p>The model architecture of RepPoints is illustrated in <ref type="figure">Fig. 6</ref>. It turns box representation into a set of points, specifically 9 in their experiments, which forms a pseudo box using simple processing such as min-max. Furthermore, the 9point representation coordinates with Deformable Convolution (DCN) by forming its offsets. Following the proposed FA layer, an extra IP stage is performed to refine the initial localization results. Applying our framework, RepPints is distinguishable in several aspects: (E) label assignment by assigning the nearest location to box centers as positive;</p><p>(F) 2-point representation instead of anchor-based representation; (G) the proposed 9-point representation and pseudo box formation; and (H) supervised feature adaption integrating localization results with DCNs.</p><p>Despite the remarkable performance validated by our experiments, the potential root is surprising: In comparison with 2-point representation, the 9-point representation demonstrates no advantages with the same LA. Note the 2-point representation is equivalent to the regression strategy of anchor-based methods like RetinaNet. We claim that different box representations are comparable in object localization and it is further verified in detailed experiments in the next section. In contrast, the supervised feature adaptation is critical for slender object detection and substantially improves mAR on XS objects.</p><p>Through experiments in Tab. 5, we recognize FA as the potential root of advantages in slender object detection. However, the supervision that forces DCN to sample features around the borders (see <ref type="figure">Fig. 6</ref>) contradicts the property of slender objects that locations near border are more likely to be background. This thinking inspires us to propose a novel feature adaptation strategy in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improving Slender Object Detection</head><p>In Sec 3.2, we dispute the conjecture that assumes anchors or box representations are central and recognize that feature adaptation lead to improvements of slender object detection. On the other hand, inspections also reveal am- biguities in understanding the effects. The supervised feature adaptation accordingly constrains the offsets of DCN by exploiting an intermediate localization stage. Since the refinement of feature and the basis of the final prediction are coupled in sample points, the necessity is arguable. In this section, we first propose a self-adaptation strategy of features that significantly improves the detection accuracy of slender objects and then demonstrate its properties with extensive ablation experiments using our analytical framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-Adaptation of Features</head><p>Inspired by analyses in Sec. 3.2, we focus on FA to improve slender object detection. The concept of feature adaptation is kept and we generalize it to fit into the nature of slender objects. Instead of manually supervise the feature adaptation to sample features from borders, we deploy feature adaptation without explicitly constraining the sampling points, namely self-adaptation. Self-adaptation of features is characterized by two aspects. (1) It uses a DCN layer whose produced feature is supervised by an initial initial box regression task similar to <ref type="bibr" target="#b30">[30]</ref> and <ref type="bibr" target="#b36">[36]</ref>. Consequently, the feature is refined for classification. (2) The adaptation is not directly constrained by annotations to be optimal for slender objects, in contrast to <ref type="bibr" target="#b36">[36]</ref>. We use regular 2-point representation of object boxes for the initial box regression, since our experiments show its descriptive capacity is comparably powerful.</p><p>We conduct extensive experiments on different baselines with different backbones to validate the improvement of self-adaptation, especially for slender objects. The results are shown in Tab. 6. Self-adaptation that can be used as an  <ref type="table">table)</ref>, the evaluation of self-adaptation demonstrates improvements over all baselines, despite their original properties in slender object detection. The improvement is even more remarkable for stronger backbones, indicating that self-adaptation is suitable for features with higher quality. Although COCO is biased against slender objects (shown in Sec. 2), the improvements in slender objects also reflect to the overall mAP on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>In this section we use our proposed analytical framework to conduct ablation study in self-adaptation. Rigorously controlling variables, we experiment on following variants to show the impact of each of the modifications that makes self-adaptation. (I) an initial localization in addition to the final results presented by RepPoints; (J) constraining offsets of DCN using the initial object localization; (K) offsets adaptively learned from the features; and (L) a residualmanner final localization that infers upon the initial localization. Modules (I) and (L) follow the design of Rep-Points <ref type="bibr" target="#b36">[36]</ref>.</p><p>The experiments generally verify self-adaptation as the root of the improvements on slender object detection demonstrated in Tab 6. Several approaches use residual prediction that refines the prediction based on the initial regression results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">36]</ref>. However, our ablation (L) shows it brings unnecessary drift to slender object detection. Moreover, we are able to safely remove the direct supervision of feature adaptation since the experiments (K) verifies our conjecture that this supervision contradicts the property of slender objects. In <ref type="figure" target="#fig_7">Fig. 7</ref>, we visualize the sampling points of self-adaptation and other strategies. By forcing the sampling points to concentrate on the foreground of objects and to avoid the interference of background, self-adaption refines the feature to capture foreground objects and achieves better detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we investigate an important yet longoverlooked problem of slender object detection. A compre- <ref type="figure">Figure 8</ref>: Pseudo masks generated by Cascaded RCNN <ref type="bibr" target="#b1">[2]</ref> for COCO + . The boxes are the ground-truth labels and the masks are accordingly generated. Only masks for slender objects are visualized.</p><p>hensive framework is established for dissecting and comparing different object detection methods as well as their components and variants. Based on this framework, a series of key observations and insights is obtained. Furthermore, we have proposed an effective strategy for significantly improving the performance of slender object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modifications Lookup Table</head><p>To help understand the various experiment inconsistency we address in this paper, we prove a lookup table for the ablation modifications involved in the paper in Tab. 8. We also attached our code in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pseudo Mask Generation</head><p>We use images containing slender objects from Ob-jects365 <ref type="bibr" target="#b27">[27]</ref> Dataset to complement COCO. As there are 365 different categories in Objects365, we map objects categories that appear in COCO into the COCO + dataset to be mixed with the original COCO.</p><p>For slenderness estimation, we use a top-performance instance segmentation model from Detectron2 <ref type="bibr" target="#b31">[31]</ref>, an implementation of <ref type="bibr" target="#b1">[2]</ref> with a ResNeXt152 backbone. Benefiting from the pre-trained models of Dectectron2, we do not need to re-train the segmentation model. We use no test time augmentation during inference, and the resolution of the input images during testing is fixed to (800, 1333). As mentioned in Section 2.2 in our paper, the ground truth bounding boxes are used as proposals while generating the masks. Consequently, the produced pseudo masks are accurate enough and reliable to measure the slenderness of objects. Some examples of the generated pseudo masks of slender objects are shown in <ref type="figure">Figure.</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Rotated Bounding Box Evaluation</head><p>In scenarios where the rotation is required for meaningful detection, e.g. scene text detection in the wild and ship detection in aerial images, rotated boxes are applied to represent objects. As another challenging problem to be solved, the detection of oriented objects is faced with particular difficulties due to discontinuity in angles <ref type="bibr" target="#b35">[35]</ref>. Research towards this problem is making notable progress <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b38">38]</ref>, while challenges persist. We provide baseline evaluation of slender objects on both COCO and COCO+, to open up new opportunities in this field.</p><p>Similar to slenderness estimation from COCO, we approximate the angle of rotated bounding box using 90degree box representation shown in <ref type="bibr" target="#b34">[34]</ref>. Inspired by <ref type="bibr" target="#b21">[22]</ref>, we extend FasterRCNN to support rotated bounding boxes. The evaluation results is shown in Tab. 9. The shown mAP and mAR are evaluated on COCO and COCO + validation set, separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Architecture of RetinaNet and FCOS</head><p>In Sec. 3 of the paper, we use FCOS and RetinaNet as baseline models to conduct our analysis regarding the role of anchors. In case readers are not familiar with these two methods, we illustrate their model architecture in <ref type="figure" target="#fig_8">Fig. 9</ref>. RetinaNet <ref type="bibr" target="#b18">[19]</ref> is a classical and effective detector that produces localization and classification results per location. Both of its IP and LA rely on manually designed anchors.   The IP is targeted at predicting ∆ from an anchor to its matched maximum IoU object. As of LA, the IoU between anchor and object bounding boxes is used to assign labels for each anchor at each location. FCOS <ref type="bibr" target="#b28">[28]</ref> performs object detection in a different manner, namely anchor-free detector. It assigns locations inside object bounding boxes with corresponding object label, and the regression target is to predict distances from a location to box boundaries. As we demonstrated in the paper, the centerness score that underlines the center areas of objects, is key to the effectiveness of FCOS. The localization loss and prediction scores are re-weighted by the centerness scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Slenderness Prior</head><p>We have mentioned in the paper that the center prior is recognized as a central component for anchor-free detec-tors and its extension makes a trade-off strategy between regular and slender objects. Center prior suppresses spurious prediction that is distant from the object centers by re-weighting using centerness scores defined by</p><formula xml:id="formula_4">centerness = ( min(l, r) max(l, r) × min(t, b) max(t, b) ) 1 2 .<label>(4)</label></formula><p>l, r, t, b are the distance to the left, right, top, and bottom border of the bounding box, respectively. With the geometric mean, the decay is slower on the long sides of slender objects but faster on the short sides, causing insufficient training of locating slender objects. Naturally, we extend the formula to centerness * = ( min(l, r) max(l, r) × min(t, b) max(t, b) ) s ,</p><p>where s is the slenderness of objects. It favors slender objects that are challenging for precise detection and fasten the score decay of regular objects.</p><p>To validate the effectiveness of slenderness prior, we perform experiments using the baseline model of FCOS (M5) and its variant with self-adaption (M8) introduced in the paper. As the results in Tab. 10 demonstrate, this natural extension significantly improves the detection mAR for slender objects, with an acceptable sacrifice of the mAP for R objects. Despite an mAR degradation for R objects, the mAR of XS and S improve 2.1% and 1.0%, respectively. It indicates that the slenderness prior is a favorable tradeoff between slender and regular objects, as the overall mAR reaches 49.7%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Slender object examples. Images marked with red borders visualize detection results. The ski in the top left is missed by the detector due to anchor mismatching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Top: Aspect ratio from bounding box (red) and slenderness from oriented box (green). Bottom: Precision for a particular category (XS in thefigure)is undefined since the number of false positives can not be obtained. Each GT can match only one positive detection in evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Number of instances of different object categories in COCO and COCO + validation set. Clearly COCO + is more neutralized in terms of slenderness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Despite across-the-board performance drop on slender objects, COCO + is fairly balanced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of different feature adaption strategies. The sampling points are marked as red points relative to the green points (See more details in Sec. 4.1). enhancement in different detection paradigms brings significant and consistent improvement to detection of slender objects. Focusing on extra slender objects (XS in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Pipelines of methods chosen for problem dissection. Dotted boxes and arrows indicate the components only used for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>mAP (%) gap between tall and wide objects on COCO. Objects are grouped by the width/height ratio r b of bounding boxes, where XT=extra tall; T=tall; M=medium; W=wide; XW=extra wide, as defined by<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>Method</cell><cell>all</cell><cell>XT</cell><cell>T</cell><cell>M</cell><cell>W</cell><cell>XW</cell></row><row><cell cols="2">RetinaNet 36.4</cell><cell>19.2</cell><cell>26.8</cell><cell>38.1</cell><cell>24.6</cell><cell>12.7</cell></row><row><cell>Faster</cell><cell>37.9</cell><cell>23.3</cell><cell>31.4</cell><cell>39.0</cell><cell>26.1</cell><cell>16.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>RetinaNet with different sampling rates during training.</figDesc><table><row><cell></cell><cell cols="2">sample rate</cell><cell></cell><cell cols="2">COCO + mAR</cell><cell></cell><cell>COCO</cell></row><row><cell cols="2">XS S</cell><cell>R</cell><cell>all</cell><cell>XS</cell><cell>S</cell><cell>R</cell><cell>mAP</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="5">48.4 23.4 37.8 53.9 36.4</cell></row><row><cell>3</cell><cell>2</cell><cell>1</cell><cell cols="5">48.4 25.4 38.2 53.0 36.0</cell></row><row><cell cols="2">10 5</cell><cell>1</cell><cell cols="5">48.0 25.8 37.9 52.8 35.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of representative models on slender object detection metrics. 23.4 37.8 53.9 36.4 RetinaNet ResNet-101 50.5 23.6 39.7 56.0 38.7</figDesc><table><row><cell cols="2">baseline backbone</cell><cell>all</cell><cell>COCO + mAR XS S</cell><cell>R</cell><cell>COCO mAP</cell></row><row><cell cols="6">RetinaNet ResNet-50 48.4 Faster ResNet-50 47.8 25.9 37.7 51.5 37.8</cell></row><row><cell>Faster</cell><cell cols="5">ResNet-101 48.2 25.9 37.8 52.1 38.1</cell></row><row><cell>FCOS</cell><cell>ResNet-50</cell><cell cols="4">48.7 23.2 37.9 54.4 37.6</cell></row><row><cell>FCOS</cell><cell cols="5">ResNet-101 50.1 24.9 39.7 55.7 40.1</cell></row><row><cell cols="6">or performed separately. Researchers have developed</cell></row><row><cell cols="6">different paradigms for localization in recent works,</cell></row><row><cell cols="6">where it can be modeled as a regression problem</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Experiments on components making anchorfree detector (FCOS) from an anchor-based detector (Reti-naNet). A: label assign (LA); B: regression space (IP); C: loss function (LF); D: centerness re-weighting [28] (DF and LA). Details are explained in the first part of Sec. 3.2. RetinaNet A-B 43.6 18.6 33.1 49.3 32.2 RetinaNet A-C 46.2 21.0 35.8 51.6 33.7 RetinaNet A-D 48.8 22.4 37.9 54.2 37.4 in Sec. 2 and shown in</figDesc><table><row><cell>baseline</cell><cell>w/</cell><cell>all</cell><cell>COCO + mAR XS S</cell><cell>R</cell><cell>COCO mAP</cell></row><row><cell cols="2">RetinaNet -</cell><cell cols="4">48.4 23.4 37.8 53.9 36.4</cell></row><row><cell>FCOS</cell><cell cols="5">A-D 48.7 23.2 37.9 54.4 37.6</cell></row><row><cell cols="2">RetinaNet A</cell><cell cols="4">37.2 15.0 26.6 42.3 30.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Experiments on different box representations. E: nearest label assign (LA), F: 2-point box representation (DF), G: 9-point box representation[36] (DF), H: feature adaptation (FA). Details are explained in later half of Sec. 3.2.</figDesc><table><row><cell>baseline</cell><cell>w/</cell><cell>all</cell><cell>COCO + mAR XS S</cell><cell>R</cell><cell>COCO mAP</cell></row><row><cell cols="2">RetinaNet -</cell><cell cols="4">48.4 23.4 37.8 53.9 36.4</cell></row><row><cell cols="6">RepPoints E-H 47.0 26.2 38.1 51.0 38.4</cell></row><row><cell cols="2">RetinaNet E</cell><cell cols="4">46.6 20.7 35.7 51.6 33.5</cell></row><row><cell cols="2">RetinaNet E-F</cell><cell cols="4">42.5 22.5 33.2 46.9 32.1</cell></row><row><cell cols="6">RetinaNet E-G 42.2 19.6 33.0 46.6 32.1</cell></row><row><cell cols="6">RetinaNet E-H 46.5 25.3 37.4 50.7 38.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on feature adaption. Modifications I to L are introduced in Sec. 4.2, and the original and our improved versions of the mentioned methods are marked with * and †, respectively. The mAP and mAR (XS) are evaluated on COCO and COCO + , where XS stands for extra slender objects.</figDesc><table><row><cell>#</cell><cell>baseline</cell><cell>modules</cell><cell>mAP</cell><cell>mAR(XS)</cell></row><row><cell>M1</cell><cell cols="2">RepPoints I+L</cell><cell>37.9</cell><cell>24.9</cell></row><row><cell>M2  *</cell><cell cols="2">RepPoints I+J+L</cell><cell>38.4</cell><cell>25.1</cell></row><row><cell>M3</cell><cell cols="2">RepPoints I+J</cell><cell>38.1</cell><cell>27.6</cell></row><row><cell>M4  †</cell><cell cols="2">RepPoints I+K</cell><cell>39.4</cell><cell>28.4</cell></row><row><cell>M5  *</cell><cell>FCOS</cell><cell>-</cell><cell>37.6</cell><cell>23.2</cell></row><row><cell>M6</cell><cell>FCOS</cell><cell>I</cell><cell>37.7</cell><cell>23.9</cell></row><row><cell>M7</cell><cell>FCOS</cell><cell>I+J</cell><cell>39.1</cell><cell>24.6</cell></row><row><cell>M8  †</cell><cell>FCOS</cell><cell>I+K</cell><cell>39.0</cell><cell>24.5</cell></row><row><cell>M9  *</cell><cell cols="2">RetinaNet -</cell><cell>37.4</cell><cell>23.4</cell></row><row><cell>M10</cell><cell cols="2">RetinaNet I</cell><cell>38.4</cell><cell>24.2</cell></row><row><cell>M11</cell><cell cols="2">RetinaNet I+J</cell><cell>39.1</cell><cell>24.3</cell></row><row><cell cols="3">M12  † RetinaNet I+K</cell><cell>39.4</cell><cell>24.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Lookup table for our model assessment.</figDesc><table><row><cell cols="2"># Stage</cell><cell>Description</cell><cell>Reference</cell></row><row><cell cols="2">A LA</cell><cell cols="2">Assign in-object locations as positive FCOS [28]</cell></row><row><cell cols="2">B IP</cell><cell>Regression from object center</cell><cell>FCOS [28]</cell></row><row><cell cols="2">C LF</cell><cell>IoU Los</cell><cell>IoUNet [13], FCOS [28]</cell></row><row><cell cols="2">D LA, DF</cell><cell>Center-prior</cell><cell>FCOS [28]</cell></row><row><cell cols="2">E LA</cell><cell>Assign object center as positive</cell><cell>RepPoints [36]</cell></row><row><cell cols="2">F IP</cell><cell>2-point box representation</cell><cell>RetinaNet [19], RepPoints [36]</cell></row><row><cell cols="2">G IP</cell><cell>9-point box representation</cell><cell>RepPoints [36]</cell></row><row><cell cols="2">H FA</cell><cell>Supervised feature adaptation</cell><cell>RepPoints [36]</cell></row><row><cell>I</cell><cell>FA</cell><cell>Initial localization prediction</cell><cell>RepPoints [36]</cell></row><row><cell>J</cell><cell>FA</cell><cell>Supervised DCN</cell><cell>RepPoints [36]</cell></row><row><cell cols="2">K FA</cell><cell>Self-adaptation of features</cell><cell>-</cell></row><row><cell cols="2">L IP</cell><cell>Residual localization prediction</cell><cell>RepPoints [36]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Rotated box evaluation on validation datasets. IoU is now mesured between rotated bounding boxes to match prediction and ground truth.</figDesc><table><row><cell>backbone</cell><cell>all</cell><cell cols="2">COCO + RBox mAR XS S</cell><cell>R</cell><cell>RBox mAP</cell></row><row><cell>ResNet-50</cell><cell>37.5</cell><cell>18.3</cell><cell>27.6</cell><cell>41.5</cell><cell>27.7</cell></row><row><cell cols="2">ResNet-101 38.9</cell><cell>19.4</cell><cell>29.6</cell><cell>42.6</cell><cell>29.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Validation of the slenderness prior. FA is module K presented in the paper, and SP is short for slenderness prior.</figDesc><table><row><cell>baseline FA SP</cell><cell>all</cell><cell>COCO + mAR XS S</cell><cell>R</cell><cell>COCO mAP</cell></row><row><cell>FCOS</cell><cell cols="4">48.9 23.9 38.6 54.5 37.7</cell></row><row><cell>FCOS</cell><cell cols="4">49.4 24.2 39.0 55.1 39.0</cell></row><row><cell>FCOS</cell><cell cols="4">49.7 26.3 40.0 54.2 38.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sod-mtgan: Small object detection via multi-task generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dense object nets: Learning dense visual object descriptors by and for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Peter R Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="373" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augfpn: Improving multi-scale feature learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12595" to="12604" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yodsawalai</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qieyun</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of deep learningbased object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="128837" to="128868" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>C. Cortes, N. D. Lawrence, D. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Convolutional neural network based automatic object detection on aerial images. IEEE geoscience and remote sensing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksej</forename><surname>Igorševo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avramović</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="740" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8430" to="8439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection of floating plastics from satellite and unmanned aerial systems (plastic litter project 2018)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Topouzelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apostolos</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shungudzemwoyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="175" to="183" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anchor box optimization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1286" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Objects detection for remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scale-transferrable object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11172</idno>
		<title level="m">Learning data augmentation strategies for object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
