<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Light CNN for Deep Face Representation with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20171">AUGUST 2017 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">A Light CNN for Deep Face Representation with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20171">AUGUST 2017 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Convolutional Neural Network, Face Recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on https://github.com/AlfredXiangWu/LightCNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the last decade, convolutional neural network (CNN) has become one of the most popular techniques for solving computer vision problems. Numerous vision tasks, such as image classification <ref type="bibr" target="#b0">[1]</ref>, object detection <ref type="bibr" target="#b1">[2]</ref>, face recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, have benefited from the robust and discriminative representation learned via CNNs. As a result, their performances have been greatly improved, for example, the accuracy on the challenging labeled faces in the wild (LFW) benchmark has been improved from 97% <ref type="bibr" target="#b4">[5]</ref> to 99% <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b3">[4]</ref>. This improvement mainly benefits from which CNN can learn robust face embedding from the training data with lots of subjects. To achieve optimal accuracy, the scale of the training dataset for CNN has been consistently increasing. Some large-scale face datasets have been released such as CASIA-WebFace <ref type="bibr" target="#b5">[6]</ref>, CelebFaces+ <ref type="bibr" target="#b3">[4]</ref>, VGG face dataset <ref type="bibr" target="#b6">[7]</ref>, UMDFace <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, MS-Celeb-1M <ref type="bibr" target="#b10">[11]</ref> and VGGFace2 dataset <ref type="bibr" target="#b11">[12]</ref>. However, these large-scale datasets often contain massive noisy signals especially when they are automatically collected via image search engines or from movies. This paper studies a Light CNN framework to learn a deep face representation from the large-scale data with massive noisy labels. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we define a Max-Feature-Map (MFM) operation to obtain a compact representation and perform feature filter selection. MFM is an alternative of ReLU to suppress low-activation neurons in each layer, so that it can be considered as a special implementation of maxout activation <ref type="bibr" target="#b12">[13]</ref> to separate noisy signals and informative signals. Our Light CNN architecture includes MFM, small convolution filters and Network in Network, and is trained on the MS-Celeb-1M dataset. To handle noisy labeled images, we propose a semantic bootstrapping method to automatically re-label * Zhenan Sun is the corresponding author. X. Wu, R. He, Z. <ref type="bibr">Sun</ref>   training data via pre-trained deep networks. We assume that by given similar percepts, the model is capable of giving same predictions consistently. Intuitively, too much skepticism of the original training label may lead to a wrong relabeling. Hence, it is important to balance the trade-off between the prediction and original labels. Extensive experimental evaluations demonstrate that the proposed Light CNN achieves stateof-the-art results on five face benchmarks without supervised fine-tuning. The contributions are summarized as follows: 1) This paper introduces MFM operation, a special case of maxout to learn a Light CNN with a small number of parameters. Compared to ReLU whose threshold is learned from training data, MFM adopts a competitive relationship so that it has better generalization ability and is applicable on different data distributions. 2) The Light CNNs based on MFM are designed to learn a universal face representation. We propose three Light arXiv:1511.02683v4 [cs.CV] 12 Aug 2018 CNN models following the ideas of AlexNet, VGG and ResNet, respectively. The proposed models lead to better performance in terms of speed and storage space. 3) A semantic bootstrapping method via a pretrained deep network is proposed to handle noisy labeled images in a large-scale dataset. Inconsistent labels can be effectively detected by the probabilities of predictions, and then relabeled or removed for training. 4) The proposed single model with a 256-D representation obtains state-of-the-art results on various face benchmarks, i.e., large-scale, video-based, cross-age face recognition, heterogenous and cross-view face recognition datasets. The models contain fewer parameters and extract a face representation faster than other open source face models. The paper is organized as follows. In Section II, we briefly review some related work on face recognition and noisy label problems. Section III describes the proposed Light CNN framework and the semantic bootstrapping method. Finally, we present experimental results in Section IV and conclude this paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CNN based Face Recognition</head><p>Modern face recognition methods often regard CNNs as robust feature extractors. Earlier DeepFace <ref type="bibr" target="#b4">[5]</ref> trains CNN on 4.4M face images and uses CNN as a feature extractor for face verification. It achieves 97.35% accuracy on LFW with a 4096-D feature vector. As an extension of DeepFace, Taigman et al. <ref type="bibr" target="#b13">[14]</ref> apply a semantic bootstrapping method to select an efficient training set from a large dataset. Besides, it also discusses more stable protocols <ref type="bibr" target="#b14">[15]</ref> of LFW, indicating the robustness of face features more representatively. To further improve the accuracy, Sun et al. <ref type="bibr" target="#b3">[4]</ref> resorts to a multi-patch ensemble model. An ensemble of 25 CNN models is trained on different local patches and Joint Bayesian is applied to obtain a robust embedding space. In <ref type="bibr" target="#b15">[16]</ref>, verification loss and classification loss are further combined to increase interclass distance and decrease intra-class distance. The ensemble model obtains 99.47% on LFW.</p><p>Then, triplet loss is introduced into face recognition by FaceNet <ref type="bibr" target="#b7">[8]</ref>. FaceNet is trained on about 100-200M face images with 8M face identities in total. Since the selection of triplet pairs is important to achieve satisfying accuracy, FaceNet presents an online triplet mining method for training triplet-based CNN and achieves good accuracy (99.63%). Parkhi et al. <ref type="bibr" target="#b6">[7]</ref> train a VGG network <ref type="bibr" target="#b16">[17]</ref> on 2622 identities of 2.6M images collected from Internet and then fine-tune the model by a triplet-based metric learning method like FaceNet, which the accuracy achieves 98.95% on LFW. Tran et al. <ref type="bibr" target="#b17">[18]</ref> also propose a domain specific data augmentation to increase training data and obtains comparable performance on LFW.</p><p>The performance improvement of face recognition mainly benefits from CNN and large-scale face datasets. However, large-scale datasets often contain massive noisy labels especially when they are automatically collected from internet. Therefore, learning a Light CNN from the large-scale face data with massive noisy labels is of great significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Noisy Label Problems</head><p>Noisy label is an important issue in machine learning when datasets tend to be large. Some methods <ref type="bibr" target="#b18">[19]</ref> have been devoted to deal with noisy label problems. These methods can generally be classified into three categories. In the first category, robust loss <ref type="bibr" target="#b19">[20]</ref> is designed for classification tasks, which means the learned classification models are robust to the presence of label noise. The second category <ref type="bibr" target="#b20">[21]</ref> aims to improve the quality of training data by identifying mislabeled instances. The third category <ref type="bibr" target="#b21">[22]</ref> directly models the distribution of noisy labels during learning. The advantage of this category is that it allows using information about noisy labels during learning.</p><p>Recently, learning with noisy labeled data also draws much attention in deep learning, because deep learning is a datadriven approach and accurate label annotation is quite expensive. Mnih and Hinton <ref type="bibr" target="#b22">[23]</ref> introduce two robust loss functions for noisy label aerial images. However, their method is only applicable for binary classification. Sukhbaatar et al. <ref type="bibr" target="#b23">[24]</ref> consider multi-class classification for modeling class dependent noise distribution. They propose a bottom-up noise model to change the label probabilities output for back-propagation and a top-down model to do so given noisy labels before feeding data. Moreover, with the notion of perceptual consistency, the work of <ref type="bibr" target="#b24">[25]</ref> extends the softmax loss function by weakly supervised training. The idea is to dynamically update the targets of the prediction objective function based on the current model. They use a simple convex combination of training labels and the predictions of a current model to generate training targets. Although some strategies have been studied for noisy label problem, massive noisy label is still an open issue for deep learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ARCHITECTURE</head><p>In this section, we first propose Max-Feature-Map operation for CNN to simulate neural inhibition, resulting in a new Light CNN framework for face analysis and recognition. Then, a semantic bootstrapping method for noisy labeled training dataset is addressed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Max-Feature-Map Operation</head><p>A large-scale face dataset often contains noisy signals. If the errors incurred by these noisy signals are not properly handled, CNN will learn a biased result. Rectified Linear Unit (ReLU) <ref type="bibr" target="#b25">[26]</ref> activation separates noisy signals and informative signals by a threshold (or bias) to determine the activation of one neuron. If the neuron is not active, its output value will be 0. However, this threshold might lead to the loss of some information especially for the first several convolution layers, because these layers are similar to Gabor filters (i.e., both positive and negative responses are respected). To alleviate this problem, Leaky Rectified Linear Units (LReLU) <ref type="bibr" target="#b26">[27]</ref>, Parametric Rectified Linear Units (PReLU) <ref type="bibr" target="#b27">[28]</ref> and Exponential Linear Units (ELU) <ref type="bibr" target="#b28">[29]</ref> have been proposed.</p><p>In neural science, lateral inhibition (LI) <ref type="bibr" target="#b29">[30]</ref> increases the contrast and sharpness in visual or audio response and aids the mammalian brain in perceiving contrast within an image. Taking visual LI as an example, if an excitatory neural signal is released to horizontal cells, the horizontal cells will send an inhibitory signal to its neighboring or related cells. This inhibition produced by horizontal cells creates a more concentrated and balanced signal to cerebral cortex. As for auditory LI, if certain sound frequencies create a greater contribute to inhibition than excitation, tinnitus can be suppressed. Considering LI and noisy signals, we expect the activation function in one convolution layer to have the following characteristics:</p><p>1) Since large-scale dataset often contains various types of noise, we expect that noisy signals and informative signals can be separated. 2) When there is a horizontal edge or line in an image, the neuron corresponding to horizontal information is excited whereas the neuron corresponding to vertical information is inhibited.</p><p>3) The inhibition of one neuron is free of parameters so that it does not depend on training data extensively. To achieve the above characteristics, we propose the Max-Feature-Map (MFM) operation, which is an extension of Maxout activation <ref type="bibr" target="#b12">[13]</ref>. However, the basic motivation of MFM and Maxout are different. Maxout aims to approximate an arbitrary convex function via enough hidden neurons. More neurons are used, better approximation results are obtained. Generally, the scale of a Maxout network is larger than that of a ReLU network. MFM resorts to max function to suppress the activations of a small number of neurons so that MFM based CNN models are light and robust. Although MFM and Maxout all use a max function for neuron activation, MFM cannot be treated as a convex function approximation. We define two types of MFM operations to obtain competitive feature maps.</p><p>Given an input convolution layer x n ∈ R H×W , where n = {1, ..., 2N }, W and H denote the spatial width and height of the feature map. The MFM 2/1 operation which combines two feature maps and outputs element-wise maximum one as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(c) can be written aŝ</p><formula xml:id="formula_0">x k ij = max(x k ij , x k+N ij )<label>(1)</label></formula><p>where the channel of the input convolution layer is</p><formula xml:id="formula_1">2N , 1 ≤ k ≤ N, 1 ≤ i ≤ H, 1 ≤ j ≤ W . As is shown in Eq. (1), the outputx via MFM operation is in R H×W ×N .</formula><p>The gradient of Eq. (1) takes the following form,</p><formula xml:id="formula_2">∂x k ij ∂x k ij = 1, if x k ij ≥ x k+N ij 0, otherwise ∂x k ij ∂x k+N ij = 0, if x k ij ≥ x k+N ij 1, otherwise<label>(2)</label></formula><p>By using MFM 2/1, we generally obtain 50% informative neurons from input feature maps via the element-wise maximum operation across feature channels. Furthermore, as shown in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>, to obtain more comparable feature maps, the MFM 3/2 operation, which inputs three feature maps and removes the minimal one element-wise, can be defined as:</p><formula xml:id="formula_3">  x k1 ij = max(x k ij , x k+N ij , x k+2N ij ) x k2 ij = median(x k ij , x k+N ij , x k+2N ij )<label>(3)</label></formula><p>where x n ∈ R H×W , 1 ≤ n ≤ 3N, 1 ≤ k ≤ N and median(·) is the median value of input feature maps. The gradient of MFM 3/2 is similar to Eq. (2), in which the value of gradient is 1 when the feature map x k ij is activated, and it is set to be 0 otherwise. In this way, we select and reserve 2/3 information from input feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Light CNN Framework</head><p>In the context of CNN, MFM operation plays a similar role to local feature selection in biometrics. MFM selects the optimal feature at each location learned by different filters. It results in binary gradient (1 and 0) to excite or suppress one neuron during back propagation. The binary gradient plays a similar role of famous ordinal measure <ref type="bibr" target="#b30">[31]</ref> which is widely used in biometrics.</p><p>A CNN with MFM can obtain a compact representation while the gradients of MFM layers are sparse. Due to the sparse gradient of MFM, on the one hand, when doing back propagation in training stage, stochastic gradient descent (SGD) can only make effects on the neuron of response variables; on the other hand, when extracting features for testing, MFM can obtain more competitive nodes from previous convolution layers by activating the maximum of two feature maps. These observations demonstrate the valuable properties of MFM, i.e., MFM could perform feature selection and facilitate to generate sparse connections.</p><p>In this section, we discuss three architectures for our Light CNN framework. The first one is constructed by 4 convolution layers with Max-Feature-Map operations and 4 max-pooling layers like Alexnet <ref type="bibr" target="#b31">[32]</ref> (as shown in <ref type="table" target="#tab_1">Table I</ref>). It contains about 4,095K parameters and 1.5G FLOPS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Filter Size /Stride Output Size #Params</p><formula xml:id="formula_4">Conv1 9 × 9/1 120 × 120 × 96 7.7K MFM1 - 120 × 120 × 48 - Pool1 2 × 2/2 60 × 60 × 48 - Conv2 5 × 5/1 56 × 56 × 192 230.4K MFM2 - 56 × 56 × 96 - Pool2 2 × 2/2 28 × 28 × 96 - Conv3 5 × 5/1 24 × 24 × 256 614K MFM3 - 24 × 24 × 128 - Pool3 2 × 2/2 12 × 12 × 128 - Conv4 4 × 4/1 9 × 9 × 384 786K MFM4 - 9 × 9 × 192 - Pool4 2 × 2/2 5 × 5 × 192 - fc1 - 512 2,457K MFM fc1 - 256 - Total - - 4,095K</formula><p>Since Network in Network (NIN) <ref type="bibr" target="#b32">[33]</ref> can potentially do feature selection between convolution layers and the number of parameters can be reduced by using small convolution kernels like in VGG <ref type="bibr" target="#b16">[17]</ref>, we integrate NIN and a small convolution kernel size into the network with MFM. The constructed 9layer Light CNN contains 5 convolution layers, 4 Network in Network (NIN) layers, Max-Feature-Map layers and 4 maxpooling layers as shown in <ref type="table" target="#tab_1">Table II</ref>. The Light CNN-9 contains about 5,556K parameters and 1G FLOPS in total, which is deeper and faster than the Light CNN-4 model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Filter Size /Stride, Pad Output Size #Params</p><formula xml:id="formula_5">Conv1 5 × 5/1, 2 128 × 128 × 96 2.4K MFM1 - 128 × 128 × 48 - Pool1 2 × 2/2 64 × 64 × 48 - Conv2a 1 × 1/1 64 × 64 × 96 4.6K MFM2a - 64 × 64 × 48 - Conv2 3 × 3/1, 1 64 × 64 × 192 165K MFM2 - 64 × 64 × 96 - Pool2 2 × 2/2 32 × 32 × 96 - Conv3a 1 × 1/1 32 × 32 × 192 18K MFM3a - 32 × 32 × 96 - Conv3 3 × 3/1, 1 32 × 32 × 384 331K MFM3 - 32 × 32 × 192 - Pool3 2 × 2/2 16 × 16 × 192 - Conv4a 1 × 1/1 16 × 16 × 384 73K MFM4a - 16 × 16 × 192 - Conv4 3 × 3/1, 1 16 × 16 × 256 442K MFM4 - 16 × 16 × 128 - Conv5a 1 × 1/1 16 × 16 × 256 32K MFM5a - 16 × 16 × 128 - Conv5 3 × 3/1, 1 16 × 16 × 256 294K MFM5 - 16 × 16 × 128 - Pool4 2 × 2/2 8 × 8 × 128 - fc1 - 512 4,194K MFM fc1 - 256 - Total - - 5,556K</formula><p>With the development of residual networks <ref type="bibr" target="#b0">[1]</ref>, the very deep convolution neural networks are widely used and often obtain high performance in various computer vision tasks. We also introduce the idea of residual blocks to Light CNN and design a 29-layer convolution network for face recognition. The residual block contains two 3 × 3 convolution layers and two MFM operations without batch normalization. There are 12,637K parameters and about 3.9G FLOPS in Light CNN-29. The details of Light CNN-29 are presented in <ref type="table" target="#tab_1">Table III</ref>.</p><p>Note that there are some differences between the proposed residual block with MFM operations and the original residual block <ref type="bibr" target="#b0">[1]</ref>. On the one hand, we remove batch normalization from the original residual block. Although batch normalization is efficient to accelerate the convergence of training and avoid overfitting, in practice, batch normalization are domain specific which may be failed when test samples come from differen domains compared with training data. Besides, batch statistics may diminish when the size of training minibatches are small.</p><p>On the other hand, we employ the fully connected layer instead of the global average pooling layer on the top. In our training scheme, input images are all aligned, so that each node for high-level feature maps contains both semantic and spatial information which may be damaged by the global average pooling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Filter Size /Stride, Pad Output Size #Params</p><formula xml:id="formula_6">Conv1 5 × 5/1, 2 128 × 128 × 96 2.4K MFM1 - 128 × 128 × 48 - Pool1 2 × 2/2 64 × 64 × 48 - Conv2 x 3 × 3/1, 1 3 × 3/1, 1 × 1 64 × 64 × 48 82K Conv2a 1 × 1/1 64 × 64 × 96 4.6K MFM2a - 64 × 64 × 48 - Conv2 3 × 3/1, 1 64 × 64 × 192 165K MFM2 - 64 × 64 × 96 - Pool2 2 × 2/2 32 × 32 × 96 - Conv3 x 3 × 3/1, 1 3 × 3/1, 1 × 2 32 × 32 × 96 662K Conv3a 1 × 1/1 32 × 32 × 192 18K MFM3a - 32 × 32 × 96 - Conv3 3 × 3/1, 1 32 × 32 × 384 331K MFM3 - 32 × 32 × 192 - Pool3 2 × 2/2 16 × 16 × 192 - Conv4 x 3 × 3/1, 1 3 × 3/1, 1 × 3 16 × 16 × 192 3981K Conv4a 1 × 1/1 16 × 16 × 384 73K MFM4a - 16 × 16 × 192 - Conv4 3 × 3/1, 1 16 × 16 × 256 442K MFM4 - 16 × 16 × 128 - Conv5 x 3 × 3/1, 1 3 × 3/1, 1 × 4 16 × 16 × 128 2356K Conv5a 1 × 1/1 16 × 16 × 256 32K MFM5a - 16 × 16 × 128 - Conv5 3 × 3/1, 1 16 × 16 × 256 294K MFM5 - 16 × 16 × 128 - Pool4 2 × 2/2 8 × 8 × 128 - fc1 - 512 4,194K MFM fc1 - 256 - Total - - 12,637K</formula><p>C. Semantic Bootstrapping for Noisy Labels Bootstrapping, also called "self-training", provides a simple and effective approach for sample distribution estimation. Its basic idea is that the forward process on a training sample can be modeled by re-sampling and performing inference from original labeled samples to relabeled ones. It can estimate standard errors and confidence intervals for a complex data distribution and is also appropriate to control the stability of the estimation. Let x ∈ X and t denote data and labels, respectively. The CNN based on softmax loss function regresses x onto t, and the distribution of its predictions can be represented as a conditional probability p(t|f (x)), i p(t i |f (x)) = 1. The maximum probability p(t i |f (x)) determines the most convincing prediction label. Based on the above theories, we propose a semantic bootstrapping method to sample the training data from the large dataset with massive noisy labels. Firstly, we train the Light CNN-9 model on CASIA-WebFace and fine-tune it on the original noisy labeled MS-Celeb-1M dataset. Second, we employ the trained model to relabel the noisy labeled dataset according to the conditional probabilities p(t i |f (x)). And then we retrain Light CNN-9 on the relabeled dataset. Finally, we further re-sample the original noisy labeled dataset by the retrained model and construct the cleaned MS-Celeb-1M dataset. The details and discussions for bootstrapping the MS-Celeb-1M dataset are shown in Section IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate our Light CNN models on various face recognition tasks. We first introduce the training methodology and databases, and then present the comparison with state-of-the-art face recognition methods, as well as algorithmic analysis and detailed evaluation. Finally, we discuss the effectiveness of the semantic bootstrapping method for selecting training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Methodology and Preprocessing</head><p>To train the Light CNN, we randomly select one face image from each identity as the validation set and the remaining images as the training set. The open source deep learning framework Caffe <ref type="bibr" target="#b33">[34]</ref> is employed to implement the CNN model <ref type="bibr" target="#b0">1</ref> . Dropout is used for fully connected layers and the ratio is set to 0.7. The momentum is set to 0.9, and the weight decay is set to 5 × 10 −4 for convolution layers and a fullyconnected layer except the fc2 layer. Obviously, the fc1 fullyconnected layer contains the face representation that can be used for face verification. Note that the fc2, which contains large number of parameters with the increasing of identities in the training dataset, are not used for feature extraction. Thus we increase the weight decay of fc2 layer to 5×10 −3 to avoid overfitting. The learning rate is set to 1 × 10 −3 initially and reduced to 5×10 −5 gradually. The parameter initialization for convolutional layers and fully-connected layers is Xavier and Gaussian, respectively.</p><p>The CASIA-WebFace and MS-Celeb-1M datasets are used for training in the experiments. To alleviate the influence of large illumination discrepancy, we use gray-scale face images instead of RGB images for training and testing. When training, the face images are aligned to 144×144 by the five landmarks <ref type="bibr" target="#b34">[35]</ref> (as shown in <ref type="figure" target="#fig_1">Fig 2)</ref> and then randomly cropped to 128 × 128 as inputs. Besides, each pixel (ranged between [0, 255]) is dividing by 255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Testing Protocols</head><p>Four types of face data are used to systematically evaluate the performance of the proposed Light CNN. These databases correspond to large-scale, low-resolution and heterogeneous face recognition (or verification) tasks, respectively. Note that we do not re-train or fine-tune the Light CNN model on any testing database. That is, the training sets of all five testing databases are excluded for model training and finetuning. We directly extract features and compute the similarity of these features measured in cosine similarity.</p><p>The first type is the commonly used LFW dataset <ref type="bibr" target="#b37">[38]</ref> that contains 13,233 images of 5,749 people. It contains three protocols as follows:</p><p>* For the standard verification protocol <ref type="bibr" target="#b37">[38]</ref>, all face images are divided into 10 folds, each of which contains 600 face pairs without identity overlap. * For the probe-gallery identification testing <ref type="bibr" target="#b14">[15]</ref>  <ref type="bibr" target="#b39">[40]</ref>, YouTube Celebrities (YTC) <ref type="bibr" target="#b40">[41]</ref> and Celebrity-1000 <ref type="bibr" target="#b41">[42]</ref> that are widely used to evaluate the performance of video-based face recognition methods.</p><p>* The YTF dataset contains 3,425 videos of 1,595 subjects. Due to low resolution and motion blur, the quality of images in the YTF dataset is worse than in LFW. As for the evaluation protocol, the database is divided into 10 splits, each includes 250 positive pairs and 250 negative ones. As in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we randomly select 100 samples from each video and compute average similarities.  * MegaFace aims at the evaluation of face recognition algorithms at million-scale. It includes probe and gallery set. The probe set is FaceScrub <ref type="bibr" target="#b45">[46]</ref>, which contains 100K images of 530 identities, and the gallery set consists of about 1 million images from 690K different subjects. * The IJB-A dataset contains 5,712 images and 2,085 videos from 500 identities. All the images and videos are evaluated with two standard protocols, namely, 1:1 face verification and 1:N face identification. * The IJB-B dataset is an extension of IJB-A, which contains 11,754 images and 7,011 videos from 1,845 identities. In this paper, we evaluate the Light CNN models on the mixed media (frames and stills) 1:1 verification protocol and open set 1:N protocol using mixed media (frames, stills) as probe. The fourth type is the cross-domain databases, including CACD-VS <ref type="bibr" target="#b46">[47]</ref>, Multi-PIE <ref type="bibr" target="#b47">[48]</ref> and the CASIA NIR-VIS 2.0 database <ref type="bibr" target="#b48">[49]</ref>.</p><p>* The CACD-VS dataset <ref type="bibr" target="#b46">[47]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>FAR=0.1% DIR@FAR=1% HighDimLBP <ref type="bibr" target="#b38">[39]</ref> 41. <ref type="bibr" target="#b65">66</ref> 18.07 WebFace <ref type="bibr" target="#b5">[6]</ref> 80. <ref type="bibr" target="#b25">26</ref> 28.90 CenterLoss <ref type="bibr" target="#b36">[37]</ref> 93. <ref type="bibr" target="#b34">35</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Method Comparison</head><p>In this subsection, we train three Light CNN models with MFM 2/1 by the cleaned MS-Celeb-1M dataset. The cleaned MS-Celeb-1M dataset contains 79,077 identities totally about 5,049,824 images that are selected by the proposed semantic bootstrapping method. The details of bootstrapping the dirty MS-Celeb-1M dataset is presented in Section IV-E. To certificate the effectiveness of three architectures of the Light CNN, we evaluate our architectures on four different types of face images, including face in the wild, video-based face dataset, large-scale face dataset and cross-domain face dataset. The 256-D deep features are extracted from the output of fully connected layer after MFM operation (MFM fc1). The similarity scores are computed by cosine distance and the results of three Light CNN architectures, denoted as Light CNN-4, Light CNN-9 and Light CNN-29, respectively, are shown in tables IV-VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Labeled Face in the Wild Database</head><p>On the LFW database (as shown in <ref type="table" target="#tab_1">Table IV</ref>), we evaluate our 4-layer, 9-layer and 29-layer Light CNN models with unsupervised setting, which means our models are not trained or fine-tuned on LFW in a supervised way. The results of our models on the LFW verification protocol are better than those of DeepFace <ref type="bibr" target="#b4">[5]</ref>, DeepID2+ <ref type="bibr" target="#b15">[16]</ref>, WebFace <ref type="bibr" target="#b5">[6]</ref>, VGG <ref type="bibr" target="#b6">[7]</ref>, CenterLoss <ref type="bibr" target="#b36">[37]</ref> and SeetaFace <ref type="bibr" target="#b35">[36]</ref> for a single net. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rank-1 Accuracy (%) LMKML <ref type="bibr" target="#b54">[55]</ref> 78.20 MMDML <ref type="bibr" target="#b55">[56]</ref> 78.50 MSSRC <ref type="bibr" target="#b56">[57]</ref> 80.75 SFSR <ref type="bibr" target="#b57">[58]</ref> 85.74 RRNN <ref type="bibr" target="#b58">[59]</ref> 86.60 CRG <ref type="bibr" target="#b59">[60]</ref> 86.70 VGG <ref type="bibr" target="#b6">[7]</ref> 93  Although several business methods have achieved ultimate accuracy on 6000-pairs face verification task, a more practical criterion may be the verification rate at the extremely low false acceptance rate (eg., VR@FAR=0). We achieve 97.50% at VR@FAR=0 for Light CNN-29, while other methods' results are lower than 70%. Moreover, open-set identification rate at a low false acceptance rate is even more challenging but meaningful in real applications. As shown in <ref type="table" target="#tab_1">Table IV</ref> On the BLUFR protocols (as shown in <ref type="table">Table.</ref> V), Light CNN obtains 98.88% on TPR@FAR=0.1% for face verification and 92.29% on DIR@FAR=1% for open-set identification, which also obtains better results compared to other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Video-based Face Recognition Databases</head><p>Due to low resolution and motion blur, the quality of images in YTF is worse than in LFW. The Light CNN-29 obtains 95.54% without fine-tuning on YTF by using a single model, which outperforms other state-of-the-art methods, such as DeepFace, DeepID2+, WebFace, FaceNet, SeetaFace, VGG and CenterLoss.</p><p>As is shown in  <ref type="bibr" target="#b55">[56]</ref>, Mean Sequence Sparse Representation-based Classification (MSSRC) <ref type="bibr" target="#b56">[57]</ref>, Simultaneous Feature and Sample Reduction (SFSR) <ref type="bibr" target="#b57">[58]</ref>, Recurrent Regression Neural Network (RRNN) <ref type="bibr" target="#b58">[59]</ref>, Covariate-Relation Graph (CRG) <ref type="bibr" target="#b59">[60]</ref> and VGG <ref type="bibr" target="#b6">[7]</ref>. Obviously, the Light CNN-29 obtains 94.18% rank-1 accuracy that is superior to other state-of-the-art methods.</p><p>We further evaluate the performance of the Light CNN models on Celebrity-100. The main competitors are Multi- Method Accuracy (%) HD-LBP <ref type="bibr" target="#b63">[64]</ref> 81.60 HFA <ref type="bibr" target="#b64">[65]</ref> 84.40 CARC <ref type="bibr" target="#b46">[47]</ref> 87.60 VGG <ref type="bibr" target="#b6">[7]</ref> 96.00 SeetaFace <ref type="bibr" target="#b35">[36]</ref> 95 task Joint Sparse Representation (MTJSR) <ref type="bibr" target="#b50">[51]</ref>, Eigen Probabilistic Elastic Part (Eigen-PEP) <ref type="bibr" target="#b52">[53]</ref>, Deep Extreme Learning Machines (DELM) <ref type="bibr" target="#b51">[52]</ref> and Neural Aggregation Network (NAN) <ref type="bibr" target="#b53">[54]</ref>. In Table VI(a), the Light CNN-29 outperforms its competitors such as MTJSR <ref type="bibr" target="#b50">[51]</ref>, DELM <ref type="bibr" target="#b51">[52]</ref>, Eigen-RER <ref type="bibr" target="#b52">[53]</ref> and GoogleNet+AvePool <ref type="bibr" target="#b53">[54]</ref> on both close-set and open-set protocols. The performance of the Light CNN-29 is worse than that of GoogleNet+NAN <ref type="bibr" target="#b53">[54]</ref>. This is because the Light CNN models are not trained on Celebrity-1000 and we only employ average pooling along each feature dimension for aggregation as described in <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Large-Scale Face Recognition Databases</head><p>On the challenging MegaFace database (as shown in Table VII(a)), we compare the Light CNNs against FaceNet <ref type="bibr" target="#b7">[8]</ref>, NTechLAB, CenterLoss <ref type="bibr" target="#b36">[37]</ref>, Beijing Faceall Co., Barebones FR and 3DiVi Company. The 29-layer Light CNN achieves 73.75% on rank-1 accuracy and 85.13% on VR@FAR=10 −6 which outperforms Barebones FR, Center-Loss, Beijing Faceall Co. and 3DiVi Company. Besides, the Light CNNs obtain satisfying results compared with some commercial face recognition systems such as Google FaceNet and NTechLAB. Note that they achieve better performance than our models due to the large-scale private training dataset (500M for Google and 18M for NTechLAB) and unknown preprocessing techniques. The CMC and ROC curves are shown in <ref type="table">Table.</ref>VII(b) and <ref type="table">Table.</ref>VII(c), respectively.</p><p>The performance comparison of the Light CNNs with other state-of-the-arts on IJB-A and IJB-B is given in table VII(d). It is observed that the proposed Light CNN-29 obtains comparable results with VGGFace2 <ref type="bibr" target="#b11">[12]</ref> and DA-GAN [63] on both 1:1 verification and 1:N identification tasks on IJB-A. Note that VGGFace2 employs a much more computational complex model (SENet-50) that is trained on MS-Celeb-1M and finetuned on VGGFace2 dataset and DA-GAN synthesizes profile face images as the training data. The proposed Light CNN-29 is only trained on MS-Celeb-1M and obtains significant improvements especially on TAR@FAR=0.001 (93.9% vs 93.0%).</p><p>Besides, the IJB-B dataset is an extension of IJB-A, which contains more subjects and images. Under the mixed media verification and identification protocols, the Light CNN-29 significantly improves the performance on TAR@FAR=1e-4 (87.7% vs 83.1%) and Rank-1 (91.9% vs 90.2%) compared with VGGFace2 <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Cross Domain Face Recognition Databases</head><p>Table VIII(a) shows the results on the CACD-VS dataset. The results of our models on CACD-VS is 98.55% and outperform other age-invariant face recognition algorithms <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref> and two open source models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b35">[36]</ref>. This indicates that our Light CNNs are potentially robust for agevariant problems.</p><p>We also compare the Light CNNs with multi-view face recognition methods <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b67">[68]</ref> and pose-aware face image synthesis <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b68">[69]</ref> in <ref type="table" target="#tab_1">Table VIII</ref> <ref type="bibr">(b)</ref>. It is obvious that the Light CNN-29 obtains great performance on Multi-PIE, where the accuracy on ±60 • is about 95.0%. Note that all the compared methods are trained on Multi-PIE, while the Light CNN models are trained on MS-Celeb-1M where the imaging condition is quite different from Multi-PIE. An extension of our Light CNN model also achieves higher accuracy by rotating faces <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>. The results indicate that the Light CNN framework can efficiently capture the characteristics of different identities and obtain features invariant to pose and illumination for face recognition.</p><p>It is interesting to observe that our Light CNN also performs very well on NIR-VIS dataset. It not only outperforms the three CNN methods but also significantly improves stateof-the-art results such as TRIVET <ref type="bibr" target="#b72">[73]</ref> and IDR <ref type="bibr" target="#b73">[74]</ref> that are trained on the CASIA NIR-VIS 2.0 dataset in a supervised way. We improve the best rank-1 accuracy from 95.82%±0.76% to 96.72%±0.23% and VR@FAR=0.1% is further improved from 94.03%±1.06% to 94.77%±0.43%. The ROC curves compared with other methods are shown in <ref type="table">Table.</ref> VIII(c). Note that, different from the compared methods, Light CNNs are not fine-tuned on the CASIA NIR-VIS 2.0 dataset. Particularly, our Light CNN is also applicable for Mesh and VIS heterogenous face recognition <ref type="bibr" target="#b76">[77]</ref>.</p><p>The improvement made by out Light CNN models may be attributed to the parameter-free activation functions. Obviously, compared with other CNN methods that are not trained on the cross-modal NIR-VIS dataset, our Light CNNs based on MFM operations rely on a competitive relationship rather than a threshold of ReLU so that it is naturally adaptive to different appearances from different modalities. All the experiments suggest that the proposed Light CNNs obtain discriminative face representations and have good generalization ability for various face recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Analysis</head><p>MFM operation plays an important role in our Light CNN models and it would be interesting to take a closer look into it. Hence we give a detail analysis of MFM on the Light CNN-9 model in this subsection.</p><p>First, we compare the performance of MFM 2/1 and MFM 3/2 with ReLU, PReLU and ELU on the LFW database. The basic network is Light CNN-9 as is shown in table II. We simply change activation functions and it is obvious that the output channels of ReLU, PReLU and ELU for each layer are 2× compared with MFM 2/1, and 1.5× compared with MFM 3/2. The experimental results of different activation functions are shown in <ref type="table" target="#tab_1">Table IX</ref>. According to <ref type="bibr" target="#b77">[78]</ref>, wider models should obtain better performance. However, we observe that MFM 2/1 and MFM 3/2 are generally superior to the other three activation functions.</p><p>The reason is that MFM uses a competitive relationship rather than a threshold (or bias) to active a neuron. Since the training and testing sets are from different data sources, MFM has better generalization ability to different sources. Compared with MFM 2/1, MFM 3/2 can further improve performance, indicating that when using MFM, it would be better to keep only a small number of neurons to be inhibited so that more information can be preserved to the next convolution layer. That is, the ratio between input neurons and output neurons should be set to between 1 and 2.</p><p>Second, as shown in <ref type="table" target="#tab_1">Table IV</ref>-VIII in Section IV-C, it is obvious that all the proposed Light CNN-4, Light CNN-9 and Light CNN-29 models obtain good performance on various face recognition datasets. It is shown that MFM operation is suitable for the different general CNN architectures such as AlexNet, VGG and ResNet.  Third, we analyze the performance of residual blocks containing MFM operations on two validation datasets. One is LFW which contains 6,000 pairs for evaluations and the other, denoted as VAL1, contains 1,424 face images of 866 identities, that is, 573 positive pairs and 1,012,603 negative pairs. The images in VAL1 are strictly independent from MS-Celeb-1M, as they do not share the same source of data.</p><p>In <ref type="figure" target="#fig_3">Fig. 3</ref>, we present the performance of different configurations of Light CNN-29. Obviously, the performance of Light CNN-29 with global average pooling on both LFW and VAL1 is lower than that of Light CNN-29, because the global average pooling does not consider the spatial information of each node in high-level feature map.</p><p>In terms of Batch Normalization (BN), as shown in <ref type="figure" target="#fig_3">Fig.  3(a)</ref>, the model with BN achieves comparative performance as the one without BN. However, Light CNN-29 substantially outperforms the one with BN as shown in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. This is because most of the images in LFW are portraits of celebrities collected from Internet, which is similar to how MS-Celeb-1M is constructed. However, the images in VAL1 are obtained offline, so that they are strictly independent from the training dataset used. Obviously, BN diminishes when testing samples are quite independent from the used training datasets, because the means and variances in BN depend on the statistics of the training samples. Based on the above observations, in the Light CNN-29 model, we remove batch normalization and use fully-connected layers instead of global average pooling layers, leading to higher generalization ability.</p><p>In addition, computational efficiency is also a critical aspect in comprehensive evaluation of face recognition systems. Very deep CNNs or multi-patch ensembles are common ways to improve recognition accuracy, while they are often time consuming for practical applications. To verify the computational efficiency of our Light CNNs, we compare our CNN with five widely used models, i.e., FaceNet <ref type="bibr" target="#b7">[8]</ref>, WebFace <ref type="bibr" target="#b5">[6]</ref>, the VGG released model <ref type="bibr" target="#b6">[7]</ref>, the open source SDK SeetaFace <ref type="bibr" target="#b35">[36]</ref> and CenterLoss <ref type="bibr" target="#b36">[37]</ref>. As shown in <ref type="table" target="#tab_17">Table X</ref>, the size of our biggest Light CNN model (Light CNN-29) is 10× smaller than that of the wellknown VGG model, while the CPU time is about 5× faster.</p><p>Compared with the open source face SDK SeetaFace and CenterLoss, Light CNN also performs well in terms of time cost, the number of parameters and feature dimension. Since FaceNet <ref type="bibr" target="#b7">[8]</ref> and WebFace <ref type="bibr" target="#b5">[6]</ref> don't release the models, we report the number of parameters referring to their paper. Obviously, the model of FaceNet <ref type="bibr" target="#b7">[8]</ref> is approximate 10× larger than Light CNN-29. In terms of the time cost, the network of FaceNet contains 1.6B FLOPS, while the Light CNN-29 is 3.9G FLOPS. Considering the model of WebFace <ref type="bibr" target="#b5">[6]</ref>, although the model size is small, its performance has a large margin compared with Light CNN (shown in <ref type="table" target="#tab_1">Table IV and Table  V</ref>). The results indicate that Light CNN is potentially more suitable and practical on embedding devices and smart phones for real-time applications than its competitors. Particularly, MFM can result in a convolutional neural network with a small parameter space and a small feature dimension. If MFM is well studied and carefully designed, the learned CNN can use a smaller parameter space to achieve better recognition accuracy.</p><p>Besides, we implement Light CNNs on MaPU <ref type="bibr" target="#b78">[79]</ref>, a novel architecture which is suitable for data-intensive computing with great power efficiency and sustained computation throughput. The Light CNN-9 for feature extractions are only about 40ms on MaPU, which is implemented by floating-point calculation. It is shown that our light CNNs can be deployed on embedded systems without any precision degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Noisy Label Data Bootstrapping</head><p>In this subsection, we verify the efficiency of the proposed semantic bootstrapping method on the MS-Celeb-1M database. We select Light CNN-9 for semantic bootstrapping. The testing is performed on LFW. Since LFW only contains 6,000 pairs for evaluations, it is not obvious to justify the effectiveness of methods. We introduce two private datasets in which face images are subject to variations in viewpoint, resolution and illumination. The first dataset, denoted as VAL1, contains 1,424 face images of 866 identities, consisting of 573 positive pairs and 1,012,603 negative pairs (i.e., totally 1,013,176 pairs). The second dataset contains 675 identities First, we train a light CNN model on the CASIA-WebFace database that contains about 50K images of 10,575 identities in total. Then, we fine-tune our light CNN on MS-Celeb-1M, initialized by the pre-trained model on CASIA-WebFace. Since MS-Celeb-1M contains 99,891 identities, the fully-connected layer from feature representation (256-d) to identity label, which is treated as the classifier, has a large number of parameters (256*99,891=25,572,096). To alleviate the difficulty of CNN convergence, we firstly set the learning rate of all the convolution layers to 0, so that the softmax loss only contributes to the last fully-connected layer to train a classifier. When it is about to converge, the learning rate of all the convolution layers is set to the same, and then the learning rate is gradually decreased from 1e-3 to 1e-5.</p><p>Second, we employ the trained model in the first step to make predictions on the MS-Celeb-1M dataset and obtain the probabilityp i and labelt i for each sample x i ∈ X. Since the abilities of perceptual consistency can be influenced by the noisy labeled data in the training set, the strict bootstrapping rules are employed to select samples. We accept the re-labeling sample whose prediction labelt is the same as the ground truth label t and whose probabilityp i is greater than the threshold p 0 . As shown in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>, we set p 0 to [0.6, 0.7, 0.8, 0.9] to construct four re-labeling datasets. Obviously, the best performance is obtained when p 0 is set to 0.7. In this way, the MS-Celeb-1M re-labeling dataset, defined as MS-1M-1R,  contains 79,077 identities totally 4,086,798 images. Third, MS-1M-1R is used to retrain the Light CNN model following the training methodology in Section IV-A. Furthermore, the original noisy labeled MS-Celeb-1M database is resampled by the model trained on MS-1M-1R. Assuming that there are few noisy labeled data in MS-1M-1R, we accept the following samples: 1) The predictiont is the same as the ground truth label t; 2) The predictiont is different from the ground truth label t, but the probability p i is greater than the threshold p 1 . Obviously, the lower threshold p 1 is set, the more dangerous we sample an error labeled face image. But if the threshold is set too high, this step is not useful due to less sampled images. According to <ref type="figure" target="#fig_5">Fig. 4(b)</ref>, although LFW obtains nearly the same accuracy for different thresholds, we set p 1 to 0.7 due to the best performance on VAL1 and VAL2. The dataset after two times bootstrapping, denoted as MS-1M-2R, contains 5,049,824 images for 79,077 identities.</p><p>Finally, we retrain Light CNN-9 on MS-1M-2R. <ref type="table" target="#tab_1">Table  XI</ref> shows experimental results of the CNN models learned on different subsets. We have the following observations: 1) The MS-Celeb-1M database contains massive noisy labels. If the noisy labels are correctly dealt with, the performance on the two testing datasets can be improved. Our semantic bootstrapping method provides a practical way to deal with the noisy labels on the MS-Celeb-1M database. 2) Verification performance benefits from larger datasets. The model trained on the original MS-Celeb-1M database with noisy labels outperforms the model trained on the CASIA-WebFace database in terms of both ROC and AUC. 3) After two bootstrapping steps, the number of identities drops from 99,891 to 79,077 and performance improvement tends to be smaller. These indicate that our semantic bootstrapping method can obtain a purer training dataset that could in turn result in a light CNN with higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have developed a Light CNN framework to learn a robust face representation on noisy labeled dataset. Inspired by neural inhibition and maxout activation, we proposed a Max-Feature-Map operation to obtain a compact and low dimensional face representation. Small kernel sizes of convolution layers, Network in Network layers and Residual Blocks have been implemented to reduce parameter space and improve performance. One advantage of our framework is that it is faster and smaller than other published CNN methods. It extracts one face representation using about 121ms on a single core i7-4790, and it only consists 12,637K parameters for a Light CNN-29 model. Besides, an effective semantic bootstrapping has been proposed to handle the noisy label problem. Experimental results on various face recognition tasks verify that the proposed light CNN framework has potential value for some real-time face recognition systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) ReLU: h(x)=max(0,x 1 ) (b) Maxout: h(x)=max(x i ) (c) MFM 2/1: h(x)=max(x 1 , x 2 ) (d) MFM 3/2: h 1 (x)=max(x i ), h 2 (x)=median(x i ) Acomparison of different types of neural inhibition. (a) ReLU suppresses a neuron by thresholding magnitude responses. (b) Maxout with enough hidden units makes a piecewise linear approximation to an arbitrary convex function. (c) MFM 2/1 suppresses a neuron by a competitve relationship. It is the simplest case of maxout activations. (d) MFM 3/2 activates two neurons and suppresses one neuron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Face image alignment for training dataset. (a) is the facial points detection results and (b) is the normalized face image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>* The YTC dataset is composed of 1,910 videos from 47 subjects with a high compression rate and large appearance variations. Following the standard evaluation protocols, the YTC testing set is divided into five-fold cross validation. Each fold contains 423 videos, where the gallery set contains 141 videos and the others are considered as the probe set. * The Celebrity-1000 dataset contains 159,726 video sequences from 1000 subjects covering various resolutions, illuminations and poses. There are two types of protocols: close-set and open-set. For the close-set protocol, the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Comparisons on different configurations of Light CNN-29. (a) shows the ROC curves of LFW. (b) shows the ROC curves of VAL1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The perfomrance trained on MS-Celeb-1M-2R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The performance of LFW, VAL1 and VAL2. The models are trained on the cleaned datasets with different threshold settings to sample images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and T. Tan are with National Laboratory of Pattern Recognition, CASIA, Center for Research on Intelligent Perception and Computing, CASIA, Center for Excellence in Brain Science and Intelligence Technology, CAS and University of Chinese Academy of Sciences, Beijing, China,100190.</figDesc><table /><note>E-mail:alfredxiangwu@gmail.com, {rhe, znsun, tnt}@nlpr.ia.ac.cn</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>ARCHITECTURES OF THE LIGHT CNN-4 MODEL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc>ARCHITECTURES OF THE LIGHT CNN-9 MODEL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III THE</head><label>III</label><figDesc>ARCHITECTURES OF THE LIGHT CNN-29 MODEL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The second type is the collections of video-based face recognition databases containing the YouTube Face (YTF) databse</figDesc><table><row><cell>, there</cell></row><row><cell>are two new protocols called the close-set and open-set</cell></row><row><cell>identification. 1) For the close-set task, the gallery set</cell></row><row><cell>contains 4,249 identities, each with only a single face</cell></row><row><cell>image, and the probe one contains 3,143 face images</cell></row><row><cell>belonging to the</cell></row></table><note>same set of identities. The performance is measured by Rank-1 identification accuracy. 2) For the open-set task, the gallery set includes 3,143 images of 596 identities. The probe set includes 10,090 images which are constructed by 596 genuine probes and 9,494 impostor ones. The accuracy is evaluated by the Rank- 1 Detection and Identification Rate (DIR), which is calculated according to genuine probes matched in Rank- 1 at a 1% False Alarm Rate (FAR) of impostor ones that are not rejected.* The Benchmark of Large-scale Unconstrained Face Recognition (BLUFR) [39] is a new benchmark for LFW evaluations, which contains both verification and open- set identification. There are 10-fold experiments, with each fold containing about 156,915 genuine matching and 46,960,863 impostor matching on average for perfor- mance evaluation. It is more challenging and generalized compared to LFW.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>WITH OTHER STATE-OF-THE-ART METHODS ON THE LFW AND YTF DATASETS. THE UNRESTRICTED PROTOCOL FOLLOWS THE LFW UNRESTRICTED SETTING AND THE UNSUPERVISED PROTOCOL MEANS THE MODEL IS NOT TRAINED ON LFW IN SUPERVISED WAY.</figDesc><table><row><cell>Method</cell><cell cols="3">#Net Acc on LFW VR@FAR=0</cell><cell>Protocol</cell><cell cols="3">Rank-1 DIR@FAR=1% Acc on YTF</cell></row><row><cell>DeepFace [5]</cell><cell>7</cell><cell>97.35</cell><cell>46.33</cell><cell>unrestricted</cell><cell>64.90</cell><cell>44.50</cell><cell>91.40</cell></row><row><cell>Web-Scale [14]</cell><cell>4</cell><cell>98.37</cell><cell>-</cell><cell>unrestricted</cell><cell>82.50</cell><cell>61.90</cell><cell>-</cell></row><row><cell>DeepID2+ [16]</cell><cell>25</cell><cell>99.47</cell><cell>69.36</cell><cell>unrestricted</cell><cell>95.00</cell><cell>80.70</cell><cell>93.20</cell></row><row><cell>WebFace [6]</cell><cell>1</cell><cell>97.73</cell><cell>-</cell><cell>unrestricted</cell><cell>-</cell><cell>-</cell><cell>90.60</cell></row><row><cell>FaceNet [8]</cell><cell>1</cell><cell>99.63</cell><cell>-</cell><cell>unrestricted</cell><cell>-</cell><cell>-</cell><cell>95.10</cell></row><row><cell>SeetaFace [36]</cell><cell>1</cell><cell>98.62</cell><cell>-</cell><cell>unrestricted</cell><cell>92.79</cell><cell>68.13</cell><cell>-</cell></row><row><cell>VGG [7]</cell><cell>1</cell><cell>97.27</cell><cell>52.40</cell><cell>unsupervised</cell><cell>74.10</cell><cell>52.01</cell><cell>92.80</cell></row><row><cell>CenterLoss [37]</cell><cell>1</cell><cell>98.70</cell><cell>61.40</cell><cell>unsupervised</cell><cell>94.05</cell><cell>69.97</cell><cell>94.90</cell></row><row><cell>Light CNN-4</cell><cell>1</cell><cell>97.97</cell><cell>79.20</cell><cell>unsupervised</cell><cell>88.79</cell><cell>68.03</cell><cell>90.72</cell></row><row><cell>Light CNN-9</cell><cell>1</cell><cell>98.80</cell><cell>94.97</cell><cell>unsupervised</cell><cell>93.80</cell><cell>84.40</cell><cell>93.40</cell></row><row><cell>Light CNN-29</cell><cell>1</cell><cell>99.33</cell><cell>97.50</cell><cell>unsupervised</cell><cell>97.33</cell><cell>93.62</cell><cell>95.54</cell></row><row><cell cols="4">training and testing subsets contain the same identities</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and they are divided into four scales: 100, 200, 500 and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1000 for probe-gallery identification. For the open-set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>protocol, the generic training set contains 200 subjects and the remaining 800 subjects are used in testing stage. The probe and gallery set are used in testing stage and they are further divided into four scale: 100, 200, 400 and 800. The third type is the very challenging large-scale databases, including the MegaFace [43], IJB-A [44] and IJB-B [45] datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V THE</head><label>V</label><figDesc>PERFORMANCE ON LFW BLUFR PROTOCOLS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI THE</head><label>VI</label><figDesc>PERFORMANCE ON VIDEO-BASED FACE RECOGNITION DATABASES. (a) Comparison of Rank-1 accuracy (%) with other state-of-the-art methods on the Celebrity-1000 dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Average Rank-1 Accuracy on YTC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset.</cell></row><row><cell>Method</cell><cell>100</cell><cell cols="2">Close-Set 200 500</cell><cell>1000</cell><cell>100</cell><cell cols="2">Open-Set 200 400</cell><cell>800</cell></row><row><cell>MTJSR [51]</cell><cell>50.60</cell><cell>40.80</cell><cell>35.46</cell><cell>30.04</cell><cell>46.12</cell><cell>39.84</cell><cell>37.51</cell><cell>33.50</cell></row><row><cell>DELM [52]</cell><cell>49.80</cell><cell>45.21</cell><cell>38.88</cell><cell>28.83</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Eigen-RER [53]</cell><cell>50.60</cell><cell>45.02</cell><cell>39.97</cell><cell>31.94</cell><cell>51.55</cell><cell>46.15</cell><cell>42.33</cell><cell>35.90</cell></row><row><cell>GoogleNet+AvePool [54]</cell><cell>84.46</cell><cell>78.93</cell><cell>77.68</cell><cell>73.41</cell><cell>84.11</cell><cell>79.09</cell><cell>78.40</cell><cell>75.12</cell></row><row><cell>GoogleNet+NAN [54]</cell><cell>90.44</cell><cell>83.33</cell><cell>82.27</cell><cell>77.17</cell><cell>88.76</cell><cell>85.21</cell><cell>82.74</cell><cell>79.87</cell></row><row><cell>Light CNN-4</cell><cell>79.68</cell><cell>71.48</cell><cell>67.95</cell><cell>63.19</cell><cell>77.04</cell><cell>70.50</cell><cell>70.38</cell><cell>64.61</cell></row><row><cell>Light CNN-9</cell><cell>81.27</cell><cell>74.37</cell><cell>72.96</cell><cell>67.71</cell><cell>77.82</cell><cell>75.84</cell><cell>74.54</cell><cell>68.90</cell></row><row><cell>Light CNN-29</cell><cell>88.54</cell><cell>81.70</cell><cell>79.62</cell><cell>76.31</cell><cell>85.99</cell><cell>82.38</cell><cell>81.32</cell><cell>77.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII THE</head><label>VII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="10">PERFORMANCE ON LARGE-SCALE FACE RECOGNITION DATABASES.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">(a) MegaFace performance comparison with other</cell><cell></cell><cell cols="5">(b) The CMC for MegaFace.</cell><cell></cell><cell cols="5">(c) The ROC for MegaFace.</cell></row><row><cell cols="5">methods on rank-1 identification accuracy with 1</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">million distractors and verification TAR for 10 −6</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAR.</cell><cell>Method</cell><cell>Rank-1</cell><cell cols="2">VR@FAR=10 −6</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NTechLAB FaceNet v8 [8] Beijing Faceall Co. 3DiVi Company Barebones FR CenterLoss [37]</cell><cell>73.300 70.496 64.803 33.705 59.363 65.234</cell><cell>85.081 86.473 67.118 36.927 59.036 76.510</cell><cell></cell><cell>Identification Rate</cell><cell>0.6 0.7 0.5</cell><cell></cell><cell></cell><cell></cell><cell>3DiVi Company Barebones_FR Beijing FaceAll Co. CenterLoss NTechLAB</cell><cell>True Positive Rate</cell><cell>0.6 0.7 0.5 0.4</cell><cell></cell><cell></cell><cell></cell><cell>3DiVi Company Barebones_FR Beijing FaceAll Co. CenterLoss NTechLAB</cell></row><row><cell></cell><cell>Light CNN-4</cell><cell>60.236</cell><cell>62.341</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell>FaceNet LightCNN-9</cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>FaceNet LightCNN-9</cell></row><row><cell cols="2">Light CNN-9 Light CNN-29</cell><cell>67.109 73.749</cell><cell>85.133 77.456</cell><cell></cell><cell></cell><cell>10 0 0.3</cell><cell>10 1</cell><cell>10 2</cell><cell># distractors 10 3</cell><cell>10 4 LightCNN-29 10 5</cell><cell>10 6</cell><cell>10 -7 0.2</cell><cell>10 -6</cell><cell>10 -5</cell><cell>False Postive Rate 10 -4 10 -3</cell><cell>10 -2 LightCNN-29</cell><cell>10 -1</cell><cell>10 0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IJB-A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IJB-B</cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell></cell><cell>1:1 Verification</cell><cell></cell><cell cols="4">1:N Identification</cell><cell></cell><cell cols="2">1:1 Verification</cell><cell></cell><cell></cell><cell></cell><cell cols="2">1:N Identification</cell></row><row><cell></cell><cell cols="2">FAR=0.001</cell><cell>FAR=0.01</cell><cell>FAR=0.1</cell><cell cols="2">Rank-1</cell><cell cols="2">Rank-5</cell><cell cols="2">FAR=1e-4</cell><cell>FAR=1e-3</cell><cell cols="2">FAR=1e-2</cell><cell></cell><cell>Rank-1</cell><cell>Rank-5</cell></row><row><cell cols="2">VGGFace [7]</cell><cell>62.0±4.3</cell><cell>83.4±2.1</cell><cell>95.4±0.5</cell><cell cols="2">92.5±0.8</cell><cell cols="3">97.2±0.5</cell><cell>53.5</cell><cell>71.1</cell><cell cols="2">85.0</cell><cell></cell><cell>75.2±3.8</cell><cell>84.3±3.2</cell></row><row><cell cols="2">Bansalit et al [10]</cell><cell>73.0</cell><cell>87.4</cell><cell>96.0</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Sohn et al [61]</cell><cell>64.9±2.2</cell><cell>86.4±0.7</cell><cell>97.0±0.1</cell><cell cols="2">89.5±0.3</cell><cell cols="3">95.7±0.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Crosswhite et al [62]</cell><cell>83.6±2.7</cell><cell>93.9±1.3</cell><cell>97.9±0.4</cell><cell cols="2">92.8±1.0</cell><cell cols="3">97.7±0.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">NAN [54]</cell><cell>88.1±1.1</cell><cell>94.1±0.8</cell><cell>97.8±0.3</cell><cell cols="2">95.8±0.5</cell><cell cols="3">98.0±0.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DA-GAN [63]</cell><cell>93.0±0.5</cell><cell>97.6±0.7</cell><cell>99.1±0.3</cell><cell cols="2">97.1±0.7</cell><cell cols="3">98.9±0.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">VGGFace2 [12]</cell><cell>92.1±1.4</cell><cell>96.8±0.6</cell><cell>99.0±0.2</cell><cell cols="2">98.2±0.4</cell><cell cols="3">99.3±0.2</cell><cell>83.1</cell><cell>90.8</cell><cell cols="2">95.6</cell><cell></cell><cell>90.2±3.6</cell><cell>94.6±2.2</cell></row><row><cell cols="2">Whitelam et al [45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>54.0</cell><cell>70.0</cell><cell cols="2">84.0</cell><cell></cell><cell>79.0</cell><cell>85.0</cell></row><row><cell cols="2">Light CNN-4</cell><cell>60.1±5.3</cell><cell>79.4±3.2</cell><cell>91.4±2.1</cell><cell cols="2">88.5±1.8</cell><cell cols="3">93.2±0.7</cell><cell>30.5</cell><cell>62.4</cell><cell cols="2">76.5</cell><cell></cell><cell>67.5±4.2</cell><cell>78.3±5.1</cell></row><row><cell cols="2">Light CNN-9</cell><cell>83.4±1.7</cell><cell>90.3±1.0</cell><cell>95.4±0.7</cell><cell cols="2">91.6±0.8</cell><cell cols="3">95.3±0.9</cell><cell>31.7</cell><cell>67.4</cell><cell cols="2">79.9</cell><cell></cell><cell>72.3±5.8</cell><cell>80.4±4.2</cell></row><row><cell cols="2">Light CNN-29</cell><cell>93.9±0.9</cell><cell>96.9±0.4</cell><cell>98.7±0.1</cell><cell cols="2">97.7±0.3</cell><cell cols="3">99.0±0.1</cell><cell>87.7</cell><cell>92.0</cell><cell cols="2">95.3</cell><cell></cell><cell>91.9±1.5</cell><cell>94.8±0.9</cell></row></table><note>(d) The performance of 1:1 verification and 1:N identification on the IJB-A and IJB-B datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table VI</head><label>VI</label><figDesc></figDesc><table><row><cell>(b), we compare the Light CNN</cell></row><row><cell>with other video-based face recognition methods such as Lo-</cell></row><row><cell>calized Multi-Kernel Metric Learning (LMKML) [55], Multi-</cell></row><row><cell>Manifold Deep Metric Learning (MMDML)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VIII THE</head><label>VIII</label><figDesc>PERFORMANCE ON CROSS DOMAIN FACE RECOGNITION DATABASES (a) Accuracy of different methods on CACD-VS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE IX COMPARISON</head><label>IX</label><figDesc>WITH DIFFERENT ACTIVATION FUNCTIONS ON LFW VERIFICATION AND IDENTIFICATION PROTOCOL BY THE LIGHT CNN-9</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MODEL.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell cols="7">Accuracy Rank-1 DIR@FAR=1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ReLU [26]</cell><cell>98.30</cell><cell cols="2">88.58</cell><cell></cell><cell></cell><cell>67.56</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PReLU [28]</cell><cell>98.17</cell><cell cols="2">88.30</cell><cell></cell><cell></cell><cell>66.30</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ELU [29]</cell><cell>97.70</cell><cell cols="2">84.70</cell><cell></cell><cell></cell><cell>62.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MFM 2/1</cell><cell>98.80</cell><cell cols="2">93.80</cell><cell></cell><cell></cell><cell>84.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MFM 3/2</cell><cell>98.83</cell><cell cols="2">94.97</cell><cell></cell><cell></cell><cell>88.59</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rate</cell><cell>0.96</cell><cell></cell><cell></cell><cell></cell><cell>Rate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Positive</cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell>Positive</cell><cell>0.85</cell><cell></cell><cell></cell><cell></cell></row><row><cell>True</cell><cell>0.94</cell><cell></cell><cell></cell><cell></cell><cell>True</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.93</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.92</cell><cell></cell><cell>Light CNN-29 + BN</cell><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell cols="2">Light CNN-29 + BN</cell></row><row><cell></cell><cell>0.91</cell><cell></cell><cell cols="2">Light CNN-29 + Global AVE Pooling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Light CNN-29 + Global AVE Pooling</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Light CNN-29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Light CNN-29</cell></row><row><cell></cell><cell>0.9</cell><cell>0</cell><cell cols="2">0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05</cell><cell></cell><cell>10 -6 0.7</cell><cell>10 -5</cell><cell>10 -4</cell><cell>10 -3</cell><cell>10 -2</cell><cell>10 -1</cell><cell>10 0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>False Positive Rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>False Positive Rate</cell></row><row><cell></cell><cell cols="4">(a) The ROC curves on LFW.</cell><cell></cell><cell cols="5">(b) The ROC curves on VAL1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE X THE</head><label>X</label><figDesc>TIME COST AND THE NUMBER OF PARAMETERS OF OUR MODEL COMPARED WITH VGG, CENTERLOSS RELEASED MODEL AND SEETAFACE. THE SPEED IS TESTED ON A SINGLE CORE I7-4790.</figDesc><table><row><cell>Model</cell><cell>#Param</cell><cell>#Dim</cell><cell>Times</cell></row><row><cell>FaceNet [8]</cell><cell>140,694K</cell><cell>128</cell><cell>-</cell></row><row><cell>WebFace [6]</cell><cell>5,015K</cell><cell>320</cell><cell>-</cell></row><row><cell>VGG [7]</cell><cell>134,251K</cell><cell>4096</cell><cell>581ms</cell></row><row><cell>SeetaFace [36]</cell><cell>50,021K</cell><cell>2048</cell><cell>245ms</cell></row><row><cell>CenterLoss [37]</cell><cell>19,596K</cell><cell>1024</cell><cell>160ms</cell></row><row><cell>Light CNN-4</cell><cell>4,095K</cell><cell>256</cell><cell>75ms</cell></row><row><cell>Light CNN-9</cell><cell>5,556K</cell><cell>256</cell><cell>67ms</cell></row><row><cell>Light CNN-29</cell><cell>12,637K</cell><cell>256</cell><cell>121ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE XI THE</head><label>XI</label><figDesc>PERFORMANCE ON VAL1, VAL2 AND LFW FOR LIGHT CNN-9 MODEL TRAINED ON DIFFERENT DATABASES. IT COMPARES THE PERFORMANCE OF LIGHT CNN MODEL TRAINED ON CASIA-WEBFACE, MS-CELEB-1M, MS-CELEB-1M AFTER 1 TIMES BOOTSTRAPPING(MS-1M-1R) AND MS-CELEB-1M AFTER 2 TIMES BOOTSTRAPPING(MS-1M-2R). THE AREA UNDER PRECISION-RECALL CURVE(AUC) AND VERIFICATION RATE(VR)@FASLE ACCEPTANCE RATE(FAR) FOR DIFFERENT MODELS ARE PRESENTED.</figDesc><table><row><cell>LFW</cell><cell>ACC (%)</cell><cell>FAR=1% (%)</cell><cell>FAR=0.1% (%)</cell></row><row><cell>CASIA</cell><cell>98.13</cell><cell>96.73</cell><cell>87.13</cell></row><row><cell>MS-Celeb-1M</cell><cell>98.47</cell><cell>98.13</cell><cell>94.97</cell></row><row><cell>MS-1M-1R</cell><cell>98.80</cell><cell>98.43</cell><cell>95.43</cell></row><row><cell>MS-1M-2R</cell><cell>98.80</cell><cell>98.60</cell><cell>96.77</cell></row><row><cell>VAL1</cell><cell cols="3">AUC (%) FAR=0.1% (%) FAR=0.01% (%)</cell></row><row><cell>CASIA</cell><cell>89.72</cell><cell>92.50</cell><cell>84.82</cell></row><row><cell>MS-Celeb-1M</cell><cell>92.03</cell><cell>94.42</cell><cell>88.48</cell></row><row><cell>MS-1M-1R</cell><cell>94.82</cell><cell>96.86</cell><cell>92.15</cell></row><row><cell>MS-1M-2R</cell><cell>95.34</cell><cell>97.03</cell><cell>93.54</cell></row><row><cell>VAL2</cell><cell cols="3">AUC (%) FAR=0.1% (%) FAR=0.01% (%)</cell></row><row><cell>CASIA</cell><cell>62.82</cell><cell>62.84</cell><cell>44.46</cell></row><row><cell>MS-Celeb-1M</cell><cell>75.79</cell><cell>77.93</cell><cell>61.38</cell></row><row><cell>MS-1M-1R</cell><cell>81.04</cell><cell>82.66</cell><cell>68.91</cell></row><row><cell>MS-1M-2R</cell><cell>82.94</cell><cell>84.55</cell><cell>71.39</cell></row><row><cell cols="4">and totally 3,277 images, which is denoted as VAL2. VAL2</cell></row><row><cell cols="4">contains 2,632,926 pairs that are composed of 4,015 positive</cell></row><row><cell cols="4">and 2,628,911 negative pairs. All the face images in VAL1</cell></row><row><cell cols="4">and VAL2 are strictly independent from both CASIA-WebFace</cell></row><row><cell cols="4">and MS-Celeb-1M datasets. Considering highly imbalanced</cell></row><row><cell cols="4">dataset evaluations, we employ Receiver Operator Characteris-</cell></row><row><cell cols="4">tic (ROC) curves and Precision-Recall (PR) curves to evaluate</cell></row><row><cell cols="4">the performance of the models retrained via bootstrapping.</cell></row><row><cell cols="4">Both VR@FAR for ROC curves and AUC for PR curves are</cell></row><row><cell>reported.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/AlfredXiangWu/face verification experiment</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank the associate editor and the reviewers for their valuable comments and advice. This work is funded by State Key Development Program (Grant No. 2016YFB1001001) and the National Natural Science Foundation of China (Grants No. 61622310 and 61427811).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1506.02640</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition by discriminant analysis with gabor tensor representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1411.7923</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Umdfaces: An annotated face dataset for training deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The do&apos;s and don&apos;ts for cnn-based face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1710.08092</idno>
		<ptr target="http://arxiv.org/abs/1710.08092" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unconstrained face recognition: Identifying a person of interest from a media collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2144" to="2157" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do we really need to collect millions of faces for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning with annotation noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Klebanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instance pruning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating a kernel fisher discriminant in the presence of label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamics of pattern formation in lateral-inhibition type neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ordinal measures for iris recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2211" to="2226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">VIPLFaceNet: An open source deep face recognition sdk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A benchmark study of large-scale unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face tracking and recognition with visual constraints in real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Toward large-population face identification in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1874" to="1884" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-b face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Biometrics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face recognition and retrieval using cross-age reference coding with cross-age celebrity dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="804" to="815" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multipie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The CASIA NIR-VIS 2.0 face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-view perceptron: a deep model for learning face identity and view representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visual classification with multitask joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4349" to="4360" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Representation learning with deep extreme learning machines for efficient image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uzair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<idno>abs/1503.02445</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Eigen-pep for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image set classification using holistic multiple order statistics features and localized multi-kernel metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-manifold deep metric learning for image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Face recognition in movie trailers via mean sequence sparse representation-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simultaneous feature and sample reduction for image-set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Articial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recurrent regression for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<idno>abs/1607.06999</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image set representation and classification with covariate-relation graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR Asian Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for face recognition in unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3210" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dual-agent gans for photorealistic and identity preserving profile face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: Highdimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hidden factor analysis for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep learning identitypreserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-view deep network for cross-view classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multi-task convolutional neural network for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1702.04710</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Shared representation learning for heterogenous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">NIR-VIS heterogeneous face recognition via cross-spectral joint dictionary learning and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Transferring deep representation for NIR-VIS heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning invariant deep representation for nir-vis face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Articial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesiss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pose-guided photorealistic face rotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Demeshnet: Blind face inpainting for deep meshface verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Mapu: A novel mathematical computing architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
