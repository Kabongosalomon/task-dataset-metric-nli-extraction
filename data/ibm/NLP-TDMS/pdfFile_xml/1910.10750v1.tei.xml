<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
						</author>
						<title level="a" type="main">6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closedloop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Estimating 6D pose of objects, i.e., translation and orientation in 3D, offers a concise and informative form of state representation for robotic applications, such as manipulation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> and navigation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>. In robotic manipulation, the ability of tracking object 6D poses in realtime gives rise to fast feedback control <ref type="bibr" target="#b23">[24]</ref>. Pioneering work in 6D tracking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref> has achieved remarkable accuracy and robustness given the 3D model of an object instance, often referred as instance-level 6D tracking. However, the assumption of known 3D model can be brittle in realistic settings, where perfect geometry of novel objects is hard to acquire. In this work, we propose to study the problem of categorylevel 6D tracking, where the goal is to develop category-level models capable of tracking novel object instances within a specific category.</p><p>The problem of category-level tracking has been studied extensively in 2D domains. Classical methods rely on handcrafted features as object representations for visual tracking <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">49]</ref>. Recent work has embarked on an exploration of new computational tools, in particular, deep neural networks, and large amounts of training data to improve tracking performance under visual variations and heavy occlusions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref>. However, a model for 6D pose tracking would have to handle the larger search space of all possible poses due to the increased dimensionality, leading to a substantial computational burden over 2D visual trackers.</p><p>One remedy is to reduce category-level 6D tracking to a 3D detection and 6D pose estimation problem. 3D detection and pose estimation have been studied in a large <ref type="figure">Fig. 1</ref>. Overview of 6-PACK: Our model learns to robustly detect and track a set of 3D category-based keypoints (yellow dots) based on anchors (red dots) sampled around the previously estimated object pose using RGB-D images. The keypoints from two consecutive frames are then used to compute the 6D object pose change via least-squares optimization. The entire process is fast (&gt;10fps) to enable real-time robot interaction. body of literature, especially in the context of autonomous driving <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>. Most relevant to us is NOCS <ref type="bibr" target="#b45">[46]</ref> which introduced a category-level model to estimate the 6D pose of objects from RGB-D images. NOCS transforms every object pixel to a shared coordinate frame as keypoints for pose estimation. However, estimating poses from a large number of crude keypoints makes their method susceptible to noises from clutter and occlusion. Furthermore, these trackingby-detection methods cannot leverage temporal information from previous frames. In contrast, we seek to develop a tracking model that learns compact and discriminative object representations for robust registration and leverages temporal consistency for efficient search.</p><p>To this end, we propose 6-PACK, a vision-based 6D-Pose Anchor-based Category-level Keypoint tracker. 6-PACK tracks a small set of keypoints in RGB-D videos and estimates object pose by accumulating relative pose changes over time (see <ref type="figure">Fig. 1</ref>). This method does not require the known 3D model. Instead, it circumvents the need of defining and estimating the absolute 6D pose via a novel anchor mechanism analogous to the proposal methodology used in 2D object detection <ref type="bibr" target="#b35">[36]</ref>. These anchors offer a base for generating 3D keypoints. Unlike previous methods that require manual keypoint annotations <ref type="bibr" target="#b30">[31]</ref>, we propose an unsupervised learning approach that discovers the optimal set of 3D keypoints for tracking. These keypoints serve as a compact representation of the object, from which its pose difference between two adjacent frames can be efficiently estimated <ref type="bibr" target="#b1">[2]</ref>. This keypoint-based representation leads to robust and real-time 6D pose tracking.</p><p>We show experimentally improved generalization and robustness in category-level 6D tracking compared to the traditional registration-based tracker <ref type="bibr" target="#b39">[40]</ref>, the state-of-theart tracking-by-detection method <ref type="bibr" target="#b45">[46]</ref>, and other ablative baselines. 6-PACK substantially outperforms all baselines on the recently introduced NOCS-REAL275 dataset <ref type="bibr" target="#b45">[46]</ref>. The proposed tracker runs at 10Hz on a GTX1070 GPU. We deploy this tracker on a Toyota HSR Robot and demonstrate its utility in real-time manipulation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Historically, 6D object visual pose estimation has relied on matching the current view of an object to a given template model. The pose can then be recovered from solving an error minimization problem between correspondences, either a PnP, a reprojection or a distance error, depending on the nature of the template <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. While conceptually simple and efficient in practice, the performances of these methods degrade significantly under clutter or variable lighting conditions due to errors in feature matching.</p><p>Recent 6D object pose estimation methods instead learns to directly match input images with renderings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> or silhouette <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> of template object models. PoseRBPF <ref type="bibr" target="#b12">[13]</ref> compares the latent code of the input image and that of the model rendering to recover the rotation part of the object pose. However, the requirements of knowing the object models confine these methods to known object instances.</p><p>The problem of category-level pose estimation has amassed significant interests in research areas such as autonomous driving <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref> due to the availability of large-scale datasets <ref type="bibr" target="#b15">[16]</ref>. Most relevant to us is a class of methods that combines the classical keypoint matching ideas and modern learning techniques by directly predicting either category-level semantic keypoints <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44]</ref> or 3D bounding box corners <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. However, supervised keypoints learning require large amounts of labelled data, and the manually annotated keypoints or the bounding box corners may not be the optimal landmarks to track. Moreover, defining features a priori (so-called feature engineering) has largely been outperformed by data-driven methods that learn the best features for the task <ref type="bibr" target="#b4">[5]</ref>.</p><p>A notable exception is NOCS <ref type="bibr" target="#b45">[46]</ref>, which learns to project every input object pixel into a category-level canonical 3D space as keypoint. The pose is then recovered by registering all projected keypoints to an "average" category object model. However, as we show experimentally, the reliance on dense keypoint correspondences makes NOCS susceptible to noise from occlusion. In contrast, our novel keypoint detector is trained end-to-end only with the final pose tracking objective to generate a small but robust set of 3D keypoints for tracking without direct keypoint supervision.</p><p>Our keypoint generator is closely related to the large corpus of keypoint detection and matching methods. Classical methods detect and match keypoints based on handdesigned local features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. Modern learning approaches learns to detect keypoints via optimizing fullysupervised <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref> or semi-supervised objectives <ref type="bibr" target="#b18">[19]</ref>. Recently, KeypointNet <ref type="bibr" target="#b40">[41]</ref> shows the benefits of learning to generate keypoints in 3D space without supervision by exploiting geometric consistency across multiple views. The model generalizes to new object instances within known categories in a synthetic domain. However, as we show in our experimental evaluation, KeypointNet fails to scale to a realworld since the model has been trained to define keypoints on single models of objects centered at the origin of coordinates. This approach suffers when exposed to noisy RGB-D data from a real scene where the objects have occlusions and are not centered, increasing the boundaries of the 3D space where the keypoints could be found. Our method introduces a novel anchoring mechanism that allows the model to generate keypoints only in the most relevant subspace. The strategy significantly improves the quality of the generated keypoints and reduces the number of keypoints needed for estimating the pose, enabling our model to track objects in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM DEFINITION</head><p>For any instance of an object category, given its initial 6D pose, p 0 ∈ SE(3), and the category it belongs to (e.g., mug, bowl, laptop, . . . ), we define category-level 6D pose tracking as the problem of estimating continuously the change of pose of the object between consecutive timesteps t − 1 and t, ∆p t ∈ SE(3). A change of pose consists of change in rotation ∆R t ∈ SO(3) and a change in translation</p><formula xml:id="formula_0">∆t t ∈ R 3 , ∆p t = [∆R t |∆t t ].</formula><p>The absolute pose can be then retrieved by applying recursively the last estimated change of pose:</p><formula xml:id="formula_1">p t = ∆p t · p t−1 = ∆p t · ∆p t−1 · · · p 0 .</formula><p>The initial pose is the translation and rotation with respect to the camera frame of a canonical frame defined similarly for all instances of the same category. This setup was defined in NOCS <ref type="bibr" target="#b45">[46]</ref> for the related problem of category-level 6D pose estimation. For example, for the category camera, the frame is placed at the centroid of the object with the xaxis pointing in the direction of the camera objective and the y-axis pointing upwards. Similar to prior 6D tracking work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>, we assume the initial pose of the object is given. However, different from these approaches, our method is robust to errors in this initial pose, as shown in our experimental evaluation (Sec. V).</p><p>Our method is inspired by recent works on learning to predict 3D keypoints for 6D object pose estimation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>. Following <ref type="bibr" target="#b40">[41]</ref>, we define 3D keypoints as points in 3D space k ∈ R 3 that are geometrically and semantically consistent throughout a temporal sequence. <ref type="figure">Fig. 1</ref> illustrates an example of such keypoints. Concretely, given two consecutive input frames (I t−1 , I t ), the problem is to predict matching keypoint lists (k t−1 , k t ) from the two frames. Then based on rigid body assumption, the change of pose ∆p t can be recovered by solving a point set alignment problem with least-squares optimization <ref type="bibr" target="#b1">[2]</ref>. an attention mechanism over a grid of anchor points (Sec. IV-A) generated around the predicted pose of the object. Each anchor summarizes the volume around it with a distanceweighted sum of the individual features of the RGB-D points in its surrounding. This information allows to find a coarse centroid of the object in the new RGB-D frame and guide the following search of keypoints around it, which is more efficient than searching keypoints in the entire unconstrained 3D space of previous approaches <ref type="bibr" target="#b40">[41]</ref>. Second, 6-PACK uses the anchor feature to generate keypoints for both symmetric and non-symmetric categories (Sec. IV-B and <ref type="figure">Fig. 2</ref> left and top-right). Different from previous methods (e.g., kPAM <ref type="bibr" target="#b30">[31]</ref>), these keypoints are learned in an unsupervised manner so they are the most robust and informative for tracking based on training data.</p><p>Finally, the keypoints of the current and previous frames are passed to a least-squares optimization <ref type="bibr" target="#b1">[2]</ref> that calculates the inter-frame change in pose. Based on this motion, we extrapolate the pose in the next frame to center the next distribution of anchor points.</p><p>The process starts at the location indicated by a given initial pose, p 0 . We allow for this initial pose to contain an error that we reject with an initial iterative procedure. We generate a set of keypoints and refine the given pose to be at the centroid of the set, p 0 . The centroid of the keypoints is close to the instance centroid as it is imposed in the training process (Sec. IV-B). We then run the generation and correction again, a total number of T = 10 times. We select the refined pose that is closest to the centroid of generated keypoints as initial pose for the tracking procedure, since this refined pose is most likely to be the correct one for the instance of the class. The initialization process reduces the effort of providing a very accurate initial pose p 0 and increases the robustness of the category-level tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Anchor-based Attention Mechanism</head><p>Directly generating a set of ordered 3D keypoints for pose tracking is challenging due to the large output space. Prior work on automatic keypoint generation <ref type="bibr" target="#b40">[41]</ref> did not address this problem: in their setup, keypoints are generated for a single object within a bounded, origin-centered sphere. However, in our setup, the object can be anywhere in 3D space within the field of view of the RGB-D frame. Thus, the keypoints can be anywhere in the unbounded 3D space.</p><p>The stated problem of 3D keypoint generation in unbounded space resembles that of 2D object detection, where the goal is to draw a tight bounding box around the target object on the 2D image. A successful solution to this problem is to begin by drawing a grid of anchor points in the image and find the anchor that is closest to the object center. This procedure locates coarsely the object and simplifies the second step, the generation of a more accurate bounding box proposal around the anchor <ref type="bibr" target="#b34">[35]</ref>. Inspired by this idea, we propose an attention mechanism over a grid of 3D anchors around the predicted current object location. Each anchor contains a feature representation of the surrounding volume around it. The model learns to attend, based on this feature, to the anchor closest to the centroid of the object. The 3D keypoints could then be generated as offset points from the selected anchor (Sec. IV-B). By splitting the tracking problem into coarse attention-based anchor selection and fine-grained keypoint generation, our tracker has potential to tackle larger region of search space (improving robustness and perceivable inter-frame motion) while maintaining high tracking quality.</p><p>As mention before, each anchor contains a feature representing the 3D points in the volume surrounding it. As showed by prior work <ref type="bibr" target="#b44">[45]</ref>, it is possible to combine color and geometric information into a fused feature to be used for pose estimation. Therefore, we apply the DenseFusion <ref type="bibr" target="#b44">[45]</ref> feature embedding to all colored 3D points from the RGB-D image within the grid of anchors, and use a distanceweighted averaging to pool them into each anchor to generate the anchor embedding.</p><p>Let's assume the grid has N anchor points, a i ∈ R 3 , and contains M colored points from the RGB-D frame, x j ∈ R 3 . The vector of distances from an anchor point to all colored points is thus</p><formula xml:id="formula_2">d i = [d i0 , d i1 , · · · , d iM ].</formula><p>Then, the weights for the features of the colored points are defined as w = softmax(d i ). Based on these weights we perform a distanceweighted average pooling to generate the anchor embedding ψ i of the anchor a i , ψ i = j w j φ j , where φ j is the point embedding of x j generated by the DenseFusion encoder.</p><p>Once we have generated a per-anchor geometric and color feature, we train an attention network that learns to detect the one closest to the object centroid. Our attention network takes as input each anchor-level embedding ψ i and produces as output a confidence score c i per anchor. The attention network is trained to assign the highest confidence score to the anchor closest to the centroid of the object with supervision. Thus, given the ground-truth position of the object centroid o gt , the loss function can be written as:</p><formula xml:id="formula_3">L anc = 1 N i c i (||a i − o gt || 2 − β)<label>(1)</label></formula><p>where β = min(||a i − o gt || 2 ), i = 1 . . . N refers to the minimum possible distance from the anchor to the object centroid. We use a two-layer MLP as the attention network. During the evaluation, 6-PACK selects the anchor with the highest confidence score and generates the keypoints as offsets to this anchor, as explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised 3D Keypoints Generation</head><p>With the anchor-based attention mechanism, 6-PACK identified the anchor a i and associated feature ψ i with the highest confidence score. Now, 6-PACK will use this feature to generate the final set of 3D keypoints, [k 0 , . . . , k K ], to track the instance of the object category. We propose a keypoint generation neural network that uses as input the anchor feature and generates a K × 3 dimensional output containing an ordered list of keypoints. Since the list is ordered, we do not need to find correspondences between keypoints of consecutive frames to estimate the change in pose.</p><p>As mentioned before, we train our keypoint generation network in an unsupervised manner that does not require of manual annotation, and that, compared to supervised methods, has led to improved transferability on categorylevel orientation estimation <ref type="bibr" target="#b40">[41]</ref>. We render the unsupervised training of our keypoint generation network as optimizing the multi-view consistency between the keypoints generated in consecutive frames. In other words, suppose K keypoints are generated in each of two consecutive frames; the training objective is to place the keypoints in the current view at the location that corresponds to the keypoints of the previous frame, transformed by the ground truth inter-frame motion.</p><p>This objective can be formalized in the following multi-view consistency loss:</p><formula xml:id="formula_4">L mvc = 1 K i ||k t i − [∆R gt t |∆t gt t ] · k t−1 i || (2)</formula><p>where [∆R gt t |∆t gt t ] = ∆p gt t is the ground truth inter-frame change in pose.</p><p>The multi-view consistency loss only guarantees interframe consistency between features locations independently of the perspective or the visible part of the object. However, this does not guarantee these locations are optimal for our final goal, to estimate the change of pose (e.g., all keypoints could end up all at the same location). To tackle this problem we turn the keypoint-based pose estimation step into a differentiable pose estimation loss function and combine it with the multi-view consistency loss in the training process <ref type="bibr" target="#b40">[41]</ref>. This loss function consists of a translation loss L tra and rotation loss L rot :</p><formula xml:id="formula_5">L tra = ||(k t −k t−1 ) − ∆t gt t || (3) L rot = 2 arcsin( 1 2 √ 2 ||∆R t − ∆R gt t ||)<label>(4)</label></formula><p>wherek t andk t−1 are the centroids of the keypoints in previous and current frames, and ∆R t is the inter-frame change in orientation estimated based on the generated keypoint sets using least-squares optimization <ref type="bibr" target="#b1">[2]</ref>. Thus, these losses force the keypoints to be generated such that the ground truth change of pose can be computed from them. We also integrate a separation loss L sep and silhouette consistency loss L sil defined by Suwajanakorn et al. <ref type="bibr" target="#b40">[41]</ref>. The separation loss forces keypoints to maintain some distance to each other to avoid degenerate configurations and improve the pose estimation. The silhouette consistency loss forces keypoints to be closer to the object's surface to improve interpretability.</p><p>In addition to the main objectives introduced above, we impose a centroid loss that forces the centroid of the generated set of keypoints to be at the centroid of the object, L cen , useful to correct for noise in the given initial pose as explained at the beginning of this section. The final overall training loss is a weighted sum of the 6 terms introduced above, where the weightings are determined by their relative magnitude and importance.</p><p>3D Keypoint Generation for Classes with Symmetry Axes: The presented multi-view consistency and pose loss functions do not handle well symmetries on instances of object categories since identifying the rotation along the symmetry axis is an unsolvable problem. We propose a coordinate system transformation ρ() that transforms the coordinates of points into a space that is rotation-invariant around the axis of symmetry. <ref type="figure">Fig. IV-B</ref> illustrates the transformation for a case with five points on an instance of bowl. Suppose some common axis of symmetry for all instances in this category, s gt , passing through the Cartesian origin of coordinates (y-axis in <ref type="figure">Fig. IV-B)</ref>; we transform the </p><formula xml:id="formula_6">L sym mvc = 1 K i ||ρ(k t i ) − ρ([∆R gt t |∆t gt t ]k t−1 i )||<label>(5)</label></formula><p>The pose estimation loss also needs to be adapted for categories with symmetry axes. While the translation loss remains unaltered, we redefine the rotation loss as the angular difference between the predicted change ∆ŝ and the groundtruth change ∆s t gt in orientation of the symmetry axis. The rotation loss for these categories is then simply:</p><formula xml:id="formula_7">L sym R = arccos ∆s t gt · ∆ŝ t ||∆s t gt || · ||∆ŝ t ||<label>(6)</label></formula><p>V. EXPERIMENTS In this section, we would like to answer the following questions: 1) Does our method indeed generate robust 3D keypoints that are suitable for 6D pose tracking? 2) Does our anchor-based attention mechanism improve the overall tracking performance? 3) How robust is our method against variable levels of noise in pose initialization, and 4) Is our method efficient enough for real-time applications such as closed-loop object manipulation?</p><p>To answer questions 1), 2) and 3), we evaluate our method and compare it to multiple baselines on the NOCS-REAL275 <ref type="bibr" target="#b45">[46]</ref> dataset, the only real-world benchmark dataset for category-level object 6D pose tracking. To answer question 4), we deploy our model on a real robot platform and test the model with another ten unseen objects and show that the model can be successfully used in a collaborative pouring and a "toasting" tasks. Dataset: We use the NOCS-REAL275 dataset, which contains six categories including bottle, bowl, camera, can, laptop, and mug. Three of them are categories with axes of symmetry. The training set consists of 275K discrete frames of synthetic data generated with 1085 models of instances of the classes from ShapeNetCore <ref type="bibr" target="#b7">[8]</ref> with random poses, and seven real videos with ground truth poses depicting in total three instances of objects of each category. The testing set has six real videos depicting in total three different (unseen) instances for each object category with 3,200 frames in total.</p><p>Evaluation Metrics: We follow NOCS <ref type="bibr" target="#b45">[46]</ref> and report the following metrics: 1) 5°5 cm <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, the percentage of tracking results with orientation error &lt; 5°and translation error &lt; 5 cm, and 2) IoU25 <ref type="bibr" target="#b14">[15]</ref>, percentage of volume overlap between the prediction and ground-truth 3D bounding box that is larger than 25%, In addition, we include other traditionally used metrics from the tracking community: 3) R err , mean of the orientation error in degrees, and 4) T err , mean of the translation error in centimeters.</p><p>Baselines: We compare three model variants with baselines to showcase the effectiveness of our design choices:</p><p>• NOCS <ref type="bibr" target="#b45">[46]</ref>: The state-of-the-art category-level 6D pose estimation method that uses per-pixel prediction. • ICP <ref type="bibr" target="#b49">[50]</ref>: The standard point-to-plane ICP algorithm implemented in Open3D. • KeypointNet <ref type="bibr" target="#b40">[41]</ref>: The implementation of our model without the anchor-based attention mechanism, which direct generates 3D keypoints in the 3D space. Evaluation Results on the NOCS-REAL275 dataset: <ref type="table" target="#tab_0">Table I</ref> contains the results on the testing set of all six categories of the NOCS dataset. We compare two variants of our model (with and without temporal prediction) and several current state-of-the-art category-level 6D pose estimation methods. To evaluate the robustness against noisy initial poses, we inject up to 4cm of uniformly sampled random translation noise. We also measure robustness against missing frames by dropping 450 frames out of 3200 frames are uniformly from the testing videos.</p><p>6-PACK outperforms the second best method NOCS <ref type="bibr" target="#b45">[46]</ref> by more than 15% in 5°5 cm metric and 12% in IoU25 metric. <ref type="figure" target="#fig_3">Fig. 5</ref> shows some examples of ours and NOCS pose estimations (left), as well as visualizations of the generated and matched keypoints (right) by 6-PACK. These results indicate that, compared to NOCS, which uses all input pixels as keypoints, our method detects compact and robust 3D keypoints that are best suited for category-based 6D tracking.</p><p>Additionally, 6-PACK outperforms KeypointNet <ref type="bibr" target="#b40">[41]</ref> by a large margin in all metrics. The IoU25 metric reveals that KeypointNet frequently loses track of the object (50.3% across the evaluation set). As discussed in Sec. II, Keypoint-Net is not designed to handle the large unbounded keypoint generation space of our real-world setting. On the other hand, our method avoids losing track of category instances in evaluation (IoU25 &gt;94%). Our anchor-based mechanism increases the space of search avoiding drifting.</p><p>To further examine the robustness and stability of different methods, we compute the mean performance without the first x frames. This way we can measure how much of the performance is due to the ground truth initial pose (frames closer to the initial frame are easy to track). <ref type="figure" target="#fig_2">Fig. 4</ref> depicts the mean accuracy (&lt;5°5 cm percentage). We observe that the performance of all methods decreases, except for NOCS because it is a pose estimation method, not a pose tracker. However, the performance of 6-PACK is more than 10% higher than the NOCS throughout the whole process and stops decreasing approx. 100 after the initial frame. We also evaluate the sensitivity of our proposed method in using different number of keypoints on the laptop category. Our model with 8-keypoints output achieves 62.4% in the 5°5cm metric, surpassing the 4-keypoints 55.2% and the 16-keypoints 48.6% variants. 8-keypoints offer the best trade-off between information compression and redundancy.</p><p>Integration on a Real Robot: Finally, we also deploy our model trained on the NOCS dataset directly to control a real-world robot platform from a computer with an NVIDIA GTX1070 GPU and an Intel Core i7-6700K CPU. The robot used in the experiment is a Toyota HSR (Human Support Robot) equipped with an Asus Xtion RGB-D sensor, a holonomic mobile base, and a two-finger gripper. In this conditions, 6-PACK tracks poses at 10 Hz with less than 30% of the GPU storage (around 2 GB).</p><p>In our tests, we move manually for circa 30 s different instances of the known object categories in front of the robot while the robot tracks them and/or follows them with its gaze. For two of the object categories (bowl and bottle) the robot also performs a manipulation task based on the tracking information (pouring or tossing) at the end of the tracking sequence. To quickly provide an initial coarse pose estimation to initiate tracking, we place and detect a checkerboard that delimits the front face of a 3D bounding box around the target object. We test the tracking performance on bowl (4 instances), bottle (3), laptop (2) and can (2), see video on the project website. Our method successfully tracks without loss the objects in more than 60% of the trials, achieving high accuracy (object visually within the estimated bounding box).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We presented 6-PACK, a category-level 6D object pose tracker. Our tracker is based on a novel anchor-based keypoint generation neural network that detects reliably the same keypoints on different instances of the same category and uses them to estimate the inter-frame change in pose. Our method is trained in an unsupervised manner to allow the network to select the best keypoints for tracking. We compared 6-PACK to 3D geometry methods and deep learning models, showing that our method achieves state-of-the-art performance on a challenging category-based 6D object pose tracking benchmark. Furthermore, we deployed 6-PACK on an HSR robot platform and showed that our method enables real-time tracking and robot interaction.</p><p>ACKNOWLEDGEMENT This work has been partially supported by JD.com American Technologies Corporation ("JD") under the SAIL-JD AI Research Initiative and by an ONR MURI award (1186514-1-TBCJE). This article solely reflects the opinions and conclusions of its authors and not JD or any entity associated with JD.com. We also want to thank Toyota Research Institute for the Human Support Robot which we used to perform our real robot experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>IV. MODEL 6 - 4 Fig. 2 .</head><label>642</label><figDesc>PACK performs category-level 6D pose tracking in the following manner(Fig. 2, bottom-right). First, 6-PACK uses Anchor-Based Keypoint Generation: First, we (1) crop an enlarged volume around the predicted pose of the instance, normalize to a unit, and (2) generate a grid of anchor points on the volume. Then, we (3) use a DenseFusion-based<ref type="bibr" target="#b44">[45]</ref> network to compute fused geometric and color features for the M points in the volume, and (4) average-pool them into N anchor features based on their distance. The anchor features are (5) used by an attention network to select the closest to the centroid, which is then used to (6) generate an ordered set of keypoints. Overall Workflow (bottom right): The anchor-based keypoint generation is applied on the previous and current frames to obtain two sets of ordered keypoints to compute the inter-frame change in pose. This pose is applied to the previously computed instance pose to obtain the current estimated 6D pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Object Classes with Symmetry Axes: An instance of the class bowl with an axis of symmetry along y. Left: perspective view of the object and Cartesian coordinates of five keypoints. Right: Bird's eye view of the object and same points transformed into the symmetry-invariant coordinates (d and θ indicated for keypoint 2). location of a keypoint point k i from (x, y, z) in the Cartesian coordinates to the triplet (d, h, θ) defined as: • Distance to the symmetry axis d: The distance from k i point to the symmetry axis, s gt . • Height along the symmetry axis h: Distance between the orthogonal projection of k i onto the symmetry axis and the Cartesian origin of coordinates. • Relative angle θ: The separating angle between the radial vector connecting the point k i to the symmetry axis and the radial vector of the next keypoint encountered when advancing clockwise around the s gt . The new coordinates are closely related to cylindrical coordinates, where we replace the absolute rotation angle by a relative inter-keypoint angle. Based on the symmetryinvariant transformation, we redefine the multi-view consistency loss for symmetric categories as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Stability Evaluation over Time: Each point on the curve represents the mean success rate (&lt;5°5 cm percentage) on the interval of frames starting from the frame indicated by the x-axis until the end of the sequence, [x, END]. 6-PACK and its variants and KeypointNet are initialized with pose with five different uniformly-sampled translation noise between ±2 cm. Frames closer to the first frame are easier to estimate since the initial (noisy) pose is given. 6-PACK shows a relatively constant performance even if we do subtract the first easier frames from the computation (drop of only 5% without the first 75 frames).• Ours without temporal prediction: An ablation of 6-PACK where the predicted pose in the next frame is the previous estimated pose. • Ours: 6-PACK where the predicted pose in the next frame extrapolates from the last estimated inter-frame change of pose (constant velocity model).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Qualitative Comparison between 6-PACK and NOCS: Depicted frames are 100 tracking frames after the initial frame. (b) Visualization of the Generated Keypoints: Generated and matched keypoints in two largely separated frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">EVALUATION OF 6D POSE ON NOCS-REAL275</cell></row><row><cell></cell><cell></cell><cell>NOCS</cell><cell>ICP</cell><cell>Keypoint</cell><cell>Ours w/o</cell><cell></cell></row><row><cell></cell><cell></cell><cell>[46]</cell><cell>[50]</cell><cell>Net [41]</cell><cell>temporal</cell><cell>Ours</cell></row><row><cell></cell><cell>5°5cm</cell><cell>5.5</cell><cell>10.1</cell><cell>5.9</cell><cell>23.7</cell><cell>24.5</cell></row><row><cell>bottle</cell><cell>IoU25 Rerr</cell><cell>48.7 25.6</cell><cell>29.9 48.0</cell><cell>23.1 28.5</cell><cell>92.0 15.7</cell><cell>91.1 15.6</cell></row><row><cell></cell><cell>Terr</cell><cell>14.4</cell><cell>15.7</cell><cell>9.5</cell><cell>4.2</cell><cell>4.0</cell></row><row><cell></cell><cell>5°5cm</cell><cell>62.2</cell><cell>40.3</cell><cell>16.8</cell><cell>53.0</cell><cell>55.0</cell></row><row><cell>bowl</cell><cell>IoU25 Rerr</cell><cell>99.6 4.7</cell><cell>79.7 19.0</cell><cell>74.7 9.8</cell><cell>100.0 5.3</cell><cell>100.0 5.2</cell></row><row><cell></cell><cell>Terr</cell><cell>1.2</cell><cell>4.7</cell><cell>8.2</cell><cell>1.6</cell><cell>1.7</cell></row><row><cell></cell><cell>5°5cm</cell><cell>0.6</cell><cell>12.6</cell><cell>1.8</cell><cell>8.4</cell><cell>10.1</cell></row><row><cell>camera</cell><cell>IoU25 Rerr</cell><cell>90.6 33.8</cell><cell>53.1 80.5</cell><cell>30.9 45.2</cell><cell>91.0 43.9</cell><cell>87.6 35.7</cell></row><row><cell></cell><cell>Terr</cell><cell>3.1</cell><cell>12.2</cell><cell>8.5</cell><cell>5.5</cell><cell>5.6</cell></row><row><cell></cell><cell>5°5cm</cell><cell>7.1</cell><cell>17.2</cell><cell>4.3</cell><cell>25.0</cell><cell>22.6</cell></row><row><cell>can</cell><cell>IoU25 Rerr</cell><cell>77.0 16.9</cell><cell>40.5 47.1</cell><cell>42.6 28.8</cell><cell>89.9 12.5</cell><cell>92.6 13.9</cell></row><row><cell></cell><cell>Terr</cell><cell>4.0</cell><cell>9.4</cell><cell>13.1</cell><cell>5.0</cell><cell>4.8</cell></row><row><cell></cell><cell>5°5cm</cell><cell>25.5</cell><cell>14.8</cell><cell>49.2</cell><cell>62.4</cell><cell>63.5</cell></row><row><cell>laptop</cell><cell>IoU25 Rerr</cell><cell>94.7 8.6</cell><cell>50.9 37.7</cell><cell>94.6 6.5</cell><cell>97.8 4.9</cell><cell>98.1 4.7</cell></row><row><cell></cell><cell>Terr</cell><cell>2.4</cell><cell>9.2</cell><cell>4.4</cell><cell>2.5</cell><cell>2.5</cell></row><row><cell></cell><cell>5°5cm</cell><cell>0.9</cell><cell>6.2</cell><cell>3.1</cell><cell>22.4</cell><cell>24.1</cell></row><row><cell>mug</cell><cell>IoU25 Rerr</cell><cell>82.8 31.5</cell><cell>27.7 56.3</cell><cell>52.0 61.2</cell><cell>100.0 20.3</cell><cell>95.2 21.3</cell></row><row><cell></cell><cell>Terr</cell><cell>4.0</cell><cell>9.2</cell><cell>6.7</cell><cell>1.8</cell><cell>2.3</cell></row><row><cell></cell><cell>5°5cm</cell><cell>17.0</cell><cell>16.9</cell><cell>13.5</cell><cell>32.5</cell><cell>33.3</cell></row><row><cell>Overall</cell><cell>IoU25 Rerr</cell><cell>82.2 20.2</cell><cell>47.0 48.1</cell><cell>53.0 30.0</cell><cell>95.1 17.1</cell><cell>94.2 16.0</cell></row><row><cell></cell><cell>Terr</cell><cell>4.9</cell><cell>10.5</cell><cell>8.4</cell><cell>3.4</cell><cell>3.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Least-squares fitting of two 3-d point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Blostein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="698" to="700" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">6-dof model-based tracking of arbitrarily shaped 3d objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Münch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5204" to="5209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Silhonet: An rgb method for 3d object pose estimation and grasp planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06893</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brief: Binary robust independent elementary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="778" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial learning of structure-aware fully convolutional networks for landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time 3d model-based tracking using edge and keypoint features for robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4048" to="4055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The moped framework: Object recognition and pose estimation for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1284" to="1306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Poserbpf: A rao-blackwellized particle filter for 6d object pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bretl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09304</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic articulated real-time tracking for robot manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia Cifuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Issac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1546" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentationdriven 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3385" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3028" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth-based object tracking using a robust gaussian filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Issac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Cifuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trimpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="608" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time perception meets reactive motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Issac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mainprice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Cifuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Berenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1864" to="1871" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cornernetlite: Efficient keypoint based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08900</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Keypoint recognition using randomized trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1465" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Point matching as a classification problem for fast and robust object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pilet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="II" to="II" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno>ArXiv:1804.00175</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Kpam: Keypoint affordances for category-level robotic manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06684</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep tracking: Seeing beyond seeing using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04670</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Machine learning for highspeed corner detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, Citeseer</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dart: Dense articulated real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haehnel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Generalized-icp</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10790</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Normalized object coordinate space for categorylevel 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02970</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Probabilistic object tracking using a range camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3195" to="3202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10871</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>ArXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06396</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
