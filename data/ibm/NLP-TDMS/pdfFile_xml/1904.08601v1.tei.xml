<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Optics for Monocular Depth Estimation and 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Chang</surname></persName>
							<email>jchang10@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
							<email>gordon.wetzstein@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford Univsersity</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Optics for Monocular Depth Estimation and 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation and 3D object detection are critical for scene understanding but remain challenging to perform with a single image due to the loss of 3D information during image capture. Recent models using deep neural networks have improved monocular depth estimation performance, but there is still difficulty in predicting absolute depth and generalizing outside a standard dataset. Here we introduce the paradigm of deep optics, i.e. end-to-end design of optics and image processing, to the monocular depth estimation problem, using coded defocus blur as an additional depth cue to be decoded by a neural network. We evaluate several optical coding strategies along with an end-to-end optimization scheme for depth estimation on three datasets, including NYU Depth v2 and KITTI. We find an optimized freeform lens design yields the best results, but chromatic aberration from a singlet lens offers significantly improved performance as well. We build a physical prototype and validate that chromatic aberrations improve depth estimation on real-world results. In addition, we train object detection networks on the KITTI dataset and show that the lens optimized for depth estimation also results in improved 3D object detection performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth awareness is crucial for many 3D computer vision tasks, including semantic segmentation <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b9">10]</ref>, 3D object detection <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>, 3D object classification <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">30]</ref>, and scene layout estimation <ref type="bibr" target="#b48">[48]</ref>. The required depth information is usually obtained with specialized camera systems, for example using time-of-flight, structured illumination, pulsed LiDAR, or stereo camera technology. However, the need for custom sensors, highpower illumination, complex electronics, or bulky device form factors often makes it difficult or costly to employ these specialized devices in practice.</p><p>Single-image depth estimation with conventional cameras has been an active area of research. Traditional approaches make use of pre-defined image features that are <ref type="bibr">Figure 1</ref>. We apply deep optics, i.e. end-to-end design of optics and image processing, to build an optical-encoder, CNN-decoder system for improved monocular depth estimation and 3D object detection.</p><p>statistically correlated with depth, e.g. shading, perspective distortions, occlusions, texture gradients, and haze <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b17">18]</ref>. Recently, significant improvements have been achieved by replacing hand-crafted features with learned features via convolutional neural networks (CNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref>. While these methods tend to perform decently within consistent datasets, they do not generalize well to scenes that were not part of the training set. In essence, the problem of estimating a depth map from pictorial cues alone is ill-posed. Optically encoding depth-dependent scene information has the potential to remove some of the ambiguities inherent in all-in-focus images, for example using (coded) defocus blur <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b0">1]</ref> or chromatic aberrations <ref type="bibr" target="#b43">[43]</ref>. However, it is largely unclear how different optical coding strategies compare to one another and what the best strategy for a specific task may be.</p><p>Inspired by recent work on deep optics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b11">12]</ref>, we interpret the monocular depth estimation problem with coded defocus blur as an optical-encoder, electronic-decoder system that can be trained in an end-to-end manner. Although co-designing optics and image processing is a core idea in computational photography, only differentiable estimation algorithms, such as neural networks, allow for true end-toend computational camera designs. Here, error backprograpagation in the training phase not only optimizes the net-work weights but also the physical lens parameters. With the proposed deep optics approach, we evaluate several variants of optical coding strategies for two important 3D scene understanding problems: monocular depth estimation and 3D object detection.</p><p>In a series of experiments, we demonstrate that the deep optics approach optimizes the accuracy of depth estimation across several datasets. Consistent with previous work, we show that optical aberrations that are typically considered undesirable for image quality are highly beneficial for encoding depth cues. Our results corroborate that defocus blur provides useful information, but we additionally find that adding astigmatism and chromatic aberrations even further improves accuracy. By jointly optimizing a freeform lens, i.e. the spatially varying surface height of a lens, with the CNNs weights we achieve the best results. Surprisingly, we find that the accuracy of optimized lenses is only slightly better than standard defocus with chromatic aberrations. This insight motivates the use of simple cameras with only a single lens over complex lens systems when prioritizing depth estimation quality, which we validate with an experimental prototype.</p><p>We also evaluate the benefits of deep optics for higherlevel 3D scene understanding tasks. To this end, we train a PointNet <ref type="bibr" target="#b29">[29]</ref> 3D object detection network on the KITTI dataset. We find that, compared to all-in-focus monocular images, images captured through the optimized lenses also perform better in 3D object detection, a task which requires semantic understanding on top of depth estimation to predict 3D bounding boxes on object instances.</p><p>In sum, our experiments demonstrate that an optimized lens paired with a concurrently trained neural network can improve depth estimation without sacrificing higher-level image understanding. Specifically, we make the following contributions:</p><p>• We build a differentiable optical image formation model that accounts for either fixed (defocus, astigmatism, chromatic aberration) or optimizable (freeform or annular) lens designs, which we integrate with a differentiable reconstruction algorithm, i.e. a CNN. • We evaluate the joint optical-electronic model with the various lens settings on three datasets (Rectangles, NYU Depth-v2, KITTI). The optimized freeform phase mask yields the best results, with chromatic aberrations coming in a close second. • We build a physical prototype and validate that captured images with chromatic aberrations achieve better depth estimation than their all-in-focus counterparts. • We train a 3D object detection network with the optimized lens and demonstrate that the benefits of improved depth estimation carry through to higher level 3D vision. Note that the objective of our work is not to develop the state-of-the-art network architecture for depth estimation, but to understand the relative benefits of deep optics over fixed lenses. Yet, our experiments show that deep optics achieves lower root-mean-square errors on depth estimation tasks with a very simple U-Net <ref type="bibr" target="#b34">[34]</ref> compared to more complex networks taking all-in-focus images as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Monocular Depth Estimation Humans are able to infer depth from a single image, provided enough contextual hints that allow the viewer to draw from past experiences. Deep monocular depth estimation algorithms aim at mimicking this capability by training neural networks to perform this task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref>. Using various network architectures, loss functions, and supervision techniques, monocular depth estimation can be fairly successful on consistent datasets such as KITTI <ref type="bibr" target="#b6">[7]</ref> and NYU Depth <ref type="bibr" target="#b38">[38]</ref>. However, performance is highly dependent on the training dataset. To address this issue, several recent approaches have incorporated physical camera parameters into their image formation model, including focal length <ref type="bibr" target="#b13">[14]</ref> and defocus blur <ref type="bibr" target="#b0">[1]</ref>, to implicitly encode 3D information into a 2D image. We build on these previous insights and perform a significantly more extensive study that evaluates several types of fixed lenses as well as fully optimizable camera lenses for monocular depth estimation and 3D object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Photography for Depth Estimation</head><p>Modifying camera parameters for improved depth estimation is a common approach in computational photography. For example, coding the amplitude <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b49">49]</ref> or phase <ref type="bibr" target="#b20">[21]</ref> of a camera aperture has been shown to improve depth reconstruction. Chromatic aberrations have also been shown to be useful for estimating the depth of a scene <ref type="bibr" target="#b43">[43]</ref>. Whereas conventional defocus blur is symmetric around the focal plane, i.e. there is one distance in front of the focal plane that has the same PSF as another distance behind the focal plane, defocus blur with chromatic aberrations is unambiguous. In all these approaches, depth information is encoded into the image in a way that makes it easier for an algorithm to succeed at a certain task, such as depth estimation. In this paper, we combine related optical coding techniques with more contemporary deep-learning methods. The primary benefit of a deep learning approach over previous work is that these allow a loss function applied to a high-level vision task, e.g. object detection, to directly influence physical camera parameters in a principled manner, such as the lens surface.</p><p>Deep Optics Deep learning can be used for jointly training camera optics and CNN-based estimation methods. This approach was recently demonstrated for applications in extended depth of field and superresolution imaging <ref type="bibr" target="#b39">[39]</ref>, image classification <ref type="bibr" target="#b1">[2]</ref>, and multicolor localization microscopy <ref type="bibr" target="#b25">[25]</ref>. For example, Hershko et al. <ref type="bibr" target="#b25">[25]</ref> proposed to learn a custom diffractive phase mask that produced highly wavelength-dependent point spread functions (PSFs), allowing for color recovery from a grayscale camera. In our applications, an optical lens model also creates depthdependent PSFs with chromatic aberrations. However, our deep camera is designed for computer vision applications rather than microscopy. The work closest to ours is that of Haim et al. <ref type="bibr" target="#b11">[12]</ref>, who designed a diffractive phase mask consisting of concentric rings to induce chromatic aberrations that could serve as depth cues <ref type="bibr" target="#b11">[12]</ref>. The training process optimized the ring radii and phase shifts within two or three annular rings but did not allow for deviation from this simple parametric lens model. In our experiments, we systematically evaluate the comparative performances of non-optimized aberrated lenses as well as fully optimizable freeform lenses. Unlike previous work, we explore applications in depth estimation and also 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Differentiable Image Formation Model</head><p>To optimize optical lens elements that best encode depthdependent scene information, we model light transport in the camera using wave optics. This is not only physically accurate but also allows for both refractive and diffractive optical elements to be optimized. Due to the fact that the light in a natural scene is incoherent, we only rely on a coherent light transport model to simulate the depth-and wavelenth-dependent point spread function (PSF) of the system, which we then use to simulate sensor images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modeling Conventional Cameras</head><p>We begin by building a camera model consisting of a single convex thin lens with focal length f at a distance s from the sensor (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The relationship between the in-focus distance and the sensor distance is given by the thin-lens equation:</p><formula xml:id="formula_0">1 f = 1 d + 1 s<label>(1)</label></formula><p>Hence an object at a distance d in front of the lens appears in focus at a distance s behind the lens. When imaging a real-world scene, there are likely to be objects at multiple depths that are imaged with different PSFs. To simulate the PSF at a depth z, we consider a point emitter of wavelength λ centered on the optical axis located a distance z away from the center of the thin lens. Our general approach is to propagate the wave of light through the optical system to the sensor. To begin, we first propagate the light emitted by the point, represented as a spherical wave, to the lens. The complex-valued electric field immediately before the lens is given by:</p><formula xml:id="formula_1">U in (x, y) = exp(ik x 2 + y 2 + z 2 )<label>(2)</label></formula><p>where k = 2π/λ is the wavenumber. The next step is to propagate this wave field through the lens by multiplying the input by a phase delay, t(x, y), induced at each location on the lens. Such a phase shift of a wave is physically produced by light slowing down as it propagates through the denser material of the optical element. The thickness profile, ∆(x, y), of a convex thin lens with index of refraction n(λ) in a paraxial regime <ref type="bibr" target="#b8">[9]</ref> is</p><formula xml:id="formula_2">∆(x, y) = ∆ 0 − x 2 + y 2 2f (n(λ) − 1)<label>(3)</label></formula><p>where ∆ 0 is the center thickness. Note that the refractive index is wavelength-dependent, which is necessary to model chromatic aberrations correctly. Converting thickness to the corresponding phase shift, φ = k(n − 1)∆, and neglecting the constant phase offset from ∆ 0 , the phase transformation is</p><formula xml:id="formula_3">t(x, y) = e iφ(x,y) = exp −i k 2f (x 2 + y 2 )<label>(4)</label></formula><p>Additionally, since a lens has some finite aperture size, we insert an amplitude function A(x, y) that blocks all light in regions outside the open aperture. To find the electric field immediately after the lens, we multiply the amplitude and phase modulation of the lens with the input electric field:</p><formula xml:id="formula_4">U out (x, y) = A(x, y) t(x, y) U in (x, y)<label>(5)</label></formula><p>Finally, the field propagates a distance s to the sensor with the exact transfer function <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_5">H s (f x , f y ) = exp iks 1 − (λf x ) 2 − (λf y ) 2<label>(6)</label></formula><p>where (f x , f y ) are spatial frequencies. This transfer function is applied in the Fourier domain as:</p><formula xml:id="formula_6">U sensor (x , y ) = F −1 F {U out (x, y)} · H s (f x , f y ) (7)</formula><p>where F denotes the 2D Fourier transform. Since the sensor measures light intensity, we take the magnitude-squared to find the final PSF:</p><formula xml:id="formula_7">PSF λ,z (x , y ) = |U sensor (x , y )| 2<label>(8)</label></formula><p>By following this sequence of forward calculations, we can generate a 2D PSF for each depth and wavelength of interest. Since the lens was initially positioned to focus at a distance d, we can expect the PSF for z = d to have the sharpest focus and to spread out away from this focal plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modeling Freeform Lenses</head><p>Several variables such as focal length, focus distance, and aperture size are modeled by the above formulation. For maximum degrees of freedom to shape the PSF, we can also treat the optical element as a freeform lens by assuming that is has an additional arbitrary thickness profile ∆ ff (x, y). The corresponding phase delay is</p><formula xml:id="formula_8">t ff (x, y) = exp [jk(n ff (λ) − 1)∆ ff (x, y)]<label>(9)</label></formula><p>where n ff (λ) is the wavelength-dependent index of refraction of the lens material. We parametrize ∆ ff with the Zernike basis (indices 1-36, <ref type="bibr" target="#b27">[27]</ref>), which leads to smoother surfaces. The intensity PSF of a freeform lens is then</p><formula xml:id="formula_9">PSF λ,z (x, y; λ) = |F −1 {F{A · t lens · t ff · U in } · H s }| 2 (x, y)<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth-Dependent Image Formation</head><p>We can use these simulated PSFs to approximate a captured image of a 3D scene on an RGB sensor. To this end, we use a layered representation that models the scene as a set of planar surfaces at a discrete number of depth planes <ref type="bibr" target="#b12">[13]</ref>. This allows for precomputation of a fixed number of PSFs corresponding to each depth plane. We make a few modifications here to suit our datasets consisting of pairs of all-in-focus RGB images and their discretized depth maps. For an all-in-focus image L, a set of j = 1 . . . J discrete depth layers, and occlusion masks {M j }, we calculate our final image by:</p><formula xml:id="formula_10">I λ = J j=1 (L λ * PSF λ,j ) · M j<label>(11)</label></formula><p>where * denotes 2D convolution for each color channel centered on λ. The occlusion masks {M j } represent the individual layers of the quantized depth map. To ensure smooth transitions between the masks of a scene, we additionally blur each of the quantized layers and re-normalize them, such that j M j = 1 at each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Depth Estimation</head><p>In this section, we detail our experiments for deep optics for monocular depth estimation with encoded blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network and Training</head><p>For depth estimation, we connect our differentiable image formation model to a U-Net <ref type="bibr" target="#b34">[34]</ref> that takes as input either the simulated sensor images or the original all-infocus dataset images. The network consists of 5 downsampling layers ({Conv-BN-ReLU}×2→MaxPool2×2) followed by 5 upsampling layers with skip connections (Conv T +Concat→{Conv-BN-ReLU}×2). The output is the predicted depth map, at the same resolution as the input image. We use the standard ADAM optimizer with a meansquare-error (MSE) loss on the logarithmic depth. We train the models for 40,000 iterations at a learning rate of .001 and batch size of 3. We additionally decay the learning rate to 1e-4 for the Rectangles dataset.</p><p>We evaluate on (1) a custom Rectangles dataset, which consists of white rectangles against a black background places at random depths (see Supplement), (2) the NYU Depth v2 dataset with standard splits, and (3) a subset of the KITTI depth dataset (5500 train, 749 val) that overlaps with the object detection dataset for which we obtained dense "ground truth" depth maps from Ma et al. <ref type="bibr" target="#b22">[23]</ref>. We train on full-size images. We calculate loss for NYU Depth on the standard crop size, and for KITTI only on the official sparse ground truth depth.</p><p>For the Rectangles and NYU Depth datasets, we initialize the phase mask as an f/8, 50 mm focal length lens, focused to 1 m. For the KITTI dataset, we initialize an f/8, 80 mm focal length lens, focused to 7.6 m. When the lens is being optimized, we also initialize the U-Net with the optimized weights for the fixed lens, and each training step adjusts the parameters of the lens (Zernike coefficients for freeform, ring heights for annular) and the U-Net. We use 12 depth bins in our simulations, spaced linearly in inverse depth. When optimizing a freeform lens for the KITTI dataset, we reduce this to 6 intervals due to GPU memory constraints and train for 30,000 iterations; then we freeze the lens and increase back to 12 intervals to fine-tune the U-Net for an additional 30,000 iterations. <ref type="table" target="#tab_0">Table 1</ref> shows a summary of results for all datasets. Examples of simulated sensor images and predicted depth maps from NYU Depth and KITTI are shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (see</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplement for Rectangles).</head><p>We observe common trends across all datasets. When using the all-in-focus images, errors are highest. This is most intuitive to understand with the Rectangles dataset. If there is a randomly-sized white rectangle floating in space that is always in focus, there are no depth cues for the network to recognize, and the network predicts the mean depth for every rectangle. Depth from defocus-only improves performance, but there is still ambiguity due to symmetric blur along inverse depth in both directions from the focal plane. Astigmatism (see Supplement for details) helps resolve this ambiguity, and the inherent chromatic aberration of a singlet lens further improves results.</p><p>We optimize two freeform lenses for each dataset. The annular lens consist of three concentric layers of different heights, inspired by <ref type="bibr" target="#b11">[12]</ref>. While these optimized lenses outperformed all-in-focus experiments, they did not yield higher accuracy than chromatic aberration from a fixed lens. In contrast, the optimized freeform lens showed the best results, demonstrating the ability of the end-to-end optimization to learn a new freeform lens that better encodes depth information. For NYU Depth, we found that additionally initializing ∆ ff with astigmatism yielded the better results. <ref type="table">Table 2</ref> additionally compares default metrics on the NYU Depth test set with reported results from previous works. These comparisons suggest that adding this optical encoder portion of the model can yield results on par with state-of-the-art methods with more heavyweight and carefully designed networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results</head><p>We build a prototype for monocular depth estimation using chromatic aberration on real-world scenes. Our camera consisted of a Canon EOS Rebel T5 camera and a biconvex singlet lens (f = 35mm, Thorlabs) with a circular aperture  (D = 0.8 mm). We captured a series of images of a point white light source to calibrate the modeled PSFs with the captured PSFs, primarily by adjusting a spherical aberration parameter. We retrain a depth estimation network for the calibrated PSFs with the NYU Depth dataset, including a downsampling factor of four due to the smaller image size of dataset compared to the camera sensor. For this network, after convolution in linear intensity, we apply sRGB conversion to produce the simulated sensor image, which allows us to directly input captured sRGB camera images during evaluation. We capture images in a variety of settings with the prototype as described along with an all-in-focus pair obtained by adding a 1 mm pinhole in front of the lens (see Supplement for images). We use our retrained depth estimation network to predict a depth map from the blurry images, and we use the all-in-focus network to predict the corresponding depth map from the all-in-focus images. <ref type="figure" target="#fig_3">Fig. 5</ref> shows a few  examples; more are included in the supplement. Depth estimation with the optical model performs significantly better on the captured images, as physical depth information is actually encoded into the image, allowing the network to rely not just on dataset priors for prediction. A limitation of our prototype was its smaller field of view, due to camera vignetting and the spatially varying nature of the real PSF, which prevented capture of full indoor room scenes. This could be improved by adding another lens to correct for other aberrations <ref type="bibr" target="#b3">[4]</ref> or by including these variations in the image formation model <ref type="bibr" target="#b14">[15]</ref>.  <ref type="table">Table 3</ref>. Object detection performance measured by 2D AP % (IoU = 0.5) and 3D AP % (IoU = 0.5) on our validation split of the KITTI object detection dataset using the all-in-focus and optimized mask models. Higher values are bolded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">3D Object Detection</head><p>To assess whether an optical system optimized for improved depth estimation is beneficial for higher-level 3D scene understanding as well, we evaluate 3D object detection performance on the KITTI dataset using the same optical system. 3D object detection requires recognizing instances of different objects as well as regressing an oriented 3D bounding box around each object instance. Depth information, whether implicitly contained in an image or explic- itly provided from a depth sensor, is critical for this task, as is evidenced in the large gap in performance between the RGB and RGB+LIDAR methods shown in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>We train a 3D object detection network specific to the freeform lens optimized for KITTI depth estimation. In particular, we use a Frustrum PointNet v1 (FPointNet, <ref type="bibr" target="#b29">[29]</ref>), which was demonstrated to work with both sparse LIDAR point clouds and dense depth images. FPointNet first uses 2D bounding box predictions on the RGB image to generate frustrum proposals that bound a 3D search space; then 3D segmentation and box estimation occur on the 3D point cloud contained within each frustrum. In our modified network, we substitute the ground truth LIDAR point clouds with our estimated depth maps projected into a 3D point cloud. As in the original method, ground truth 2D boxes augmented with random translation and scaling are used during training, but estimated 2D bounding boxes from a separately trained 2D object detection network (Faster R-CNN, <ref type="bibr" target="#b32">[32]</ref>) are used during validation. Since we require accurate dense ground truth depth maps to generate our simulated sensor images, we report results for our validation split, for which we did obtain a reliable dense depth map. For comparison, we train the same networks with all-infocus images and their estimated depth maps. More details on our implementation of these networks and assessment on the test set is included in the Supplement. <ref type="table" target="#tab_3">Tables 3 and 4</ref>. Average precision (AP) values are computed by the standard PASCAL protocol, as described in the KITTI development kit. 2D object detection performance is similar between the all-in-focus and optimized systems, which implies that even though the sensor images from the optimized optical element appear blurrier than the all-infocus images, the networks are able to extract comparable information from the two sets of images. More notably, 3D object detection improves with the optimized optical system, indicating that the FPointNet benefits from the improved depth maps enabled with the optimized lens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of our object detection experiments are shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Throughout our experiments, we demonstrate that a joint optical-encoder, electronic-decoder model outperforms the corresponding optics-agnostic model using all-in-focus images. We build a differentiable optical image formation layer that we join with a depth estimation network to allow for end-to-end optimization from camera lens to network weights. The fully optimized system yields the most accurate depth estimation results, but we find that native chromatic aberrations can also encode valuable depth information. Additionally, to verify that improved depth encoding does not need to sacrifice other important visual content, we show that the lens optimized for depth estimation maintains 2D object detection performance while further improving 3D object detection from a single image.</p><p>As mentioned, our conclusions are primarily drawn from the relative performance between our results. We do not claim to conclusively surpass existing methods, as we use the ground truth or pseudo-truth depth map in simulating our sensor images, and we are limited to an approximate, discretized, layer-based image formation model. There may be simulation inaccuracies that are not straightforward to disentangle unless the entire dataset was recaptured through the different lenses. Nonetheless, our real-world experimental results are promising in supporting the advantage of optical depth encoding, though more extensive experiments, especially with a larger field-of-view, would be valuable. We are interested in future work to see how an optical layer can further improve leading methods, whether for monocular depth estimation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b5">6]</ref> or other visual tasks.</p><p>More broadly, our results consistently support the idea that incorporating the camera as an optimizable part of the network offers significant benefits over considering the image processing completely separately from image capture. We have only considered the camera as a single static optical layer in this paper, but there may be potential in more complex designs as research in both optical computing and computer vision continues to advance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>PSF simulation model. (Top) Optical propagation model of point sources through a phase mask placed in front of a thin lens. PSFs are simulated by calculating intensity of the electric field at the sensor plane. (Bottom) Sample PSFs from thin lens defocus only, with chromatic aberrations, and using an optimized mask initialized with astigmatism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Depth-dependent image formation. Given a set of lens parameters, an all-in-focus image, and its binned depth map, the image formation model generates the appropriate PSFs and applies depth-dependent convolution to simulate the corresponding sensor image, which is then passed into a U-Net for depth estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Depth estimation. (Top) Examples with RMSE (m) from the NYU Depth v2 dataset with all-in-focus, defocus, chromatic aberration, and optimized models. The simulated sensor image from the optimized system is also shown. (Bottom) Examples with RMSE (m) from the KITTI dataset (cropped to fit) with all-in-focus and optimized models; the sensor image from the optimized model is also shown. All depth maps use the same colormap, but the maximum value is 7 m for NYU Depth and 50 m for KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Real-world capture and depth estimation. (Top) Captured and calibrated depth-dependent PSFs, displayed at the same scale. (Bottom) Examples of images captured using our prototype with a zoomed region inset, depth estimation with chromatic aberration, and depth estimation from the corresponding all-in-focus image (not shown). Depth map colorscale is the same for all depth maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Depth estimation error with different optical models for various datasets. RMSEs are reported for linear and log (base e or 10) scaling of depth (m or log(m)). Lowest errors are bolded, and second-lowest are italicized. The KITTI* dataset is our KITTI dataset subset.</figDesc><table><row><cell></cell><cell cols="2">Rectangles</cell><cell cols="2">NYU Depth v2</cell><cell cols="2">KITTI*</cell></row><row><cell>Optical model</cell><cell>RMSE lin</cell><cell>RMSE log</cell><cell>RMSE lin</cell><cell>RMSE log10</cell><cell>RMSE lin</cell><cell>RMSE log</cell></row><row><cell>All-in-focus</cell><cell>0.4626</cell><cell>0.3588</cell><cell>0.9556</cell><cell>0.1452</cell><cell>2.9100</cell><cell>0.1083</cell></row><row><cell>Defocus, achromatic</cell><cell>0.2268</cell><cell>0.1805</cell><cell>0.4814</cell><cell>0.0620</cell><cell>2.5400</cell><cell>0.0776</cell></row><row><cell>Astigmatism, achromatic</cell><cell>0.1348</cell><cell>0.0771</cell><cell>0.4561</cell><cell>0.0559</cell><cell>2.3634</cell><cell>0.0752</cell></row><row><cell>Chromatic aberration</cell><cell>0.0984</cell><cell>0.0563</cell><cell>0.4496</cell><cell>0.0556</cell><cell>2.2566</cell><cell>0.0702</cell></row><row><cell>Optimized, annular</cell><cell>0.1687</cell><cell>0.1260</cell><cell>0.4817</cell><cell>0.0623</cell><cell>2.7998</cell><cell>0.0892</cell></row><row><cell>Optimized, freeform</cell><cell>0.0902</cell><cell>0.0523</cell><cell>0.4325</cell><cell>0.0520</cell><cell>1.9288</cell><cell>0.0621</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>3D object localization AP % (bird's eye view) and 3D object detection AP % (IoU= 0.7) for the car class. The listed numbers from literature are reported on the official test set; results from our methods are reported on our validation split.</figDesc><table><row><cell>3D object localization</cell><cell>3D object detection</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep depth from defocus: how can defocus blur improve 3d estimation using dense neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trouvé-Peloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Champagnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="307" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wetzstein. Hybrid optical-electronic convolutional neural networks with optimized diffractive optics for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12324</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral focal sweep: Extended depth of field from chromatic aberrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Cossairt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Introduction to Fourier optics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Macmillan Learnng, 4 edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth estimation from a single image using deep learned phase coded mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elmalem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A layer-based restoration framework for variable-aperture photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning depth from single images with deep neural network embedding focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4676" to="4689" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-quality computational imaging through simple lenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Hullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Labitzke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="149" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Obtaining shape from shading information. The psychology of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="page" from="115" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">4d frequency analysis of computational cameras for depth of field extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">97</biblScope>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multicolor localization microscopy by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shechtman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01637</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Illumination planning for object recognition in structured environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1994 IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zernike polynomials and atmospheric turbulence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Noll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOsA</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="211" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A new sense for depth of field. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Building part-based object detectors via 3d geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1745" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end optimization of optics and image processing for achromatic extended depth of field and super-resolution imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Inferring spatial layout from a single image via depth-ordered grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Passive depth estimation using chromatic aberration and a depth from defocus approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trouvé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Champagnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Le</forename><surname>Besnerais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Idier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="7152" to="7164" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM transactions on graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiscale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5354" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Estimating the 3d layout of indoor scenes and its clutter from depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1273" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Coded aperture pairs for depth from defocus and defocus deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="72" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
