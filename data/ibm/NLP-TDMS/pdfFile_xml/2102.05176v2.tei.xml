<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer learning based few-shot classification using optimal transport mapping from preprocessed latent space of backbone neural network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Chobola</surname></persName>
							<email>choboto1@fit.cvut.cz</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vašata</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kordík</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague Thakurova</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer learning based few-shot classification using optimal transport mapping from preprocessed latent space of backbone neural network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>MetaDL Challenge 2020 focused on image classification tasks in few-shot settings. This paper describes second best submission in the competition. Our meta learning approach modifies the distribution of classes in a latent space produced by a backbone network for each class in order to better follow the Gaussian distribution. After this operation which we call Latent Space Transform algorithm, centers of classes are further aligned in an iterative fashion of the Expectation Maximisation algorithm to utilize information in unlabeled data that are often provided on top of few labelled instances. For this task, we utilize optimal transport mapping using the Sinkhorn algorithm. Our experiments show that this approach outperforms previous works as well as other variants of the algorithm, using K-Nearest Neighbour algorithm, Gaussian Mixture Models, etc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Few-shot learning is increasingly popular because it can handle machine learning tasks with just a few learning examples. It is also more biologically plausible and closer to what we observe in nature. While learning a new task, one normally does not start from a randomly initialised neural network presenting hundreds of thousands of examples in several thousands epochs.</p><p>When you are told to remember a person from a picture, you are able to distinguish this person from others even when you see her in different positions or environments. In machine learning, this is called one shot learning. The task of one shot learning is to learn new classes given only one instance available for each class. Three-way five-shot learning means learning three classes given five training instances each. You do not learn classifiers from scratch, but you typically use neural networks trained on similar tasks using much more data. This also reflects the natural situation when the visual perception is already well trained on similar tasks when trying to remember a new person from the picture. This process can be also called meta learning or transfer learning as one uses a pretrained neural network called a backbone network. Also, in a few-shot learning scenario, Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. you can often utilise unlabelled instances apart of those few labelled samples that are available for the task.</p><p>MetaDl challenge 2020 1 focused on few shot learning of image classification tasks. Participants trained a metalearner on a meta-train set and produced a learner which was subsequently used to train on classification tasks generated from the meta-test set and evaluated. The goal was to discover learners with ability to quickly adapt to new unseen image classification tasks.</p><p>Our submissions scored second in the final leaderboard. This paper describes methods we have experimented with and the architecture of the meta-learning pipeline responsible for second best result in the competition. The architecture of our solution mainly follows (Hu, Gripon, and Pateux 2020) with important improvements in the preprocessing of latent space output of the backbone model B. The main improvement is in the different normalization of the transformed feature vectors which resembles the Gaussian distribution assumption better. Since this is the key assumption for the proper functionality of the Sinkhorn mapping algorithm, it leads to more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>There are several different approaches to few shot learning. The survey <ref type="bibr" target="#b7">(Wang et al. 2020</ref>) is a good resource to learn about general overview and taxonomy of few shot learning methods. Prototypical networks <ref type="bibr" target="#b3">(Snell, Swersky, and Zemel 2017)</ref> and the Siamese networks (Koch, Zemel, and Salakhutdinov 2015) focus on learning embeddings transforming the data in a way that it can be recognised with a simple classifier. This approach is further enhanced by relation networks <ref type="bibr" target="#b5">(Sung et al. 2018)</ref> which is able to classify images of new classes by predicting distances between query images and the few examples of each new class.</p><p>Another interesting direction aims at the learning process itself. In (Ravi and Larochelle 2017) a recurrent network based meta-learner model learns the exact optimization algorithm used to train another learner neural network classifier in the few-shot setup. Meta-transfer learning <ref type="bibr" target="#b4">(Sun et al. 2019</ref>) adapts a deep neural network for few shot learning tasks. Transfer is achieved by learning scaling and shifting functions of DNN weights for each task.</p><p>We further extend the direction of few-shot learning research that is leveraging classification capabilities in robust backbone models (neural networks) pretrained on similar tasks. These transfer learning based methods need to find mapping of few-shot classes to similar classes used to train the backbone model.</p><p>In <ref type="bibr" target="#b1">(Rohrbach, Ebert, and Schiele 2013)</ref> the Propagated Semantic Transfer has been applied to employ semantic knowledge transfer to original classes, combine the transferred predictions with labels for the novel classes, exploit the manifold structure of novel classes by graph based learning and improve the local neighborhood in such graph structures by replacing the raw feature-based representation with an attribute-based representation.</p><p>When transferring the knowledge, deep embeddings are far superior, compared to weight transfer, as a starting point for novel tasks as investigated in <ref type="bibr" target="#b2">(Scott, Ridgeway, and Mozer 2018)</ref>. Another similar approach is TransMatch <ref type="bibr" target="#b8">(Yu et al. 2020)</ref>, where a feature extractor is pre-trained on original classes and subsequently used to initialize few-shot classifier weights for the novel classes, the classifier is also updated with a semisupervised learning method.</p><p>Our research proceeds from (Hu, Gripon, and Pateux 2020), where the latent space produced by a backbone deep network is preprocessed by a power transform and optimaltransport algorithm maps original classes to novel classes while centres on new classes are iteratively adjusted. This approach has shown significant improvement in accuracy in our experiments. The importance of feature transformation for few-shot learning is confirmed by <ref type="bibr" target="#b7">(Wang et al. 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model description</head><p>Formally, in a few-shot learning task one has a dataset D containing a part D S with a few labelled samples from w classes and a part D Q with some unlabelled samples. The goal is to predict the classes for samples in D Q . We will assume that D S contains exactly s labelled samples for each class and D Q contains exactly q unlabelled samples for each class. Hence, there are ws samples in D S and wq samples in D Q . The i-th sample from D will be denoted by x i and if it is from D S we will denote its label by y i .</p><p>Moreover, let us assume that there is another dataset D B corresponding to some related task, such as image classification to some novel classes. This dataset can be used to train the backbone model b which maps the initial space into some latent feature space L = R d . In order to train such a model one might train the neural network for classification and then remove the last classification layers as we did in the experiments. Or an encoder part of an autoencoder might be used.</p><p>The next step is to preprocess the points in the latent space to be prepared for the final prediction algorithm that estimates the labels. As was recently researched this step is crucial and may lead to significant improvements of the result, see <ref type="bibr" target="#b7">(Wang et al. 2019</ref>). To proceed we will further assume that the features obtained from the backbone model B are non-negative, i.e. L = R d + . This is often the case when one extracts b as a part of some neural network with the ReLU activation function on inner layers. Let us denote by B the dataset D transformed by b and by B S and B Q its parts corresponding to D S and D Q , respectively.</p><p>In the preprocessing we transform the dataset D of points in the latent space L to a final dataset F of points in the final feature space F = R r , where the dimension r = min{d, w(s + q)} is the minimum of the dimension d of L and the number of points in the dataset D. The preprocessing is a composition of three steps and we will call it the Latent Space Transform algorithm (LST). The first is the power transform combined with the semi-normalization of each point given by</p><formula xml:id="formula_0">f 1 (u) = (u + ε) β (u + ε) β δ 2 for all u ∈ L,</formula><p>where the power is taken component-wise, ε = 10 −6 is the normalization parameter, and · is the Euclidean norm. The hyperparameter β controls the strength of the power transform and the hyperparameter δ controls the strength of the normalization, where δ = 1 means the full normalization and δ = 0 yields no normalization at all. The power transform is known to help stabilising the variance and making the data more Gaussian distribution-like by reducing its skewness, see <ref type="bibr" target="#b0">(Box and Cox 1964)</ref>. The normalization on the other hand leads to the projection on the unit sphere which is not compatible with the assumption used later in the optimal-transport that the components of points in the same class are independent with Gaussian distribution of the same variance. Hence, the semi-normalization controlled by the hyperparameter δ enables for having some variance in the perpendicular direction to the unit sphere surface and thus does not a priori break the compatibility of the resulting distribution with the Gaussian assumption. Let us denote the dataset with all points in B transformed using f 1 by F 1 and F 1,S , F 1,Q analogously. The second step is the removal of unnecessary dimensions using the QR decomposition of the transposition of the already preprocessed data matrix F 1 ∈ R w(s+q),d corresponding to dataset F 1 , F T 1 = QR and thus we define F 2 = F 1 Q so that F 2 ∈ R w(s+q),r , where r = min{d, w(s + q)}, and the corresponding dataset is denoted by F 2 . We again denote by F 2,S and F 2,Q the parts of F 2 that corresponds to samples originally in D S and D Q , respectively. It corresponds to the change of the orthonormal basis in the R d and throwing away the dimensions that are zero for the data points.</p><p>The last preprocessing step is the centering and further semi-normalization given by  <ref type="figure">Figure 1</ref>: In order to predict the class label of a test example, we transform the image using a backbone CNN to the latent space and preprocess vectors by the Latent Space Transform algorithm that helps to transform distribution of individual classes to Gaussian like. Then a test example is processed and compared to the class centres that have been iteratively adjusted using a Sinkhorn mapping with unlabeled data projected to the latent space in the same way. The closest class is assigned to the test example as prediction.</p><formula xml:id="formula_1">f 3 (u) = u −ū u γ 2 , whereū = 1 w(s + q) w(s+q) i=1 u i CNN Backbone</formula><p>is the centroid (component-wise average) of the dataset F 2 . Again, the hyperparameter γ allows to control the strength of the normalization. For γ &lt; 1 the resulting points are only partially normalized and one may expect to better resemble the Gaussian distribution assumed in the next step. The typical result for the final Euclidean norms of transformed points is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Let us denote the final preprocessed dataset by F and its respective parts corresponding to original parts D S and D Q by F S and F Q , respectively.</p><p>Once the preprocessing of the dataset is finished, the actual optimal-transport can begin. In this part we directly follow (Hu, Gripon, and Pateux 2020). The preliminary assumption of the method is the independent Gaussian distributions of all components of points in individual classes with class centres c 1 , . . . , c w as parameters. Moreover, it is assumed that all the Gaussian distributions have the same variance λ/2, where λ is the hyperparameter. Under this assumption the maximum a posteriori estimate (MAP) y 1 , . . . ,ŷ wq of the labels of unlabelled samples f 1 , . . . , f wq from F Q corresponds to</p><formula xml:id="formula_2">{ŷ j } wq j=1 , {ĉ k } w k=1 = arg max {yj },{c k } i P (y i |f i ) = arg max {yj },{c k } i P (f i |y i )P (y i ) = arg max {yj },{c k } i e −λ −1 fi−cy i 2 2 P (y i ).</formula><p>This is directly related to the Optimal Transport theory, see <ref type="bibr">(Hu, Gripon, and Pateux 2020;</ref><ref type="bibr" target="#b0">Cuturi 2013;</ref><ref type="bibr" target="#b0">Berman 2020;</ref><ref type="bibr" target="#b6">Villani 2003)</ref>, and one may use the iterative expectationmaximization like approach incorporating the Sinkhorn algorithm to get the MAP estimate. It consists of repeating of two steps, where the first is the construction of the mapping matrix M * with elements M * ij = P (y i = j) which is maximizing the previous term for a given centres c 1 , . . . , c w and the second step is the estimation of class centres that is for the fixed mapping matrix again optimizing the previous term. For the Sinkhorn algorithm, see (Cuturi 2013) the mapping matrix is defined as</p><formula xml:id="formula_3">M * = Sinkhorn(L, a, b, λ) = arg min M∈U (a,b) i,j M ij L ij + λH(M),</formula><p>where U (a, b) is a set of positive matrices in R wq×w for which the rows sums to a vector a and columns sums to a vector b, L ∈ R wq×w is the cost function consisting of Euclidean distances between unlabelled instances and class centres, that is L ij = f i − c j 2 2 , the hyperparameter λ is a regularisation coefficient forcing the entropy H(M) = − ij M ij log M ij to become smaller, a denotes the distribution of the amount that each unlabelled example uses for class allocation, i.e. a is the vector of ones with wq elements, and b denotes the distribution of the amount of unlabelled examples allocated to each class, i.e. b is the vector with w elements that equals to q.</p><p>The iterative approach starts with initialising the class centres from the labelled samples in F S . Then the mapping matrix M * is calculated using the Sinkhorn algorithm. It is then used to re-estimate the class centres via the update using</p><formula xml:id="formula_4">µ j = fi∈F Q M * ij f i + f k ∈F S ,y k =j f k s + wq i=1 M * ij .</formula><p>To avoid unnecessarily big steps in centre estimations, the new centre is set to be c j = c j + α(µ j − c j ), where the α is the learning rate. The number of iterations is fixed to n steps . Once the iteration process finishes, the labels of the samples from F Q might be estimated from the last mapping matrix asŷ i = arg max j M * ij . The overview of the algorithm is given in Algorithm 1. The overall process of our approach is depicted in <ref type="figure">Figure 1</ref>. The code is available at https://github.com/ctom2/latent-spacetransform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Optimal map algorithm</head><p>Parameters: w, s, q, λ, α, n steps Initialisation: c j = 1 s f k ∈F S ,y k =j f k repeat n steps times:</p><formula xml:id="formula_5">L ij = ||f i − c j || 2 , ∀i, j; M * = Sinkhorn(L, p = 1 wq , q = q1 w , λ); Calculate µ j ; c j = c j + α(µ j − c j ); end returnŷ i = arg max j M * ij</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>The performance of the stated methods was measured based on standardised few-shot classification datasets CIFAR-FS (Bertinetto et al. 2019) and CUB <ref type="bibr" target="#b7">(Wah et al. 2011)</ref>. CIFAR-FS dataset consists of images with size of 32×32 distributed into 100 classes, each containing 600 images. The dataset is split into 64 base classes, 16 validation classes and 20 novel classes. CUB dataset contains 11,788 images of birds, each with size 84 × 84, distributed over 200 classes. The dataset is split into 100 base classes, 50 validation classes and 50 novel classes.</p><p>In each testing run, w classes are randomly and uniformly drawn from novel classes, where each class consists of s instances with label and q instances without label.</p><p>Because of the high performance of WideResNet (Zagoruyko and Komodakis 2017) augmented with the S2M2 method (Mangla et al. 2020) in the few-shot setting, we chose it as the backbone architecture for our model. The latent representation of images produced by the backbone is a vector with dimension of 640. The QR decomposition reduces the said dimension to 80 in 1-shot setting, and to 100 in 5-shot setting.</p><p>All experiments are based on w = 5, q = 15 and s = 1 or 5. To evaluate the performance of the models we run 10,000 random draws to obtain mean accuracy with 95% confidence scores.</p><p>By tuning the hyperparameters of the model we observed evolution in accuracy in both 1-shot and 5-shot setting with  dependency on tested dataset. The overview with the hyperparameters can be found in <ref type="table" target="#tab_1">Table 1</ref>. The final accuracy can be seen in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table">Table 3</ref> for 1-shot and 5-shot setting, respectively. Moreover, the tables include results obtained by substituting MAP with different clustering algorithms, Gaussian Mixture model and k-means model, that take the transformed features as their input. The k-means model is initiated with centres corresponding to the labeled instances in a testing run. The centres are then iteratively refined to produce better representations of the class centres. Similarly, Gaussian Mixture model is provided with initial means corresponding to the labeled examples at the beginning of each run. To compare our proposed transform method with the Power Transform (PT) (Hu, Gripon, and Pateux 2020), we performed the same substitutions for the PT+MAP model. The scores show that even by omitting the MAP part from the architecture and replacing it with simpler classification approaches while keeping the transformation intact produces competitive results. Moreover, to compare the statistical significance of the superiority of the LST+MAP model against the PT+MAP model we performed the paired t-test with p-values presented in <ref type="table" target="#tab_4">Table 4</ref>. We can see that except for the CUB dataset in 5-shot scenario the LST+MAP model is significantly better than the PT+MAP model.</p><p>In terms of execution time, we measured an average of 0.0026s per run in 1-shot setting and 0.003s per run in 5shot setting with the GPU backend. <ref type="table">Table 3</ref>: 5-shot accuracy of models based on Power Transform (PT), our proposed Latent Space Transform (LST) and WideResNet backbone. The authors of the PT+MAP model presented accuracy 93.99 ± 0.10% in 5-shot setting for CUB dataset, however we were able to obtain higher accuracy with their described model configuration.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenge submission</head><p>In this section, we describe modification to our method we have elaborated for the MetaDl challenge 2020. The main limitation of the challenge was the submission runtime which had to include backbone training time and was limited to two hours. Therefore we were not able to utilise the WRN backbone as we suggest above. Our best performing solution was relying on a lighter backbone network based on the ResNet architecture. During the backbone training, the fed images could either be left as they were, or their saturation or brightness could be changed with the probability set to 1/3 for each alteration. Moreover, the training batches also included the same images rotated by 90, 180 and 270 degrees to further improve the backbone capabilities and augment the training overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Extracted features from backbones often do not resemble Gaussian-like distributions, even though multiple algorithms are built on that assumption. In this paper we show how to transform feature vectors into better Gaussian-like distributions. By applying an iterative optimal-transport algorithm to estimate class centres empirically, the subsequent clustering method gains significant improvement over other fewshot classification methods.</p><p>Our experiments confirmed that the Latent Space Transform algorithm introduced above outperforms other forms of feature preprocessing including the Power Transform. We have also compared our approach based on optimal transport mapping to other classification methods based on Gaussian mixtures and nearest neighbours. For both CIFAR and CUB datasets, our approach proved to be superior in both 1-shot and 5-shot learning scenarios.</p><p>We have adjusted our method for the MetaDl challenge 2020 competition and scored second in the final leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Latent Space Transform algorithm produces Gaussian like distribution also for the norms of the transformed samples. The figure was produced for one batch from the CUB dataset with s = 5, q = 15, β = 0.5, δ = 0.3, and γ = 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters used in the final evaluation of the LST+MAP model.</figDesc><table><row><cell></cell><cell>1-shot</cell><cell></cell><cell>5-shot</cell><cell></cell></row><row><cell cols="5">Parameter CIFAR-FS CUB CIFAR-FS CUB</cell></row><row><cell>β</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>λ</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>α</cell><cell>0.3</cell><cell>0.4</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>n steps</cell><cell>20</cell><cell>30</cell><cell>20</cell><cell>20</cell></row><row><cell>δ</cell><cell>0.3</cell><cell>0.7</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>γ</cell><cell>0.98</cell><cell>0.95</cell><cell>0.95</cell><cell>0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: 1-shot accuracy of models based on Power Trans-</cell></row><row><cell cols="4">form (PT), our proposed Latent Space Transform (LST) and</cell></row><row><cell cols="2">WideResNet backbone.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1-shot</cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>CIFAR</cell><cell>CUB</cell></row><row><cell>PT+MAP</cell><cell>WRN</cell><cell>87.69 ± 0.23%</cell><cell>91.55 ± 0.19%</cell></row><row><cell>PT+GMM</cell><cell>WRN</cell><cell>86.96 ± 0.22%</cell><cell>90.06 ± 0.18%</cell></row><row><cell>PT+KNN</cell><cell>WRN</cell><cell>86.17 ± 0.19%</cell><cell>89.07 ± 0.17%</cell></row><row><cell>LST+MAP</cell><cell>WRN</cell><cell cols="2">87.79 ± 0.23% 91.68 ± 0.19%</cell></row><row><cell>LST+GMM</cell><cell>WRN</cell><cell>87.01 ± 0.21%</cell><cell>89.9 ± 0.18%</cell></row><row><cell>LST+KNN</cell><cell>WRN</cell><cell>85.76 ± 0.19%</cell><cell>89.26 ± 0.17%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>p-values of the paired t-test with the null hypothesis that the accuracy of the PT+MAP model is greater or equal than the accuracy of the LST+MAP model against the alternative that the accuracy of the PT+MAP model is smaller than the accuracy of the LST+MAP model.</figDesc><table><row><cell></cell><cell cols="2">1-shot</cell><cell>5-shot</cell><cell></cell></row><row><cell></cell><cell>CIFAR-FS</cell><cell>CUB</cell><cell cols="2">CIFAR-FS CUB</cell></row><row><cell>p-value</cell><cell>9.09e−5</cell><cell>1.99e−9</cell><cell>1.68e−7</cell><cell>0.78</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/26638</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the Student Summer Research Program 2020 of FIT CTU in Prague. Moreover, the research was supported by the Grant Agency of the Czech Technical University in Prague (SGS20/213/OHK3/3T/18) and the Czech Science Foundation (GAČR 18-18080S).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Leveraging the feature distribution in transferbased few-shot learning. ArXiv abs/2006.03806</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertinetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sinkhorn algorithm, parabolic optimal transport and geometric Monge-Ampère equations</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<publisher>Balasubramanian</publisher>
			<date type="published" when="1964" />
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
	<note>ICML deep learning workshop. Mangla et al. 2020. and Krishnamurthy, B. 2020. Charting the right manifold: Manifold mixup for fewshot learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapted deep embeddings: A synthesis of methods for k-shot inductive transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ridgeway</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swersky</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Topics in optimal transportation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Providence, Rhode Island</pubPlace>
		</imprint>
	</monogr>
	<note>American mathematical society</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wah</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Generalizing from a few examples: A survey on few-shot learning</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transmatch: A transfer-learning scheme for semi-supervised few-shot learning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12856" to="12864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
