<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gimme&apos; Signals: Discriminative signal encoding for multimodal activity recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Memmesheimer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Theisen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
						</author>
						<title level="a" type="main">Gimme&apos; Signals: Discriminative signal encoding for multimodal activity recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple, yet effective and flexible method for action recognition supporting multiple sensor modalities. Multivariate signal sequences are encoded in an image and are then classified using a recently proposed Effi-cientNet CNN architecture. Our focus was to find an approach that generalizes well across different sensor modalities without specific adaptions while still achieving good results. We apply our method to 4 action recognition datasets containing skeleton sequences, inertial and motion capturing measurements as well as Wi-Fi fingerprints that range up to 120 action classes. Our method defines the current best CNN-based approach on the NTU RGB+D 120 dataset, lifts the state of the art on the ARIL Wi-Fi dataset by +6.78%, improves the UTD-MHAD inertial baseline by +14.43%, the UTD-MHAD skeleton baseline by +1.13% and achieves 96.11% on the Simitate motion capturing data (80/20 split). We further demonstrate experiments on both, modality fusion on a signal level and signal reduction to prevent the representation from overloading.</p><p>All authors are with the Active Vision</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action (also referred to as activity or behaviour) recognition is a well studied field and enables application in many different areas like elderly care <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, smart homes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, surveillance <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> robotics <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and driver behaviour analysis <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>Action recognition can be defined as finding a mapping that assigns a class label to a sequence of signals. The input data can, for instance, be measurements from Inertial Measurement Units (IMU), skeleton sequences, motion capturing sequences or image streams. We tackle the action recognition problem on a signal level as this is a common basis for a variety of input modalities or features that can be transformed into multivariate signal sequences. A common basis is important for the generalization across different modalities.</p><p>Some sensors like IMUs, Wi-Fi receivers yield multivariate signals directly, other sensors like RGB-D cameras provide skeleton estimates indirectly. Skeleton estimates can be transformed easily into multivariate signals by considering their joint axes. This also holds for human poses that can be estimated on camera streams using recent methods <ref type="bibr" target="#b15">[16]</ref>. Predicting the action class from multivariate signal sequences can then be seen as finding discriminative representations for signals.</p><p>Convolutional neural networks have shown great performance in classification tasks. We, therefore, propose a representation that transforms multivariate signal sequences into images. Recent proposed Convolutional Neural Network <ref type="figure">Fig. 1</ref>: We propose a representation that is suitable for multimodal action recognition. The <ref type="figure">Figure shows</ref> representations for skeletal data from the NTU <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> dataset, Inertial data from the UTD-MHAD <ref type="bibr" target="#b2">[3]</ref> dataset and WiFi Fingerprints from the ARIL <ref type="bibr" target="#b3">[4]</ref> dataset.</p><p>(CNN) architectures use architecture search conditioned on maximizing the accuracy while minimizing the floating-point operations <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Therefore they are good candidates for use in robotic systems. <ref type="figure">Figure 1</ref> gives an exemplary overview of the variety of modalities that our proposed representation can be used for. We evaluated the approach on 5 datasets containing different modalities. Many proposed fusion approaches rely on custom-engineered sub-models per sensor modality which are usually combined in multistream architectures. In contrast, we fuse the modalities on a representation level. This has the huge benefit of having a constant computing complexity independent of the number of modalities used whereas multi-stream architectures raise in complexity with every modality added.</p><p>Our approach lifts the state of the art action recognition accuracy on the ARIL Wi-Fi dataset by +6.78% and the UTD-MHAD <ref type="bibr" target="#b2">[3]</ref> (IMU +14.43) (Skeleton +1.13%). Our approach defines the current best 2D-CNN based approach on the NTU RGB+D 120 dataset (+2.9% (cross-subject), +4.59% (crossview)%) while being outperformed by a recently proposed graph convolution approach <ref type="bibr" target="#b18">[19]</ref> achieving remarkable results. On the Simitate dataset we achieve 96.11% accuracy on motion capturing data. In total we evaluated our approach on 4 different modalities. To the best of our knowledge, there is no approach showing a comparable high flexibility in supported sensor modalities.</p><p>The main contributions of this paper are as follows:</p><p>• We propose an action recognition approach based on the encoding of signals as images for classification with an efficient 2D-CNN. • We propose filter methods on a signal level to remove signals with only a minor contribution to the action. <ref type="bibr">•</ref> We present an approach for information fusion on a signal level. By considering the action recognition problem on a signal level, our approach generalizes well across different sensor modalities. The signal reduction prevents the image representation from overloading and allows flexible addition of signal streams. By fusion on a signal level, we create a flexible framework for adding additional information for instance object estimates or the fusion of different sensor modalities. The source code for the presented method is available on github 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we present action recognition methods based on traditional feature extractors and recent advances in machine learning. Existing survey papers <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> do not include most recent publications as the action recognition field is a highly active field of research. Therefore, most recent approaches from other working groups are presented here. We put a focus on methods using skeleton sequences as input because these can be acquired on robotic systems directly from RGB-D frames or by extracting human pose features <ref type="bibr" target="#b15">[16]</ref> from video sequences. Further, large scale benchmarks <ref type="bibr" target="#b0">[1]</ref> are available for action recognition on skeleton sequences, thus a fair comparison of different approaches can be achieved.</p><p>An interesting analysis from a human visual perception point of view has been presented by Johansson <ref type="bibr" target="#b24">[25]</ref> in 1973. He found that humans are using 10-12 elements in proximal stimulus to distinguish between human motion patterns <ref type="bibr" target="#b24">[25]</ref>. This supports the use of skeletons or pose estimation maps as underlying representations for activity recognition approaches from a visual perception perspective <ref type="bibr" target="#b25">[26]</ref>. Recent advances in action recognition developed from hand crafted feature extractors to deep learning approaches like 2D-and 3D-CNNs, while in parallel LSTM based methods also improved results on large scale datasets. More recently, graph convolution approaches showed promising results.</p><p>Rahmani et al. <ref type="bibr" target="#b26">[27]</ref> presented viewpoint invariant histograms of gradient descriptors for action recognition. Vemulapalli <ref type="bibr" target="#b27">[28]</ref> represented skeleton joints as points in a Liegroup. The classification is then done by a combination of dynamic time-warping <ref type="bibr" target="#b28">[29]</ref>, Fourier temporal pyramid representation and linear SVM <ref type="bibr" target="#b27">[28]</ref>. More recent approaches suggest representing skeleton sequences as images and 2D-CNNs for recognition. Wang et al. <ref type="bibr" target="#b29">[30]</ref> encode joint trajectory maps into images based on three spatial perspectives. Caetano et al. <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> represent a combination of reference joints and a tree-structured skeleton in images. Their approach preserves spatio-temporal relations and joint relevance. Liu et al. <ref type="bibr" target="#b32">[33]</ref> study a pose map representation. The approach that comes closest to our approach is by Liu et al. <ref type="bibr" target="#b33">[34]</ref>. Liu et al. presented a combination of skeleton visualization methods and jointly trained them on multiple streams. In contrast to our approach, their underlying representation enforces custom network architectures and is constrained to skeleton sequences whereas our approach adds flexibility to other sensor modalities. Kim et al. <ref type="bibr" target="#b34">[35]</ref> presented a visual interpretable method for action recognition using temporal convolutional networks. Their approach uses a spatio-temporal representation which allows visual analysis to understand why a model predicted an action. Especially joint contributions are visually interpretable.</p><p>3D convolutions for video action recognition was popularized by Tran et al. <ref type="bibr" target="#b35">[36]</ref>. They have shown good performance on direct video action classification A three-stream network has then been proposed to integrate multiple cues sequentially via a Markov chain model <ref type="bibr" target="#b36">[37]</ref>. By the integration of additional cues from e.g. pose information, optical flow and RGB images using a Markov chain they could increase the recognition accuracy incremental with each additional cue.</p><p>CNN architectures for signal classification have also been studied previously in audio processing <ref type="bibr" target="#b37">[38]</ref>. ResNet 1D-CNN architectures have been used for joint classification and localization of activities in Wi-Fi signals <ref type="bibr" target="#b3">[4]</ref>. For activity classification on a set of inertial sensors Yang et al. <ref type="bibr" target="#b38">[39]</ref> acquire time-series signals and classify the activities using a multi-layer CNN.</p><p>Liu et al. <ref type="bibr" target="#b39">[40]</ref> presented a spatio-temporal LSTM inspired by graph-based representation of the human skeleton. They further introduced a novel trust-gating mechanism to overcome noise and occlusion. Si et al. <ref type="bibr" target="#b25">[26]</ref> presented an Attention Enhanced Graph Convolutional LSTM Network (AGC-LSTM). They use feature augmentation and a threelayer AGC-LSTM to model discriminative spatial-temporal features and yield very good results on cross-view and crosssubject experiments on skeleton sequences. Very recently Papadopoulos et al. <ref type="bibr" target="#b18">[19]</ref> proposed two novel modules to improve action recognition based on Spatial Graph Convo- lutional <ref type="bibr" target="#b40">[41]</ref> networks. The Graph Vertex Feature Encoder learns vertex features by encoding skeleton data into a new feature space. While the Dilated Hierarchical Temporal Convolutional Network introduces new convolutional layers capturing temporal dependencies. Currently their approach is leading on NTU-RGB+D 120 <ref type="bibr" target="#b0">[1]</ref> dataset. However, their specialization in skeletal representations does not allow direct adaption on different sensor modalities. Interesting fusion approaches have been presented previously. Perez et al. <ref type="bibr" target="#b41">[42]</ref> presented an approach for multi-modal fusion architecture search using RGB, depth and skeleton fusion. Song et al. <ref type="bibr" target="#b42">[43]</ref> extract visual features from different modalities around skeletal joints from RGB and optical flow representations. Whereas those approaches have focused on multiple modalities originating from one device (e.g. Microsoft Kinect) there are also methods for the fusion of sensor data from different devices. Imran et al. <ref type="bibr" target="#b43">[44]</ref> propose a three-stream architecture, with different sub-architectures per modality. A 1D-CNN for gyroscopic data, a 2D-CNN for a flow-based image classification and an RNN for skeletal classification. In the end, individual features are fused and a class label is predicted. The fused results are promising and additional modalities improved the results. Additional augmentation by signal filter methods has shown to influence the result positively as well. However, the complexity of the architecture and their sub-architectures require engineering and training overhead and lead to increased run-times by each added modality. This is an issue that we overcome by using a common representation for different modalities. Chen et al. <ref type="bibr" target="#b2">[3]</ref> fuse depth information, inertial and demonstrate positive influence. However, they also use two different approaches for each modality. Namely, they use depth motion maps for depth sequences and partitioned temporal windows for signal classification of the gyroscope signals. Most fusion methods rely on complex individual representations per modality or propose complex multi-stream architectures. In contrast, our approach allows modality fusion using matrix concatenations in a single stream. However, our approach is limited to data which can be represented as 1D signals over time. By this, our approach is directly usable for a variety of sensors used in robotics like inertial measurement units, MoCap systems or skeleton sequences and can integrate features extracted from higher dimensional image streams that result e.g. in human pose features <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>The problem of action recognition with a given set of k actions Y = {0, . . . , k} can be reformulated as a classification problem where a mapping f : R N ×M → Y must be found that assigns an action label to a given input. The input in our case is a Matrix S ∈ R N ×M where each row vector represents a discrete 1-dimensional signal and each column vector represents a sample of all sensors at one specific time step.</p><p>After signal reduction the reduced signal matrix S f ocused is transformed to an RGB image I ∈ {0, . . . , 255} H×W ×3 by normalizing the signal length M to W and the range of the signals to H. The identity of each signal is encoded in the color channel. An overview of our approach is given in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Signal reduction</head><p>To avoid cluttering of the signal representation we propose a straightforward method for signal reduction which can be used across different modalities. This allows to lay focus on signals with high information content while removing the ones with low information content.</p><p>If for example sequences of skeletons are considered many of the joints are not moving significantly throughout the performance of an action. Intuitively it can be understood that when an action is performed while standing in one place the signal of the leg movement does not contribute much to help in classifying the performed action. From this intuition we developed the assumption that low variance signals do contain less information in the context of action recognition as high variance signals. Therefore we propose to set the signals to zero which are not actively contributing to the action by applying a threshold τ to the signals standard deviation σ. In our experiments τ was defined as 20% of the maximum value of all signals.</p><p>To be more concise we define the decision function f ( s j ) for the j-th signal s j in matrix S as</p><formula xml:id="formula_0">f ( s j ) = 1, if σ( s j ) ≥ τ 0, otherwise.<label>(1)</label></formula><p>When applying this function to each signal in matrix S we receive a vector c ∈ R N which encodes in each element if the corresponding signal contributes to the action. By elementwise multiplication of each column vector of S with c S f ocus is received where all signals that do not contribute to the action are set to zero. The signals with low contribution to actions are not removed but set to zero to prevent losing the joint identity (encoded in different colors).</p><p>Reducing the signals with low contribution to the action reduces the amount of overlapping signals in the image representation which in turn allows to increase the total number of fused signals. We suggest to apply signal reduction prior to fusion, because different scaling of sensor data can result in the elimination of all signals of a sensor with lower variance as another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Signal fusion</head><p>By our formulation the fusion of signals becomes a matrix concatenation:</p><formula xml:id="formula_1">S f used = (S 1 |S 2 ),<label>(2)</label></formula><p>where S f used is the fusion of S 1 and S 2 under the assumption that both matrices have the same amount of rows, where rows represent the sequence length. This can be either achieved by subsampling the higher frequency signals or interpolating the lower frequency signals. An example for sensor fusion is the encoding of multiple identities i.e. from skeletal data with S f used = (S id1 |S id2 ), where two identities are fused. Another example is fusion of two sensor modalities with i.e. S f used = (S skeleton |S inertial ) or adding interaction context by S f used = (S skeleton |S objects ). We therefore created a simple framework to support a wide variety of possible applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Representation</head><p>To allow a CNN based classifier to discriminate well between the action classes, we aim to find a discriminative representation in the first place. For encoding the signal identity we sample discriminative colors in the HSV color space depending on the number of signals. We make the initial assumption that temporal relations are represented by the position in the image. However, network architectures of lower depth seem to not maintain a global overview of the input but focuses on local relations. Therefore we encode local temporal information by interpolating from white to the sampled color throughout the sequence length. Signal changes are encoded spatially and joint relation are preserved. <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 4</ref> give exemplary representations for skeleton and inertial sequences ( <ref type="figure">Fig. 3)</ref> and Wi-Fi CSI fingerprints <ref type="figure">(Fig. 4)</ref>. A limitation of this approach is that only lower dimensional signals can be encoded. Image sequences or their transformations like optical flow, motion history images are to high dimensional to encode on a signal level by using our representation. Extracted human pose estimates, hand-and/or object estimates from image sequences are adequate signals for encoding in this representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Augmentation</head><p>Augmentation methods have shown to successfully influence the generalization. In our case we can create artificial training data on a signal level by interpolating, sampling, scaling, filtering, adding noise to the individual signals or augment the resulting image representation. Liu et al. <ref type="bibr" target="#b33">[34]</ref> already proposed to synthesize view independent representations for skeletal motion. As we consider action recognition on a signal level these transformations would result in augmentations integrated as a pre-processing step for each modality separately. Therefore, we decided to focus on augmenting the resulting image representation which can be efficiently integrated into training pipelines. Augmentation applied to the image representation during training still allows interpretation of an effect on the underlying signals. Stretching the width describes the same action but executed in a different speed while perspective changes or rotations can synthesize slightly different executions during the demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Architecture</head><p>Most action recognition approaches based on CNNs present custom architecture designs in their pipelines <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b44">[45]</ref>. A benefit is the direct control over the number of model parameters and can be specifically engineered for data representations or use cases. Recent advances in architecture design can not be transferred directly. Searching good hyperparameters for training is then often an empirical study. Minor architecture changes can result in a completely different set of hyper-parameters. He et al. <ref type="bibr" target="#b45">[46]</ref> suggested the use of residual layers during training resulting in more stable training. Tan et al. <ref type="bibr" target="#b17">[18]</ref> recently proposed a novel architecture category based on compound scaling across all dimensions of a CNN. We take advantage of the recent development in architecture design and use an already established architecture for image classification. The recently proposed EfficientNet <ref type="bibr" target="#b17">[18]</ref> architecture is especially interesting in the robotics context as it's based on architecture search conditioned on maximizing the accuracy while minimizing the floating-point operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation</head><p>Our implementation is done in Pytorch Lightning <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, which puts a focus on reproducible research. Hyperparameters and optimizer states are logged directly into the model checkpoints. The source code is made publicly available. We used a re-implementation and pre-trained weights of the EfficientNet <ref type="bibr" target="#b17">[18]</ref> architecture. For training we used a Stochastic Gradient Decent optimizer with a learning rate of 0.1 and reduction of learning rate by a factor of 0.1 every 30 epochs with a momentum of 0.9. The learning rate reduction was inspired by He et al. <ref type="bibr" target="#b45">[46]</ref>. A batch size of 40 was used on a single Nvidia GeForce RTX 2080 TI with 11GB GDDR-6 memory. We trained for a minimum of 150 epochs and used an early stopping policy based on the accuracy after. Similar model checkpoints were created on an increased validation accuracy. For optimizing the training we used a mixed precision approach by training using 16bit float with a 32bit float batch-norm and master weights. A gradient clipping of 0.5 prevented gradient and loss overflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conducted experiments on 4 different datasets. The NTU RGB+D 120 <ref type="bibr" target="#b0">[1]</ref>, UTD-MHAD <ref type="bibr" target="#b2">[3]</ref>, ARIL <ref type="bibr" target="#b3">[4]</ref> and the Simitate <ref type="bibr" target="#b48">[49]</ref> dataset. These datasets contain in total 5 modalities. Skeleton sequences are evaluated on the recently released NTU RGB+D 120 <ref type="bibr" target="#b0">[1]</ref> and the UTD-MHAD dataset <ref type="bibr" target="#b2">[3]</ref>. The NTU RGB+D 120 dataset demonstrates the scaling capabilities of our approach as it contains 120 classes in more than 114000 sequences. The UTD-MHAD dataset <ref type="bibr" target="#b2">[3]</ref> provides 27 classes but includes IMU data beside the skeleton estimates. Therefore it is suitable to demonstrate the cross modal capabilities of our approach. We further use it for our fusion experiments. We extend these experiments with activity recognition dataset containing Wi-Fi CSI fingerprints <ref type="bibr" target="#b3">[4]</ref> and Motion Capturing data from the Simitate <ref type="bibr" target="#b48">[49]</ref> dataset. For our experiments we generated the representations of the datasets prior and used an EfficientNet-B2 <ref type="bibr" target="#b17">[18]</ref> architecture for classification. AIS in the tables denotes the additional augmentation of the training signals in image space. Results are compared to other approaches in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In the following the datasets on which the experiments where performed are introduced.</p><p>1) NTU RGB+D 120: The NTU RGB+D 120 <ref type="bibr" target="#b0">[1]</ref> dataset is a large scale action recognition dataset containing RGB+D image streams and skeleton estimates. The dataset consists of 114,480 sequences containing 120 action classes from 106 subjects in 155 different views. Cross-view and crosssubject splits are defined as protocols. For the cross-subject evaluation, the dataset is split into 53 training subjects and 53 testing subjects as reported by the dataset authors <ref type="bibr" target="#b0">[1]</ref>. For the    <ref type="table" target="#tab_1">Table I</ref> and are discussed in the next section.</p><p>2) UTD-MHAD: This dataset <ref type="bibr" target="#b2">[3]</ref> contains 27 actions of 8 individuals performing 4 repetitions each. RGB-D camera, skeleton estimates and inertial measurements are included. The RGB-D camera is placed frontal to the demonstrating person. The IMU is either attached at the hand or the leg during the movements. A cross-subject protocol is followed as proposed by the authors <ref type="bibr" target="#b2">[3]</ref>. Half of the subjects are used for training while the other half is used for validation. Results are given in <ref type="table" target="#tab_1">Table II.</ref> 3) ARIL: This dataset <ref type="bibr" target="#b3">[4]</ref> contains Wi-Fi Channel State Information (CSI) fingerprints. The CSI describes how wireless signals propagate from the transmitter to the receiver <ref type="bibr" target="#b54">[55]</ref>. A standard IEEE 802.11n Wi-Fi protocol was used to collect 1398 CSI fingerprints for 6 activities. The data is varying by location. The 6 classes represent hand gestures hand circle, hand up, hand cross, hand left, hand down, and hand right targeting the control of smart home devices. For our experiments, we use the same train/test split as was used by the authors of the dataset (1116 train sequences / 278 test sequences). Results are given in <ref type="table" target="#tab_1">Table III.</ref> 4) Simitate <ref type="bibr" target="#b48">[49]</ref>: The Simitate benchmark focuses on robotic imitation learning tasks. Hand and object data are provided from a motion capturing system in 1932 sequences containing 27 classes of different complexity. The individuals execute tasks of different kinds of activities from drawing motions with their hand over to object interactions and more complex activities like ironing. This dataset is interesting   as we can fuse human and object measurements from the motion capturing system to add context information. Good action recognition capabilities will allow direct application to symbolic imitation approaches. We use a 80/20 train/test split for our experiments. Results are given in <ref type="table" target="#tab_1">Table IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>We did our best to include results from the most recent approaches for comparison. We found that the proposed representation on a signal level archived good performances across different modalities. An improvement of +6.78% over the baseline has been achieved on a Wi-Fi CSI fingerprint-based dataset <ref type="bibr" target="#b3">[4]</ref>. Augmentation has shown a positive impact on the resulting accuracy across modalities. The resulting model based on an EfficientNet-B2 performs well in interpreting spatial relations on the color encoded signals across the experiments. For the NTU RGB+D 120 dataset we give results in <ref type="table" target="#tab_1">Table I</ref>. Related results are taken from literature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b18">[19]</ref>. A skeleton with 25 joints serves as input for the training of our model. In case multiple identities are contained they are fused with the presented signal fusion approach. We got a cross-subject accuracy of 70.8% and a cross-view accuracy of 71.59% without investment of dataset-specific model tuning. Intuitively, when considering sequential data, LSTM based approaches are considered. We highly outperform the LSTM based approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. More directly comparable are CNN based approaches <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b30">[31]</ref>. All of the mentioned approaches concentrate on finding representations limited to skeleton or human pose features while our approach considers action recognition on a signal level and therefore is transferable to other modalities as well. The discriminative representation we suggest comes closest to the one by Liu et al. <ref type="bibr" target="#b33">[34]</ref>. In combination with the proposed augmentation method and the EfficientNet-B2 based architecture, we outperform the current CNN based approaches by +2.9% (cross-subject), +4.59% (cross-view). Very recently Papadopoulos et al. <ref type="bibr" target="#b18">[19]</ref> presented an approach based on a graph convolutional network and performs 5.7% better on the cross-subject split and 8% better on a crossview split than our approach. However, this approach is also limited to graphs constructed from skeleton sequences. Graph convolutional networks could be an interesting candidate for experiments on multiple modalities in the future.</p><p>Results on the UTD-MHAD dataset are shown in <ref type="table" target="#tab_1">Table II</ref>. We compare our approach to the baseline of the authors as well as a more recent approach <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b29">[30]</ref>. While Zhao et al. <ref type="bibr" target="#b53">[54]</ref> perform better than our proposed approach we get slightly better results then Wang et al. <ref type="bibr" target="#b29">[30]</ref> and further have the benefit of being applicable on other sensor modalities. It is to note that the perfect accuracy of 100.0% in <ref type="bibr" target="#b20">[21]</ref> was falsely reported on a similar named dataset. Fused experiments are executed by fusing skeleton estimates and inertial measurements S f used = (S skeleton |S inertial ). We improve the UTD-MHAD inertial baseline <ref type="bibr" target="#b2">[3]</ref> by +14.43% and the UTD-MHAD skeleton <ref type="bibr" target="#b53">[54]</ref> baseline by +1.13%. The proposed augmentation improved results by +2.19% for Skeletons, by +8.77% for IMU data and +10.37% for the fusion with the proposed augmentation methods. Fusion in our experiments did now have an overall positive effect. The inertial measurements seem to negatively bias the predicted action. Additional sensor confidence encoding could guide future research. The experiments we conducted on the ARIL dataset are compared to a 1D-ResNet CNN <ref type="bibr" target="#b3">[4]</ref> architecture proposed by the datasets authors. Results are presented in <ref type="table" target="#tab_1">Table III</ref>. Our approach performs better by +3.12% and the additional proposed augmentation methods improved the baseline by +6.78%. Wi-Fi CSI fingerprints have the benefit of being separated by their 52 bands already. Signal reduction is therefore not necessary. The additional proposed augmentation methods increase the accuracy by another 3.66%.</p><p>On the Simitate dataset a high accuracy is achieved on an 80/20 train/test split. Results are given in <ref type="table" target="#tab_1">Table IV</ref>. Augmentation on this dataset yields only a minimal improvement. This dataset is especially interesting for adding context. In addition to the hand poses the object poses can be added by our proposed signal fusion approach. As of now, there are no comparable results published. But the results suggest applicability for symbolic imitation approaches in the future.</p><p>Most approaches focus on getting high accuracy on a single modality, whereas our approach on a signal level serves as an interesting framework for multi-modal action recognition. In total, we have shown good results across 4 modalities (Skeleton, IMU, MoCap, Wi-Fi ). To the authors knowledge, no experiment with a similar extend is known. A huge benefit is the common representation that allows immediate prototyping. Run times are constant, even when additional context or sensors are added due to the representation level fusion. The EfficientNet-B2 architecture serves as a good basis for action recognition on our representation. Additional augmentation has improved the accuracy across the conducted experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We propose to transform individual signals of different sensor modalities and represent them as an image. The resulting images are then classified using a EfficientNet-B2 architecture. Our approach was evaluated on action recognition datasets based on skeleton estimates, inertial measurements, motion capturing data and Wi-Fi CSI fingerprints. This is in contrast to many previously proposed approaches that often focus on action recognition on a single modality. For skeleton data we represent each joint and their respective axis as individual signals. For Wi-Fi we use each of the 52 CSI fingerprint channels as signals. For inertial measurement units we use each axis of the acceleration and angular velocity. For our motion capturing experiments we used each axis of the marker attached to the hand and the interacting objects. Additional context like subjects and object estimates or even the fusion of different modalities can be flexibly added by a matrix concatenation. As our approach is limited to sparse signals, we propose filtering methods on a signal level to reduce signals that do not contribute much to the action. By this, additional information can be added without overloading the image representation. We evaluated our approach on four different datasets. The NTU 120 dataset for skeleton data, the UTD-MHAD dataset for skeleton and inertial data, the ARIL dataset for Wi-Fi data and the Simitate dataset for motion capturing data. Experimental results show that our approach is achieving good results across the different sensor modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Approach overview. We propose to transform individual signals of different sensor modalities and represent them as an image. The resulting images are then recognized using a 2D convolutional neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Sample representations of the UTD-MHAD dataset: (a) and (b) represent the same class (a27) of different subjects. (c) is a sample of a different class (a1). The color encoded lines correspond to the joint signals. On the top the representation for skeletal data is shown and on the bottom their respective inertial data. Sample representations: (a) and (b) represent the same class (0) of different subjects. (c) is a sample of a different class. The color encoded lines correspond to the joint signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Results on NTU RGB+D 120. Units are in %</figDesc><table><row><cell>Approach</cell><cell>Accuracy</cell></row><row><cell>Zhao et al. [54]</cell><cell>92.8</cell></row><row><cell>Wang et al. [30]</cell><cell>85.81</cell></row><row><cell>Chen et al. (Kinect DMMs) [3]</cell><cell>66.1</cell></row><row><cell>Chen et al. (Inertial) [3]</cell><cell>67.2</cell></row><row><cell>Chen et al. (Fused) [3]</cell><cell>79.1</cell></row><row><cell>Ours (Skeleton)</cell><cell>91.14</cell></row><row><cell>Ours (Skeleton, AIS)</cell><cell>93.33</cell></row><row><cell>Ours (Inertial)</cell><cell>72.86</cell></row><row><cell>Ours (Inertial, AIS)</cell><cell>81.63</cell></row><row><cell>Ours (Fused)</cell><cell>76.13</cell></row><row><cell>Ours (Fused, AIS)</cell><cell>86.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table /><note>Results on UTD-MHAD. Units are in % cross-setup evaluation, the dataset sequences with odd setup ids are reserved while the remainder is used for training. Resulting in 16 setups used during training and 16 used for testing. Results are given in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Results on ARIL dataset. Units are in %</figDesc><table><row><cell>Approach</cell><cell>Accuracy</cell></row><row><cell>Ours (Raw)</cell><cell>95.72</cell></row><row><cell>Ours (AIS)</cell><cell>96.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Results on Simitate. Units are in %</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://github.com/airglow/gimme_signals_action_ recognition</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint activity recognition and indoor localization with wifi fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="80" to="058" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fall detection-principles and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rumeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bourke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Laighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rialle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1663" to="1666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vision-based fallen person detection for the elderly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Solbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1433" to="1442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The elderlys independent living in smart homes: A characterization of activities and sensing infrastructure survey to facilitate services development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Garcia Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="11" to="312" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning and managing context enriched behavior patterns in smart homes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roncancio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiménez-Guarín</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="191" to="205" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human activity detection and recognition for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 IEEE International Conference on Multimedia and Expo (ICME)(IEEE Cat. No. 04TH8763)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="719" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A suspicious behaviour detection using a context space model for smart surveillance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiliem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yarlagadda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="209" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The meaning of action: A review on action recognition and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced robotics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1473" to="1501" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robot navigation in large-scale social maps: An action recognition approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Charalampous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostavelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="261" to="273" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis and classification of driver behavior using in-vehicle can-bus information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Angkititrakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biennial workshop on DSP for in-vehicle and mobile systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="17" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human behavior characterization for driving style recognition in vehicle system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mercaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nardone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Sangaiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Electrical Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Driver behavioural classification from trajectory data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rigolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Gooding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE Intelligent Transportation Systems</title>
		<meeting>2005 IEEE Intelligent Transportation Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="889" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vertex feature encoding and hierarchical temporal modeling in a spatialtemporal graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09745</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rgb-d-based action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comprehensive survey of vision-based human action recognition methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1005</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review of wearable technologies for elderly care that can accurately track indoor position, recognize physical activities and monitor vital signs in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">341</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recent advances in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A review of unsupervised feature learning and deep learning for time-series modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Längkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="11" to="24" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Histogram of oriented principal components for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2430" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using dynamic time warping to find patterns in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Berndt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD workshop</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13025</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Skeleton image representation for 3d action recognition based on tree structure and reference joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05704</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2904" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ieee international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
	<note>icassp</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks on multichannel time series for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mfas: Multimodal fusion architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Pérez-Rúa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6966" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Skeleton-indexed deep multi-modal feature learning for high performance human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluating fusion of rgb-d and inertial sensors for multimodal human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ambient Intelligence and Humanized Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pytorch lightning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E A</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PytorchLightning/pytorch-lightning" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Simitate: A hybrid imitation learning benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mykhalchyshyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06002</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Global contextaware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Skeletonbased human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bayesian hierarchical dynamic model for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7733" to="7742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Channel state information from pure communication to sense and track human motion: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Qaness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ewees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Alhaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hawbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">3329</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
