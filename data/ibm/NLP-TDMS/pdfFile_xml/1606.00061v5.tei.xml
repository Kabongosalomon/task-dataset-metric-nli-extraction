<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Question-Image Co-Attention for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
							<email>jiasenlu@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
							<email>jw2yang@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<email>dbatra@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<email>parikh@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Question-Image Co-Attention for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling "where to look" or visual attention, it is equally important to model "what words to listen to" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref> has emerged as a prominent multi-discipline research problem in both academia and industry. To correctly answer visual questions about an image, the machine needs to understand both the image and question. Recently, visual attention based models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> have been explored for VQA, where the attention mechanism typically produces a spatial map highlighting image regions relevant to answering the question.</p><p>So far, all attention models for VQA in literature have focused on the problem of identifying "where to look" or visual attention. In this paper, we argue that the problem of identifying "which words to listen to" or question attention is equally important. Consider the questions "how many horses are in this image?" and "how many horses can you see in this image?". They have the same meaning, essentially captured by the first three words. A machine that attends to the first three words would arguably be more robust to linguistic variations irrelevant to the meaning and answer of the question. Motivated by this observation, in addition to reasoning about visual attention, we also address the problem of question attention. Specifically, we present a novel multi-modal attention model for VQA with the following two unique features:</p><p>Co-Attention: We propose a novel mechanism that jointly reasons about visual attention and question attention, which we refer to as co-attention. Unlike previous works, which only focus on visual attention, our model has a natural symmetry between the image and question, in the sense that the image representation is used to guide the question attention and the question representation(s) are used to guide image attention.</p><p>Question Hierarchy: We build a hierarchical architecture that co-attends to the image and question at three levels: (a) word level, (b) phrase level and (c) question level. At the word level, we embed the words to a vector space through an embedding matrix. At the phrase level, 1-dimensional convolution neural networks are used to capture the information contained in unigrams, bigrams and trigrams. Specifically, we convolve word representations with temporal filters of varying support, and then combine the various n-gram responses by pooling them into a single phrase level representation. At the question level, we use recurrent neural networks to encode the entire question. For each level of the question representation in this hierarchy, we construct joint question and image co-attention maps, which are then combined recursively to ultimately predict a distribution over the answers.</p><p>Overall, the main contributions of our work are:</p><p>• We propose a novel co-attention mechanism for VQA that jointly performs question-guided visual attention and image-guided question attention. We explore this mechanism with two strategies, parallel and alternating co-attention, which are described in Sec. 3.3; • We propose a hierarchical architecture to represent the question, and consequently construct image-question co-attention maps at 3 different levels: word level, phrase level and question level. These co-attended features are then recursively combined from word level to question level for the final answer prediction; • At the phrase level, we propose a novel convolution-pooling strategy to adaptively select the phrase sizes whose representations are passed to the question level representation; • Finally, we evaluate our proposed model on two large datasets, VQA <ref type="bibr" target="#b1">[2]</ref> and COCO-QA <ref type="bibr" target="#b16">[17]</ref>.</p><p>We also perform ablation studies to quantify the roles of different components in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref> have proposed models for VQA. We compare and relate our proposed co-attention mechanism to other vision and language attention mechanisms in literature.</p><p>Image attention. Instead of directly using the holistic entire-image embedding from the fully connected layer of a deep CNN (as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>), a number of recent works have explored image attention models for VQA. Zhu et al. <ref type="bibr" target="#b27">[28]</ref> add spatial attention to the standard LSTM model for pointing and grounded QA. Andreas et al. <ref type="bibr" target="#b0">[1]</ref> propose a compositional scheme that consists of a language parser and a number of neural modules networks. The language parser predicts which neural module network should be instantiated to answer the question. Some other works perform image attention multiple times in a stacked manner. In <ref type="bibr" target="#b24">[25]</ref>, the authors propose a stacked attention network, which runs multiple hops to infer the answer progressively. To capture fine-grained information from the question, Xu et al. <ref type="bibr" target="#b23">[24]</ref> propose a multi-hop image attention scheme. It aligns words to image patches in the first hop, and then refers to the entire question for obtaining image attention maps in the second hop. In <ref type="bibr" target="#b19">[20]</ref>, the authors generate image regions with object proposals and then select the regions relevant to the question and answer choice. Xiong et al. <ref type="bibr" target="#b22">[23]</ref> augments dynamic memory network with a new input fusion module and retrieves an answer from an attention based GRU. In concurrent work, <ref type="bibr" target="#b4">[5]</ref> collected 'human attention maps' that are used to evaluate the attention maps generated by attention models for VQA. Note that all of these approaches model visual attention alone, and do not model question attention. Moreover, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> model attention sequentially, i.e., later attention is based on earlier attention, which is prone to error propagation. In contrast, we conduct co-attention at three levels independently.</p><p>Language Attention. Though no prior work has explored question attention in VQA, there are some related works in natural language processing (NLP) in general that have modeled language attention. In order to overcome difficulty in translation of long sentences, Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> propose RNNSearch to learn an alignment over the input sentences. In <ref type="bibr" target="#b8">[9]</ref>, the authors propose an attention model to circumvent the bottleneck caused by fixed width hidden vector in text reading and comprehension. A more fine-grained attention mechanism is proposed in <ref type="bibr" target="#b17">[18]</ref>. The authors employ a word-by-word neural attention mechanism to reason about the entailment in two sentences. Also focused on modeling sentence pairs, the authors in <ref type="bibr" target="#b25">[26]</ref> propose an attention-based bigram CNN for jointly performing attention between two CNN hierarchies. In their work, three attention schemes are proposed and evaluated. In <ref type="bibr" target="#b18">[19]</ref>, the authors propose a two-way attention mechanism to project the paired inputs into a common representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We begin by introducing the notation used in this paper. To ease understanding, our full model is described in parts. First, our hierarchical question representation is described in Sec. 3.2 and the proposed co-attention mechanism is then described in Sec. 3.3. Finally, Sec. 3.4 shows how to recursively combine the attended question and image features to output answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>Given a question with T words, its representation is denoted by Q = {q 1 , . . . q T }, where q t is the feature vector for the t-th word. We denote q w t , q p t and q s t as word embedding, phrase embedding and question embedding at position t, respectively. The image feature is denoted by</p><formula xml:id="formula_0">V = {v 1 , ..., v N },</formula><p>where v n is the feature vector at the spatial location n. The co-attention features of image and question at each level in the hierarchy are denoted asv r andq r where r ∈ {w, p, s}. The weights in different modules/layers are denoted with W , with appropriate sub/super-scripts as necessary. In the exposition that follows, we omit the bias term b to avoid notational clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Hierarchy</head><p>Given the 1-hot encoding of the question words Q = {q 1 , . . . , q T }, we first embed the words to a vector space (learnt end-to-end) to get Q w = {q w 1 , . . . , q w T }. To compute the phrase features, we apply 1-D convolution on the word embedding vectors. Concretely, at each word location, we compute the inner product of the word vectors with filters of three window sizes: unigram, bigram and trigram. For the t-th word, the convolution output with window size s is given bŷ</p><formula xml:id="formula_1">q p s,t = tanh(W s c q w t:t+s−1 ), s ∈ {1, 2, 3}<label>(1)</label></formula><p>where W s c is the weight parameters. The word-level features Q w are appropriately 0-padded before feeding into bigram and trigram convolutions to maintain the length of the sequence after convolution. Given the convolution result, we then apply max-pooling across different n-grams at each word location to obtain phrase-level features</p><formula xml:id="formula_2">q p t = max(q p 1,t ,q p 2,t ,q p 3,t ), t ∈ {1, 2, . . . , T }<label>(2)</label></formula><p>Our pooling method differs from those used in previous works <ref type="bibr" target="#b9">[10]</ref> in that it adaptively selects different gram features at each time step, while preserving the original sequence length and order. We use a LSTM to encode the sequence q p t after max-pooling. The corresponding question-level feature q s t is the LSTM hidden vector at time t. Our hierarchical representation of the question is depicted in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>.</p><formula xml:id="formula_3">(b) Image A A A Ques+on 0 Q V (a) Image Ques+on x x Q V C x x W v V W q Q a q a v 1. 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>qq sv v <ref type="figure">Figure 2</ref>: (a) Parallel co-attention mechanism; (b) Alternating co-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Co-Attention</head><p>We propose two co-attention mechanisms that differ in the order in which image and question attention maps are generated. The first mechanism, which we call parallel co-attention, generates image and question attention simultaneously. The second mechanism, which we call alternating co-attention, sequentially alternates between generating image and question attentions. See <ref type="figure">Fig. 2</ref>. These co-attention mechanisms are executed at all three levels of the question hierarchy.</p><p>Parallel Co-Attention. Parallel co-attention attends to the image and question simultaneously. Similar to <ref type="bibr" target="#b23">[24]</ref>, we connect the image and question by calculating the similarity between image and question features at all pairs of image-locations and question-locations. Specifically, given an image feature map V ∈ R d×N , and the question representation Q ∈ R d×T , the affinity matrix C ∈ R T ×N is calculated by</p><formula xml:id="formula_4">C = tanh(Q T W b V )<label>(3)</label></formula><p>where W b ∈ R d×d contains the weights. After computing this affinity matrix, one possible way of computing the image (or question) attention is to simply maximize out the affinity over the locations of other modality, i.e. a v [n] = max i (C i,n ) and a q [t] = max j (C t,j ). Instead of choosing the max activation, we find that performance is improved if we consider this affinity matrix as a feature and learn to predict image and question attention maps via the following </p><formula xml:id="formula_5">H v = tanh(W v V + (W q Q)C), H q = tanh(W q Q + (W v V )C T ) a v = softmax(w T hv H v ), a q = softmax(w T hq H q ) (4) where W v , W q ∈ R k×d , w hv , w hq ∈ R k</formula><formula xml:id="formula_6">a v n v n ,q = T t=1 a q t q t<label>(5)</label></formula><p>The parallel co-attention is done at each level in the hierarchy, leading tov r andq r where r ∈ {w, p, s}.</p><p>Alternating Co-Attention. In this attention mechanism, we sequentially alternate between generating image and question attention. Briefly, this consists of three steps (marked in <ref type="figure">Fig. 2b</ref>): 1) summarize the question into a single vector q; 2) attend to the image based on the question summary q; 3) attend to the question based on the attended image feature.</p><p>Concretely, we define an attention operationx = A(X; g), which takes the image (or question) features X and attention guidance g derived from question (or image) as inputs, and outputs the attended image (or question) vector. The operation can be expressed in the following steps</p><formula xml:id="formula_7">H = tanh(W x X + (W g g)1 T ) a x = softmax(w T hx H) x = a x i x i<label>(6)</label></formula><p>where 1 is a vector with all elements to be 1. W x , W g ∈ R k×d and w hx ∈ R k are parameters. a x is the attention weight of feature X.</p><p>The alternating co-attention process is illustrated in <ref type="figure">Fig. 2 (b)</ref>. At the first step of alternating coattention, X = Q, and g is 0; At the second step, X = V where V is the image features, and the guidance g is intermediate attended question featureŝ from the first step; Finally, we use the attended image featurev as the guidance to attend the question again, i.e., X = Q and g =v. Similar to the parallel co-attention, the alternating co-attention is also done at each level of the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Encoding for Predicting Answers</head><p>Following <ref type="bibr" target="#b1">[2]</ref>, we treat VQA as a classification task.</p><p>We predict the answer based on the coattended image and question features from all three levels. We use a multi-layer perceptron (MLP) to recursively encode the attention features as shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>.</p><formula xml:id="formula_8">h w = tanh(W w (q w +v w )) h p = tanh(W p [(q p +v p ), h w ]) h s = tanh(W s [(q s +v s ), h p ]) p = softmax(W h h s )<label>(7)</label></formula><p>where W w , W p , W s and W h are the weight parameters.</p><p>[·] is the concatenation operation on two vectors. p is the probability of the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate the proposed model on two datasets, the VQA dataset <ref type="bibr" target="#b1">[2]</ref> and the COCO-QA dataset <ref type="bibr" target="#b16">[17]</ref>.</p><p>VQA dataset <ref type="bibr" target="#b1">[2]</ref> is the largest dataset for this problem, containing human annotated questions and answers on Microsoft COCO dataset <ref type="bibr" target="#b13">[14]</ref>. The dataset contains 248,349 training questions, 121,512 validation questions, 244,302 testing questions, and a total of 6,141,630 question-answers pairs. There are three sub-categories according to answer-types including yes/no, number, and other. Each question has 10 free-response answers. We use the top 1000 most frequent answers as the possible outputs similar to <ref type="bibr" target="#b1">[2]</ref>. This set of answers covers 86.54% of the train+val answers. For testing, we train our model on VQA train+val and report the test-dev and test-standard results from the VQA evaluation server. We use the evaluation protocol of <ref type="bibr" target="#b1">[2]</ref> in the experiment. COCO-QA dataset <ref type="bibr" target="#b16">[17]</ref> is automatically generated from captions in the Microsoft COCO dataset <ref type="bibr" target="#b13">[14]</ref>. There are 78,736 train questions and 38,948 test questions in the dataset. These questions are based on 8,000 and 4,000 images respectively. There are four types of questions including object, number, color, and location. Each type takes 70%, 7%, 17%, and 6% of the whole dataset, respectively. All answers in this data set are single word. As in <ref type="bibr" target="#b16">[17]</ref>, we report classification accuracy as well as Wu-Palmer similarity (WUPS) in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>We use Torch <ref type="bibr" target="#b3">[4]</ref> to develop our model. We use the Rmsprop optimizer with a base learning rate of 4e-4, momentum 0.99 and weight-decay 1e-8. We set batch size to be 300 and train for up to 256 epochs with early stopping if the validation accuracy has not improved in the last 5 epochs. For COCO-QA, the size of hidden layer W s is set to 512 and 1024 for VQA since it is a much larger dataset. All the other word embedding and hidden layers were vectors of size 512. We apply dropout with probability 0.5 on each layer. Following <ref type="bibr" target="#b24">[25]</ref>, we rescale the image to 448 × 448, and then take the activation from the last pooling layer of VGGNet <ref type="bibr" target="#b20">[21]</ref> or ResNet <ref type="bibr" target="#b7">[8]</ref> as its feature.  <ref type="bibr" target="#b23">[24]</ref> uses GoogLeNet <ref type="bibr" target="#b21">[22]</ref> and the rest all use VGGNet <ref type="bibr" target="#b20">[21]</ref>, and Ours+VGG outperforms them by 0.2% on test-dev (DMN+ <ref type="bibr" target="#b22">[23]</ref>). <ref type="table" target="#tab_2">Table 2</ref> shows results on the COCO-QA test set. Similar to the result on VQA, our model improves the state-of-the-art from 61.6% <ref type="figure">(SAN(2,CNN)</ref>  <ref type="bibr" target="#b24">[25]</ref>) to 65.4% (Ours a +ResNet). We observe that parallel co-attention performs better than alternating co-attention in this setup. Both attention mechanisms have their advantages and disadvantages: parallel co-attention is harder to train because of the dot product between image and text which compresses two vectors into a single value. On the other hand, alternating co-attention may suffer from errors being accumulated at each round. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we perform ablation studies to quantify the role of each component in our model. Specifically, we re-train our approach by ablating certain components:</p><p>• Image Attention alone, where in a manner similar to previous works <ref type="bibr" target="#b24">[25]</ref>, we do not use any question attention. The goal of this comparison is to verify that our improvements are not the result of orthogonal contributions. (say better optimization or better CNN features).  <ref type="table" target="#tab_3">Table 3</ref> shows the comparison of our full approach w.r.t these ablations on the VQA validation set (test sets are not recommended to be used for such experiments). The deeper LSTM Q + norm I baseline in <ref type="bibr" target="#b1">[2]</ref> is also reported for comparison. We can see that image-attention-alone does improve performance over the holistic image feature (deeper LSTM Q + norm I), which is consistent with findings of previous attention models for VQA <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>We now visualize some co-attention maps generated by our method in <ref type="figure" target="#fig_4">Fig. 4</ref>  image attention has different patterns across images. For the first two images, the attention transfers from objects to background regions. For the third image, the attention becomes more focused on the objects. We suspect that this is caused by the different question types. On the question side, our model is capable of localizing the key phrases in the question, thus essentially discovering the question types in the dataset. For example, our model pays attention to the phrases "what color" and "how many snowboarders". Our model successfully attends to the regions in images and phrases in the questions appropriate for answering the question, e.g., "color of the bird" and bird region. Because our model performs co-attention at three levels, it often captures complementary information from each level, and then combines them to predict the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a hierarchical co-attention model for visual question answering. Coattention allows our model to attend to different regions of the image as well as different fragments of the question. We model the question hierarchically at three levels to capture information from different granularities. what seating area is on the right ?</p><p>is the person dressed properly for the sport ? <ref type="figure">Figure 5</ref>: Visualization of co-attention maps on success cases in the COCO-QA (first three columns using Ours p +VGG) and VQA (last two columns Ours a +VGG) dataset. The layout is the same as <ref type="figure" target="#fig_4">Fig. 4</ref>.  <ref type="figure">Figure 6</ref>: Visualization of co-attention maps on failure cases in the COCO-QA (first three columns using Ours p +VGG) and VQA (last two columns Ours a +VGG) dataset. Predicted answer is in red and ground truth answer is in green. The layout is the same as <ref type="figure" target="#fig_4">Fig. 4</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowchart of our proposed hierarchical co-attention model. Given a question, we extract its word level, phrase level and question level embeddings. At each level, we apply co-attention on both the image and question. The final answer prediction is based on all the co-attended image and question features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are the weight parameters. a v ∈ R N and a q ∈ R T are the attention probabilities of each image region v n and word q t respectively. The affinity matrix C transforms question attention space to image attention space (vice versa for C T ). Based on the above attention weights, the image and question attention vectors are calculated as the weighted sum of the image features and question features, i.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>"Figure 3 :</head><label>3</label><figDesc>What color on the … up ?" (a) Hierarchical question encoding (Sec. 3.2); (b) Encoding for predicting answers (Sec. 3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Question Attention alone, where no image attention is performed. • W/O Conv, where no convolution and pooling is performed to represent phrases. Instead, we stack another word embedding layer on the top of word level outputs. • W/O W-Atten, where no word level co-attention is performed. We replace the word level attention with a uniform distribution. Phrase and question level co-attentions are still modeled. • W/O P-Atten, where no phrase level co-attention is performed, and the phrase level attention is set to be uniform. Word and question level co-attentions are still modeled. • W/O Q-Atten, where no question level co-attention is performed. We replace the question level attention with a uniform distribution. Word and phrase level co-attentions are still modeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of image and question co-attention maps on the COCO-QA dataset. From left to right: original image and question pairs, word level co-attention maps, phrase level co-attention maps and question level co-attention maps. For visualization, both image and question attentions are scaled (from red:high to blue:low). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the VQA dataset. "-" indicates the results is not available.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Open-Ended</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Multiple-Choice</cell></row><row><cell></cell><cell></cell><cell cols="2">test-dev</cell><cell></cell><cell>test-std</cell><cell></cell><cell cols="2">test-dev</cell><cell>test-std</cell></row><row><cell>Method</cell><cell cols="4">Y/N Num Other All</cell><cell>All</cell><cell cols="4">Y/N Num Other All</cell><cell>All</cell></row><row><cell>LSTM Q+I [2]</cell><cell cols="2">80.5 36.8</cell><cell cols="2">43.0 57.8</cell><cell>58.2</cell><cell cols="2">80.5 38.2</cell><cell cols="2">53.0 62.7</cell><cell>63.1</cell></row><row><cell>Region Sel. [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">77.6 34.3</cell><cell cols="2">55.8 62.4</cell><cell>-</cell></row><row><cell>SMem [24]</cell><cell cols="2">80.9 37.3</cell><cell cols="2">43.1 58.0</cell><cell>58.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [25]</cell><cell cols="2">79.3 36.6</cell><cell cols="2">46.1 58.7</cell><cell>58.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FDA [11]</cell><cell cols="2">81.1 36.2</cell><cell cols="2">45.8 59.2</cell><cell>59.5</cell><cell cols="2">81.5 39.0</cell><cell cols="2">54.7 64.0</cell><cell>64.2</cell></row><row><cell>DMN+ [23]</cell><cell cols="2">80.5 36.8</cell><cell cols="2">48.3 60.3</cell><cell>60.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours p +VGG</cell><cell cols="2">79.5 38.7</cell><cell cols="2">48.3 60.1</cell><cell>-</cell><cell cols="2">79.5 39.8</cell><cell cols="2">57.4 64.6</cell><cell>-</cell></row><row><cell>Ours a +VGG</cell><cell cols="2">79.6 38.4</cell><cell cols="2">49.1 60.5</cell><cell>-</cell><cell cols="2">79.7 40.1</cell><cell cols="2">57.9 64.9</cell><cell>-</cell></row><row><cell>Ours a +ResNet</cell><cell cols="2">79.7 38.7</cell><cell cols="2">51.7 61.8</cell><cell>62.1</cell><cell cols="2">79.7 40.0</cell><cell cols="2">59.8 65.8</cell><cell>66.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>IMG-CNN<ref type="bibr" target="#b14">[15]</ref> and SAN<ref type="bibr" target="#b24">[25]</ref> on COCO-QA. We use Ours p to refer to our parallel co-attention, Ours a for alternating co-attention.Table 1 shows results on the VQA test sets for both open-ended and multiple-choice settings. We can see that our approach improves the state of art from 60.4% (DMN+ [23]) to 62.1% (Ours a +ResNet) on open-ended and from 64.2% (FDA [11]) to 66.1% (Ours a +ResNet) on multiple-choice. Notably, for the question type Other and Num, we achieve 3.4% and 1.4% improvement on open-ended questions, and 4.0% and 1.1% on multiple-choice questions. As we can see, ResNet features outperform or match VGG features in all cases. Our improvements are not solely due to the use of a better CNN. Specifically, FDA [11] also uses ResNet [8], but Ours a +ResNet outperforms it by 1.8% on test-dev. SMem</figDesc><table /><note>There are two test scenarios on VQA: open-ended and multiple-choice. The best performing method deeper LSTM Q + norm I from [2] is used as our baseline. For open-ended test scenario, we compare our method with the recent proposed SMem [24], SAN [25], FDA [11] and DMN+ [23]. For multiple choice, we compare with Region Sel. [20] and FDA [11]. We compare with 2- VIS+BLSTM [17],</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the COCO-QA dataset. "-" indicates the results is not available.</figDesc><table><row><cell>Method</cell><cell cols="7">Object Number Color Location Accuracy WUPS0.9 WUPS0.0</cell></row><row><cell>2-VIS+BLSTM [17]</cell><cell>58.2</cell><cell>44.8</cell><cell>49.5</cell><cell>47.3</cell><cell>55.1</cell><cell>65.3</cell><cell>88.6</cell></row><row><cell>IMG-CNN [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.4</cell><cell>68.5</cell><cell>89.7</cell></row><row><cell>SAN(2, CNN) [25]</cell><cell>64.5</cell><cell>48.6</cell><cell>57.9</cell><cell>54.0</cell><cell>61.6</cell><cell>71.6</cell><cell>90.9</cell></row><row><cell>Ours p +VGG</cell><cell>65.6</cell><cell>49.6</cell><cell>61.5</cell><cell>56.8</cell><cell>63.3</cell><cell>73.0</cell><cell>91.3</cell></row><row><cell>Ours a +VGG</cell><cell>65.6</cell><cell>48.9</cell><cell>59.8</cell><cell>56.7</cell><cell>62.9</cell><cell>72.8</cell><cell>91.3</cell></row><row><cell>Ours a +ResNet</cell><cell>68.0</cell><cell>51.0</cell><cell>62.9</cell><cell>58.8</cell><cell>65.4</cell><cell>75.1</cell><cell>92.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the VQA dataset using Ours a +VGG.</figDesc><table><row><cell></cell><cell cols="2">validation</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Y/N Num Other</cell><cell>All</cell></row><row><cell>LSTM Q+I</cell><cell>79.8 32.9</cell><cell>40.7</cell><cell>54.3</cell></row><row><cell>Image Atten</cell><cell>79.8 33.9</cell><cell>43.6</cell><cell>55.9</cell></row><row><cell cols="2">Question Atten 79.4 33.3</cell><cell>41.7</cell><cell>54.8</cell></row><row><cell>W/O Q-Atten</cell><cell>79.6 32.1</cell><cell>42.9</cell><cell>55.3</cell></row><row><cell>W/O P-Atten</cell><cell>79.5 34.1</cell><cell>45.4</cell><cell>56.7</cell></row><row><cell>W/O W-Atten</cell><cell>79.6 34.4</cell><cell>45.6</cell><cell>56.8</cell></row><row><cell>Full Model</cell><cell>79.6 35.0</cell><cell>45.7</cell><cell>57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. At the word level, our model attends mostly to the object regions in an image, e.g., heads, bird. At the phrase level, the</figDesc><table><row><cell>Q: what is the man holding a snowboard on top of a snow covered? A: mountain</cell><cell>what is the man holding a snowboard on top of a snow covered</cell><cell>what is the man holding a snowboard on top of a snow covered ?</cell><cell>what is the man holding a snowboard on top of a snow covered ?</cell></row><row><cell>Q: what is the color of the bird? A: white</cell><cell>what is the color of the bird ?</cell><cell>what is the color of the bird ?</cell><cell>what is the color of the bird ?</cell></row><row><cell>Q: how many snowboarders in</cell><cell>how many snowboarders in</cell><cell>how many snowboarders in</cell><cell>how many snowboarders in</cell></row><row><cell>formation in the snow, four is</cell><cell>formation in the snow , four is</cell><cell>formation in the snow , four is</cell><cell>formation in the snow , four is</cell></row><row><cell>sitting? A: 5</cell><cell>sitting ?</cell><cell>sitting ?</cell><cell>sitting ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The ablation studies further demonstrate the roles of co-attention and question hierarchy in our final performance. Through visualizations, we can see that our model co-attends to interpretable regions of images and questions for predicting the answer.Though  our model was evaluated on visual question answering, it can be potentially applied to other tasks involving vision and language. : what is the color of the kitten? A: black Q: what are standing in tall dry grass look at the tourists? A: zebras Q: where is the woman while her baby is sleeping? A: kitchen Q: what seating area is on the right? A: park Q: is the person dressed properly for this sport? A: yes</figDesc><table><row><cell>what is the color of the kitten ?</cell><cell>what are standing in tall dry grass look at the tourists ?</cell><cell>where is the woman while her baby is sleeping ?</cell><cell>what seating area is on the right ?</cell><cell>is the person dressed properly for the sport ?</cell></row><row><cell>what is the color of the kitten ?</cell><cell>what are standing in tall dry grass look at the tourists ?</cell><cell>where is the woman while her baby is sleeping ?</cell><cell>what seating area is on the right ?</cell><cell>is the person dressed properly for the sport ?</cell></row><row><cell>what is the color of the kitten ?</cell><cell>what are standing in tall dry grass look at the tourists ?</cell><cell>where is the woman while her baby is sleeping ?</cell><cell></cell><cell></cell></row></table><note>Q</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded in part by NSF CAREER awards to DP and DB, an ONR YIP award to DP, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, a Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty awards to DB and DP, Google Faculty Research Awards to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government or any sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03556</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01455</idno>
		<title level="m">Multimodal residual learning for visual qa</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to answer questions from image using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiskỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03609</idno>
		<title level="m">Attentive pooling networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05234</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05099</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Measuring machine intelligence through visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>C Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">37</biblScope>
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
