<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Segmentation with Mixed Temporal Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Baidu</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Baidu</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Segmentation with Mixed Temporal Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main progress for action segmentation comes from densely-annotated data for fully-supervised learning. Since manual annotation for frame-level actions is timeconsuming and challenging, we propose to exploit auxiliary unlabeled videos, which are much easier to obtain, by shaping this problem as a domain adaptation (DA) problem. Although various DA techniques have been proposed in recent years, most of them have been developed only for the spatial direction. Therefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frameand video-level embedded feature spaces across domains, and further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. Finally, we evaluate our proposed methods on three challenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA outperforms the current state-of-the-art methods on all three datasets by large margins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action segmentation is of significant importance for a wide range of applications, including video surveillance and analysis of daily human activities. Given a video, the goal is to simultaneously segment the video by time and predict each segment with a corresponding action category. While video classification has shown great progress given the recent success of deep neural networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>, temporally locating and recognizing action segments in long untrimmed videos is still challenging.</p><p>Action segmentation approaches can be factorized into extracting low-level features using convolutional neural networks and applying high-level temporal models. Encouraged by the advances in speech synthesis <ref type="bibr" target="#b27">[28]</ref>, recent approaches rely on temporal convolutions to capture long range dependencies across frames using a hierarchy of temporal convolutional filters <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7]</ref>. <ref type="bibr">*</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Attentive Frame Aggregation</head><p>Temporal Convolution Temporal Convolution <ref type="figure">Figure 1</ref>: An overview of the proposed MTDA for action segmentation. "Source" refers to the data with labels, and "Target" refers to the unlabeled data with the standard transductive setting for DA. We first extract local temporal features using temporal convolution, and then obtain global temporal features with domain attentive frame aggregation. Finally, we diminish the domain discrepancy by jointly performing local and global temporal domain adaptation. Here we use the video making tea as an example.</p><p>Despite the success of these temporal models, the performance gains come from densely-annotated data for fullysupervised learning. Since manually annotating precise frame-by-frame actions is time-consuming and challenging, these methods are not easy to extend to larger scale for realworld applications.</p><p>In this paper, we regard action segmentation as a domain adaptation (DA) problem with the transductive setting <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref> given the observation that the main challenge is the distributional discrepancy caused by spatio-temporal variations across domains. For example, different people (also noted as subjects) may perform the same action with different styles in terms of spatial locations and temporal duration. The variations in the background environment also contribute to the overall domain discrepancy. To solve this problem, we propose to diminish the domain discrepancy by utilizing auxiliary unlabeled videos, which are much easier to obtain.</p><p>Videos can suffer from domain discrepancy along both the spatial and temporal directions, bringing the need of alignment for embedded feature spaces along both directions <ref type="bibr" target="#b1">[2]</ref>. However, most DA approaches have been developed only for images and not videos <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. Therefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frame-and videolevel embedded feature spaces across domains, as shown in <ref type="figure">Figure 1</ref>. We further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. To support our claims, we evaluate our approaches on three datasets with high spatio-temporal domain discrepancy: Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b7">[8]</ref>, 50Salads <ref type="bibr" target="#b34">[35]</ref>, and the Breakfast dataset <ref type="bibr" target="#b13">[14]</ref>, and achieve new state-of-the-art performance on all three datasets. Since our approach can adapt a model trained in one environment to new environments using only unlabeled videos without additional manual annotation, it is applicable to large-scale real-world scenarios, such as video surveillance.</p><p>In summary, our contributions are three-fold:</p><p>1. Local Temporal Domain Adaptation: We propose an effective adversarial-based DA method to learn domain-invariant frame-level features. To the best of our knowledge, this is the first work to utilize unlabeled videos as auxiliary data to diminish spatiotemporal variations for action segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Mixed Temporal Domain Adaptation (MTDA):</head><p>We jointly align local and global embedded feature spaces across domains by integrating additional DA mechanism which aligns the video-level feature spaces. Furthermore, we integrate the domain attention mechanism to aggregate domain-specific frames to form global video representations, leading to more effective domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Analyses:</head><p>We evaluate on three challenging real-world datasets and outperform all the previous state-of-the-art methods. We also perform analysis and ablation study on different design choices to identify key contributions of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we review the most recent work for action segmentation including the fully-and weakly-supervised setting. We also review the most related domain adaptation work for images and videos. Action Segmentation. Encouraged by the advances in speech synthesis <ref type="bibr" target="#b27">[28]</ref>, recent approaches rely on temporal convolutions to capture long-range dependencies across frames using a hierarchy of temporal convolutional filters <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7]</ref>. ED-TCN <ref type="bibr" target="#b16">[17]</ref> follows an encoderdecoder architecture with a temporal convolution and pooling in the encoder, and upsampling followed by deconvolution in the decoder. TricorNet <ref type="bibr" target="#b3">[4]</ref> replaces the convolutional decoder in the ED-TCN with a bi-directional LSTM (Bi-LSTM). TDRN <ref type="bibr" target="#b18">[19]</ref> builds on top of ED-TCN <ref type="bibr" target="#b16">[17]</ref> and use deformable convolutions instead of the normal convolution and add a residual stream to the encoder-decoder model. MS-TCN <ref type="bibr" target="#b6">[7]</ref> stacks multiple stages of temporal convolutional network (TCN) where each TCN consists of multiple temporal convolutional layers performing acausal dilated 1D convolution. With the multi-stage architecture, each stage takes an initial prediction from the previous stage and refines it. We build our approach on top of MS-TCN, focusing on developing methods to effectively exploit unlabeled videos instead of modifying the architecture. Domain Adaptation. Most recent DA approaches are based on deep learning architectures designed for addressing the domain shift problems given the fact that the deep CNN features without any DA method outperform traditional DA methods using hand-crafted features <ref type="bibr" target="#b5">[6]</ref>. Most DA methods follow the two-branch (source and target) architecture, and aim to find a common feature space between the source and target domains. The models are therefore optimized with a combination of classification and domain losses <ref type="bibr" target="#b2">[3]</ref>.</p><p>One of the main classes of methods used is Discrepancybased DA, whose metrics are designed to measure the distance between source and target feature distributions, including variations of maximum mean discrepancy (MMD) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b23">24]</ref> and the CORAL function <ref type="bibr" target="#b36">[37]</ref>. By diminishing the distance of distributions, discrepancybased DA methods reduce the gap across domains. Another common method, Adversarial-based DA, adopts a similar concept as GANs <ref type="bibr" target="#b11">[12]</ref> by integrating domain discriminators into the architectures. Through the adversarial objectives, the discriminators are optimized to classify different domains, while the feature extractors are optimized in the opposite direction. ADDA <ref type="bibr" target="#b38">[39]</ref> uses an inverted label GAN loss to split the optimization into two parts: one for the discriminator and the other for the generator. In contrast, the gradient reversal layer (GRL) is adopted in some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref> to invert the gradients so that the discriminator and generator are optimized simultaneously.</p><p>Recently, TADA <ref type="bibr" target="#b40">[41]</ref> adopts the attention mechanism to adapt the transferable regions and images. Differently, we design the attention mechanism for spatio-temporal domains, aiming to attend to the important parts of temporal dynamics for domain adaptation. Domain Adaptation for Actions. Unlike image-based DA, video-based DA is still an under-explored area. A few works focus on small-scale video DA with only few overlapping categories <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b12">13]</ref>. <ref type="bibr" target="#b35">[36]</ref> improves the domain generalizability by decreasing the effect of the background. <ref type="bibr" target="#b42">[43]</ref> maps source and target features to a common feature space using shallow neural networks. AMLS <ref type="bibr" target="#b12">[13]</ref> adapts pre-extracted C3D <ref type="bibr" target="#b37">[38]</ref> features on a Grassmann manifold obtained using PCA. However, the datasets used in the above works are too small to have enough domain shift to evaluate DA performance. Recently, Chen et al. <ref type="bibr" target="#b1">[2]</ref> propose two larger cross-domain datasets for action recognition and the state-of-the-art approach TA 3 N. However, these works focus only on the classification task while we concentrate on the more challenging temporal segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical Approach</head><p>We first introduce our baseline model which is the current state-of-the-art approach for action segmentation, MS-TCN <ref type="bibr" target="#b6">[7]</ref> (Section 3.1). And then we describe how we incorporate unlabeled video to align frame-level feature spaces (Section 3.2), and present our proposed method with the attention-based video-level domain adaptation (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Model: MS-TCN</head><p>The basic component of our baseline model is singlestage temporal convolutional network (SS-TCN), as shown in the left part of <ref type="figure">Figure 2</ref>. SS-TCN consists of multiple temporal convolutional layers performing acausal dilated 1D convolution. Dilated convolution is used to increase the temporal receptive field exponentially without the need to increase the number of parameters, which can prevent the model from over-fitting the training data. Motivated by the success of multi-stage architectures <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b26">27]</ref>, several SS-TCNs are stacked to form the multi-stage TCN (MS-TCN). Each stage takes the prediction from the previous stage and utilizes the multi-layer temporal convolution feature generator G f to obtain the frame-level features f = {f 1 , f 2 , ..., f T }, and then converts them into the framelevel predictionsŷ = {ŷ 1 ,ŷ 2 , ...,ŷ T } by a fully-connected layer G y .</p><p>The overall prediction loss function for each stage is a combination of a classification loss and a smoothing loss, which can be expressed as follows:</p><formula xml:id="formula_0">Ly = L cls + αLT −M SE<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-stage Temporal Convolution Network (SS-TCN)</head><p>. . . . . .</p><formula xml:id="formula_1">. . . . . . . . . . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-layer Temporal</head><formula xml:id="formula_2">Convolution ! " ℒ $% GRL Domain Classifier &amp; $% domain pred. ' &amp; $% ℒ ( ! ) * ) " Figure 2:</formula><p>We perform local temporal DA by applying the domain classifier G ld to the final embedded features f in one stage. A gradient reversal layer (GRL) is added between G ld and f so that f is trained to be domain-invariant.ŷ is the frame-level predictions for each stage. L y and L ld are the prediction loss and local domain loss, respectively.</p><p>where L cls is a cross-entropy loss, L T −M SE is a truncated mean squared error used to reduce the difference between adjacent frame-level prediction to improve the smoothness, and α is the trade-off weight for the smoothness loss. To train the complete model, we minimize the sum of the losses over all stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Temporal Domain Adaptation</head><p>Despite the progress of MS-TCN on action segmentation, there is still a large room for improvement. The main challenge is the distributional discrepancy caused by spatiotemporal variations across domains. For example, different subjects may perform the same action completely different due to personalized spatio-temporal styles. Therefore, the problem becomes generalizing the model across domains. In this paper, we propose to reduce the domain discrepancy by performing unsupervised DA with auxiliary unlabeled videos.</p><p>Encouraged by the success of adversarial-based DA approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, for each stage, we feed the frame-level features f to an additional shallow binary classifiers, called the local domain classifiers G ld , to discriminate whether the data is from the source or target domain. Before backpropagating the gradients to the main model, a gradient reversal layer (GRL) is inserted between G ld and the main model to invert the gradient, as shown in <ref type="figure">Figure 2</ref>. During adversarial training, G f is learned by maximizing the domain discrimination loss L ld , while G ld is learned by minimizing L d with the domain label d.</p><p>Therefore, the feature generator G f will be optimized to gradually align the feature distributions between the two domains. In this paper, we note the adversarial local domain classifierĜ ld as the combination of a GRL and a domain classifier G ld , and investigate the integration of G ld for different stages. From our experiments, the best performance happens when G ld s are integrated into middle stages. For more details, please see Section 4.5.</p><p>The overall loss function becomes a combination of the baseline prediction loss L y and the local domain loss L ld , which can be expressed as follows:</p><formula xml:id="formula_3">L = Ns Ly − Ns β l L ld (2) L ld = 1 T T j=1 L ld (G ld (fj), dj)<label>(3)</label></formula><p>where N s is the total stage number, N s is the number of selected stages, and T is the number of frames from each video. L ld is a binary cross entropy loss function, and β l is the trade-off weight for local domain loss L ld .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mixed Temporal Domain Adaptation</head><p>One main drawback of integrating DA into local framelevel features f is that the video-level feature space is still not fully aligned. Although f is learned using the context and dependencies from neighbor frames, the temporal receptive field still not guarantees to cover the whole video length. Furthermore, aligning video-level feature spaces also helps to generate domain-adaptive frame-level predictions for action segmentation. Therefore, we propose to jointly align local frame-level feature spaces and global video-level feature spaces, as shown in <ref type="figure" target="#fig_2">Figure 5</ref>. Global Temporal Domain Adaptation. To achieve this goal, we first aggregate f = {f 1 , f 2 , ..., f T } using temporal pooling to form video-level feature V . Since each feature f t captures context in different time by temporal convolution, V still contain temporal information despite the naive temporal pooling method. After obtaining V , we add another domain classifier (noted as global domain classifier G gd ) to explicitly align the embedded feature spaces of video-level features.</p><p>Therefore, the global domain loss L gd is added into the overall loss, which can be expressed as follows:</p><formula xml:id="formula_4">L = Ns Ly − Ns (β l L ld + βgL gd )<label>(4)</label></formula><formula xml:id="formula_5">L gd = L gd (G gd (G tf (f )), d)<label>(5)</label></formula><p>where L gd is also a binary cross entropy loss function, and β g is the trade-off weight for global domain loss L gd . Domain Attention Mechanism. Although aligning videolevel feature spaces across domains benefits action segmentation, not all the frame-level features are equally important to align. In order to effectively align overall temporal dynamics, we want to focus more on aligning the frame-level features which have larger domain discrepancy. Therefore, we assign larger attention weights to those features, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>.  <ref type="figure">Figure 4</ref>: The details of the domain attention mechanism, consisting of two modules: domain attentive temporal pooling (left) and domain attentive entropy (right). Both modules use the domain predictiond to make their inputs domain attentive with a residual connection. L ae is the attentive entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Target</head><p>Hence, we integrate each stage with the domain attention mechanism, as shown in <ref type="figure">Figure 4</ref>, which utilizes the entropy criterion to generate the domain attention value for each frame-level feature as below:</p><formula xml:id="formula_6">wj = 1 − H(dj)<label>(6)</label></formula><p>whered j is the domain prediction from G ld . H(p) = − k p k · log(p k ) is the entropy function to measure uncertainty. w j increases when H(d j ) decreases, which means the domains can be distinguished well. We also add a residual connection for more stable optimization. Finally, we aggregate the attended frame-level features with temporal pooling to generate the video-level feature h, which is noted  as domain attentive temporal pooling (DATP) and can be expressed as:</p><formula xml:id="formula_7">h = 1 T T j=1 (wj + 1) · fj<label>(7)</label></formula><p>In addition, we add the minimum entropy regularization to refine the classifier adaptation. However, we only want to minimize the entropy for the videos that are similar across domains. Therefore, we attend to the videos which have low domain discrepancy, so that we can focus more on minimizing the entropy for these videos. The attentive entropy loss L ae can be expressed as follows:</p><formula xml:id="formula_8">Lae = 1 T T j=1 (1 + H(dj)) · H(ŷj)<label>(8)</label></formula><p>whered andŷ is the output of G ld and G y , respectively. We also adopt the residual connection for stability. By adding Equation <ref type="formula" target="#formula_8">(8)</ref> into Equation <ref type="formula" target="#formula_4">(4)</ref>, and replacing G tf (f ) with h by Equation <ref type="formula" target="#formula_7">(7)</ref>, the overall loss of our final proposed Mixed Temporal Domain Adaptation (MTDA), as shown in <ref type="figure" target="#fig_2">Figure 5</ref>, can be expressed as follows:</p><formula xml:id="formula_9">L = Ns Ly − Ns (β l L ld + βgL gd − µLae)<label>(9)</label></formula><p>where µ is the weight for the attentive entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate how our approaches can diminish spatialtemporal discrepancy for action segmentation, we choose three challenging datasets: Georgia Tech Egocentric Activities (GTEA) <ref type="bibr" target="#b7">[8]</ref>, 50Salads <ref type="bibr" target="#b34">[35]</ref>, and the Breakfast dataset <ref type="bibr" target="#b13">[14]</ref>, which separate the training and testing sets by different people (noted as subjects), resulting in high spatio-temporal variations. By following the transductive setting for DA, "Source" refers to the original training set, and "Target" refers to the testing set without labels. With these three datasets, we show how our approaches adapt the same actions across different people by decreasing the spatio-temporal variations across videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The GTEA dataset contains 28 videos including 7 kitchen activities performed by 4 subjects. All the videos were recorded by a camera that is mounted on the actor's head. There are totally 11 action classes including background. On average, each video is around one minute long with 20 action instances. We use 4-fold cross-validation for evaluation by leaving one subject out.</p><p>The 50Salads dataset contains 50 videos for salad preparation activities performed by 25 subjects. There are totally 17 action classes. On average, each video contains 20 action instances and is 6.4 minutes long. For evaluation, we apply 5-fold cross-validation by leaving five subjects out.</p><p>The Breakfast dataset is the largest among the three datasets with 1712 videos for breakfast preparation activities performed by 52 subjects. The videos were recorded in 18 different kitchens with 48 action classes where each video contains 6 action instances on average and is around 2.7 minutes long. For evaluation, we utilize the standard 4-fold cross-validation by leaving 13 subjects out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>For all the three datasets, we use the following evaluation metrics as in <ref type="bibr" target="#b16">[17]</ref>: frame-wise accuracy (Acc), segmental edit score, and segmental F1 score at the IoU threshold k%, denoted as F1@k (k = {10, 25, 50}).</p><p>While frame-wise accuracy is one of the most common evaluation metrics for action segmentation, it does not take into account the temporal dependencies of the prediction, causing large qualitative differences with similar frame-wise accuracy. In addition, long action classes have higher impact on this metric than shorter action classes, making this metric not able to reflect over-segmentation errors. To address the above limitations, the segmental edit score penalizes over-segmentation by measuring the ordering of predicted action segments independent of slight temporal shifts. Finally, another suitable metric segmental F1 score (F1@k) becomes popular recently since it is found that the score numbers better indicate the qualitative segmentation results. F1@k also penalizes over-segmentation errors while ignoring minor temporal shifts between the predictions and ground truth. F1@k is determined by the total number of actions but not depends on the duration of each action instance, which is similar to mean average precision (mAP) with intersection-over-union (IoU) overlap criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation</head><p>Our implementation is based on the PyTorch <ref type="bibr" target="#b29">[30]</ref> framework. We extract I3D <ref type="bibr" target="#b0">[1]</ref> features for the video frames and use these features as input to our model. The video frame rates are the same as <ref type="bibr" target="#b6">[7]</ref>. For fair comparison, we adopt the same architecture design choices of MS-TCN <ref type="bibr" target="#b6">[7]</ref> as our We set the number of filters to 64 in all the layers of the model and the filter size is 3. For optimization, we use the Adam optimizer and the batch size equals to 1. Since the target data size is smaller than the source data, each target data is loaded randomly multiple times in each epoch during training. For the weighting of loss functions, we follow the common strategy as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> to gradually increase β l and β g from 0 to 1. the weighting α for smoothness loss is 0.15 as in <ref type="bibr" target="#b6">[7]</ref> and µ is chosen as 1 × 10 −4 via the grid-search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental Results</head><p>We first compare with the baseline model MS-TCN [7] to see how our approaches effectively utilize the unlabeled videos for action segmentation. "Source only" means the model is trained only with source labeled videos. And then we compare the proposed approach to the state-of-the-art methods on all three datasets. Local Temporal Domain Adaptation. By integrating domain classifiers with frame-level features f, the results on all three datasets with respect to all the metrics are improved significantly, as shown in the row "DA (L)" in <ref type="table">Table 1</ref>. For example, on the GTEA dataset, our approach outperforms the baseline by 4.6% for F1@50, 5.5% for the edit score and 3.8% for the frame-wise accuracy. Although "DA (L)" mainly works on the frame-level features, they are learned using the context from neighbor frames, so they still contain temporal information, which is critical to diminish the temporal variations for actions across domains. Mixed Temporal Domain Adaptation. Despite the improvement from local temporal DA, the temporal recep-tive fields of frame-level features are still not guaranteed to cover the whole video length. Therefore, we aggregate frame-level features to generate a video-level feature for each video and apply additional domain classifier on it. However, aggregating frames by temporal pooling without considering the importance of each frame does not ensure better performance, especially for the Breakfast dataset, which contains much higher domain discrepancy than the other two. The F1 score and frame-wise accuracy both have slightly worse results, as shown in the row "DA (L + G)" in <ref type="table">Table 1</ref>. Therefore, we apply the domain attention mechanism to aggregate frames more effectively, leading to better global temporal DA performance. For example, on the Breakfast dataset, "DA (L + G + A)" outperforms "DA (L)" by 1.4% for F1@50, 1.9% for the edit score and 0.7% for the frame-wise accuracy, as shown in <ref type="table">Table 1</ref>.</p><p>Our final method "DA (L + G + A)", which is also MTDA, outperforms the baseline by large margins (e.g. 6.4% for F1@50, 6.8% for the edit score and 3.7% for the frame-wise accuracy on GTEA; 8.0% for F1@50, 7.3% for the edit score and 2.5% for the frame-wise accuracy on 50Salads), as demonstrated in <ref type="table">Table 1</ref>.</p><p>Comparison with the State-of-the-Art. Here we compare the proposed MTDA to the state-of-the-art methods, and MTDA outperforms all the previous methods on the three datasets with respect to three evaluation metrics: F1 score, edit distance, and frame-wise accuracy, as shown in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>For the GTEA dataset, the authors of MS-TCN <ref type="bibr" target="#b6">[7]</ref> also fine-tune the I3D features to improve the performance (e.g. from 85.8% to 87.5% for F1@10). Our MTDA outperforms the fine-tuned MS-TCN even without any fine-tuning process since we learn the temporal features more effectively from unlabeled videos, which is more important for action segmentation.</p><p>cGAN <ref type="bibr" target="#b8">[9]</ref> utilizes supplementary modalities including depth maps and optical flow with an auxiliary network. cGAN outperforms MS-TCN in terms of the F1 score and edit score on the 50Salads dataset. Our MTDA outperforms cGAN, indicating more effective way to utilizes auxiliary data.</p><p>For the Breakfast dataset, the authors of MS-TCN [7] also use the improved dense trajectories (IDT) features, which encode only motion information and outperform the I3D features since the encoded spatial information is not the critical factor for the Breakfast dataset. Our MTDA outperforms the IDT-version of MS-TCN by a large margin with the same I3D features. This shows that our DATP module effectively aggregate frames by considering the temporal structure for action segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study and Analysis</head><p>Integration of G ld and Stages. Since we use Multi-stage TCN <ref type="bibr" target="#b6">[7]</ref> as our baseline model and develop our approaches   upon it, it raises a question: How to effectively perform DA by integrating the domain classifiers to a multi-stage architecture? Our architecture contains four stages as in <ref type="bibr" target="#b6">[7]</ref>. First, we integrate G ld into one stage and the results are demonstrated in <ref type="table" target="#tab_4">Table 3</ref>. The results show that the best performance happens when G ld s are integrated into middle stages, such as S2 or S3. S1 is not a good choice for DA  <ref type="figure">Figure 6</ref>: The qualitative results of temporal action segmentation on GTEA for the activity CofHoney. The video snapshots are shown in the first row in a temporal order (from left to right). "Source only" refers to the baseline model MS-TCN <ref type="bibr" target="#b6">[7]</ref>. because of two reasons: 1) S1 corresponds to low-level and transferable features with less discriminability where DA shows limited effects <ref type="bibr" target="#b21">[22]</ref>. 2) S1 capture less temporal information from neighbor frames, representing less temporal receptive fields, which is critical for action segmentation. However, higher stages (e.g. S4) are not always better. We conjecture that it is because higher stages are used to refine the prediction. They may affect the semantic structure of feature representations, which is important to DA. In our case, integrating G ld into S2 provides the best overall performance.</p><p>We also add multiple domain classifiers to adjacent stages. However, multi-stage DA does not always guarantee improved performance. For example, {S1, S2} has worse results than {S2} in terms of F1@{10, 25, 50}. Since {S2} and {S3} provide the best single-stage DA performance, we use {S2, S3}, which performs the best, as the final model for all our approaches in all the experiments. Qualitative results. In addition to evaluating the quantitative performance using the above metrics, it is also common to evaluate the qualitative performance to ensure that the prediction results are aligned with human vision. Here we compare our approaches with the baseline model MS-TCN <ref type="bibr" target="#b6">[7]</ref> and the ground truth, as shown in <ref type="figure">Figure 6</ref>. MS-TCN fails to predict open before the long pour action in the middle part of the video, and falsely predict pour before stir in the end of the video, as shown in the "Source only" row. With local and global temporal DA, our approach can detect all the actions happened in the video, as shown in the row "DA (L + G)". Finally, with the domain attention mechanism, our proposed MTDA also removes the falsely predicted action pour. For more qualitative results, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we consider action segmentation as a DA problem and reduce the domain discrepancy by performing unsupervised DA with auxiliary unlabeled videos. To diminish domain discrepancy for both the spatial and temporal directions, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frame-and videolevel embedded feature spaces across domains, and further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. The comprehensive experiment results validate that our approach outperforms all the previous state-of-the-art methods. Our approach can adapt models effectively by using auxiliary unlabeled videos, leading to further possible applications to large-scale problems, such as video surveillance and human activity analysis. For the future work, we would like to develop DA approaches with self-supervised learning to reduce the need of additional unlabeled videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">More Qualitative results</head><p>Here we show more examples to compare our approaches with the baseline model MS-TCN <ref type="bibr" target="#b6">[7]</ref> and the ground truth, as shown in <ref type="figure" target="#fig_5">Figures 7 and 8</ref>. GTEA dataset. For the example Hotdog <ref type="figure" target="#fig_5">(Figure 7a</ref>), MS-TCN fails to predict the put action before the take action in the early part of the video, and the predicted fold action in the end of the video has much shorter time duration than it should be, as shown in the "Source only" row. With local and global temporal DA, our approach can detect all the actions with proper time duration, as shown in the row "DA (L + G)", and the domain attention mechanism further helps refine the time duration for each predicted action. For another example Pealate <ref type="figure" target="#fig_5">(Figure 7b</ref>), our final approach "DA (L + G + A)" also produces the best action segmentation result, which is the closest to the ground truth, compared with the baseline and other methods. 50Salads and Breakfast datasets. In addition to the GTEA dataset, we also evaluate the qualitative performance on another two challenger datasets: 50Salads and Breakfast, as demonstrated in <ref type="figure" target="#fig_6">Figure 8</ref>. The 50Salads dataset is challenging since each video is long and contains around 20 fine-grained action classes. While MS-TCN confuses with some similar classes like "cut tomato" and "place tomato into bowl", our approach can produce smooth temporal segmentation, as shown in <ref type="figure" target="#fig_6">Figure 8a</ref>. The Breakfast dataset is challenging because of high spatio-temporal variations among videos since the videos are recorded in 18 different kitchens with 52 subjects. The total number of action classes is also much larger than the other two datasets. <ref type="figure" target="#fig_6">Figure 8b</ref> shows that MS-TCN falsely classifies "take cup" as "take bowl" and unable to detect the "add teabag" action for a long time. However, our proposed MTDA can continuously detect actions in the video without gaps along the temporal direction.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The overview of global temporal DA with the domain attention mechanism. Frame-level features are aggregate with different attention weights to form the videolevel feature h for global domain DA. Thicker arrows corresponds to larger attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The overall architecture of the proposed MTDA. By equipping with a local adversarial domain classifierĜ ld , a global adversarial domain classifierĜ gd , and the domain attention mechanism as shown inFigure 4, we convert a SS-TCN into a domain adaptive TCN (DA-TCN), and stack multiple stages of DA-TCN to build the final architecture. L ld and L gd is the local and global domain loss, respectively. L y is the prediction loss and L ae is the attentive entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The qualitative results of temporal action segmentation on GTEA for the activity (a) Hotdog and (b) Pealate. The video snapshots are shown in the first row in a temporal order (from left to right). "Source only" refers to the baseline model MS-TCN<ref type="bibr" target="#b6">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>The qualitative results of temporal action segmentation on (a) 50salads and (b) Breakfast. The video snapshots are shown in the first row in a temporal order (from left to right). "Source only" refers to the baseline model MS-TCN<ref type="bibr" target="#b6">[7]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Local Temporal Domain Adaptation Global Temporal Domain Adaptation Domain Attentive Frame Aggregation Source Target</head><label></label><figDesc>Work done during an internship at Baidu USA</figDesc><table><row><cell>Input</cell></row><row><cell>videos</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art on GTEA, 50Salads, and the Breakfast dataset.</figDesc><table><row><cell></cell><cell>F1@{10, 25, 50}</cell><cell>Edit Acc</cell></row><row><cell cols="3">Source only 85.8 83.4 69.8 79.0 76.3</cell></row><row><cell>{S1}</cell><cell cols="2">88.6 86.2 73.6 84.2 78.7</cell></row><row><cell>{S2}</cell><cell cols="2">89.1 87.2 74.4 84.3 79.1</cell></row><row><cell>{S3}</cell><cell cols="2">89.2 87.3 72.3 83.8 78.9</cell></row><row><cell>{S4}</cell><cell cols="2">88.1 86.4 73.0 83.0 78.8</cell></row><row><cell>{S1, S2}</cell><cell cols="2">89.0 85.8 73.5 84.8 79.5</cell></row><row><cell>{S2, S3}</cell><cell cols="2">89.6 87.9 74.4 84.5 80.1</cell></row><row><cell>{S3, S4}</cell><cell cols="2">88.3 86.8 73.9 83.6 78.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The experimental results for the integration of G ld and different stages of MS-TCN<ref type="bibr" target="#b6">[7]</ref> on the GTEA dataset. {S n } means adding G ld to the nth stage. The stages with smaller n are closer to inputs.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal attentive alignment for large-scale video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Tricornet: A hybrid temporal convolutional and recurrent network for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07818</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial network for continuous fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep domain adaptation in action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deodhare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<publisher>ICLRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ts-lstm and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="87" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attend and interact: Higher-order object interactions for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EEuropean Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop</title>
		<imprint>
			<publisher>NIPSW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international joint conference on Pervasive and ubiquitous computing (UbiComp)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human action recognition across datasets by foreground-weighted histogram decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>ECCVW</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dual many-toone-encoder-based transfer learning for cross-dataset human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="137" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saminger-Platz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
