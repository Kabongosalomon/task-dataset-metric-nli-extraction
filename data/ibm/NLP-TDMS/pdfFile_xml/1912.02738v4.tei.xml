<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetaFun: Meta-Learning with Iterative Functional Updates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Ton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
						</author>
						<title level="a" type="main">MetaFun: Meta-Learning with Iterative Functional Updates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a functional encoder-decoder approach to supervised meta-learning, where labeled data is encoded into an infinite-dimensional functional representation rather than a finitedimensional one. Furthermore, rather than directly producing the representation, we learn a neural update rule resembling functional gradient descent which iteratively improves the representation. The final representation is used to condition the decoder to make predictions on unlabeled data. Our approach is the first to demonstrates the success of encoder-decoder style meta-learning methods like conditional neural processes on largescale few-shot classification benchmarks such as miniImageNet and tieredImageNet, where it achieves state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of meta-learning is to be able to generalise to new tasks from the same task distribution as the training tasks. In supervised meta-learning, a task can be described as making predictions on a set of unlabelled data points (target) by effectively learning from a set of data points with labels (context). Various ideas have been proposed to tackle supervised meta-learning from different perspectives <ref type="bibr" target="#b1">(Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b28">Ravi &amp; Larochelle, 2016;</ref><ref type="bibr" target="#b7">Finn et al., 2017;</ref><ref type="bibr" target="#b15">Koch, 2015;</ref><ref type="bibr" target="#b33">Snell et al., 2017;</ref><ref type="bibr" target="#b36">Vinyals et al., 2016;</ref><ref type="bibr" target="#b32">Santoro et al., 2016;</ref><ref type="bibr" target="#b31">Rusu et al., 2019)</ref>. In this work, we are particularly interested in a family of meta-learning models that use an encoder-decoder pipeline, such as Neural Processes <ref type="bibr" target="#b9">(Garnelo et al., 2018a;</ref>. The encoder is a permutation-invariant function on the context that summarises the context into a task representation, while the decoder produces a predictive model for the targets, conditioned on the task representation.</p><p>The objective of meta-learning is then to learn the encoder and the decoder such that the produced predictive model generalises well to the targets of new tasks.</p><p>Previous works in this category such as the Conditional Neural Process (CNP) and the Neural Process (NP) <ref type="bibr" target="#b9">(Garnelo et al., 2018a;</ref> use sum-pooling operations to produce finite-dimensional, vectorial, task representations. In this work, we investigate the idea of summarising tasks with infinite-dimensional functional representations. Although there is a theoretical guarantee that sum-pooling of instancewise representations can express any set function (universality) <ref type="bibr" target="#b39">(Zaheer et al., 2017;</ref><ref type="bibr" target="#b6">Bloem-Reddy &amp; Teh, 2020)</ref>, in practice CNP and NP tend to underfit the context . This observation is in line with the theoretical finding that for universality, the dimension of the task representation should be at least as large as the cardinality of the context set, if the encoder is a smooth function <ref type="bibr" target="#b37">Wagstaff et al. (2019)</ref>. We develop a method that explicitly uses functional representations. Here the effective dimensionality of the task representation grows with the number of context points, which addresses the aforementioned issues of fixed-dimensional representations. Moreover, in practice it is difficult to model interactions between data points with only sum-pooling operations. This issue can be partially addressed by inserting modules such as relation networks <ref type="bibr" target="#b34">(Sung et al., 2018;</ref><ref type="bibr" target="#b31">Rusu et al., 2019)</ref> or set transformers <ref type="bibr" target="#b16">(Lee et al., 2019a)</ref> before sum-pooling. However, only within-context but not context-target interactions can be modelled by these modules. The construction of our functional representation involves measuring similarities between all data points, which naturally contains information regarding interactions between elements in either the context or the target. Furthermore, rather than producing the functional representation in a single pass, we develop an approach that learns iterative updates to encode the context into the task representation. In general, learning via iterative updates is often easier than directly learning the final representation, because of the error-correcting opportunity at each iteration. For example, an iterative parameterisation of the encoder in Variational Autoencoders <ref type="bibr">(VAEs)</ref> has been demonstrated to be effective in reducing the amortisation gap <ref type="bibr" target="#b20">(Marino et al., 2018)</ref>, while in meta-learning, both learning to learn methods <ref type="bibr" target="#b1">(Andrychowicz et al., 2016;</ref><ref type="bibr">Ravi &amp; Larochelle, Figure 1</ref>. To illustrate the iterative procedure in MetaFun, we consider a simpler case where our functional representation is just a predictor for the task. (A) The figure depicts a 1D regression task with the current predictor. (B) Local updates are computed by evaluating the functional representation (the current predictor) on the context inputs, and comparing it to the corresponding context outputs. Here we simply measure differences between evaluations (predictions) and outputs. (C) We apply functional pooling to aggregate local updates into a global functional update, which generalises the local updates to the whole input domain. (D) The functional update is applied to the current functional representation with a learning rate α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2016) and</head><p>Model Agnostic Meta Learning (MAML) <ref type="bibr" target="#b7">(Finn et al., 2017)</ref> use iterative updating procedures to adapt to new tasks, although these update rules operate in parameter space rather than function space. Therefore, it is reasonable to conjecture that iterative structures are favourable inductive biases for the task encoding process.</p><p>In summary, the primary contribution of this work is a metalearning approach that learns to summarise a task using a functional representation constructed via iterative updates. We apply our approach to solve meta-learning problems on both regression and classification tasks, and achieve state-ofthe-art performance on heavily benchmarked datasets such as miniImageNet <ref type="bibr" target="#b36">(Vinyals et al., 2016)</ref> and tieredImageNet <ref type="bibr" target="#b29">(Ren et al., 2018)</ref>, which has never been demonstrated with encoder-decoder meta-learning methods without MAMLstyle gradient updates. We also conducted an ablation study to understand the effects of the different model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MetaFun</head><p>Meta-learning, or learning to learn, leverages past experiences to quickly adapt to tasks T ∼ p(T ) drawn iid from some task distribution. In supervised meta-learning, a task T takes the form of</p><formula xml:id="formula_0">T = { , {(x i , y i )} i∈C , {(x j , y j )} j∈T }, where x i , x j ∈ X are inputs, y i , y j ∈ Y</formula><p>outputs, is the loss function to be minimised, {(x i , y i )} i∈C is the context, and {(x j , y j )} j∈T is the target. We consider the process of learning as constructing a predictive model using the task context and refer to the mapping from context {(x i , y i )} i∈C to a predictive model f = Φ({(x i , y i )} i∈C ; φ) as the learning model parameterised by φ. In our formulation, the objective of meta-learning is to optimise the learning model such that the expected loss on the target under f is minimised, formally written as:</p><formula xml:id="formula_1">f = Φ({(x i , y i )} i∈C ; φ) φ * = arg min φ E T ∼p(T )   1 |T| j∈T (f (x j ), y j )   , (1) where both {(x i , y i )} i∈C , {(x j , y j )} i∈T come from task T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Learning Functional Task Representation</head><p>Like previous works such as CNP and NP, we construct the learning model using an encoder-decoder pipeline, where the encoder Φ e ({(x i , y i )} i∈C ; φ e ) is a permutationinvariant function of the context producing a task representation. In past works, pooling operations are usually used to enforce permutation-invariance. CNP and NP use sum-pooling:</p><formula xml:id="formula_2">r = i∈C r i , where r i = h(x i , y i ; φ e</formula><p>) is a representation for context pair x i , y i , and r is a fixed-dimensional task representation. Instead, we introduce functional-pooling operations, which also enforce permutation-invariance but output a function that can loosely be interpreted as an infinite-dimensional representation.</p><p>Definition 2.1 (Functional pooling). Let k(·, ·) be a realvalued similarity measure, and {(x i , r i )} i∈C be a set of key-value pairs with x i ∈ X , r i ∈ R. Functional pooling is a mapping FUNPOOLING : (X × R) |C| → H defined as</p><formula xml:id="formula_3">r(·) = FUNPOOLING({(x i , r i )} i∈C ) = i∈C k(·, x i )r i , (2)</formula><p>where the output is a function r : X → R and H is a space of such functions.</p><p>In practice, we only need to evaluate this function on a finite query set {(x j , y j )} j∈Q (consisting of both contexts and targets; see below). That is, we only need to compute R = [r(x 1 ), . . . , r(x |Q| )] , which can be easily implemented using matrix operations. We consider two types of FUNPOOLING here, though others are possible. The kernel-based FUNPOOLING reads as,</p><formula xml:id="formula_4">R = KFP (Q, K, V ) := k rbf (Q, K)V ,<label>(3)</label></formula><p>where k rbf is the RBF kernel, Q = [a(x 1 ), . . . , a(x |Q| )] is a matrix whose rows are queries, a(·) is a transformation mapping inputs into features, K = [a(x 1 ), . . . , a(x |C| )] a matrix whose rows are keys, and V = [r 1 , . . . , r |C| ] a matrix whose rows are values (using terminology from the attention literature). Parameterising input transformation a At each iteration, we first evaluate the current functional representation at both context and target points. Then the shared local update function u takes in each context point and the corresponding evaluation as inputs, and produces local update ui. Next, we apply (kernel-based or attention-based) functional pooling to aggregate local updates ui into a functional update ∆r(·), which for each query is a linear combination of local updates ui weighted by similarities between this query and all keys. Finally, the functional updates are evaluated for both the context and the target, and are applied to the corresponding evaluations of functional representation with a learning rate α.</p><p>with a deep neural network can be seen as using deep kernels <ref type="bibr" target="#b38">(Wilson et al., 2016)</ref> as the similarity measure. The second type of FUNPOOLING is given by dot-product attention,</p><formula xml:id="formula_5">R = DFP (Q, K, V ) := softmax(QK / d k )V ,<label>(4)</label></formula><p>where d k is the dimension of the query/key vectors.</p><p>Our second core idea is that rather than producing the task representation in a single pass (like previous encoderdecoder meta-learning approaches), we start from an initial representation r (0) (·), and iteratively produce improved representations r (1) (·), . . . , r (T ) (·). At each step, a parameterised local update rule u compares r (t) (·) to the context input/output pairs, producing local update values</p><formula xml:id="formula_6">u i = u(x i , y i , r (t) (x i )) for each i ∈ C.</formula><p>These can then be aggregated into a global update to the task representation using functional pooling,</p><formula xml:id="formula_7">u i = u(x i , y i , r (t) (x i )) ,</formula><formula xml:id="formula_8">∆r (t) (·) = FUNPOOLING({(x i , u i )} i∈C ) , r (t+1) (·) = r (t) (·) − α∆r (t) (·) ,<label>(5)</label></formula><p>where α is the step size. Once the local update function u and the functional pooling operations are parameterised by neural networks, Equation (5) defines a neural update rule operating directly in function space. The functional update ∆r (t) (·) depends on the current representation r (t) (·) and the context {(x i , y i )} i∈C . <ref type="figure">Figure 1</ref> illustrates our iterative procedure in a simplified setting.</p><p>The final task representation can then be decoded into a predictor f (·) = Φ d (r (T ) (·); φ d ). The specific parametric forms of the decoder take different forms for regression and classification, and are described in Section 2.2. The decoder requires the evaluation of functional representation r (T ) (x) at x only for predicting f (x). Therefore, it is unnecessary to compute the functional representations r(·) (including their functional updates) on all input points. Instead, we compute them only on the context {x i } i∈C and target inputs {x j } j∈T . We use</p><formula xml:id="formula_9">r (t) = [r (t) (x 1 ) . . . r (t) (x |C| ), r (t) (x 1 ) . . . r (t) (x |T| )]</formula><p>to denote a matrix where each row is r (t) (x) evaluated on either context or target inputs, and let Q = [a(x 1 ) . . . a(x |C| ), a(x 1 ) . . . a(x |T| )] . Equation <ref type="formula" target="#formula_8">(5)</ref> can be implemented using matrix computations as follows,</p><formula xml:id="formula_10">u (t) i = u(x i , y i , r (t) i ) ,<label>(6)</label></formula><formula xml:id="formula_11">U (t) = [u (t) 1 , . . . , u (t) |C| ] ,<label>(7)</label></formula><formula xml:id="formula_12">∆r (t) = KFP or DFP Q, K, U (t) ,<label>(8)</label></formula><formula xml:id="formula_13">r (t+1) = r (t) − α∆r (t)<label>(9)</label></formula><p>where r (t) i denotes the i-th row of r (t) . To obtain a prediction f j for the target (x j , y j ), we decode the final representation for this target point:</p><formula xml:id="formula_14">f j = Φ d (r (T ) |C|+j ; φ d )</formula><p>, and the overall training loss can be written as:</p><formula xml:id="formula_15">L(u, a, φ d ) = 1 |T| j∈T (f j , y j ) ,<label>(10)</label></formula><p>where the predictions f j depend on u, a and φ d .</p><p>Assuming the width and depth of all our neural network components are bounded by W and D respectively, and the output dimension of u is also less than W , the time complexity of our approach is O W |C|(|C| + |T|) + W 2 D(|C|T + |T|) , and the space complexity is O (|C| + W T )(|C| + |T|) + W 2 D . For few-shot problems, |C| and |T| are typically small, and T ≤ 6 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">MetaFun for Regression and Classification</head><p>While the proposed framework can be applied to any supervised learning task, the specific parameterisation of learnable components can affect the model performance. In this section, we specify the parametric forms of our model that work well on regression and classification tasks.</p><p>Regression For regression tasks, we parameterise the local update function u(·) using a multi-layer perceptron as</p><formula xml:id="formula_16">u([x i , y i , r(x i )]) = MLP ([x i , y i , r(x i )]), i ∈ C, where [·]</formula><p>is concatenation. We also use an MLP to parametrise the input transformation a(·) in the functional pooling. The decoder in this case is given by w = MLP (r(x)), another MLP 1 that outputs w, which then parameterises the predictive model f = MLP (x; w).</p><p>Note that our model can easily be modified to incorporate Gaussian uncertainty by adding an extra output vector for the predictive standard deviation: P (y|x) = N (µ w (x), σ w (x)), w = MLP (r(x)). For further architecture details, see Appendix.</p><p>Classification For K-way classification, we divide the latent functional representation r(x) into K parts [r 1 (x), . . . , r K (x)], where r k (x) corresponds to the class k. Consequently, the local update function u(·) also has K parts, that is,</p><formula xml:id="formula_17">u([x i , y i , r(x i )]) = [u 1 (·), . . . , u K (·)]. In this case, y i = [y 1 i , . . . , y K i ]</formula><p>is the class label expressed as a one-hot vector; the u k is defined as follows,</p><formula xml:id="formula_18">u k ([x i , y i , r(x i )]) = y k i u + (m(r k (x i )), m i ) + (1 − y k i )u − (m(r k (x i )), m i ) ,<label>(11)</label></formula><p>where m i = K k=1 m(r k (x i )) summarises representations of all classes, and m, u + , u − are parameterised by separate MLPs. With this formulation, we update the class representations using either u + (when the label matches k) or u − 1 It might be desirable to use other parameterisations of the input transformation a(·), and the decoder f (·), e.g., f (x) = MLP ([x, r(x)])), or feeding r(x) to each layer of the MLP.</p><p>(when the label is different to k), so that labels are not concatenated to the inputs, but directly used to activate different model components, which is crucial for model performance. Furthermore, interactions between data points in classification problems include both within-class and between-class interactions. Our approach is able to integrate two types of interactions by having separate functional representation for each class and computing local updates for each class differently based on class membership of each data point. In fact, this formulation resembles the structure of the local update rule in functional gradient descent for classification tasks, which is a special case of our approach (see Section 3). Same as in regression tasks, the input transformation a(·) in the functional pooling is still an MLP. The parametric form of the decoder is the same as in Latent Embedding Optimisation (LEO) <ref type="bibr" target="#b31">(Rusu et al., 2019)</ref>. The class representation r k (x) generates weights w k ∼ N (µ(r k (x)), σ(r k (x))) where µ and σ are MLPs or just linear functions, and the final prediction is given by</p><formula xml:id="formula_19">P (y = k|x) = softmax(x T w) k ,<label>(12)</label></formula><p>where w = [w 1 , . . . , w K ], k = 1, . . . , K. Hyperparameters of all components are described in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Functional Gradient Descent Functional gradient descent <ref type="bibr" target="#b21">(Mason et al., 1999;</ref><ref type="bibr" target="#b13">Guo et al., 2001)</ref> is an optimisation algorithm used to minimise the objective function by moving in the direction of the negative gradient in function space. To ensure smoothness, we may work with functions in a Reproducing kernel Hilbert space (RKHS) <ref type="bibr" target="#b2">(Aronszajn, 1950;</ref><ref type="bibr" target="#b5">Berlinet &amp; Thomas-Agnan, 2011)</ref> defined by a kernel k(x, x ). Given a function f in the RKHS, we are interested in minimising the supervised loss L(f ) = i∈C (f (x i ), y i ) with respect to f . We can do so by computing the functional derivative and use it to iteratively update f (see Appendix for more details),</p><formula xml:id="formula_20">f (t+1) (x) = f (t) (x) − α i∈C k(x, x i )∇ (f (t) (x i ), y i ) (13) with step size α, and ∇ (f (t) (x i ), y i ) denotes gradient w.r.t.</formula><p>to predictions in the loss function .</p><p>The update rule in Equation <ref type="formula" target="#formula_8">(5)</ref> becomes that of functional gradient descent in Equation <ref type="formula" target="#formula_4">(13)</ref> when</p><formula xml:id="formula_21">(i) A trivial decoder f (x) = Φ d (r(x); φ d )(x) = r(x)</formula><p>is used, so the functional representation r(x) is the same as the predictive model f (x).</p><p>(ii) Kernel functional pooling KFP is used and the kernel function is fixed.</p><p>(iii) Using gradient-based local update function u(x, y, f (x)) = ∇ (f (x), y).</p><p>Furthermore, for a K-way classification problem, we predict</p><formula xml:id="formula_22">K-dimensional logits f (x) = [f 1 (x), .</formula><p>. . , f K (x)] , and use cross entropy loss as follows:</p><formula xml:id="formula_23">(f (x), y) = − K k=1 y k log e f k (x) K k =1 e f k (x) ,<label>(14)</label></formula><p>where y = [y 1 , . . . , y K ] is the one-hot label for x.</p><p>The gradient-based local update function is now</p><formula xml:id="formula_24">∇ (f (x), y) = [∂ 1 (f (x), y), . . . , ∂ K (f (x), y)] where ∂ k (f (x), y) is partial derivative w.r.t. each predictive logit: ∂ k (f (x), y) = e f k (x) K k =1 e f k (x) − y k .<label>(15)</label></formula><p>Here ∂ k (f (x), y) is analogous to u k (·) in Equation <ref type="formula" target="#formula_18">(11)</ref>, which is the local update function for class k.</p><p>If m, u + , u − in Equation <ref type="formula" target="#formula_18">(11)</ref> are specified rather than being learned, more specifically:</p><formula xml:id="formula_25">m(f k (x)) = e f k (x) u + (m(f k (x)), m) = m(f k (x)) m − 1 u − (m(f k (x)), m) = m(f k (x)) m m = K k=1 m(f k (x)) ,<label>(16)</label></formula><p>Equation <ref type="formula" target="#formula_8">(15)</ref> can be rewritten as:</p><formula xml:id="formula_26">∂ k (f (x), y) = y k u + (m(f k (x)), m) + (1 − y k )u − (m(f k (x)), m) ,<label>(17)</label></formula><p>which has a similar form as Equation <ref type="formula" target="#formula_18">(11)</ref>.</p><p>Therefore, our approach can be seen as an extension of functional gradient descent, with an additional learning capacity due learnable neural modules which afford more flexibility. From this perspective, our approach tackles supervised meta-learning problems by learning an optimiser in function space.</p><p>Supervised Meta-Learning Various ideas have been proposed to solve the problem of supervised meta-learning. <ref type="bibr" target="#b1">Andrychowicz et al. (2016)</ref>; <ref type="bibr" target="#b28">Ravi &amp; Larochelle (2016)</ref> learn the neural optimisers from previous tasks which can be used to optimise models for new tasks. However, these learned optimisers operate in parameter space rather than function space as we do. MAML <ref type="bibr" target="#b7">(Finn et al., 2017)</ref> learns the initialisation from which models are further adapted for a new task by a few gradient descent steps. <ref type="bibr" target="#b15">Koch (2015)</ref>; <ref type="bibr" target="#b33">Snell et al. (2017)</ref>; <ref type="bibr" target="#b36">Vinyals et al. (2016)</ref> explore the idea of learning a metric space from previous tasks in which data points are compared to each other to make predictions at test time. <ref type="bibr" target="#b32">Santoro et al. (2016)</ref> demonstrate that Memory-Augmented Neural Networks (MANN) can rapidly integrate the data for a new task into memory, and utilise this stored information to make predictions.</p><p>Our approach, in line with previous works such as CNP and NP, adopt an encoder-decoder pipeline to tackle supervised meta-learning. The encoder in CNP corresponds to a summation of instance-level representations produced by a shared instance encoder. NPs, on the other hand, use a probabilistic encoder with the same parametric form as CNP, but producing a distribution of stochastic representation. The Attentive Neural Process (ANP)  adds a deterministic path in addition to the stochastic path in NP. The deterministic path produces a target-specific representation, which can be interpreted as applying functional pooling (implemented with multihead attention <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>) to instance-wise representation. However, the representation is directly produced in a single pass rather than iteratively improved as we do, and only regression applications are explored as opposed to few-shot image classification. In fact, to achieve high performance for classification tasks, it is crucial for CNP to only apply sum-pooling within each class <ref type="bibr" target="#b9">(Garnelo et al., 2018a)</ref>, and it is unclear how to follow similar practices in ANP with both within-class and betweenclass interactions still being modelled. Recently, <ref type="bibr" target="#b11">Gordon et al. (2019)</ref> have also extended CNP to use functional representations, but for the purpose of incorporating translation equivariance in the inputs as an inductive bias rather than increasing representational capacity as we do. Their approach uses convnets to impose translation equivariance and does not learn a flexible iterative encoder.</p><p>Pooling operations are usually used in encoder-decoder meta-learning to enforce permutation invariance in the encoder. As an example, encoders in both CNP and NP use simple sum-pooling operations. More expressive pooling operations have been proposed to model interactions between data points. <ref type="bibr" target="#b24">Murphy et al. (2019)</ref> introduces Janossy pooling which applies permutation-sensitive functions to all reorderings and averages the outputs, while <ref type="bibr" target="#b16">Lee et al. (2019a)</ref> use pooling by multihead attention (PMA), which uses a finite query set to attend to the processed key-value pairs. Loosely speaking, attention-based functional pooling can be seen as having the whole input domain X as the query set in PMA.</p><p>Gradient-Based Meta-Learning Interestingly, many gradient-based meta-learning methods such as MAML can also be cast into an encoder-decoder formulation, because a gradient descent step is a valid permutation-invariant func-tion. For a model f (·, θ) parameterised by θ, one gradient descent step on the context loss has the following form,</p><formula xml:id="formula_27">θ t+1 = θ t − α i∈C ∇ θ (f (x i ; θ t ), y i ) ,<label>(18)</label></formula><p>where is the loss function, α is the learning rate, and θ t are the model parameters after t gradient steps. This corresponds to a special case of permutation-invariant functions where we take the instance-wise encoder to be</p><formula xml:id="formula_28">h t (x i , y i ; θ t ) = θ t /|C| − α∇ θ (f (x i ; θ t ), y i ) and apply sum-pooling θ t+1 = i h t (x i , y i ; θ t ).</formula><p>Multiple gradientdescent steps also result in a permutation-invariant function, which can be proved by induction. We refer to this as a gradient-based encoder. What follows is that popular metalearning methods such as MAML can be seen as part of the encoder-decoder formulation. More specifically, in MAML, we learn an initialisation of the model parameters θ 0 from training tasks, and adapt to new tasks by running T gradient steps from the learned initialisation. Therefore, θ T can be seen as the task representation (albeit very highdimensional) produced by a gradient-based encoder. The success of MAML on a variety of tasks can be partially explained by the high-dimensional representation and the iterative adaptation by gradient descent, supporting our usage of a functional ('infinite-dimensional') representation and iterative updating procedure. Note, however, that the update rule in MAML operates in parameter space rather than function space as in our case.</p><p>Under the same encoder-decoder formulation, a comparison regarding MAML and MetaFun can be made, which partially explains why MetaFun can be desirable: Firstly, the updates in MAML must lie in its parametric space, while there is no parametric constraint in MetaFun, which is better illustrated in <ref type="figure">Figure 3</ref>. Secondly, MAML uses gradient-based updates, while MetaFun uses learned local updates, which potentially contains more information than gradient. Finally, MAML does not explicitly consider interactions between data points, while both within-context and context-target interactions are modelled in MetaFun.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed model on both few-shot regression and classification tasks. In all experiments that follow, we partition the data into training, validation and test meta-sets, each containing data from disjoint tasks. For quantitative results, we train each model with 5 different random seeds and report the mean and the standard deviation of the test accuracy. For further details on hyperparameter tuning, see the Appendix. All experiments are performed using TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>, and the code is available online 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">1-D Function Regression</head><p>We first explore a 1D sinusoid regression task where we visualise the updating procedure in function space, providing intuition for the learned functional updates. Then we incorporate Gaussian uncertainty into the model, and compare our predictive uncertainty against that of a GP which generates the data.  <ref type="figure">U(0, π)</ref>. The x-coordinates are uniformly sampled from U(−5.0, 5.0). <ref type="figure">Figure 3</ref> shows that our proposed algorithm learns a smooth transition from the initial state to the final prediction at t = T = 5. Note that although only 5 context points on a single phase of the sinusoid are given at test time, the final iteration makes predictions close to the ground truth across the whole period. As a comparison, we use MAML as an example of updating in parameter space. The original MAML (40 units × 2 hidden layers) can fit the sinusoid quite well after several iterations from the learned initialisation. However the prediction is not as good, particularly on the left side where there are no context points (see <ref type="figure">Figure 3 B</ref>). As we increase the model size to large MAML (256 units × 3 hidden layers), updates become much smoother <ref type="figure">(Figure 3 C)</ref> and the predictions are closer to the ground truth. We further conduct experiments with a very wide MAML (1024 units × 3 hidden layers), but the performance cannot be further improved <ref type="figure">(Figure 3  D)</ref>. In <ref type="table" target="#tab_0">Table 1</ref>, we compare the mean squared error averaged across tasks. MetaFun performs much better than all MAMLs, even though less parameters (116611 parameters) are used compared to large MAML (132353 parameters). Predictive uncertainties As another simple regression example, we demonstrate that MetaFun, like CNP, can produce good predictive uncertainties. We use synthetic data generated using a GP with an RBF kernel and Gaussian observation noise (µ = 0, σ = 0.1), and our decoder produces both predictive means and variances. As in <ref type="bibr" target="#b14">Kim et al. (2019)</ref>, we found that MetaFun-DFP can produce somewhat piece-wise constant mean predictions which is less appealing in this situation. On the other hand, MetaFun-KFP (with deep kernels) performed much better, as can be seen in <ref type="figure">Figure 4</ref>. We consider the cases of 5 or 15 context points, and compare our predictions to those for the oracle GP. In both cases, our model gave very good predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification: miniImageNet and tieredImageNet</head><p>The miniImageNet dataset <ref type="bibr" target="#b36">(Vinyals et al., 2016)</ref> consists of 100 classes selected randomly from the ILSVRC-12 dataset <ref type="bibr" target="#b30">(Russakovsky et al., 2015)</ref>, and each class contains 600 randomly sampled images. We follow the split in <ref type="bibr" target="#b28">Ravi &amp; Larochelle (2016)</ref>, where the dataset is divided into training (64 classes), validation (16 classes), and test (20 classes) meta-sets. The tieredImageNet dataset <ref type="bibr" target="#b29">(Ren et al., 2018)</ref> contains a larger subset of the ILSVRC-12 dataset. These classes are further grouped into 34 higher-level nodes. These nodes are then divided into training (20 nodes), validation (6 nodes), and test (8 nodes) meta-sets. This dataset is considered more challenging because the split is near the root of the ImageNet hierarchy <ref type="bibr" target="#b29">(Ren et al., 2018)</ref>. For both datasets, we use the pre-trained features provided by <ref type="bibr" target="#b31">Rusu et al. (2019)</ref>.</p><p>Following the commonly used experimental setting, each few-shot classification task consists of 5 randomly sampled classes from a meta-set. Within each class, we have either 1 example (1-shot) or 5 examples (5-shot) as context, and 15 examples as target. For all experiments, hyperparameters are chosen by training on the training meta-set, and comparing target accuracy on the validation meta-set. We conduct randomised hyperparameters search <ref type="bibr" target="#b4">(Bergstra &amp; Bengio, 2012)</ref>, and the search space is given in Appendix. Then with the model configured by the chosen hyperparameters, we train on the union of the training and validation meta-sets, and report final target accuracy on the test meta-set.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we compare our approach to other meta-learning methods. The numbers presented are the mean and standard deviation of 5 independent runs. The table demonstrates that our model outperforms previous state-of-the-art on 1shot and 5-shot classification tasks for the more challenging tieredImageNet. As for miniImageNet, we note that previous work, such as MetaOptNet-SVM <ref type="bibr" target="#b17">(Lee et al., 2019b)</ref>, used significant data augmentation to regularise their model and hence achieved superior results. For a fair comparison, we also equipped each model with data augmentation and reported accuracy with/without data augmentation. However, MetaOptNet-SVM <ref type="bibr" target="#b17">(Lee et al., 2019b</ref>) uses a different data augmentation scheme involving horizontal flip, random crop, and color (brightness, contrast, and saturation) jitter. On the other hand, MetaFun, <ref type="bibr" target="#b27">Qiao et al. (2018)</ref> and LEO <ref type="bibr" target="#b31">(Rusu et al., 2019)</ref>, only use image features averaging representation of different crops and their horizontal mirrored versions. In 1-shot cases, MetaFun matches previous state-of-the-art performance, while in 5-shot cases, we get significantly better results. In <ref type="table" target="#tab_1">Table 2</ref>, results for both MetaFun-DFP (using dot-product attention) and MetaFun-KFP (using deep kernels) are reported. Although both of  <ref type="bibr" target="#b36">(Vinyals et al., 2016)</ref> 43.56 ± 0.84% 55.31 ± 0.73% Meta-learner LSTM <ref type="bibr" target="#b28">(Ravi &amp; Larochelle, 2016)</ref> 43.44 ± 0.77% 60.60 ± 0.71% MAML <ref type="bibr" target="#b7">(Finn et al., 2017)</ref> 48.70 ± 1.84% 63.11 ± 0.92% LLAMA <ref type="bibr" target="#b12">(Grant et al., 2018)</ref> 49.40 ± 1.83% -REPTILE <ref type="bibr" target="#b25">(Nichol et al., 2018)</ref> 49.97 ± 0.32% 65.99 ± 0.58% PLATIPUS  50.13 ± 1.86% -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Without data augmentation):</head><p>Meta-SGD <ref type="bibr" target="#b18">(Li et al., 2017)</ref> 54.24 ± 0.03% 70.86 ± 0.04% SNAIL <ref type="bibr" target="#b22">(Mishra et al., 2018)</ref> 55.71 ± 0.99% 68.88 ± 0.92% <ref type="bibr" target="#b3">Bauer et al. (2017)</ref> 56.30 ± 0.40% 73.90 ± 0.30% <ref type="bibr" target="#b23">Munkhdalai et al. (2018)</ref> 57.10 ± 0.70% 70.04 ± 0.63% TADAM <ref type="bibr" target="#b26">(Oreshkin et al., 2018)</ref> 58.50 ± 0.30% 76.70 ± 0.30% <ref type="bibr" target="#b27">Qiao et al. (2018)</ref> 59 <ref type="formula">.</ref>  <ref type="bibr" target="#b33">(Snell et al., 2017)</ref> 53.31 ± 0.89% 72.69 ± 0.74% Relation Net [in <ref type="bibr" target="#b19">Liu et al. (2019)]</ref> 54.48 ± 0.93% 71.32 ± 0.78% Transductive Prop. Nets <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> 57 them demonstrate state-of-the-art performance, MetaFun-KFP generally outperforms MetaFun-DFP for 5-shot problems, but performs slightly worse for 1-shot problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>As stated in Section 2.2, our model has three learnable components: the local update function, the functional pooling, and the decoder. In this section we explore the effects of using different versions of these components. We also investigate how the model performance would change with different numbers of iterations. <ref type="table" target="#tab_4">Table 3</ref> demonstrates that neural network parameterised local update functions, described in Section 2.1, consistently outperforms gradient-based local update function, despite the latter having build-in inductive biases. Interestingly, the choice between dot-product attention and deep kernel in functional pooling is problem dependent. We found that MetaFun with deep kernels usually perform better than MetaFun with dot product attention on 5-shot classification tasks, but worse on 1-shot tasks. We conjecture that the deep kernel is better able to fuse the information across the 5 images per class compared to attention. In the comparative experiments in Section 4.2 we reported results on both.</p><p>In addition, we investigate how a simple Squared Exponential (SE) kernel would perform on these few-shot classification tasks. This corresponds to using an identity input transformation function a in deep kernels. <ref type="table" target="#tab_4">Table 3</ref> shows that using SE kernel is consistently worse than using deep kernels, showing that the heavily parameterised deep kernel is necessary for these problems.  <ref type="figure">Figure 5</ref>. This figure illustrates the accuracy of our approach for varying number of iterations T = 1, . . . , 6, over different few-shot learning problems. For each problem, we use the same configuration of hyperparameters except for the number of iterations and the choice between attention and deep kernels. Error bars (standard deviations) are given by training the same model 5 times with different random seeds.</p><p>Next, we looked into directly applying functional gradient descent with parameterised deep kernel to these tasks. This corresponds to removing the decoder and using deep kernels and gradient-based local update function (see Section 3). Unsurprisingly, this did not fare as well, given as it only has one trainable component (the deep kernel) and the updates are directly applied to the predictions rather than a latent functional representation.</p><p>Finally, <ref type="figure">Figure 5</ref> illustrates the effects of using different numbers of iterations T . On all few-shot classification tasks, we can see that using multiple iterations (two is often good enough) always significantly outperform one iteration. We also note that this performance gain diminishes as we add more iterations. In Section 4.2 we treated the number of iterations as one of the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we propose a novel functional approach for meta-learning called MetaFun. The proposed approach learns to generate a functional task representation and an associated functional update rule, which allows to iteratively update the task representation directly in the function space. We evaluate MetaFun on both few-shot regression and classification tasks, and demonstrate that it matches or exceeds previous state-of-the-art results on miniImageNet and tieredImageNet few-shot classification tasks.</p><p>Interesting future research directions include a) exploring a stochastic encoder and hence working with stochastic functional representations, akin to the Neural Process (NP), and b) using local update functions and the functional pooling components whose parameters change with iterations instead of sharing them across iterations, where the added flexibility could lead to further performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Functional Gradient Descent</head><p>Functional gradient descent <ref type="bibr" target="#b21">(Mason et al., 1999;</ref><ref type="bibr" target="#b13">Guo et al., 2001)</ref> is an iterative optimisation algorithm for finding the minimum of a function. However, the function to be minimised is now a function on functions (functional). Formally, a functional L : H → R is a mapping from a function space H to a 1D Euclidean space R. Just like gradient descent in parameter space which takes steps proportional to the negative of the gradient, functional gradient descent updates f following the gradient in function space. In this work, we only consider a special function space called RKHS (Appendix A.1), and calculate functional gradients in RKHS (Appendix A.2). The algorithm is further detailed in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Reproducing Kernel Hilbert Space</head><p>A Hilbert space H extends the notion of Euclidean space by introducing inner product ·, · H which describes the concept of distance or similarity in this space. A RKHS H k is a Hilbert space of real-valued functions on X with the reproducing property that for all x ∈ X there exists a unique k x ∈ H k such that the evaluation functional E x (f ) = f (x) can be represented by taking the inner product of this element k x and f , formally as:</p><formula xml:id="formula_29">E x (f ) = k x , f H k .<label>(19)</label></formula><p>Since k x ∈ H k for any x ∈ X , we can define a kernel function k(x, x ) : X × X → R by letting</p><formula xml:id="formula_30">k(x, x ) = k x (x) = k x , k x H k .<label>(20)</label></formula><p>Using properties of inner product, it is easy to show that the kernel function k(x, x ) is symmetric and positive definite, and we call it the reproducing kernel of the Hilbert space H k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Functional Gradients</head><p>Functional derivative can be thought of as describing the rate of change of the output with respect to the input in a functional. Formally, functional derivative at point f in the direction of g is defined as:</p><formula xml:id="formula_31">∂L ∂f (g) = lim →0 L(f + g) − L(f ) ,<label>(21)</label></formula><p>which is a function of g. This is known as Fréchet derivative in a Banach space, of which the Hilbert space is a special case.</p><p>Functional gradient, denoted as ∇ f L, is related to functional derivative by the following equation:</p><formula xml:id="formula_32">∂L ∂f (g) = ∇ f L, g H k .<label>(22)</label></formula><p>Thanks to the reproducing property, it is straightforward to calculate functional derivative of an evaluation functional in RKHS:</p><formula xml:id="formula_33">E x (f + g) = f + g, k x H k = f, k x H k + g, k x H k (23) ∂E x ∂f (g) = k x , g H k<label>(24)</label></formula><p>Therefore, the functional gradient of an evaluation functional is:</p><formula xml:id="formula_34">∇ f E x = k x .<label>(25)</label></formula><p>For a learning task with loss function and a context set {(x i , y i )} i∈C , the overall supervised loss on the context can be written as:</p><formula xml:id="formula_35">L(f ) = i∈C (f (x i ), y i ).<label>(26)</label></formula><p>In this case, the functional gradient of L can be easily calculated by applying the chain rule:</p><formula xml:id="formula_36">∇ f L = i∈C (f (x i ), y i )k xi (27) = i∈C k(·, x i ) (f (x i ), y i ).<label>(28)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Functional Gradient Descent</head><p>To optimise the overall loss on the entire context in Equation <ref type="formula" target="#formula_10">(26)</ref>, we choose a suitable learning rate α, and iteratively update f with:</p><formula xml:id="formula_37">f (t+1) (x) = f (t) (x) − α∇ f L(f (t) )(x) (29) = f (t) (x) − α i∈C k(x, x i ) (f (t) (x i ), y i )<label>(30)</label></formula><p>In order to evaluate the final model f T (x) at iteration T , we only need to compute</p><formula xml:id="formula_38">f (T ) (x) = f (0) (x) − T −1 t=0 α i∈C k(x, x i ) (f (t) (x i ), y i ),<label>(31)</label></formula><p>which does not depend on function values outside the context from previous iterations t &lt; T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head><p>We run experiments on Nvidia's GeForce GTX 1080 Ti, and it typically takes about 20-40 minutes to train a few-shot model on a single GPU card until early-stopping is triggered (after seeing 10k-100k tasks). For miniImageNet and tieredImageNet, we conduct randomised hyperparameters search <ref type="bibr" target="#b4">(Bergstra &amp; Bengio, 2012)</ref> for hyperparameters tunning. Typically, 64 configurations of hyperparameters are sampled for each problem, and the best configuration is chosen by comparing accuracy on the validation set. The considered range of hyperparameters is given in <ref type="table">Table 4</ref>, and the chosen hyperparameters are shown in <ref type="table">Table 5</ref>. For regression tasks, we simply use hyperparameters listed in <ref type="table" target="#tab_7">Table 6</ref> for both MetaFun-DFP and MetaFun-KFP. <ref type="table">Table 4</ref>. Considered Range of Hyperparameters. The random generators such as randint or uniform use numpy.random syntax, so the first argument is inclusive while the second argument is exclusive. Whenever a list is given, it means uniformly sampling from the list. u+ and u− will be followed by a linear transformation with an output dimension of dim-reprs. Outer learning rate 10 −5 × uniform(-5, -4) Initial inner learning rate [0.1, 1.0, 10.0] Dropout rate uniform(0.0, 0.5) Orthogonality penalty weight 10 uniform(-4, -2) L2 penalty weight 10 uniform(-10, -8) Label smoothing [0.0, 0.1, 0.2] <ref type="table">Table 5</ref>. Results of randomised hyperparameters search. Hyperparameters shown in this table are not guaranteed to be optimal within the considered range, because we conduct randomised hyperparameters search. However, models configured with these hyperparameters perform reasonably well, and we used them to report final results comparing to other methods. Furthermore, dropout is only applied to the inputs. Orthogonality penalty weight and L2 penalty weight are used in exactly the same way as in <ref type="bibr" target="#b31">Rusu et al. (2019)</ref>. Inner learning rate α is trainable so only an initial inner learning rate is given in the 2.69 × 10 −3 2.73 × 10 −4 1.06 × 10 −4 7.33 × 10 −3 L2 penalty weight 1.19 × 10 −9 1.68 × 10 −9 4.90 × 10 −9 6.22 × 10 −9 Label smoothing 0.2 0.2 0.1 0.1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>This figure illustrates the iterative computation of functional representation in MetaFun.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>MetaFun is able to learn smooth updates, and recover the ground truth function almost perfectly. While the updates given by MAMLs are relatively not smooth, especially for MAML with less parameters. Predictive uncertainties for MetaFun matches those for the oracle GP very closely in both 5-shot and 15-shot cases. The model is trained on random context size ranging from 1 to 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Few-shot regression on sinusoid. MAML can beneift from more parameters, but MetaFun still outperforms all MAMLs despite less parameters being used compared to large MAML. We report mean and standard deviation of 5 independent runs.</figDesc><table><row><cell>Model</cell><cell>5-shot MSE</cell><cell>10-shot MSE</cell></row><row><cell>Original MAML</cell><cell>0.390 ± 0.156</cell><cell>0.114 ± 0.010</cell></row><row><cell>Large MAML</cell><cell>0.208 ± 0.009</cell><cell>0.061 ± 0.004</cell></row><row><cell>Very Wide MAML</cell><cell>0.205 ± 0.013</cell><cell>0.059 ± 0.010</cell></row><row><cell>MetaFun</cell><cell cols="2">0.040 ± 0.008 0.017 ± 0.005</cell></row></table><note>Visualisation of functional updates We train a T -step MetaFun with dot-product functional pooling, on a simple sinusoid regression task from Finn et al. (2017), where each task uses data points of a sine wave. The amplitude A and phase b of the sinusoid varies across tasks and are randomly sampled during training and test time, with A ∈ U(0.1, 5.0) and b ∈</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Few-shot Classification Test Accuracy</figDesc><table><row><cell>miniImageNet 5-way miniImageNet 5-way</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation Study. We conduct independent randomised hyperparameter search for each number presented, and reported means and standard deviations over 5 independent runs for each.</figDesc><table><row><cell cols="2">Functional pooling</cell><cell></cell><cell cols="3">Local update Decoder function</cell><cell cols="2">MiniImageNet 1-shot 5-shot</cell><cell></cell><cell cols="3">tieredImageNet</cell></row><row><cell cols="2">Attention</cell><cell></cell><cell>NN</cell><cell></cell><cell></cell><cell>62.12 ± 0.30%</cell><cell>77.78 ± 0.12%</cell><cell cols="3">67.72 ± 0.14%</cell><cell cols="2">82.81 ± 0.15%</cell></row><row><cell cols="3">Deep Kernel</cell><cell>NN</cell><cell></cell><cell></cell><cell>61.16 ± 0.15%</cell><cell>78.20 ± 0.16%</cell><cell cols="2">67.27 ± 0.20%</cell><cell cols="3">83.28 ± 0.12%</cell></row><row><cell cols="2">Attention</cell><cell></cell><cell cols="2">Gradient</cell><cell></cell><cell>59.63 ± 0.19%</cell><cell>75.84 ± 0.04%</cell><cell cols="2">62.55 ± 0.10%</cell><cell></cell><cell cols="2">78.18 ± 0.09%</cell></row><row><cell cols="3">Deep Kernel</cell><cell cols="2">Gradient</cell><cell></cell><cell>59.73 ± 0.21%</cell><cell>76.41 ± 0.14%</cell><cell cols="2">65.24 ± 0.11%</cell><cell></cell><cell cols="2">80.31 ± 0.16%</cell></row><row><cell cols="2">SE Kernel</cell><cell></cell><cell>NN</cell><cell></cell><cell></cell><cell>60.04 ± 0.19%</cell><cell>75.25 ± 0.12%</cell><cell cols="2">60.81 ± 0.30%</cell><cell></cell><cell cols="2">79.70 ± 0.20%</cell></row><row><cell cols="3">Deep Kernel</cell><cell cols="2">Gradient</cell><cell></cell><cell>57.67 ± 0.16%</cell><cell>73.55 ± 0.04%</cell><cell cols="2">62.53 ± 0.17%</cell><cell></cell><cell cols="2">76.86 ± 0.07%</cell></row><row><cell></cell><cell cols="3">miniImageNet 1 shot</cell><cell></cell><cell cols="2">miniImageNet 5 shot</cell><cell cols="2">tieredImageNet 1 shot</cell><cell cols="4">tieredImageNet 5 shot</cell></row><row><cell>T=1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">T=1</cell><cell></cell><cell>T=1</cell><cell cols="2">T=1</cell><cell></cell><cell></cell></row><row><cell>T=2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">T=2</cell><cell></cell><cell>T=2</cell><cell cols="2">T=2</cell><cell></cell><cell></cell></row><row><cell>T=3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">T=3</cell><cell></cell><cell>T=3</cell><cell cols="2">T=3</cell><cell></cell><cell></cell></row><row><cell>T=4</cell><cell></cell><cell></cell><cell></cell><cell cols="2">T=4</cell><cell></cell><cell>T=4</cell><cell cols="2">T=4</cell><cell></cell><cell></cell></row><row><cell>T=5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">T=5</cell><cell></cell><cell>T=5</cell><cell cols="2">T=5</cell><cell></cell><cell></cell></row><row><cell>T=6</cell><cell></cell><cell></cell><cell></cell><cell cols="2">T=6</cell><cell></cell><cell>T=6</cell><cell cols="2">T=6</cell><cell></cell><cell></cell></row><row><cell>59%</cell><cell>60%</cell><cell>61%</cell><cell>62%</cell><cell>63%</cell><cell cols="2">74% 75.25% 76.5% 77.75% 79%</cell><cell cols="2">64% 65.25% 66.5% 67.75% 69%</cell><cell>76%</cell><cell>78%</cell><cell>80%</cell><cell>82%</cell><cell>84%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MetaFun-DFP</cell><cell>MetaFun-KFP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Architecture Shared MLP m nn-sizes × nn-layers MLP for positive labels u + nn-sizes × nn-layers MLP for negative labels u − nn-sizes × nn-layers Key/query transformation MLP a dim(x) × embedding-layers Decoder linear with output dimension dim(x)</figDesc><table><row><cell>Hyperparameters</cell><cell>Considered Range</cell></row><row><cell>num-iters</cell><cell>randint(2, 7)</cell></row><row><cell>nn-layers</cell><cell>randint(2, 4)</cell></row><row><cell>embedding-layers</cell><cell>randint(1, 3)</cell></row><row><cell>nn-sizes</cell><cell>[64, 128]</cell></row><row><cell>dim-reprs</cell><cell>=nn-sizes</cell></row><row><cell>Initial representation r 0</cell><cell>[zero, constant, parametric]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>× 10 −3 2.58 × 10 −3 L2 penalty weight 1.32 × 10 −10 2.60 × 10 −10 1.92 × 10 −9 1.63 × 10 −9 × 10 −5 8.60 × 10 −5 8.01 × 10 −5 4.50 × 10 −5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>table.</cell><cell></cell></row><row><cell></cell><cell cols="3">miniImageNet</cell><cell></cell><cell>tieredImageNet</cell></row><row><cell>Hyperparameters (for MetaFun-DFP)</cell><cell cols="2">1-shot</cell><cell>5-shot</cell><cell cols="2">1-shot</cell><cell>5-shot</cell></row><row><cell>num-iters</cell><cell>2</cell><cell></cell><cell>5</cell><cell></cell><cell>3</cell><cell>5</cell></row><row><cell>nn-layers</cell><cell>3</cell><cell></cell><cell>2</cell><cell></cell><cell>2</cell><cell>3</cell></row><row><cell>embedding-layers</cell><cell>2</cell><cell></cell><cell>2</cell><cell></cell><cell>1</cell><cell>1</cell></row><row><cell>nn-sizes</cell><cell>64</cell><cell></cell><cell>128</cell><cell></cell><cell>128</cell><cell>128</cell></row><row><cell>Initial state</cell><cell cols="2">zero</cell><cell>constant</cell><cell cols="2">constant</cell><cell>constant</cell></row><row><cell>Outer learning rate</cell><cell cols="2">8.56 × 10 −5</cell><cell>3.71 × 10 −5</cell><cell cols="2">5.55 × 10 −5 5.78 × 10 −5</cell></row><row><cell>Initial inner learning rate</cell><cell cols="2">0.1</cell><cell>10.0</cell><cell></cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Dropout rate</cell><cell cols="2">0.397</cell><cell>0.075</cell><cell cols="2">0.123</cell><cell>0.223</cell></row><row><cell cols="5">Orthogonality penalty weight 1.37 Label smoothing 3.28 × 10 −3 1.56 × 10 −3 0.2 0.2</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">miniImageNet</cell><cell></cell><cell>tieredImageNet</cell></row><row><cell>Hyperparameters (for MetaFun-KFP)</cell><cell cols="2">1-shot</cell><cell>5-shot</cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>num-iters</cell><cell></cell><cell>3</cell><cell>6</cell><cell></cell><cell>4</cell><cell>4</cell></row><row><cell>nn-layers</cell><cell></cell><cell>3</cell><cell>2</cell><cell></cell><cell>2</cell><cell>3</cell></row><row><cell>embedding-layers</cell><cell></cell><cell>2</cell><cell>2</cell><cell></cell><cell>1</cell><cell>1</cell></row><row><cell>nn-sizes</cell><cell></cell><cell>64</cell><cell>64</cell><cell></cell><cell>64</cell><cell>128</cell></row><row><cell>Initial state</cell><cell cols="2">zero</cell><cell>parametric</cell><cell cols="2">parametric</cell><cell>zero</cell></row><row><cell cols="2">Outer learning rate 4.21 Initial inner learning rate</cell><cell>0.1</cell><cell>0.1</cell><cell></cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Dropout rate</cell><cell cols="2">0.424</cell><cell>0.359</cell><cell></cell><cell>0.115</cell><cell>0.148</cell></row><row><cell>Orthogonality penalty weight</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Hyperparameters for regression tasks. Local update function and the predictive model will be followed by linear transformations with output dimension of dim-reprs and dim(y) accordingly.</figDesc><table><row><cell>Components</cell><cell>Architecture</cell></row><row><cell>Local update function</cell><cell>nn-sizes × nn-layers</cell></row><row><cell cols="2">Key/query transformation MLP a nn-sizes × embedding-layers</cell></row><row><cell>Decoder</cell><cell>nn-sizes × nn-layers</cell></row><row><cell>Predictive model</cell><cell>nn-sizes × (nn-layers-1)</cell></row><row><cell>Hyperparameters</cell><cell>Considered Range</cell></row><row><cell>num-iters</cell><cell>5</cell></row><row><cell>nn-layers</cell><cell>3</cell></row><row><cell>embedding-layers</cell><cell>3</cell></row><row><cell>nn-sizes</cell><cell>128</cell></row><row><cell>dim-reprs</cell><cell>=nn-sizes</cell></row><row><cell>Initial representation r 0</cell><cell>zero</cell></row><row><cell>Outer learning rate</cell><cell>10 −4</cell></row><row><cell>Initial inner learning rate</cell><cell>0.1</cell></row><row><cell>Dropout rate</cell><cell>0.0</cell></row><row><cell>Orthogonality penalty weight</cell><cell>0.0</cell></row><row><cell>L2 penalty weight</cell><cell>0.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A tensorflow implementation of our model is available at github.com/jinxu06/metafun-tensorflow</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jonathan Schwarz for valuable discussion, and the anonymous reviewers for their feedback. Jin Xu and Yee Whye Teh acknowledge funding from Tencent AI Lab through the Oxford-Tencent Collaboration on Large Scale Machine Learning project. Jean-Francois Ton is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix for MetaFun: Meta-Learning with Iterative Functional Updates</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American mathematical society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Discriminative kshot learning using probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Świątkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reproducing kernel Hilbert spaces in probability and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic symmetries and invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bloem-Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/19-322.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="1" to="61" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic modelagnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1690" to="1699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Neural processes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Bruinsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13556</idno>
		<title level="m">Convolutional conditional neural processes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Norm-based regularization of boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Submitted to</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attentive neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Metalearning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Sgd</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Learning to learn quickly for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Iterative amortized inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3403" to="3412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Functional gradient techniques for combining hypotheses. Advances in Large Margin Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metalearning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osborne</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09006</idno>
		<title level="m">On the limitations of representing functions on sets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
