<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sub-Image Anomaly Detection with Deep Pyramid Correspondences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
							<email>niv.cohen2@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
							<email>yedid.hoshen@mail.huji.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sub-Image Anomaly Detection with Deep Pyramid Correspondences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>anomaly detection</term>
					<term>nearest-neighbors</term>
					<term>feature pyramid</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nearest neighbor (kNN) methods utilizing deep pre-trained features exhibit very strong anomaly detection performance when applied to entire images. A limitation of kNN methods is the lack of segmentation map describing where the anomaly lies inside the image. In this work we present a novel anomaly segmentation approach based on alignment between an anomalous image and a constant number of the similar normal images. Our method, Semantic Pyramid Anomaly Detection (SPADE) uses correspondences based on a multi-resolution feature pyramid. SPADE is shown to achieve state-of-the-art performance on unsupervised anomaly detection and localization while requiring virtually no training time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans observe many images throughout their lifetimes, most of which are of little interest. Occasionally, an image indicating an opportunity or danger appears. A key human ability is to detect the novel images that deviate from previous patterns triggering particular vigilance on the part of the human agent. Due to the importance of this function, allowing computers to detect anomalies is a key task for artificial intelligence.</p><p>As a motivational example, let us consider the setting of an assembly-line fault detection. Assembly lines manufacture many instances of a particular product. Most products are normal and fault-free. Unfortunately, on isolated occasions, the manufactured products contain some faults e.g. dents, wrong labels or part duplication. As reputable manufacturers strive to keep a consistent quality of products, prompt detection of the faulty products is very valuable. As mentioned earlier, humans are quite adept at anomaly detection, however having a human operator oversee every product manufactured by the assembly line has several key limitations: i) high wages earned by skilled human operators ii) limited human attention span ( <ref type="bibr" target="#b13">[14]</ref> states this can be as low as 20 minutes!) iii) a human operator cannot be replicated between different assembly lines. iv) different operators typically do not maintain a consistent quality level. Anomaly detection therefore calls for computer vision solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2005.02357v3 [cs.CV] 3 Feb 2021</head><p>Although visual anomaly detection is very valuable, it is also quite challenging. One challenge common to all anomaly detection methods is the unexpectedness of anomalies. Typically in supervised classification, test classes come from the a similar distribution to the train data. In most anomaly detection settings, the distribution of anomalies is not observed during training time. Different anomaly detection methods differ by the way the anomalies are observed at training time. In this paper, we deal with the setting where at training time only normal data (but no anomalies) are observed. This is a practically useful setting, as obtaining normal data (e.g. products that contain no faults) is usually easy. This setting is sometimes called semi-supervised ( <ref type="bibr" target="#b6">[7]</ref>). As this notation is ambiguous, we shall refer to this setting as the normal-only training setting. An easier scenario is fully supervised i.e. both normal and anomalous examples are presented with labels during training. As this training setting is similar to standard supervised classification, a mature task with effective solutions, it will not be dealt with in this work.</p><p>Another challenge particular to visual anomaly detection (rather than nonimage anomaly detection methods) is the localization of anomalies i.e. segmenting the parts of the image which the algorithm deems anomalous. This is very important for the explainability of the decision made by the algorithm as well as for building trust between operators and novel AI systems. It is particularly important for anomaly detection, as the objective is to detect novel changes not seen before which humans might not be familiar with. In this case, the computer may teach the human operator of the existence of new anomalies or alternatively the human may decide that an anomaly is not of interest thus not rejecting the product, resulting in cost-saving</p><p>We present a new method for solving the task of sub-image anomaly detection and segmentation. Our method does not require an extended training stage, it is fast, robust and achieves state of the art performance. Out methods consists of several stages: i) image feature extraction using a pre-trained deep neural network (e.g. an ImageNet trained ResNet) ii) nearest neighbor retrieval of the nearest K normal images to the target iii) finding dense pixel-level correspondence between the target and the normal images, target image regions that do not have near matches in the retrieved normal images are labeled as anomalous. Our method is extensively evaluated on an industrial product dataset (MVTech) as well as a surveillance dataset in a campus setting (Shanghai Tech Campus). Our method achieves state-of-the-art performance both on image-level and pixellevel anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Anomaly detection has attracted a large body of work over the last several decades. We present an overview of image-level and sub-image anomaly detection methods.</p><p>Image-level methods: We review methods that detect if an image is anomalous that are not particularly designed for segmenting the anomaly within the image.</p><p>Note that many of these methods are not specific to images. There are three main classes of methods for image-level anomaly detection: reconstruction-based, distribution-based and classification-based.</p><p>Reconstruction-based methods learn a set of basis functions on the training data, and attempt to reconstruct the test image using a sparse set of these basis functions. If the test image cannot be faithfully reconstructed using the basis functions, it is denoted as anomalous, as it is likely that it came from a different basis from that of the normal training data. Different methods vary in terms of the set of basis functions and loss functions they use. Popular choices of basis functions include: K-means <ref type="bibr" target="#b14">[15]</ref>, K nearest neighbors (kNN) <ref type="bibr" target="#b8">[9]</ref>, principal component analysis (PCA) <ref type="bibr" target="#b19">[20]</ref>. The loss functions used vary between simple vector metrics such as Euclidean or L 1 losses and can use more complex perceptual losses such as structural similarity (SSIM) <ref type="bibr" target="#b31">[32]</ref>. Recently deep learning methods have broadened the toolbox of reconstruction-based methods. Principal components have been extended to non-linear functions learned by autoencoders <ref type="bibr" target="#b26">[27]</ref>, including both denoising as well as variational autoencoders (VAEs). Deep perceptual loss functions <ref type="bibr" target="#b33">[34]</ref> significantly improve over traditional perceptual loss functions. The main disadvantages of reconstruction-based loss functions are: i) sensitivity to the particular loss measure used for evaluating the quality of reconstruction, making their design non-obvious and hurting performance ii) determining the correct functional basis.</p><p>The second class of methods is distribution-based. The main principle is to model the probability density function (PDF) of the distribution of the normal data. Test samples are evaluated using the PDF, and test samples with low probability density values are designated as anomalous. Different distribution-based methods differ by the distributional assumptions that they make, the approximations used to estimate the true PDF, and by the training procedure. Parametric methods include Gaussian or mixture of Gaussians (GMM). Kernel density estimation <ref type="bibr" target="#b20">[21]</ref> is a notable non-parametric method. Nearest neighbors <ref type="bibr" target="#b8">[9]</ref> can also be seen as a distributional (as it performs density estimation), but note that we also designated it a reconstruction-based method. Recently deep learning methods have improved performance, particularly by mapping high-dimensional data distributions into a lower and denser space. PDF estimation is typically easier in lower dimensional spaces. Learning the deep projection and distributional modeling can be done jointly as done by <ref type="bibr" target="#b35">[36]</ref>. Another recent development, adversarial training, was also applied to anomaly detection e.g. ADGAN <ref type="bibr" target="#b7">[8]</ref>. Although in principle distributional-methods are very promising, they suffer from some critical drawbacks: i) real image data rarely follows simple parametric distributional assumptions ii) non-parametric methods have high sample complexity and often require large training set that is often not available in practice.</p><p>Recently, classification-based methods have achieved dominance for imagelevel anomaly detection. One such paradigm is one-class support vector machines (OC-SVM) <ref type="bibr" target="#b27">[28]</ref>. One of its most successful variants is support vector data description (SVDD) <ref type="bibr" target="#b29">[30]</ref> which can be seen as a finding the minimal sphere which contains at least a given fraction of the data. These methods are very sensitive to the feature space used giving rise to both kernel methods as well as deep methods <ref type="bibr" target="#b25">[26]</ref> for learning features. Another set of methods is based on self-supervised learning. Golan and El-Yaniv <ref type="bibr" target="#b10">[11]</ref> proposed a RotNet-based <ref type="bibr" target="#b9">[10]</ref> approach, which performs geometric transformations on the input data and trains a network that attempts to recognize the transformation used. They use the idea that the trained classifier will generalize well to new normal images but not to anomalous images allowing it to be used as an anomaly detection criterion. Hendrycks et al. <ref type="bibr" target="#b17">[18]</ref> improved the architecture and training procedure achieving strong performance. Bergman and Hoshen <ref type="bibr" target="#b3">[4]</ref> combined this work with an SVDD type criterion and extended it to non-image data. Very recently Bergman et al. <ref type="bibr" target="#b2">[3]</ref> showed that the features learned using such self-supervised methods are not competitive with generic ImageNet-based feature extractors. A simple method based on kNN (or efficient approximations) significantly outperformed such self-supervised methods.</p><p>Sub-image methods: The methods previously described tackled the task of classifying a whole image as normal or anomalous, and most of the techniques were not specific to images. The task of segmenting the particular pixels containing anomalies is special to images and has achieved far less attention from the deep learning community. Napoletano et al. <ref type="bibr" target="#b23">[24]</ref> extracted deep features from small overlapping patches, and used a K-means based classifier over dimensionality reduced features. Bergmann et al. <ref type="bibr" target="#b4">[5]</ref> evaluated both a ADGAN and autoencoder approaches on MVTech finding complementary strengths. More recently, Venkataramanan et al. <ref type="bibr" target="#b30">[31]</ref> used an attention-guided VAE approach combining multiple methods (GAN loss <ref type="bibr" target="#b12">[13]</ref>, GRADCAM <ref type="bibr" target="#b28">[29]</ref>). Bergmann et al. <ref type="bibr" target="#b5">[6]</ref> used a student-teacher based autoencoder approach employing pre-trained Im-ageNet deep features (still requiring an expensive training stage). In this work, we present a novel sub-image alignment approach which is more accurate, faster, more stable than previous methods and does not require a dedicated training stage. To support research on sub-image anomaly detection, high quality datasets for evaluating this task have been introduced, such as: MVTech [5] -a dataset simulating an industrial fault detection where the objective is to detect parts of images of products that contain faults such as dents or missing parts. The ShanghaiTech Campus dataset [23] -a dataset simulating a surveillance setting where cameras observe a busy campus and the objective is to detect anomalous objects and activities such as fights. Hendrycks et al. <ref type="bibr" target="#b16">[17]</ref> also proposed a new dataset containing anomalies such as road hazards. We evaluate our work on the two most used datasets, MVTech and ShanghaiTech Campus (STC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Correspondence-based Sub-Image Anomaly Detection</head><p>We present our method for sub-image anomaly detection. Our method consists of several parts: i) image feature extraction ii) K nearest neighbor normal image retrieval iii) pixel alignment with deep feature pyramid correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Extraction</head><p>The first stage of our method is the extraction of strong image level features. The same features are later used for pixel-level image alignment. There are multiple options for extracting features. The most commonly used option is selfsupervised feature learning, that is, learning features from scratch directly on the input normal images. Although it is an attractive option, it is not obvious that the features learned on small training datasets will indeed be sufficient for serving as high-quality similarity measures. The analysis performed in Bergman et al. <ref type="bibr" target="#b3">[4]</ref> illustrates that self-supervised features underperform ImageNet-trained ResNet features for the purposes of anomaly detection. We therefore used a ResNet feature extractor pre-trained on the ImageNet dataset. As image-level features we used the feature vector obtained after global-pooling the last convolutional layer. Let us denote the global feature extractor F , for a given image x i , we denote the extracted features f i :</p><formula xml:id="formula_0">f i = F (x i )<label>(1)</label></formula><p>At initialization, the features for all training images (which are all normal) are computed and stored. At inference, only the features of the target image are extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">K Nearest Neighbor Normal Image Retrieval</head><p>The first stage in our method is determining which images contain anomalies using DN2 <ref type="bibr" target="#b3">[4]</ref>. For a given test image y, we retrieve its K nearest normal images from the training set, N k (f y ). The distance is measured using the Euclidean metric between the image-level feature representations.</p><formula xml:id="formula_1">d(y) = 1 K f ∈N K (fy) f − f y 2<label>(2)</label></formula><p>Images are labelled at this stage as normal or anomalous. Positive classification is determined by verifying if the kNN distance is larger than a threshold τ . It is expected that most images are normal, and only few images are designated as anomalous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sub-image Anomaly Detection via Image Alignment</head><p>After being labelled as anomalous at the image-level stage, the objective is to locate and segment the pixels of one or multiple anomalies. In the case that the image was falsely classified as anomalous, our objective would be to mark no pixels as anomalous.</p><p>As a motivational idea, let us consider aligning the test image to a retrieved normal image. By finding the differences between the test and normal image, we would hope to detect the anomalous pixels. This naive method has several flaws i) assume that there are multiple normal parts the object may possibly consist of, alignment to particular normal images may fail ii) for small datasets or objects that experience complex variation, we may never in fact find a normal training image which is similar to the test image in every respect triggering false positive detections iii) computing the image difference would be very sensitive to the loss function being used.</p><p>To overcome the above issues, we present a multi-image correspondence method. We extract deep features at every pixel location p ∈ P using feature extractor F (x i , p) of the relevant test and normal training images. The details of the feature extractor will be described in Sec. 3.4. We construct a gallery of features at all pixel locations of the K nearest neighbors G = {F (x 1 , p)|p ∈ P } ∪ {F (x 2 , p)|p ∈ P }}.. ∪ {F (x K , p)|p ∈ P }}. The anomaly score at pixel p, is given by the average distance between the features F (y, p) and its κ nearest features from the gallery G. The anomaly score of pixel p in target image y is therefore given by:</p><formula xml:id="formula_2">d(y, p) = 1 κ f ∈Nκ(F (y,p)) f − F (y, p) 2<label>(3)</label></formula><p>For a given threshold θ, a pixel is determined as anomalous if d(y, p) &gt; θ, that is, if we cannot find a closely corresponding pixel in the K nearest neighbor normal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature Pyramid Matching</head><p>Alignment by dense correspondences is an effective way of determining the parts of the image that are normal vs. those that are anomalous. In order to perform the alignment effectively, it is necessary to determine the features for matching. As in the previous stage, our method uses features from a pre-trained deep ResNet CNN. The ResNet results in a pyramid of features. Similarly to image pyramids, earlier layers (levels) result in higher resolution features encoding less context. Later layers encode lower resolution features which encode more context but at lower spatial resolution. To perform effective alignment, we describe each location using features from the different levels of the feature pyramid. Specifically, we concatenate features from the output of the last M blocks, the results for different numbers of M is shown in the experimental section. Our features encode both fine-grained local features and global context. This allows us to find correspondences between the target image and K ≥ 1 normal images, rather than having to explicitly align the images, which is more technically challenging and brittle. Our method is scalable and easy to deploy in practice. We will show in Sec. 4 that our method achieves the state-of-the-art sub-image anomaly segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>In all experiments, we use a Wide-ResNet50 × 2 feature extractor, which was pre-trained on ImageNet. MVTec images were resized to 256 × 256 and cropped to 224 × 224. STC images were resized to 256 × 256. Due to the large size of STC dataset, we subsampled its training data to roughly 5000 images. To be comparable with <ref type="bibr" target="#b30">[31]</ref>, we subsampled the STC test set by a factor of 5. All metrics were calculated at 256 × 256 image resolution, and we used cv2.INTERAREA for resizing when needed. Unless otherwise specified, we used features from the ResNet at the end of the first block (56 × 56), second block (28 × 28) and third block <ref type="bibr">(14 × 14)</ref>, all with equal weights. We used K = 50 nearest neighbours for the MVTtec experiments and K = 1 nearest neighbours for the STC experiments (due to the runtime considerations). In all experiments we used κ = 1.</p><p>After achieving the pixel-wise anomaly score for each image, we used smoothed the results with a Gaussian filter (σ = 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform an extensive evaluation of our method against the state-of-the-art in sub-image anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MVTec</head><p>To simulate anomaly detection in industrial settings, <ref type="bibr" target="#b4">[5]</ref> Bregmann et al. presented a dataset consisting of images from 15 different classes. 5 classes consist of textures such as wood or leather. The other 10 classes contain objects (mostly rigid). For each class, the training set is composed of normal images. The test set is composed of normal images as well as images containing different types of anomalies. This dataset therefore follows the standard protocol where no anomalous images are used in training. The anomalies in this dataset are more finegrained than those typically used in the literature e.g. in CIFAR10 evaluation, where anomalous images come from a completely different image category. Instead, anomalies take the form of a slightly scratched hazelnut or a lightly bent cable. As the anomalies are at the sub-image level, i.e. only affect a part of the image, the dataset provides segmentation maps indicating the precise pixel positions of the anomalous regions.</p><p>An example of the operation of our method on the MVTec dataset can be observed in <ref type="figure" target="#fig_1">Fig. 2</ref>. The anomalous object (a hazelnut) contain a scratch. The retrieved nearest neighbor normal image, contains a complete nut without scratches. By search for correspondences between the two images, our method is able to find correspondences for the normal image regions but not for the anomalous region. This results in an accurate detection of the anomalous region of the image. The anomalous images pixels are presented on the right-most image.</p><p>We compared our method against several methods that were introduced over the last several months, as well as longer standing baseline such as OCSVM and nearest neighbors. For each setting, we compared against the methods that reported the suitable metric.</p><p>We first compare the quality of deep nearest neighbor matching as a means for finding anomalous images. This is computed by the distance between the test image and the nearest neighbor normal images. Larger distances indicate more anomalous images. We compared the ROC area under the curve (ROCAUC) of the first step of our method and other state-of-the-art methods for image level anomaly detection. We report the average ROCAUC across the 15 classes. Please note that the first stage of our method is identical with DN2 <ref type="bibr" target="#b2">[3]</ref>. This comparison is important as it verifies if deep nearest neighbors are effective on these dataset. The comparison is presented in Tab. 1. Our method is shown to outperform a range of state-of-the-art methods utilizing a range of self-supervised anomaly detection learning techniques. This gives evidence that deep features trained on ImageNet (which is very different from ImageNet dataset) are very effective even on datasets that are quite different from ImageNet.</p><p>We proceed to evaluate our method on the task of pixel-level anomaly detection. The objective here is to segment the particular pixels that contain anoma- lies. We evaluate our method using two established metrics. The first is per-pixel ROCAUC. This metric is calculated by scoring each pixel by the distance to its K nearest correspondences. By scanning over the range of thresholds, we can compute the pixel-level ROCAUC curve. The anomalous category is designated as positive. It was noted by several previous works that ROCAUC is biased in favor of large anomalies. In order to reduce this bias, Bergmann et al <ref type="bibr" target="#b5">[6]</ref> propose the PRO (per-region overlap) curve metric. They first separate anomaly masks into their connected components, therefore dividing them into individual anomaly regions. By changing the detection threshold, they scan over false positive rates (FPR), for each FPR they compute PRO i.e. the proportion of the pixels of each region that are detected as anomalous. The PRO score at this FPR is the average coverage across all regions. The PRO curve metric computes the integral across FPR rates from 0 to 0.3. The PRO score is the normalized value of this integral. In Tab. 2, we compare our methods on the per-pixel ROCAUC metric against state-of-the-art results reported by Bergmann et al. <ref type="bibr" target="#b4">[5]</ref> as well as newer results by Venkataramanan et al. <ref type="bibr" target="#b30">[31]</ref>. Most of the methods use different varieties of autoencoders, including the top-performer CAVGA-R u . Our method significantly outperforms all methods. This attest to the strength of our pyramid based correspondence approach. In Tab. 3, we compare our method in terms of PRO. As explained above, this is another per-pixel accuracy measure which gives larger weight to anomalies which cover few pixels. Our method is compared with the auto-encoder with pre-trained features based approach of Bregmann et al. <ref type="bibr" target="#b5">[6]</ref> and the baselines presented in their paper. Our approach achieves significantly better results than all previous methods. We note than Bregmann et al also presented an ensemble approach with better results. While our method does not use ensembles (which will probably improve our method too), we outperform the ensemble approach as well. We present more qualitative results of our method in <ref type="figure" target="#fig_0">Fig. 1</ref> that show that our method is able to recover accurate masks of the anomalous regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shanghai Tech Campus Dataset</head><p>We evaluate our method on the Shanghai Tech Campus dataset. It simulates a surveillance setting, where the input consists of videos captured by surveillance cameras observing a busy campus. The dataset contains 12 scenes, each scene consists of training videos and a smaller number of test images. The training videos do not contain anomalies while the test videos contain normal and anomalous images. Anomalies are defined as pedestrians performing non-standard ac- The predicted anomalous image pixels. We can see how in this example, SPADE detects the anomalous image region by finding the correspondence with the nearest-neighbor image. The anomalous parts did not have correspondences in the normal image and were therefore detected. tivities (e.g. fighting) as well as any moving object which is not a pedestrian (e.g. motorbikes). We began by evaluating our first stage for detecting image-level anomalies against other state-of-the-art methods. We show in Tab. 4 that our first stage has comparable performance to the top performing method <ref type="bibr" target="#b18">[19]</ref>. More interestingly, we compare in Tab. 5 the pixel-level ROCAUC performance with the best reported method, CAVGA-R u <ref type="bibr" target="#b30">[31]</ref>. Our method significantly outperforms the best reported method by a significant margin. Note that we compared to the best method that did not use anomaly supervision, as we do not use it and as anomaly supervision is often not available in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct an ablation study on our method in order to understand the relative performance of its different parts. In Tab. 6, we compare using different level of the feature pyramid. We experienced that using activations of too high resolution (56 × 56) significantly hurts performance (due to limited context) while using the higher levels on their own results in diminished performance (due to lower resolution). Using a combination of all features in the pyramid results in the best performance. In Tab. 7, we compared using the top K neighboring normal images as performed by our first stage vs. choosing them randomly from the dataset. We observe that choosing the kNN images improves performance. This does not affect all classes equally. As an example, we report the numbers for the class "Grid" which has much variation between images. For this category, using the kNN images results in much better performance than randomly choosing K images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Anomaly detection via alignment: Most current sub-image anomaly detection methods take the approach of learning a large parametric function for auto-encoding images, making the assumption that anomalous regions will not be reconstructed well. Although this approach does achieve some success, we take a much simpler approach. Similarly to image alignment methods and differently from other sub-image anomaly detection methods, our method does not require feature training and can work on very small datasets. A difference between our method and standard image alignment is that we find correspondences between the target image and parts of K normal images, as opposed to an entire single normal image in simple alignment approaches. The connection with alignment methods, can help in speeding up our method e.g. by combining it with the  <ref type="table">Table 7</ref>. Evaluating the effectiveness of our kNN retrieval state. We use here 10 nearest neighbours, chosen according to stage 1, or randomly selected. We also show the "Grid" class to indicate that stage 1 is more important to some classes then others PatchMatch <ref type="bibr" target="#b1">[2]</ref> method which used locality for significant speedup of the kNN search.</p><p>The role of context for anomaly detection: The quality of the alignment between the anomalous image and retrieved normal images is strongly affected by the quality of extracted features. Similarly to other works dealing with detection and segmentation, the context is very important. Local context is needed for achieving segmentation maps with high-pixel resolutions. Such features are generally found in the shallow layers of a deep neural networks. Local context is typically insufficient for alignment without understanding the global context i.e. location of the part within the object. Global context is generally found in the deepest layers of a neural network, however global context features are of low resolution. The combination of features from different levels allows both global context and local resolution giving high quality correspondences. The idea is quite similar to that in Feature Pyramid Networks <ref type="bibr" target="#b21">[22]</ref>.</p><p>Optimizing runtime performance: Our method is significantly reliant on the K nearest neighbors algorithm. The complexity of kNN scales linearly with the size of the dataset used for search which can be an issue when the dataset is very large or of high dimensionality. Our approach is designed to mitigate the complexity issues. First, we compute the initial image-level anomaly classification on global-pooled features which are 2048 dimensional vectors. Such kNN computation can be achieved very quickly for moderate sized datasets and different speedup techniques (e.g. KDTrees) can be used for large scale datasets. The anomaly segmentation stage requires pixel-level kNN computation which is significantly slower than image-level kNN. However, our method limits the sub-image kNN search to only the K nearest neighbors of the anomalous image significantly limiting computation time. We assume that the vast majority of images are normal, therefore only a small fraction of images require the next stage of anomaly segmentation. Additionally, the anomaly segmentation stage is required for explainability and trust building with the human operators, but in many cases it is not time-critical therefore putting a laxer requirement on computation time. Our method is therefore quite suitable for practical deployment from a complexity and runtime perspective.</p><p>Pre-trained vs. learned features: Previous sub-image anomaly detection methods have either used self-learned features or a combination of self-learned and pre-trained images features. Self-learned approaches in this context, typically train an autoencoder and use its reconstruction error for anomaly detection. Other approaches have used a combination of pre-trained and self-learned methods e.g. methods that use perceptual losses and <ref type="bibr" target="#b5">[6]</ref> which uses a pre-trained encoder. Our numerical results have shown that our method significantly outperforms such approaches. We believe that given the limited supervision and small dataset size in normal-only training set as tackled in this work, it is rather hard to beat very deep pre-trained networks. We therefore use pre-trained features and do not attempt to modify them. The strong results achieved by our method attest to the effectiveness of this approach. We believe that future work should focus on methods for finetuning the deep pre-trained features for this particular task and expect it it improve over our method. That not-withstanding the ease of deployment and generality of our approach should make it a good choice in many practical settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a novel alignment-based method for detecting and segmenting anomalies inside images. Our method relies on K nearest neighbors of pixel-level feature pyramids extracted by pre-trained deep features. Our method consists of two stages, which are designed to achieve high accuracy and reasonable computational complexity. Our method was shown to outperform the strongest current methods on two realistic sub-image anomaly detection datasets, while being much simpler. The ease of deployment enjoyed by our method should make it a good candidate for practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work was partly supported by the Federmann Cyber Security Research Center in conjunction with the Israel National Cyber Directorate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An evaluation of SPADE on detecting anomalies between flowers with or without insects (taken from one category of 102 Category Flower Dataset<ref type="bibr" target="#b24">[25]</ref>) and bird varieties (taken from Caltech-UCSD Birds 200)<ref type="bibr" target="#b32">[33]</ref>. (left to right) i) An anomalous image ii) The retrieved top normal neighbor image iii) The mask detected by SPADE iv) The predicted anomalous image pixels. SPADE was able to detect the insect on the anomalous flower (top), the white colors of the anomalous albatross (center) and the red spot on the anomalous bird (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(left to right) i) An anomalous image ii) The retrieved top normal neighbor image iii) The mask detected by SPADE iv)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>(top rows) Anomaly detection on images from the Cable and Grid categories of the MVTec dataset (bottom) Detecting bike anomaly on the STC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Image-level anomaly detection accuracy on MVTec (Average ROCAUC %) Geom [11] GANomaly [1] AEL2 ITAE [19] SPADE Sub-Image anomaly detection accuracy on MVTec (ROCAUC %)</figDesc><table><row><cell cols="2">Average</cell><cell>67.2</cell><cell>76.2</cell><cell>75.4</cell><cell>83.9</cell><cell>85.5</cell><cell></cell></row><row><cell></cell><cell cols="7">AESSIM AEL2 AnoGAN CNN Dict TI VM CAVGA-Ru SPADE</cell></row><row><cell>Carpet</cell><cell>87</cell><cell>59</cell><cell>54</cell><cell>72</cell><cell>88 -</cell><cell>-</cell><cell>97.5</cell></row><row><cell>Grid</cell><cell>94</cell><cell>90</cell><cell>58</cell><cell>59</cell><cell>72 -</cell><cell>-</cell><cell>93.7</cell></row><row><cell>Leather</cell><cell>78</cell><cell>75</cell><cell>64</cell><cell>87</cell><cell>97 -</cell><cell>-</cell><cell>97.6</cell></row><row><cell>Tile</cell><cell>59</cell><cell>51</cell><cell>50</cell><cell>93</cell><cell>41 -</cell><cell>-</cell><cell>87.4</cell></row><row><cell>Wood</cell><cell>73</cell><cell>73</cell><cell>62</cell><cell>91</cell><cell>78 -</cell><cell>-</cell><cell>88.5</cell></row><row><cell>Bottle</cell><cell>93</cell><cell>86</cell><cell>86</cell><cell>78</cell><cell>-82</cell><cell>-</cell><cell>98.4</cell></row><row><cell>Cable</cell><cell>82</cell><cell>86</cell><cell>78</cell><cell>79</cell><cell>--</cell><cell>-</cell><cell>97.2</cell></row><row><cell>Capsule</cell><cell>94</cell><cell>88</cell><cell>84</cell><cell>84</cell><cell>-76</cell><cell>-</cell><cell>99.0</cell></row><row><cell>Hazelnut</cell><cell>97</cell><cell>95</cell><cell>87</cell><cell>72</cell><cell>--</cell><cell>-</cell><cell>99.1</cell></row><row><cell>Metal nut</cell><cell>89</cell><cell>86</cell><cell>76</cell><cell>82</cell><cell>-60</cell><cell>-</cell><cell>98.1</cell></row><row><cell>Pill</cell><cell>91</cell><cell>85</cell><cell>87</cell><cell>68</cell><cell>-83</cell><cell>-</cell><cell>96.5</cell></row><row><cell>Screw</cell><cell>96</cell><cell>96</cell><cell>80</cell><cell>87</cell><cell>-94</cell><cell>-</cell><cell>98.9</cell></row><row><cell>Toothbrush</cell><cell>92</cell><cell>93</cell><cell>90</cell><cell>77</cell><cell>68</cell><cell>-</cell><cell>97.9</cell></row><row><cell>Transistor</cell><cell>90</cell><cell>86</cell><cell>80</cell><cell>66</cell><cell>--</cell><cell>-</cell><cell>94.1</cell></row><row><cell>Zipper</cell><cell>88</cell><cell>77</cell><cell>78</cell><cell>76</cell><cell>--</cell><cell>-</cell><cell>96.5</cell></row><row><cell>Average</cell><cell>87</cell><cell>82</cell><cell>74</cell><cell>78</cell><cell>75 77</cell><cell>89</cell><cell>96.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Sub-Image anomaly detection accuracy on MVTec (PRO %) Student 1-NN OC-SVM 2-AE VAE SSIM-AE CNN-Dict SPADE</figDesc><table><row><cell>Carpet</cell><cell cols="2">69.5 51.2</cell><cell>35.5</cell><cell>45.6 50.1</cell><cell>64.7</cell><cell>46.9</cell><cell>94.7</cell></row><row><cell>Grid</cell><cell cols="2">81.9 22.8</cell><cell>12.5</cell><cell>58.2 22.4</cell><cell>84.9</cell><cell>18.3</cell><cell>86.7</cell></row><row><cell>Leather</cell><cell cols="2">81.9 44.6</cell><cell>30.6</cell><cell>81.9 63.5</cell><cell>56.1</cell><cell>64.1</cell><cell>97.2</cell></row><row><cell>Tile</cell><cell cols="2">91.2 82.2</cell><cell>72.2</cell><cell>89.7 87.0</cell><cell>17.5</cell><cell>79.7</cell><cell>75.9</cell></row><row><cell>Wood</cell><cell cols="2">72.5 50.2</cell><cell>33.6</cell><cell>72.7 62.8</cell><cell>60.5</cell><cell>62.1</cell><cell>87.4</cell></row><row><cell>Bottle</cell><cell cols="2">91.8 89.8</cell><cell>85.0</cell><cell>91.0 89.7</cell><cell>83.4</cell><cell>74.2</cell><cell>95.5</cell></row><row><cell>Cable</cell><cell cols="2">86.5 80.6</cell><cell>43.1</cell><cell>82.5 65.4</cell><cell>47.8</cell><cell>55.8</cell><cell>90.9</cell></row><row><cell>Capsule</cell><cell cols="2">91.6 63.1</cell><cell>55.4</cell><cell>86.2 52.6</cell><cell>86.0</cell><cell>30.6</cell><cell>93.7</cell></row><row><cell>Hazelnut</cell><cell cols="2">93.7 86.1</cell><cell>61.6</cell><cell>91.7 87.8</cell><cell>91.6</cell><cell>84.4</cell><cell>95.4</cell></row><row><cell>Metal nut</cell><cell cols="2">89.5 70.5</cell><cell>31.9</cell><cell>83.0 57.6</cell><cell>60.3</cell><cell>35.8</cell><cell>94.4</cell></row><row><cell>Pill</cell><cell cols="2">93.5 72.5</cell><cell>54.4</cell><cell>89.3 76.9</cell><cell>83.0</cell><cell>46.0</cell><cell>94.6</cell></row><row><cell>Screw</cell><cell cols="2">92.8 60.4</cell><cell>64.4</cell><cell>75.4 55.9</cell><cell>88.7</cell><cell>27.7</cell><cell>96.0</cell></row><row><cell cols="3">Toothbrush 86.3 67.5</cell><cell>53.8</cell><cell>82.2 69.3</cell><cell>78.4</cell><cell>15.1</cell><cell>93.5</cell></row><row><cell>Transistor</cell><cell cols="2">70.1 68.0</cell><cell>49.6</cell><cell>72.8 62.6</cell><cell>72.5</cell><cell>62.8</cell><cell>87.4</cell></row><row><cell>Zipper</cell><cell cols="2">93.3 51.2</cell><cell>35.5</cell><cell>83.9 54.9</cell><cell>66.5</cell><cell>70.3</cell><cell>92.6</cell></row><row><cell>Average</cell><cell>85.7</cell><cell>64</cell><cell>47.9</cell><cell>79 63.9</cell><cell>69.4</cell><cell>51.5</cell><cell>91.7</cell></row><row><cell cols="9">Table 4. Image-level anomaly detection accuracy on STC (Average ROCAUC %)</cell></row><row><cell cols="9">TSC [23] StackRNN [23] AE-Conv3D [35] MemAE [12] AE(2D) [16] ITAE [19] SPADE</cell></row><row><cell>67.9</cell><cell>68.0</cell><cell></cell><cell>69.7</cell><cell>71.2</cell><cell>60.9</cell><cell></cell><cell>72.5</cell><cell>71.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Pixel-level anomaly detection accuracy on STC (Average ROCAUC %)</figDesc><table><row><cell cols="4">AEL2 AESSIM CAVGA-Ru [31] SPADE</cell></row><row><cell>74</cell><cell>76</cell><cell>85</cell><cell>89.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Pyramid level ablation for sub-image anomaly detection accuracy on MVTec</figDesc><table><row><cell>(PRO %)</cell><cell></cell></row><row><cell cols="2">Used layers size: (14) (28) (56) SPADE</cell></row><row><cell>Carpet</cell><cell>93.5 93.4 91.0 94.7</cell></row><row><cell>Grid</cell><cell>80.9 88.0 89.1 86.7</cell></row><row><cell>Leather</cell><cell>96.6 97.5 97.3 97.2</cell></row><row><cell>Tile</cell><cell>74.5 65.9 73.8 75.9</cell></row><row><cell>Wood</cell><cell>84.7 87.7 87.5 87.4</cell></row><row><cell>Bottle</cell><cell>93.7 94.7 88.3 95.5</cell></row><row><cell>Cable</cell><cell>89.3 87.3 73.5 90.9</cell></row><row><cell>Capsule</cell><cell>90.5 92.8 91.4 93.7</cell></row><row><cell>Hazelnut</cell><cell>92.7 95.8 96.2 95.4</cell></row><row><cell>Metal nut</cell><cell>91.3 93.1 86.1 94.4</cell></row><row><cell>Pill</cell><cell>89.2 94.4 96.3 94.6</cell></row><row><cell>Screw</cell><cell>90.7 95.9 96.1 96.0</cell></row><row><cell>Toothbrush</cell><cell>90.9 93.5 94.5 93.5</cell></row><row><cell>Transistor</cell><cell>91.3 72.1 62.5 87.4</cell></row><row><cell>Zipper</cell><cell>90.9 92.4 92.5 92.6</cell></row><row><cell>Average</cell><cell>89.38 89.6 87.74 91.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ganomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM ToG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10445</idno>
		<title level="m">Deep nearest neighbor anomaly detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02357</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A geometric framework for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Portnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of data mining in computer security</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The appropriate and effective use of security technologies in US schools: A guide for schools and law enforcement agencies. US Department of Justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Office of Justice Programs</publisher>
		</imprint>
		<respStmt>
			<orgName>National Institute of</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<title level="m">A benchmark for anomaly segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04606</idno>
		<title level="m">Deep anomaly detection with outlier exposure</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inverse-transform autoencoder for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10676</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Outlier detection with kernel density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pokrajac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by cnn-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gornitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>MLSD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention guided anomaly detection and localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahalanobis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08616</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<title level="m">Caltech-ucsd birds 200</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
