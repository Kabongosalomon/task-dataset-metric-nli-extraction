<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Anomaly Detection Using Geometric Transformations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Technion -Israel Institute of Technology Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science Technion -Israel Institute of Technology Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Anomaly Detection Using Geometric Transformations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of anomaly detection in images, and present a new detection technique. Given a sample of images, all known to belong to a "normal" class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects). The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images. The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images. We present extensive experiments using the proposed detector, which indicate that our technique consistently improves all known algorithms by a wide margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Future machine learning applications such as self-driving cars or domestic robots will, inevitably, encounter various kinds of risks including statistical uncertainties. To be usable, these applications should be as robust as possible to such risks. One such risk is exposure to statistical errors or inconsistencies due to distributional divergences or noisy observations. The well-known problem of anomaly/novelty detection highlights some of these risks, and its resolution is of the utmost importance to mission critical machine learning applications. While anomaly detection has long been considered in the literature, conclusive understanding of this problem in the context of deep neural models is sorely lacking. For example, in machine vision applications, presently available novelty detection methods can suffer from poor performance in some problems, as demonstrated by our experiments.</p><p>In the basic anomaly detection problem, we have a sample from a "normal" class of instances, emerging from some distribution, and the goal is to construct a classifier capable of detecting outof-distribution "abnormal" instances <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b0">1</ref> There are quite a few variants of this basic anomaly detection problem. For example, in the positive and unlabeled version, we are given a sample from the "normal" class, as well as an unlabeled sample that is contaminated with abnormal instances. This contaminated-sample variant turns out to be easier than the pure version of the problem (in the sense that better performance can be achieved) <ref type="bibr" target="#b1">[2]</ref>. In the present paper, we focus on the basic (and harder) version of anomaly detection, and consider only machine vision applications for which deep models (e.g., convolutional neural networks) are essential.</p><p>There are a few works that tackle the basic, pure-sample-anomaly detection problem in the context of images. The most successful results among these are reported for methods that rely on one of the following two general schemes. The first scheme consists of methods that analyze errors in reconstruction, which is based either on autoencoders or generative adversarial models (GANs) trained over the normal class. In the former case, reconstruction deficiency of a test point indicates abnormality. In the latter, the reconstruction error of a test instance is estimated using optimization to find the approximate inverse of the generator. The second class of methods utilizes an autoencoder trained over the normal class to generate a low-dimensional embedding. To identify anomalies, one uses classical methods over this embedding, such as low-density rejection <ref type="bibr" target="#b7">[8]</ref> or single-class SVM <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. A more advanced variant of this approach combines these two steps (encoding and then detection) using an appropriate cost function, which is used to train a single neural model that performs both procedures <ref type="bibr" target="#b23">[24]</ref>.</p><p>In this paper we consider a completely different approach that bypasses reconstruction (as in autoencoders or GANs) altogether. The proposed method is based on the observation that learning to discriminate between many types of geometric transformations applied to normal images, encourages learning of features that are useful for detecting novelties. Thus, we train a multi-class neural classifier over a self-labeled dataset, which is created from the normal instances and their transformed versions, obtained by applying numerous geometric transformations. At test time, this discriminative model is applied on transformed instances of the test example, and the distribution of softmax response values of the "normal" train images is used for effective detection of novelties. The intuition behind our method is that by training the classifier to distinguish between transformed images, it must learn salient geometrical features, some of which are likely to be unique to the single class.</p><p>We present extensive experiments of the proposed method and compare it to several state-of-the-art methods for pure anomaly detection. We evaluate performance using a one-vs-all scheme over several image datasets such as CIFAR-100, which (to the best of our knowledge) have never been considered before in this setting. Our results overwhelmingly indicate that the proposed method achieves dramatic improvements over the best available methods. For example, on the CIFAR-10 dataset (10 different experiments), we improved the top performing baseline AUROC by 32% on average. In the CatsVsDogs dataset, we improve the top performing baseline AUROC by 67%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The literature related to anomaly detection is extensive and beyond the scope of this paper (see, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref> for wider scope surveys). Our focus is on anomaly detection in the context of images and deep learning. In this scope, most published works rely, implicitly or explicitly, on some form of (unsupervised) reconstruction learning. These methods can be roughly categorized into two approaches.</p><p>Reconstruction-based anomaly score. These methods assume that anomalies possess different visual attributes than their non-anomalous counterparts, so it will be difficult to compress and reconstruct them based on a reconstruction scheme optimized for single-class data. Motivated by this assumption, the anomaly score for a new sample is given by the quality of the reconstructed image, which is usually measured by the 2 distance between the original and reconstructed image. Classic methods belonging to this category include Principal Component Analysis (PCA) <ref type="bibr" target="#b14">[15]</ref>, and Robust-PCA <ref type="bibr" target="#b3">[4]</ref>. In the context of deep learning, various forms of deep autoencoders are the main tool used for reconstruction-based anomaly scoring. Xia et al. <ref type="bibr" target="#b31">[32]</ref> use a convolutional autoencoder with a regularizing term that encourages outlier samples to have a large reconstruction error. Variational autoencoder is used by An and Cho <ref type="bibr" target="#b0">[1]</ref>, where they estimate the reconstruction probability through Monte-Carlo sampling, from which they extract an anomaly score. Another related method, which scores an unseen sample based on the ability of the model to generate a similar one, uses Generative Adversarial Networks (GANS) <ref type="bibr" target="#b12">[13]</ref>. Schlegl et al. <ref type="bibr" target="#b24">[25]</ref> use this approach on optical coherence tomography images of the retina. Deecke et al. <ref type="bibr" target="#b6">[7]</ref> employ a variation of this model called ADGAN, reporting slightly superior results on CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> and MNIST <ref type="bibr" target="#b18">[19]</ref>.</p><p>Reconstruction-based representation learning. Many conventional anomaly detection methods use a low-density rejection principle <ref type="bibr" target="#b7">[8]</ref>. Given data, the density at each point is estimated, and new samples are deemed anomalous when they lie in a low-density region. Examples of such methods are kernel density estimation (KDE) <ref type="bibr" target="#b21">[22]</ref>, and Robust-KDE <ref type="bibr" target="#b15">[16]</ref>. This approach is known to be problematic when handling high-dimensional data due to the curse of dimensionality. To mitigate this problem, practitioners often use a two-step approach of learning a compact representation of the data, and then applying density estimation methods on the lower-dimensional representation <ref type="bibr" target="#b3">[4]</ref>. More advanced techniques combine these two steps and aim to learn a representation that facilitates the density estimation task. Zhai et al. <ref type="bibr" target="#b35">[36]</ref> utilize an energy-based model in the form of a regularized autoencoder in order to map each sample to an energy score, which is the estimated negative logprobability of the sample under the data distribution. Zong et al. <ref type="bibr">[38]</ref> uses the representation layer of an autoencoder in order to estimate parameters of a Gaussian mixture model.</p><p>There are few approaches that tackled the anomaly detection problem without resorting to some form of reconstruction. A recent example was published by Ruff et al. <ref type="bibr" target="#b23">[24]</ref>, who have developed a deep one-class SVM model. The model consists of a deep neural network whose weights are optimized using a loss function resembling the SVDD <ref type="bibr" target="#b26">[27]</ref> objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>In this paper, we consider the problem of anomaly detection in images. Let X be the space of all "natural" images, and let X ⊆ X be the set of images defined as normal. Given a sample S ⊆ X, and a type-II error constraint (rate of normal samples that were classified as anomalies), we would like to learn the best possible (in terms of type-I error) classifier h S (x) : X → {0, 1}, where h S (x) = 1 ⇔ x ∈ X, which satisfies the constraint. Images that are not in X are referred to as anomalies or novelties.</p><p>To control the trade-off between type-I and type-II errors when classifying, a common practice is to learn a scoring (ranking) function n S (x) : X → R, such that higher scores indicate that samples are more likely to be in X. Once such a scoring function has been learned, a classifier can be constructed from it by specifying an anomaly threshold (λ):</p><formula xml:id="formula_0">h λ S (x) = 1 n S (x) ≥ λ 0 n S (x) &lt; λ.</formula><p>As many related works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14]</ref>, in this paper we also focus only on learning the scoring function n S (x), and completely ignore the constrained binary decision problem. A useful (and common practice) performance metric to measure the quality of the trade-off of a given scoring function is the area under the Receiver Operating Characteristic (ROC) curve, which we denote here as AUROC. When prior knowledge on the proportion of anomalies is available, the area under the precision-recall curve (AUPR) metric might be preferred <ref type="bibr" target="#b5">[6]</ref>. We also report on performance in term of this metric in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discriminative Learning of an Anomaly Scoring Function Using Geometric Transformations</head><p>As noted above, we aim to learn a scoring function n S (as described in Section 3) in a discriminative fashion. To this end, we create a self-labeled dataset of images from our initial training set S, by using a class of geometric transformations T . The created dataset, denoted S T , is generated by applying each geometric transformation in T on all images in S, where we label each transformed image with the index of the transformation that was applied on it. This process creates a self-labeled multi-class dataset (with |T | classes) whose cardinality is |T ||S|. After the creation of S T , we train a multi-class image classifier whose objective is to predict, for each image, the index of its generating transformation in T . At inference time, given an unseen image x, we decide whether it belongs to the normal class by first applying each transformation on it, and then applying the classifier on each of the |T | transformed images. Each such application results in a softmax response vector of size |T |. The final normality score is defined using the combined log-likelihood of these vectors under an estimated distribution of "normal" softmax vectors (see details below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Creating and Learning the Self-Labeled Dataset</head><p>Let T = {T 0 , T 1 , . . . , T k−1 } be a set of geometric transformations, where for each 1 ≤ i ≤ k −1, T i : X → X , and T 0 (x) = x is the identity transformation. The set T is a hyperparameter of our method, on which we elaborate in Section 6. The self-labeled set S T is defined as</p><formula xml:id="formula_1">S T {(T j (x), j) : x ∈ S, T j ∈ T } .</formula><p>Thus, for any x ∈ S, j is the label of T j (x). We use this set to straightforwardly learn a deep k-class classification model, f θ , which we train over the self-labeled dataset S T using the standard cross-entropy loss function. To this end, any useful classification architecture and optimization method can be employed for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dirichlet Normality Score</head><p>We now define our normality score function n S (x). Fix a set of geometric transformations T = {T 0 , T 1 , . . . , T k−1 }, and assume that a k-class classification model f θ has been trained on the selflabeled set S T (as described above). For any image x, let y(x) softmax (f θ (x)), i.e., the vector of softmax responses of the classifier f θ applied on x. To construct our normality score we define:</p><formula xml:id="formula_2">n S (x) k−1 i=0 log p(y(T i (x))|T i ),</formula><p>which is the combined log-likelihood of a transformed image conditioned on each of the applied transformations in T , under a naïve (typically incorrect) assumption that all of these conditional distributions are independent. We approximate each conditional distribution to be y(</p><formula xml:id="formula_3">T i (x))|T i ∼ Dir(α i ), where α i ∈ R k + , x ∼ p X (x), i ∼ Uni(0, k − 1), and p X (x)</formula><p>is the real data probability distribution of "normal" samples. Our choice of the Dirichlet distribution is motivated by two reasons. First, it is a common choice for distribution approximation when samples (i.e., y) reside in the unit k − 1 simplex. Second, there are efficient methods for numerically estimating the maximum likelihood parameters <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>. We denote the estimation byα i . Using the estimated Dirichlet parameters, the normality score of an image x is:</p><formula xml:id="formula_4">n S (x) = k−1 i=0   log Γ( k−1 j=0 [α i ] j ) − k−1 j=0 log Γ([α i ] j ) + k−1 j=0 ([α i ] j − 1) log y(T i (x)) j   .</formula><p>Since allα i are constant w.r.t x, we can ignore the first two terms in the parenthesis and redefine a simplified normality score, which is equivalent in its normality ordering:</p><formula xml:id="formula_5">n S (x) = k−1 i=0 k−1 j=0 ([α i ] j − 1) log y(T i (x)) j = k−1 i=0 (α i − 1) · log y(T i (x)).</formula><p>As demonstrated in our experiments, this score tightly captures normality in the sense that for two images x 1 and x 2 , n S (x 1 ) &gt; n S (x 2 ) tend to imply that x 1 is "more normal" than x 2 . For each i ∈ {0, . . . , k − 1}, we estimateα i using the fixed point iteration method described in <ref type="bibr" target="#b20">[21]</ref>, combined with the initialization step proposed by Wicker et al. <ref type="bibr" target="#b30">[31]</ref>. Each vectorα i is estimated based on the set S i = {y(T i (x))|x ∈ S}. We note that the use of an independent image set for estimatingα i may improve performance. A full and detailed algorithm is available in the supplementary material.</p><p>A simplified version of the proposed normality score was used during preliminary stages of this research:n S (x)</p><formula xml:id="formula_6">1 k k−1 j=0 [y (T j (x))] j .</formula><p>This simple score function eliminates the need for the Dirichlet parameter estimation, is easy to implement, and still achieves excellent results that are only slightly worse than the above Dirichlet score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we describe our experimental setup and evaluation method, the baseline algorithms we use for comparison purposes, the datasets, and the implementation details of our technique (architecture used and geometric transformations applied). We then present extensive experiments on the described publicly available datasets, demonstrating the effectiveness of our scoring function. Finally, we show that our method is also effective at identifying out-of-distribution samples in labeled multi-class datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline Methods</head><p>We compare our method to state-of-the-art deep learning approaches as well as a few classic methods.</p><p>One-Class SVM. The one-class support vector machine (OC-SVM) is a classic and popular kernelbased method for novelty detection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. It is typically employed with an RBF kernel, and learns a collection of closed sets in the input space, containing most of the training samples. Samples residing outside of these enclosures are deemed anomalous. Following <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7]</ref>, we use this model on raw input (i.e., a flattened array of the pixels comprising an image), as well as on a low-dimensional representation obtained by taking the bottleneck layer of a trained convolutional autoencoder. We name these models RAW-OC-SVM and CAE-OC-SVM, respectively. It is very important to note that in both these variants of OC-SVM, we provide the OC-SVM with an unfair significant advantage by optimizing its hyperparameters in hindsight; i.e., the OC-SVM hyperparameters (ν and γ) were optimized to maximize AUROC and taken to be the best performing values among those in the parameter grid: ν ∈ {0.1, 0.2, . . . , 0.9}, γ ∈ {2 −7 , 2 −6 , . . . , 2 2 }. Note that the hyperparameter optimization procedure has been provided with a two-class classification problem. There are, in fact, methods for optimizing these parameters without hindsight knowledge <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref>. These methods are likely to degrade the performance of the OC-SVM models. The convolutional autoencoder is chosen to have a similar architecture to that of DCGAN <ref type="bibr" target="#b22">[23]</ref>, where the encoder is adapted from the discriminator, and the decoder is adapted from the generator.</p><p>In addition, we compare our method to a recently published, end-to-end variant of OC-SVM called One-Class Deep SVDD <ref type="bibr" target="#b23">[24]</ref>. This model, which we name E2E-OC-SVM, uses an objective similar to that of the classic SVDD <ref type="bibr" target="#b26">[27]</ref> to optimize the weights of a deep architecture. However, there are constraints on the used architecture, such as lack of bias terms and unbounded activation functions. The experimental setup used by the authors is identical to ours, allowing us to report their published results as they are, on CIFAR-10.</p><p>Deep structured energy-based models. A deep structured energy-based model (DSEBM) is a state-of-the-art deep neural technique, whose output is the energy function (negative log probability) associated with an input sample <ref type="bibr" target="#b35">[36]</ref>. Such models can be trained efficiently using score matching in a similar way to a denoising autoencoder <ref type="bibr" target="#b28">[29]</ref>. Samples associated with high energy are considered anomalous. While the authors of <ref type="bibr" target="#b35">[36]</ref> used a very shallow architecture in their model (which is ineffective in our problems), we selected a deeper one when using their method. The chosen architecture is the same as that of the encoder part in the convolutional autoencoder used by CAE-OC-SVM, with ReLU activations in the encoding layer.</p><p>Deep Autoencoding Gaussian Mixture Model. A deep autoencoding Gaussian mixture model (DAGMM) is another state-of-the-art deep autoencoder-based model, which generates a lowdimensional representation of the training data, and leverages a Gaussian mixture model to perform density estimation on the compact representation <ref type="bibr">[38]</ref>. A DAGMM jointly and simultaneously optimizes the parameters of the autoencoder and the mixture model in an end-to-end fashion, thus leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The architecture of the autoencoder we used is similar to that of the convolutional autoencoder from the CAE-OC-SVM experiment, but with linear activation in the representation layer. The estimation network is inspired by the one in the original DAGMM paper.</p><p>Anomaly Detection with a Generative Adversarial Network. This network, given the acronym ADGAN, is a GAN based model, which learns a one-way mapping from a low-dimensional multivariate Gaussian distribution to the distribution of the training set <ref type="bibr" target="#b6">[7]</ref>. After training the GAN on the "normal" dataset, the discriminator is discarded. Given a sample, the training of ADGAN uses gradient descent to estimate the inverse mapping from the image to the low-dimensional seed. The seed is then used to generate a sample, and the anomaly score is the 2 distance between that image and the original one. In our experiments, for the generative model of the ADGAN we incorporated the same architecture used by the authors of the original paper, namely, the original DCGAN architecture <ref type="bibr" target="#b22">[23]</ref>. As described, ADGAN requires only a trained generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets</head><p>We consider four image datasets in our experiments: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b17">[18]</ref>, CatsVsDogs <ref type="bibr" target="#b8">[9]</ref>, and fashion-MNIST <ref type="bibr" target="#b32">[33]</ref>, which are described below. We note that in all our experiments, pixel values of all images were scaled to reside in [−1 <ref type="bibr">, 1]</ref>. No other pre-processing was applied.</p><p>• CIFAR-10: consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images, divided equally across the classes.</p><p>• CIFAR-100: similar to CIFAR-10, but with 100 classes containing 600 images each. This set has a fixed train/test partition with 500 training images and 100 test images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses, which we use in our experiments.</p><p>• Fashion-MNIST: a relatively new dataset comprising 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. In order to be compatible with the CIFAR-10 and CIFAR-100 classification architectures, we zero-pad the images so that they are of size 32x32.</p><p>• CatsVsDogs: extracted from the ASIRRA dataset, it contains 25,000 images of cats and dogs, 12,500 in each class. We split this dataset into a training set containing 10,000 images, and a test set of 2,500 images in each class. We also rescale each image to size 64x64. The average dimension size of the original images is roughly 360x400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Protocol</head><p>We employ a one-vs-all evaluation scheme in each experiment. Consider a dataset with C classes, from which we create C different experiments. For each 1 ≤ c ≤ C, we designate class c to be the single class of normal images. We take S to be the set of images in the training set belonging to class c. The set S is considered to be the set of "normal" samples based on which the model must learn a normality score function. We emphasize that S contains only normal samples, and no additional samples are provided to the model during training. The normality score function is then applied on all images in the test set, containing both anomalies (not belonging to class c) and normal samples (belonging to class c), in order to evaluate the model's performance. As stated in Section 3, we completely ignore the problem of choosing the appropriate anomaly threshold (λ) on the normality score, and quantify performance using the area under the ROC curve metric, which is commonly utilized as a performance measure for anomaly detection models. We are able to compute the ROC curve since we have full knowledge of the ground truth labels of the test set. <ref type="bibr" target="#b1">2</ref> Hyperparameters and Optimization Methods For the self-labeled classification task, we use 72 geometric transformations. These transformations are specified in the supplementary material (see also Section 6 discussing the intuition behind the choice of these transformations). Our model is implemented using the state-of-the-art Wide Residual Network (WRN) model <ref type="bibr" target="#b34">[35]</ref>. The parameters for the depth and width of the model for all 32x32 datasets were chosen to be 10 and 4, respectively, and for the CatsVsDogs dataset (64x64), 16 and 8, respectively. These hyperparameters were selected prior to conducting any experiment, and were fixed for all runs. <ref type="bibr" target="#b2">3</ref> We used the Adam <ref type="bibr" target="#b16">[17]</ref> optimizer with default hyperparameters. Batch size for all methods was set to 128. The number of epochs was set to 200 on all benchmark models, except for training the GAN in ADGAN for which it was set to 100 and produced superior results. We trained the WRN for 200/|T | epochs on the self-labeled set S T , to obtain approximately the same number of parameter updates as would have been performed had we trained on S for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>In <ref type="table">Table 1</ref> we present our results. The table is composed of four blocks, with each block containing several anomaly detection problems derived from the same dataset (for lack of space we omit class names from the tables, and those can be found in the supplementary material). For example, the first row contains the results for an anomaly detection problem where the normal class is class 0 in CIFAR-10 (airplane), and the anomalous instances are images from all other classes in CIFAR-10 (classes 1-9). In this row (as in any other row), we see the average AUROC results over five runs and the corresponding standard error of the mean for all baseline methods. The results of our algorithm are shown in the rightmost column. OC-SVM variants and ADGAN were run once due to their time complexity. The best performing method in each row appears in bold. For example, in the CatsVsDogs experiments where dog (class 1) is the "normal" class, the best baseline (DSEBM) achieves 0.561 AUROC. Note that the trivial average AUROC is always 0.5, regardless of the proportion of normal vs. anomalous instances. Our method achieves an average AUROC of 0.888. <ref type="table">Table 1</ref>. Our relative advantage is most prominent when focusing on the larger images. All baseline methods, including OC-SVM variants, which enjoy hindsight information, only achieve performance that is slightly better than random guessing in the CatsVsDogs dataset. On the smaller-sized images, the baselines can perform much better. In most cases, however, our algorithm significantly outperformed the other methods. Interestingly, in many cases where the baseline methods struggled with separating normal samples from anomalies, our method excelled. See, for instance, the cases of automobile (class 1) and horse (class 7; see the CIFAR-10 section in the table). Inspecting the results on CIFAR-100 (where 20 super-classes defined the partition), we observe that our method was challenged by the diversity inside the normal class. In this case, there are a few normal classes on which our method did not perform well; see e.g., non-insect invertebrates (class 13), insects (class 7), and household electrical devices (class 5). In Section 6 we speculate why this might happen. We used the super-class partitioning of CIFAR-100 (instead of the 100 base classes) because labeled data for single base classes is scarce. On the fashion-MNIST dataset, all methods, excluding DAGMM, performed very well, with a slight advantage to our method. The fashion-MNIST dataset was designed as a drop-in replacement for the original MNIST dataset, which is slightly more challenging. Classic models, such as SVM with an RBF kernel, can perform well on this task, achieving almost 90% accuracy <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Several interesting observations can be made by inspecting the numbers in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Identifying Out-of-distribution Samples in Labeled Multi-class Datasets</head><p>Although it is not the main focus of this work, we have also tackled the problem of identifying out-ofdistribution samples in labeled multi-class datasets (i.e., identify images that belong to a different distribution than that of the labeled dataset). To this end, we created a two-headed classification model based on the WRN architecture. The model has two separate softmax output layers. One for categories (e.g., cat, truck, airplane, etc.) and another for classifying transformations (our method). We use the categories softmax layer only during training. At test time, we only utilize the transformations softmax layer output as described in section 4.2, but use the simplified normality score. When training on the CIFAR-10 dataset, and taking the tiny-imagenet (resized) dataset to be anomalies as done by Liang et al. <ref type="bibr" target="#b19">[20]</ref> in their ODIN method, we improved ODIN's AUROC/AUPR-In/AUPR-Out results from 92.1/89.0/93.6 to 95.7/96.1/95.4, respectively. It is important to note that in contrast to our method, ODIN is inapplicable in the pure single class setting, where there are no class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">On the Intuition for Using Geometric Transformations</head><p>In this section we explain our intuition behind the choice of the set of transformations used in our method. Any bijection of a set (having some geometric structure) to itself is a geometric transformation. Among all geometric transformations, we only used compositions of horizontal flipping, translations, and rotations in our model, resulting in 72 distinct transformations (see supplementary material for the entire list). In the earlier stages of this work, we tried a few nongeometric transformations (e.g., Gaussian blur, sharpening, gamma correction), which degraded performance and we abandoned them altogether. We hypothesize that non-geometric transformations perform worse since they can eliminate important features of the learned image set.</p><p>We speculate that the effectiveness of the chosen transformation set is affected by their ability to preserve spatial information about the given "normal" images, as well as the ability of our classifier to predict which transformation was applied on a given transformed image. In addition, for a fixed type-II error rate, the type-I error rate of our method decreases the harder it gets for the trained classifier to correctly predict the identity of the transformations that were applied on anomalies.</p><p>We demonstrate this idea by conducting three experiments. Each experiment has the following structure. We train a neural classifier to discriminate between two transformations, where the normal class is taken to be images of a single digit from the MNIST <ref type="bibr" target="#b18">[19]</ref> training set. We then evaluate our method using AUROC on a set of images comprising normal images and images of another digit from the MNIST test set. The three experiments are:  • Normal digit: '8'. Anomaly: '3'. Transformations: Identity and translation by 7 pixels. In this experiment, the transformed images are distinguishable from each other. As can be expected, our method performs well in this case, achieving an AUROC of 0.919.</p><p>To convince ourselves that high scores given by our scoring function indicate membership in the normal class, we tested how an image would need to change in order to obtain a high normality score. This was implemented by optimizing an input image using gradient ascent to maximize the simplified variant of the normality score described in section 5.5 (see, e.g., <ref type="bibr" target="#b33">[34]</ref>). Thus, we trained a classifier on the digit '3' from the MNIST dataset, with a few geometric transformations. We then took an arbitrary image of the digit '0' and optimized it. In <ref type="figure" target="#fig_1">Figure 1(a)</ref> we present two such images, where the left one is the original, and the right is the result after taking 200 gradient ascent steps that "optimize" the original image. It is evident that the '0' digits have deformed, now resembling the digit '3'. This illustrates the fact that the classification model has learned features relevant to the "normal" class. To further strengthen our hypothesis, we conducted the same experiment using images from the normal class (i.e., images of the digit '3'). We expected these images to maintain their appearance during the optimization process, since they already contain the features that should contribute to a high normality score. <ref type="figure" target="#fig_1">Figure 1(b)</ref> contains two examples of the process, where in each row, the left image is the initial '3', and the right is the result after taking 200 gradient ascent steps on it. As hypothesized, it is evident that the images remained roughly unchanged at the end of the optimization process (regardless of their different orientations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We presented a novel method for anomaly detection of images, which learns a meaningful representation of the learned training data in a fully discriminative fashion. The proposed method is computationally efficient, and as simple to implement as a multi-class classification task. Unlike best-known methods so far, our approach completely alleviates the need for a generative component (autoencoders/GANs). Most importantly, our method significantly advances the state-of-the-art by offering a dramatic improvement over the best available anomaly detection methods. Our results open many avenues for future research. First, it is important to develop a theory that grounds the use of geometric transformations. It would be interesting to study the possibility of selecting transformations that would best serve a given training set, possibly with prior knowledge on the anomalous samples. Another avenue is explicitly optimizing the set of transformations. Due to the effectiveness of our method, it is tempting to try to adapt it to other applications, for example, open-world scenario, and maybe even use it to improve multi-class classification performance. Finally, it would be interesting to attempt using our techniques to leverage deep uncertainty estimation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>, and deep active learning <ref type="bibr" target="#b9">[10]</ref>. <ref type="table">Table 1</ref>: Average area under the ROC curve in % with SEM (over 5 runs) of anomaly detection methods. For all datasets, each model was trained on the single class, and tested against all other classes. E2E column is taken from <ref type="bibr" target="#b23">[24]</ref>. OC-SVM hyperparameters in RAW and CAE variants were optimized with hindsight knowledge. The best performing method in each experiment is in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A List of Geometric Transformations Used By Our Method</head><p>In all our experiments, except for those described in Section 6 we used a fixed set of 72 geometric transformations. These transformations can be succinctly described as the composition of the following transformations, applied on an image in the order they are listed:</p><p>• The entire set of transformations is thus given by</p><formula xml:id="formula_7">T =    T rot k • T trans s h ,sw • T f lip b : b ∈ {T, F }, s h , s w ∈ {−1, 0, 1}, k ∈ {0, 1, 2, 3}   </formula><p>By taking all possible compositions, we obtain a total of 2 × 3 × 3 × 4 = 72 transformations, where each composition is fully defined by a tuple, (b, s w , s h , k). For example, the identity transformation is (F, 0, 0, 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm</head><p>We present here a full and detailed algorithm of our deep anomaly detection technique. The function Ψ(·) is the Digamma function, and its inverse is calculated numerically using five Newton-Raphson iterations. while not converged do When dealing with highly skewed datasets, precision-recall curves may be considered more informative than the area under the ROC. For completeness, we provide results of all baseline models in terms of area under the precision-recall curve. This score can be calculated in two ways: anomalies treated as the positive class (AUPR-out), and anomalies treated as the negative class (AUPR-in).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Deep Anomaly Detection Using Geometric Transformations</head><formula xml:id="formula_8">13:α i ← Ψ −1 Ψ j [α i ] j +l</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Left: original '0's. Right: the '0's optimized to a normality score learned on '3'.(b) Left: original '3's. Right: the '3's optimized to a normality score learned on '3'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Optimizing digit images to maximize the normality score • Normal digit: '8'. Anomaly: '3'. Transformations: Identity and horizontal flip. It can be expected that due to the invariance of '8' to horizontal flip, the classifier will have difficulties learning distinguishing features. Indeed, when presented with the test set containing '3' as anomalies (which do not exhibit such invariance), our method did not perform well, achieving an AUROC of 0.646.• Normal digit: '3'. Anomaly: '8'. Transformations: Identity and horizontal flip. In contrast to the previous experiment, the transformed variants of digit '3' can easily be classified to the correct transformation. Indeed, our method, using the trained model for '3', achieved 0.957 AUROC in this experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Horizontal flip: denoted as T f lip b (x), where b ∈ {T, F }. The parameter b indicates whether the flipping occurs, or the transformation is the identity. • Translation: denoted as T trans s h ,sw (x), where s h , s w ∈ {−1, 0, 1}. Applying this transformation on an image translates it by 0.25 of its height, and 0.25 of its width, in both dimensions. The direction of the translation in each axis is determined by s h and s w , where s h = s w = 0 means no translation. A reflection is used to complete missing pixels. • Rotation by multiples 90 degrees: denoted as T rot k (x), where k ∈ {0, 1, 2, 3}. Applying this transformation on an image rotates it counter-clockwise by k × 90 degrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :not converged do 4 : 7 : 1 n s∈Si s 10 :l ← 1 n</head><label>3471101</label><figDesc>Input: S: a set of "normal" images. T = {T 0 , T 1 , . . . , T k−1 }: a set of geometric transformations. f θ : a softmax classifier parametrized by θ. Output: A normality scoring function n S (x). 1: procedure GETNORMALITYSCORE(S, T , f θ ) 2: S T ← {(T j (x), j) : x ∈ S, T j ∈ T } while Train f θ on the labeled set S T for i ∈ {0, . . . , k − 1} do 8:S i ← {y(T i (x))|x ∈ S} 9:s ← s∈Si log s 11:α i ←s (k−1)(−Ψ(1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>[38] B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, and H. Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning Representations, 2018.</figDesc><table><row><cell></cell><cell>c i</cell><cell cols="2">OC-SVM RAW CAE</cell><cell>E2E</cell><cell cols="2">DAGMM DSEBM</cell><cell>AD-GAN</cell><cell>OURS</cell></row><row><cell></cell><cell>0</cell><cell>70.6</cell><cell cols="5">74.9 61.7±1.3 41.4±2.3 56.0±6.9 64.9 74.7±0.4</cell></row><row><cell></cell><cell>1</cell><cell>51.3</cell><cell cols="5">51.7 65.9±0.7 57.1±2.0 48.3±1.8 39.0 95.7±0.0</cell></row><row><cell></cell><cell>2</cell><cell>69.1</cell><cell cols="5">68.9 50.8±0.3 53.8±4.0 61.9±0.1 65.2 78.1±0.4</cell></row><row><cell></cell><cell>3</cell><cell>52.4</cell><cell cols="5">52.8 59.1±0.4 51.2±0.8 50.1±0.4 48.1 72.4±0.5</cell></row><row><cell>CIFAR-10 (32x32x3)</cell><cell>4 5 6</cell><cell>77.3 51.2 74.1</cell><cell cols="5">76.7 60.9±0.3 52.2±7.3 73.3±0.2 73.5 87.8±0.2 52.9 65.7±0.8 49.3±3.6 60.5±0.3 47.6 87.8±0.1 70.9 67.7±0.8 64.9±1.7 68.4±0.3 62.3 83.4±0.5</cell></row><row><cell></cell><cell>7</cell><cell>52.6</cell><cell cols="5">53.1 67.3±0.3 55.3±0.8 53.3±0.7 48.7 95.5±0.1</cell></row><row><cell></cell><cell>8</cell><cell>70.9</cell><cell cols="5">71.0 75.9±0.4 51.9±2.4 73.9±0.3 66.0 93.3±0.0</cell></row><row><cell></cell><cell>9</cell><cell>50.6</cell><cell cols="5">50.6 73.1±0.4 54.2±5.8 63.6±3.1 37.8 91.3±0.1</cell></row><row><cell></cell><cell cols="2">avg 62.0</cell><cell>62.4</cell><cell>64.8</cell><cell>53.1</cell><cell>60.9</cell><cell>55.3</cell><cell>86.0</cell></row><row><cell></cell><cell>0</cell><cell>68.0</cell><cell>68.4</cell><cell>-</cell><cell cols="3">43.4±3.9 64.0±0.2 63.1 74.7±0.4</cell></row><row><cell></cell><cell>1</cell><cell>63.1</cell><cell>63.6</cell><cell>-</cell><cell cols="3">49.5±2.7 47.9±0.1 54.9 68.5±0.2</cell></row><row><cell></cell><cell>2</cell><cell>50.4</cell><cell>52.0</cell><cell>-</cell><cell cols="3">66.1±1.7 53.7±4.1 41.3 74.0±0.5</cell></row><row><cell></cell><cell>3</cell><cell>62.7</cell><cell>64.7</cell><cell>-</cell><cell cols="3">52.6±1.0 48.4±0.5 50.0 81.0±0.8</cell></row><row><cell></cell><cell>4</cell><cell>59.7</cell><cell>58.2</cell><cell>-</cell><cell cols="3">56.9±3.0 59.7±6.3 40.6 78.4±0.5</cell></row><row><cell></cell><cell>5</cell><cell>53.5</cell><cell>54.9</cell><cell>-</cell><cell cols="3">52.4±2.2 46.6±1.6 42.8 59.1±1.0</cell></row><row><cell></cell><cell>6</cell><cell>55.9</cell><cell>57.2</cell><cell>-</cell><cell cols="3">55.0±1.1 51.7±0.8 51.1 81.8±0.2</cell></row><row><cell></cell><cell>7</cell><cell>64.4</cell><cell>62.9</cell><cell>-</cell><cell cols="3">52.8±3.7 54.8±1.6 55.4 65.0±0.1</cell></row><row><cell></cell><cell>8</cell><cell>66.7</cell><cell>65.6</cell><cell>-</cell><cell cols="3">53.2±4.8 66.7±0.2 59.2 85.5±0.4</cell></row><row><cell>CIFAR-100 (32x32x3)</cell><cell>9 10 11</cell><cell>70.1 83.0 59.7</cell><cell>74.1 84.1 58.0</cell><cell>---</cell><cell cols="3">42.5±2.5 71.2±1.2 62.7 90.6±0.1 52.7±3.9 78.3±1.1 79.8 87.6±0.2 46.4±2.4 62.7±0.7 53.7 83.9±0.6</cell></row><row><cell></cell><cell>12</cell><cell>68.7</cell><cell>68.5</cell><cell>-</cell><cell cols="3">42.7±3.1 66.8±0.0 58.9 83.2±0.3</cell></row><row><cell></cell><cell>13</cell><cell>65.0</cell><cell>64.6</cell><cell>-</cell><cell cols="3">45.4±0.7 52.6±0.1 57.4 58.0±0.4</cell></row><row><cell></cell><cell>14</cell><cell>50.7</cell><cell>51.2</cell><cell>-</cell><cell cols="3">57.2±1.3 44.0±0.6 39.4 92.1±0.2</cell></row><row><cell></cell><cell>15</cell><cell>63.5</cell><cell>62.8</cell><cell>-</cell><cell cols="3">48.8±1.5 56.8±0.1 55.6 68.3±0.1</cell></row><row><cell></cell><cell>16</cell><cell>68.3</cell><cell>66.6</cell><cell>-</cell><cell cols="3">54.4±3.1 63.1±0.1 63.3 73.5±0.2</cell></row><row><cell></cell><cell>17</cell><cell>71.7</cell><cell>73.7</cell><cell>-</cell><cell cols="3">36.4±2.3 73.0±1.0 66.7 93.8±0.1</cell></row><row><cell></cell><cell>18</cell><cell>50.2</cell><cell>52.8</cell><cell>-</cell><cell cols="3">52.4±1.4 57.7±1.6 44.3 90.7±0.1</cell></row><row><cell></cell><cell>19</cell><cell>57.5</cell><cell>58.4</cell><cell>-</cell><cell cols="3">50.3±1.0 55.5±0.7 53.0 85.0±0.2</cell></row><row><cell></cell><cell cols="2">avg 62.6</cell><cell>63.1</cell><cell>-</cell><cell>50.5</cell><cell>58.8</cell><cell>54.7</cell><cell>78.7</cell></row><row><cell></cell><cell>0</cell><cell>98.2</cell><cell>97.7</cell><cell>-</cell><cell cols="3">42.1±9.1 91.6±1.2 89.9 99.4±0.0</cell></row><row><cell></cell><cell>1</cell><cell>90.3</cell><cell>89.9</cell><cell>-</cell><cell cols="3">55.1±3.5 71.8±0.5 81.9 97.6±0.1</cell></row><row><cell></cell><cell>2</cell><cell>90.7</cell><cell>91.4</cell><cell>-</cell><cell cols="3">50.4±7.3 88.3±0.2 87.6 91.1±0.2</cell></row><row><cell></cell><cell>3</cell><cell>94.2</cell><cell>90.7</cell><cell>-</cell><cell cols="3">57.0±6.7 87.3±3.6 91.2 89.9±0.4</cell></row><row><cell>Fashion-</cell><cell>4</cell><cell>89.4</cell><cell>89.1</cell><cell>-</cell><cell cols="3">26.9±5.4 85.2±0.9 86.5 92.1±0.0</cell></row><row><cell>MNIST</cell><cell>5</cell><cell>91.8</cell><cell>88.5</cell><cell>-</cell><cell cols="3">70.5±9.7 87.1±0.0 89.6 93.4±0.9</cell></row><row><cell>(32x32x1)</cell><cell>6</cell><cell>83.4</cell><cell>81.7</cell><cell>-</cell><cell cols="3">48.3±5.0 73.4±4.1 74.3 83.3±0.1</cell></row><row><cell></cell><cell>7</cell><cell>98.8</cell><cell>98.7</cell><cell>-</cell><cell cols="3">83.5±11.4 98.1±0.0 97.2 98.9±0.1</cell></row><row><cell></cell><cell>8</cell><cell>91.9</cell><cell>90.6</cell><cell>-</cell><cell cols="3">49.9±7.2 86.0±3.2 89.0 90.8±0.1</cell></row><row><cell></cell><cell>9</cell><cell>99.0</cell><cell>98.6</cell><cell>-</cell><cell cols="3">34.0±3.0 97.1±0.3 97.1 99.2±0.0</cell></row><row><cell></cell><cell cols="2">avg 92.8</cell><cell>91.7</cell><cell>-</cell><cell>51.8</cell><cell>86.6</cell><cell>88.4</cell><cell>93.5</cell></row><row><cell>CatsVsDogs (64x64x3)</cell><cell cols="2">0 1 avg 51.7 50.4 53.0</cell><cell>55.2 49.9 52.5</cell><cell>---</cell><cell cols="3">43.4±0.5 47.1±1.7 50.7 88.3±0.3 52.0±1.9 56.1±1.2 48.1 89.2±0.3 47.7 51.6 49.4 88.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Thefollowing table describes the content of all single classes.</figDesc><table><row><cell cols="5">C Single Class Names D Examples from the Fashion-MNIST Dataset</cell></row><row><cell></cell><cell></cell><cell>Dataset</cell><cell>c i</cell><cell>Single Class Name</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>Airplane</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>Automobile</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>Bird</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>Cat</cell></row><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell>4 5</cell><cell>Deer Dog</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6</cell><cell>Frog</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Horse</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>Ship</cell></row><row><cell></cell><cell></cell><cell></cell><cell>9</cell><cell>Truck</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>Aquatic mammals</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>Fish</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>Flowers</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>Food containers</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>Fruit and vegetables</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>Household electrical devices</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6</cell><cell>Household furniture</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Insects</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>Large carnivores</cell></row><row><cell></cell><cell></cell><cell>CIFAR-100</cell><cell cols="2">9 Large man-made outdoor things 10 Large natural outdoor scenes</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">11 Large omnivores and herbivores</cell></row><row><cell></cell><cell></cell><cell></cell><cell>12</cell><cell>Medium-sized mammals</cell></row><row><cell></cell><cell></cell><cell></cell><cell>13</cell><cell>Non-insect invertebrates</cell></row><row><cell></cell><cell></cell><cell></cell><cell>14</cell><cell>People</cell></row><row><cell></cell><cell></cell><cell></cell><cell>15</cell><cell>Reptiles</cell></row><row><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>Small mammals</cell></row><row><cell></cell><cell></cell><cell></cell><cell>17</cell><cell>Trees</cell></row><row><cell cols="5">18 19 E Area Under the Precision-Recall Curve</cell><cell>Vehicles 1 Vehicles 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>Ankle-boot</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>Bag</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>Coat</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>Dress</cell></row><row><cell></cell><cell cols="2">Fashion-MNIST</cell><cell>4 5</cell><cell>Pullover Sandal</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6</cell><cell>Shirt</cell></row><row><cell></cell><cell></cell><cell></cell><cell>7</cell><cell>Sneaker</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>T-shirt</cell><cell>Fixed point method from [21]</cell></row><row><cell>14:</cell><cell>end while</cell><cell></cell><cell>9</cell><cell>Trouser</cell></row><row><cell cols="2">15: 16: 17: end procedure end for return n S (x)</cell><cell cols="3">k−1 i=0 (α i − 1) · log y(T i (x)) 0 CatsVsDogs 1</cell><cell>Cat Dog</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average area under the PR curve in % with SEM (computed over 5 runs) of anomaly detection methods, when anomalies are taken as the negative class (AUPR-In). For all datasets, each model was trained on the single class, and tested against all other classes. The best performing method in each experiment is in bold.</figDesc><table><row><cell>Dataset</cell><cell>c i</cell><cell cols="2">OC-SVM RAW CAE</cell><cell cols="2">DAGMM DSEBM</cell><cell>AD-GAN</cell><cell>OURS</cell></row><row><cell></cell><cell>0</cell><cell>25.6</cell><cell>33.2</cell><cell>8.3±0.5</cell><cell cols="3">14.4±2.8 17.9 28.9±0.6</cell></row><row><cell></cell><cell>1</cell><cell>56.2</cell><cell>56.5</cell><cell>12.5±0.8</cell><cell>9.3±0.5</cell><cell>8.2</cell><cell>78.7±0.3</cell></row><row><cell></cell><cell>2</cell><cell>23.0</cell><cell>24.0</cell><cell cols="4">12.5±1.1 18.5±0.1 18.0 33.1±1.4</cell></row><row><cell></cell><cell>3</cell><cell>11.2</cell><cell>12.1</cell><cell>10.2±0.2</cell><cell>9.7±0.1</cell><cell>9.0</cell><cell>33.2±0.8</cell></row><row><cell>CIFAR-10 (32x32x3)</cell><cell>4 5 6</cell><cell>27.9 9.7 20.0</cell><cell>29.2 12.2 21.7</cell><cell cols="4">12.5±2.2 23.0±0.2 21.5 42.9±0.3 9.9±0.9 12.7±0.4 9.1 52.9±0.5 17.2±1.3 17.6±0.0 13.6 48.8±2.1</cell></row><row><cell></cell><cell>7</cell><cell>11.6</cell><cell>12.4</cell><cell cols="2">11.4±0.3 10.7±0.7</cell><cell>9.0</cell><cell>81.3±0.2</cell></row><row><cell></cell><cell>8</cell><cell>25.6</cell><cell>25.7</cell><cell cols="4">13.7±1.7 27.1±1.3 16.5 68.4±0.3</cell></row><row><cell></cell><cell>9</cell><cell>40.9</cell><cell>48.5</cell><cell cols="2">12.4±2.3 17.4±2.5</cell><cell>7.3</cell><cell>58.6±0.4</cell></row><row><cell></cell><cell cols="2">avg 25.2</cell><cell>27.6</cell><cell>12.1</cell><cell>16.1</cell><cell>13.0</cell><cell>52.7</cell></row><row><cell></cell><cell>0</cell><cell>11.7</cell><cell>14.1</cell><cell>4.5±0.5</cell><cell>8.9±0.1</cell><cell>7.1</cell><cell>14.0±0.3</cell></row><row><cell></cell><cell>1</cell><cell>12.9</cell><cell>14.9</cell><cell>5.8±0.9</cell><cell>5.1±0.0</cell><cell>5.8</cell><cell>12.5±0.3</cell></row><row><cell></cell><cell>2</cell><cell>4.7</cell><cell>4.9</cell><cell>9.6±1.0</cell><cell>5.9±1.1</cell><cell>4.2</cell><cell>16.0±0.7</cell></row><row><cell></cell><cell>3</cell><cell>15.4</cell><cell>21.5</cell><cell>5.5±0.3</cell><cell>5.0±0.1</cell><cell>5.1</cell><cell>41.2±1.0</cell></row><row><cell></cell><cell>4</cell><cell>10.8</cell><cell>17.9</cell><cell>6.5±1.1</cell><cell>9.7±2.4</cell><cell>3.8</cell><cell>30.5±1.2</cell></row><row><cell></cell><cell>5</cell><cell>7.1</cell><cell>11.2</cell><cell>5.7±0.5</cell><cell>4.6±0.2</cell><cell>4.1</cell><cell>8.8±0.3</cell></row><row><cell></cell><cell>6</cell><cell>6.2</cell><cell>7.5</cell><cell>6.3±0.5</cell><cell>5.2±0.1</cell><cell>5.1</cell><cell>36.1±1.0</cell></row><row><cell></cell><cell>7</cell><cell>9.0</cell><cell>13.0</cell><cell>5.8±0.7</cell><cell>7.6±0.7</cell><cell>6.7</cell><cell>9.3±0.1</cell></row><row><cell></cell><cell>8</cell><cell>7.8</cell><cell>7.8</cell><cell>5.9±0.9</cell><cell>7.9±0.1</cell><cell>5.7</cell><cell>36.2±0.9</cell></row><row><cell>CIFAR-100 (32x32x3)</cell><cell>9 10 11</cell><cell>9.4 28.1 6.3</cell><cell>11.9 32.1 6.0</cell><cell>4.0±0.2 6.3±0.7 4.6±0.3</cell><cell cols="3">10.6±0.9 27.8±1.2 19.5 41.0±0.8 6.7 39.8±0.5 7.0±0.2 5.3 32.1±1.4</cell></row><row><cell></cell><cell>12</cell><cell>9.4</cell><cell>10.3</cell><cell>4.1±0.3</cell><cell>8.4±0.0</cell><cell>6.6</cell><cell>32.0±0.7</cell></row><row><cell></cell><cell>13</cell><cell>12.6</cell><cell>15.2</cell><cell>4.6±0.2</cell><cell>6.3±0.1</cell><cell>9.1</cell><cell>6.7±0.1</cell></row><row><cell></cell><cell>14</cell><cell>15.2</cell><cell>53.6</cell><cell>6.1±0.4</cell><cell>4.1±0.1</cell><cell>3.7</cell><cell>63.6±0.5</cell></row><row><cell></cell><cell>15</cell><cell>8.9</cell><cell>10.3</cell><cell>5.0±0.3</cell><cell>7.5±0.1</cell><cell>6.4</cell><cell>10.4±0.1</cell></row><row><cell></cell><cell>16</cell><cell>12.8</cell><cell>15.6</cell><cell>6.1±0.5</cell><cell>8.1±0.0</cell><cell>8.3</cell><cell>13.4±0.2</cell></row><row><cell></cell><cell>17</cell><cell>10.0</cell><cell>12.1</cell><cell>3.7±0.1</cell><cell>14.3±2.9</cell><cell>7.3</cell><cell>66.4±0.7</cell></row><row><cell></cell><cell>18</cell><cell>4.7</cell><cell>5.1</cell><cell>5.4±0.3</cell><cell>6.4±0.4</cell><cell>3.8</cell><cell>44.8±0.3</cell></row><row><cell></cell><cell>19</cell><cell>7.3</cell><cell>10.6</cell><cell>5.1±0.3</cell><cell>5.7±0.2</cell><cell>5.2</cell><cell>26.7±0.3</cell></row><row><cell></cell><cell cols="2">avg 10.5</cell><cell>14.8</cell><cell>5.5</cell><cell>8.3</cell><cell>6.5</cell><cell>29.1</cell></row><row><cell></cell><cell>0</cell><cell>86.1</cell><cell>84.7</cell><cell>9.5±3.5</cell><cell cols="3">55.3±7.6 41.7 95.1±0.4</cell></row><row><cell></cell><cell>1</cell><cell>62.2</cell><cell>75.3</cell><cell cols="4">12.5±2.5 22.2±1.7 27.2 91.8±0.5</cell></row><row><cell></cell><cell>2</cell><cell>53.4</cell><cell>58.7</cell><cell cols="4">11.7±4.0 41.3±0.5 39.7 46.2±0.8</cell></row><row><cell></cell><cell>3</cell><cell>70.4</cell><cell>67.8</cell><cell cols="4">13.9±4.9 47.6±7.8 60.1 53.8±1.8</cell></row><row><cell>Fashion-</cell><cell>4</cell><cell>53.4</cell><cell>51.4</cell><cell>6.0±0.2</cell><cell cols="3">35.8±0.8 38.8 54.1±0.4</cell></row><row><cell>MNIST</cell><cell>5</cell><cell>58.7</cell><cell cols="5">60.1 39.2±11.0 35.8±0.1 51.8 63.0±4.5</cell></row><row><cell>(32x32x1)</cell><cell>6</cell><cell>35.3</cell><cell>42.4</cell><cell cols="4">18.9±9.1 23.3±2.4 25.7 30.3±0.7</cell></row><row><cell></cell><cell>7</cell><cell>91.3</cell><cell cols="5">92.2 44.5±10.2 86.9±0.5 80.3 87.5±1.8</cell></row><row><cell></cell><cell>8</cell><cell>67.0</cell><cell>69.4</cell><cell>9.6±1.7</cell><cell cols="3">49.8±7.6 55.4 53.9±0.8</cell></row><row><cell></cell><cell>9</cell><cell>96.1</cell><cell>95.6</cell><cell>5.4±0.1</cell><cell cols="3">88.8±1.3 90.5 97.1±0.1</cell></row><row><cell></cell><cell cols="2">avg 67.4</cell><cell>69.8</cell><cell>17.1</cell><cell>48.7</cell><cell>51.1</cell><cell>67.3</cell></row><row><cell>CatsVsDogs (64x64x3)</cell><cell cols="2">0 1 avg 51.3 50.5 52.1</cell><cell>54.6 40.7 47.6</cell><cell cols="4">45.4±0.3 49.0±1.3 51.0 90.6±0.3 56.7±4.7 54.0±1.0 48.6 91.1±0.2 51.1 51.5 49.8 90.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average area under the PR curve in % with SEM (computed over 5 runs) of anomaly detection methods, when anomalies are taken as the positive class (AUPR-Out). For all datasets, each model was trained on the single class, and tested against all other classes. The best performing method in each experiment is in bold. 4±0.3 96.4±0.7 93.9 98.2±0.1 5 97.3 95.3 95.1±0.3 94.2±0.2 93.9 95.9±0.1 6 95.9 96.3 96.1±0.2 95.4±0.2 95.0 98.5±0.0 7 96.9 96.7 95.4±0.5 95.1±0.3 95.6 96.9±0.0 8 97.4 97.3 95.6±0.6 97.4±0.0 96.8 99.0±0.0 9 97.8 98.1 94.5±0.4 97.8±0.1 97.1 99.4±0.0 10 98.8 98.9 95.0±0.4 98.3±0.1 98.6 99.1±0.0 11 96.7 96.5 94.6±0.3 97.0±0.1 95.9 98.8±0.0</figDesc><table><row><cell>Dataset</cell><cell>c i</cell><cell cols="2">OC-SVM RAW CAE</cell><cell cols="2">DAGMM DSEBM</cell><cell>AD-GAN</cell><cell>OURS</cell></row><row><cell></cell><cell>0</cell><cell>95.2</cell><cell cols="4">95.7 88.0±0.9 91.3±1.6 93.4 95.4±0.1</cell></row><row><cell></cell><cell>1</cell><cell>95.1</cell><cell cols="4">95.2 91.9±0.6 89.7±0.4 86.3 99.5±0.0</cell></row><row><cell></cell><cell>2</cell><cell>94.8</cell><cell cols="4">95.2 90.4±1.3 92.1±0.0 94.0 96.4±0.1</cell></row><row><cell></cell><cell>3</cell><cell>91.6</cell><cell cols="4">91.0 90.6±0.3 90.3±0.1 90.2 95.0±0.1</cell></row><row><cell>CIFAR-10 (32x32x3)</cell><cell>4 5 6</cell><cell>96.4 90.8 96.0</cell><cell cols="4">96.4 90.2±2.1 95.6±0.0 96.0 98.2±0.0 91.2 90.1±1.0 93.2±0.1 90.0 98.2±0.0 95.3 93.6±0.4 94.2±0.1 93.2 97.2±0.1</cell></row><row><cell></cell><cell>7</cell><cell>91.9</cell><cell cols="4">92.0 91.9±0.2 91.4±0.1 90.4 99.4±0.0</cell></row><row><cell></cell><cell>8</cell><cell>95.0</cell><cell cols="4">95.1 89.4±0.7 95.2±0.1 94.3 99.1±0.0</cell></row><row><cell></cell><cell>9</cell><cell>95.0</cell><cell cols="4">95.1 91.8±1.4 93.4±0.6 87.3 98.8±0.0</cell></row><row><cell></cell><cell cols="2">avg 94.2</cell><cell>94.2</cell><cell>90.8</cell><cell>92.6</cell><cell>91.5</cell><cell>97.7</cell></row><row><cell></cell><cell>0</cell><cell>97.3</cell><cell cols="4">97.3 93.9±0.6 96.9±0.0 97.0 98.0±0.0</cell></row><row><cell></cell><cell>1</cell><cell>96.9</cell><cell cols="4">96.8 94.7±0.5 94.8±0.0 95.9 97.4±0.0</cell></row><row><cell></cell><cell>2</cell><cell>95.4</cell><cell cols="4">95.7 97.3±0.1 95.8±0.5 93.8 97.8±0.0</cell></row><row><cell></cell><cell>3</cell><cell>96.8</cell><cell cols="4">96.7 95.2±0.1 94.4±0.1 94.7 98.2±0.1</cell></row><row><cell>CIFAR-100 (32x32x3)</cell><cell cols="6">4 96.0 96.12 96.3 97.4 97.4 94.2±0.5 97.4±0.0 96.3 98.7±0.0</cell></row><row><cell></cell><cell>13</cell><cell>97.0</cell><cell cols="4">96.7 94.2±0.2 94.9±0.0 95.4 96.1±0.0</cell></row><row><cell></cell><cell>14</cell><cell>97.5</cell><cell cols="4">97.6 96.2±0.1 94.7±0.1 94.0 99.4±0.0</cell></row><row><cell></cell><cell>15</cell><cell>97.0</cell><cell cols="4">96.9 94.5±0.2 95.4±0.0 95.6 97.4±0.0</cell></row><row><cell></cell><cell>16</cell><cell>97.4</cell><cell cols="4">97.1 95.5±0.4 96.8±0.0 96.8 97.9±0.0</cell></row><row><cell></cell><cell>17</cell><cell>97.9</cell><cell cols="4">98.1 93.9±0.5 97.9±0.1 97.6 99.6±0.0</cell></row><row><cell></cell><cell>18</cell><cell>95.5</cell><cell cols="4">95.8 95.7±0.2 96.3±0.1 94.8 99.4±0.0</cell></row><row><cell></cell><cell>19</cell><cell>96.5</cell><cell cols="4">96.5 95.2±0.2 95.7±0.1 95.7 98.9±0.0</cell></row><row><cell></cell><cell cols="2">avg 97.0</cell><cell>96.9</cell><cell>95.2</cell><cell>96.1</cell><cell>95.7</cell><cell>98.2</cell></row><row><cell></cell><cell>0</cell><cell>99.8</cell><cell cols="4">99.7 91.3±2.1 98.9±0.1 98.9 99.8±0.0</cell></row><row><cell></cell><cell>1</cell><cell>98.6</cell><cell cols="4">98.2 94.0±0.4 95.6±0.1 97.4 99.6±0.0</cell></row><row><cell></cell><cell>2</cell><cell>98.8</cell><cell cols="4">98.8 92.9±1.4 98.5±0.1 98.4 99.0±0.0</cell></row><row><cell></cell><cell>3</cell><cell>99.3</cell><cell cols="4">98.7 95.0±0.7 98.4±0.5 98.8 98.7±0.0</cell></row><row><cell>Fashion-</cell><cell>4</cell><cell>98.6</cell><cell cols="4">98.6 85.4±2.4 98.0±0.1 98.2 99.1±0.0</cell></row><row><cell>MNIST</cell><cell>5</cell><cell>99.0</cell><cell cols="4">98.4 96.6±1.1 98.4±0.0 98.8 99.2±0.1</cell></row><row><cell>(32x32x1)</cell><cell>6</cell><cell>97.8</cell><cell cols="4">97.3 91.2±1.7 95.9±0.9 96.0 97.9±0.0</cell></row><row><cell></cell><cell>7</cell><cell>99.9</cell><cell cols="4">99.8 98.0±1.4 99.8±0.0 99.7 99.9±0.0</cell></row><row><cell></cell><cell>8</cell><cell>98.8</cell><cell cols="4">98.5 92.2±1.4 98.0±0.5 98.5 98.9±0.0</cell></row><row><cell></cell><cell>9</cell><cell>99.9</cell><cell cols="4">99.8 91.1±0.8 99.6±0.0 99.6 99.9±0.0</cell></row><row><cell></cell><cell cols="2">avg 99.0</cell><cell>98.8</cell><cell>92.8</cell><cell>98.1</cell><cell>98.4</cell><cell>99.2</cell></row><row><cell>CatsVsDogs (64x64x3)</cell><cell cols="2">0 1 avg 55.3 57.2 53.3</cell><cell cols="4">53.5 46.3±0.4 47.3±1.1 50.0 83.3±0.4 74.9 58.2±4.8 55.8±1.1 48.6 85.4±0.5 64.2 52.2 51.5 49.3 84.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Unless otherwise mentioned, the use of the adjective "normal" is unrelated to the Gaussian distribution. 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada. arXiv:1805.10917v2 [cs.LG] 9 Nov 2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A complete code of the proposed method's implementation and the conducted experiments is available at https://github.com/izikgo/AnomalyDetectionTransformations.<ref type="bibr" target="#b2">3</ref> The parameters 16, 8 were used on CIFAR-10 by the authors. Due to the induced computational complexity, we chose smaller values. When testing the parameters 16, 8 with our method on the CIFAR-10 dataset, anomaly detection results improved.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the Israel Science Foundation (grant No. 81/017).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability. SNU Data Mining Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2973" to="3009" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">International Society for Optics and Photonics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smolyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Machine Vision (ICMV 2015)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9875</biblScope>
			<biblScope unit="page">987525</biblScope>
		</imprint>
	</monogr>
	<note>Model selection for anomaly detection</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1EfylZ0Z" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal single-class classification strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nisenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asirra: A captcha that exploits interest-aligned manual image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Douceur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th ACM Conference on Computer and Communications Security (CCS)</title>
		<meeting>14th ACM Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery, Inc</publisher>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep active learning over the long tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.00941" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selective classification for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4878" to="4887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boosting uncertainty estimation for deep neural classifiers. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.08206" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view anomaly detection via robust probabilistic latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1136" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principal component analysis and factor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="115" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust kernel density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2529" to="2565" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1VGkIxRZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Estimating a dirichlet distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4390" to="4399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anomaly detection in automobile control network data with long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science and Advanced Analytics (DSAA), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="130" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hyperparameter selection of one-class support vector machine by self-adaptive data shifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="198" to="211" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A maximum likelihood approximation method for dirichlet&apos;s parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K R</forename><surname>Kalathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational statistics &amp; data analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1511" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep structured energy based models for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
	<note>ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in high-dimensional numerical data. Statistical Analysis and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The ASA Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
