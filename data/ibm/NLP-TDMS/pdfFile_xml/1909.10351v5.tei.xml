<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TinyBERT: Distilling BERT for Natural Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Information Storage System</orgName>
								<orgName type="department" key="dep2">Wuhan National Laboratory for Optoelectronics</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<email>jiang.xin@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
							<email>chen.xiao2@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Technologies Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
							<email>wangfang@hust.edu.cnyinyichun</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Information Storage System</orgName>
								<orgName type="department" key="dep2">Wuhan National Laboratory for Optoelectronics</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TinyBERT: Distilling BERT for Natural Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resourcerestricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large "teacher" BERT can be effectively transferred to a small "student" Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT.</p><p>TinyBERT 4 1 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT 4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them. Moreover, TinyBERT 6 with 6 layers performs on-par with its teacher BERT BASE .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>natural language processing (NLP). Pre-trained language models (PLMs), such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, XLNet , RoBERTa , <ref type="bibr">ALBERT (Lan et al., 2020)</ref>, T5 <ref type="bibr" target="#b15">(Raffel et al., 2019)</ref> and ELECTRA <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>, have achieved great success in many NLP tasks (e.g., the GLUE benchmark <ref type="bibr" target="#b29">(Wang et al., 2018)</ref> and the challenging multi-hop reasoning task <ref type="bibr" target="#b8">(Ding et al., 2019)</ref>). However, PLMs usually have a large number of parameters and take long inference time, which are difficult to be deployed on edge devices such as mobile phones. Recent studies <ref type="bibr">(Kovaleva et al., 2019;</ref><ref type="bibr" target="#b13">Michel et al., 2019;</ref><ref type="bibr" target="#b28">Voita et al., 2019)</ref> demonstrate that there is redundancy in PLMs. Therefore, it is crucial and feasible to reduce the computational overhead and model storage of PLMs while retaining their performances.</p><p>There have been many model compression techniques <ref type="bibr">(Han et al., 2016)</ref> proposed to accelerate deep model inference and reduce model size while maintaining accuracy. The most commonly used techniques include quantization <ref type="bibr">(Gong et al., 2014)</ref>, weights pruning <ref type="bibr">(Han et al., 2015)</ref>, and knowledge distillation (KD) <ref type="bibr" target="#b18">(Romero et al., 2014)</ref>. In this paper, we focus on knowledge distillation, an idea originated from <ref type="bibr">Hinton et al. (2015)</ref>, in a teacher-student framework. KD aims to transfer the knowledge embedded in a large teacher network to a small student network where the student network is trained to reproduce the behaviors of the teacher network. Based on the framework, we propose a novel distillation method specifically for the Transformer-based models <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>, and use BERT as an example to investigate the method for large-scale PLMs.</p><p>KD has been extensively studied in NLP <ref type="bibr">(Kim and Rush, 2016;</ref><ref type="bibr">Hu et al., 2018)</ref> as well as for pre-trained language models <ref type="bibr" target="#b20">(Sanh et al., 2019;</ref><ref type="bibr" target="#b23">Sun et al., 2019</ref><ref type="bibr" target="#b24">Sun et al., , 2020</ref>. The pre-training-then-fine-tuning paradigm firstly pre-  trains BERT on a large-scale unsupervised text corpus, then fine-tunes it on task-specific dataset, which greatly increases the difficulty of BERT distillation. Therefore, it is required to design an effective KD strategy for both training stages.</p><p>To build a competitive TinyBERT, we firstly propose a new Transformer distillation method to distill the knowledge embedded in teacher BERT. Specifically, we design three types of loss functions to fit different representations from BERT layers: 1) the output of the embedding layer; 2) the hidden states and attention matrices derived from the Transformer layer; 3) the logits output by the prediction layer. The attention based fitting is inspired by the recent findings <ref type="bibr" target="#b3">(Clark et al., 2019)</ref> that the attention weights learned by BERT can capture substantial linguistic knowledge, and it thus encourages the linguistic knowledge can be well transferred from teacher BERT to student Tiny-BERT. Then, we propose a novel two-stage learning framework including the general distillation and the task-specific distillation, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. At general distillation stage, the original BERT without fine-tuning acts as the teacher model. The student TinyBERT mimics the teacher's behavior through the proposed Transformer distillation on general-domain corpus. After that, we obtain a general TinyBERT that is used as the initialization of student model for the further distillation. At the task-specific distillation stage, we first do the data augmentation, then perform the distillation on the augmented dataset using the fine-tuned BERT as the teacher model. It should be pointed out that both the two stages are essential to improve the performance and generalization capability of TinyBERT.</p><p>The main contributions of this work are as follows: 1) We propose a new Transformer distillation method to encourage that the linguistic knowledge encoded in teacher BERT can be adequately transferred to TinyBERT; 2) We propose a novel two-stage learning framework with performing the proposed Transformer distillation at both the pretraining and fine-tuning stages, which ensures that TinyBERT can absorb both the general-domain and task-specific knowledge of the teacher BERT. 3) We show in the experiments that our TinyBERT 4 can achieve more than 96.8% the performance of teacher BERT BASE on GLUE tasks, while having much fewer parameters (∼13.3%) and less inference time (∼10.6%), and significantly outperforms other state-of-the-art baselines with 4 layers on BERT distillation; 4) We also show that a 6-layer TinyBERT 6 can perform on-par with the teacher BERT BASE on GLUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we describe the formulation of Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> and Knowledge Distillation <ref type="bibr">(Hinton et al., 2015)</ref>. Our proposed Transformer distillation is a specially designed KD method for Transformer-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer Layer</head><p>Most of the recent pre-trained language models (e.g., BERT, XLNet and RoBERTa) are built with Transformer layers, which can capture longterm dependencies between input tokens by selfattention mechanism. Specifically, a standard Transformer layer includes two main sub-layers: multi-head attention (MHA) and fully connected feed-forward network (FFN). Multi-Head Attention (MHA). The calculation of attention function depends on the three components of queries, keys and values, denoted as matrices Q, K and V respectively. The attention function can be formulated as follows:</p><formula xml:id="formula_0">A = QK T √ d k ,<label>(1)</label></formula><formula xml:id="formula_1">Attention(Q, K, V ) = softmax(A)V ,<label>(2)</label></formula><p>where d k is the dimension of keys and acts as a scaling factor, A is the attention matrix calculated from the compatibility of Q and K by dot-product operation. The final function output is calculated as a weighted sum of values V , and the weight is computed by applying softmax() operation on the each column of matrix A. According to <ref type="bibr" target="#b3">Clark et al. (2019)</ref>, the attention matrices in BERT can capture substantial linguistic knowledge, and thus play an essential role in our proposed distillation method.</p><p>Multi-head attention is defined by concatenating the attention heads from different representation subspaces as follows:</p><formula xml:id="formula_2">MHA(Q, K, V ) = Concat(h 1 , . . . , h k )W , (3)</formula><p>where k is the number of attention heads, and h i denotes the i-th attention head, which is calculated by the Attention() function with inputs from different representation subspaces. The matrix W acts as a linear transformation. Position-wise Feed-Forward Network (FFN). Transformer layer also contains a fully connected feed-forward network, which is formulated as follows:</p><formula xml:id="formula_3">FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 . (4)</formula><p>We can see that the FFN contains two linear transformations and one ReLU activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>KD aims to transfer the knowledge of a large teacher network T to a small student network S. The student network is trained to mimic the behaviors of teacher networks. Let f T and f S represent the behavior functions of teacher and student networks, respectively. The behavior function targets at transforming network inputs to some informative representations, and it can be defined as the output of any layer in the network. In the context of Transformer distillation, the output of MHA layer or FFN layer, or some intermediate representations (such as the attention matrix A) can be used as behavior function. Formally, KD can be modeled as minimizing the following objective function:</p><formula xml:id="formula_4">L KD = x∈X L f S (x), f T (x) ,<label>(5)</label></formula><p>where L(·) is a loss function that evaluates the difference between teacher and student networks, x is the text input and X denotes the training dataset. Thus the key research problem becomes how to define effective behavior functions and loss functions. Different from previous KD methods, we also need to consider how to perform KD at the pre-training stage of BERT in addition to the task-specific training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we propose a novel distillation method for Transformer-based models, and present a two-stage learning framework for our model distilled from BERT, which is called TinyBERT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer Distillation</head><p>The proposed Transformer distillation is a specially designed KD method for Transformer networks. In this work, both the student and teacher networks are built with Transformer layers. For a clear illustration, we formulate the problem before introducing our method. Problem Formulation. Assuming that the student model has M Transformer layers and teacher model has N Transformer layers, we start with choosing M out of N layers from the teacher model for the Transformer-layer distillation. Then a function n = g(m) is defined as the mapping function between indices from student layers to teacher layers, which means that the m-th layer of student model learns the information from the g(m)-th layer of teacher model. To be precise, we set 0 to be the index of embedding layer and M + 1 to be the index of prediction layer, and the corresponding layer mappings are defined as 0 = g(0) and N + 1 = g(M + 1) respectively. The effect of the choice of different mapping functions on the performances is studied in the experiment section. Formally, the student can acquire knowledge from the teacher by minimizing the following objective:</p><formula xml:id="formula_5">L model = x∈X M +1 m=0 λ m L layer (f S m (x), f T g(m) (x)),<label>(6)</label></formula><p>where L layer refers to the loss function of a given model layer (e.g., Transformer layer or embedding layer), f m (x) denotes the behavior function induced from the m-th layers and λ m is the hyper-parameter that represents the importance of the m-th layer's distillation.</p><p>Transformer-layer Distillation. The proposed Transformer-layer distillation includes the attention based distillation and hidden states based distillation, which is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The attention based distillation is motivated by the recent findings that attention weights learned by BERT can capture rich linguistic knowledge <ref type="bibr" target="#b3">(Clark et al., 2019)</ref>. This kind of linguistic knowledge includes the syntax and coreference information, which is essential for natural language understanding. Thus we propose the attention based distillation to encourage that the linguistic knowledge can be transferred from teacher (BERT) to student (TinyBERT). Specifically, the student learns to fit the matrices of multi-head attention in the teacher network, and the objective is defined as:</p><formula xml:id="formula_6">L attn = 1 h h i=1 MSE(A S i , A T i ),<label>(7)</label></formula><p>where h is the number of attention heads, A i ∈ R l×l refers to the attention matrix corresponding to the i-th head of teacher or student, l is the input text length, and MSE() means the mean squared error loss function. In this work, the (unnormalized) attention matrix A i is used as the fitting target instead of its softmax output softmax(A i ), since our experiments show that the former setting has a faster convergence rate and better performances. In addition to the attention based distillation, we also distill the knowledge from the output of Transformer layer, and the objective is as follows:</p><formula xml:id="formula_7">L hidn = MSE(H S W h , H T ),<label>(8)</label></formula><p>where the matrices H S ∈ R l×d and H T ∈ R l×d refer to the hidden states of student and teacher networks respectively, which are calculated by Equation 4. The scalar values d and d denote the hidden sizes of teacher and student models, and d is often smaller than d to obtain a smaller student network. The matrix W h ∈ R d ×d is a learnable linear transformation, which transforms the hidden states of student network into the same space as the teacher network's states.</p><p>Embedding-layer Distillation. Similar to the hidden states based distillation, we also perform embedding-layer distillation and the objective is:</p><formula xml:id="formula_8">L embd = MSE(E S W e , E T ),<label>(9)</label></formula><p>where the matrices E S and H T refer to the embeddings of student and teacher networks, respectively. In this paper, they have the same shape as the hidden state matrices. The matrix W e is a linear transformation playing a similar role as W h . Prediction-layer Distillation. In addition to imitating the behaviors of intermediate layers, we also use the knowledge distillation to fit the predictions of teacher model as in <ref type="bibr">Hinton et al. (2015)</ref>. Specifically, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits:</p><formula xml:id="formula_9">L pred = CE(z T /t, z S /t),<label>(10)</label></formula><p>where z S and z T are the logits vectors predicted by the student and teacher respectively, CE means the cross entropy loss, and t means the temperature value. In our experiment, we find that t = 1 performs well.</p><p>Using the above distillation objectives (i.e. Equations 7, 8, 9 and 10), we can unify the distillation loss of the corresponding layers between the teacher and the student network:</p><formula xml:id="formula_10">L layer =      L embd , m = 0 L hidn +L attn , M ≥m &gt; 0 L pred , m = M + 1 (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TinyBERT Learning</head><p>The application of BERT usually consists of two learning stages: the pre-training and fine-tuning. The plenty of knowledge learned by BERT in the pre-training stage is of great importance and should be transferred to the compressed model. Therefore, we propose a novel two-stage learning framework including the general distillation and the task-specific distillation, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. General distillation helps TinyBERT learn the rich knowledge embedded in pre-trained BERT, which plays an important role in improving the generalization capability of TinyBERT. The task-specific distillation further teaches TinyBERT the knowledge from the fine-tuned BERT. With the two-step distillation, we can substantially reduce the gap between teacher and student models. General Distillation. We use the original BERT without fine-tuning as the teacher and a large-scale text corpus as the training data. By performing the Transformer distillation 2 on the text from general Algorithm 1 Data Augmentation Procedure for Task-specific Distillation</p><p>Input: x is a sequence of words Params: pt: the threshold probability Na: the number of samples augmented per example K: the size of candidate set Output: D : the augmented data 1:</p><formula xml:id="formula_11">n ← 0 ; D ← [ ] 2: while n &lt; Na do 3: xm ← x 4: for i ←1 to len(x) do 5: if x[i] is a single-piece word then 6: Replace xm[i] with [MASK] 7: C ← K most probable words of BERT(xm)[i] 8:</formula><p>else 9:</p><p>C ← K most similar words of x[i] from GloVe 10:</p><p>end if 11:</p><p>Sample p ∼ Uniform(0, 1) 12:</p><p>if p ≤ pt then 13:</p><p>Replace xm[i] with a word in C randomly 14:</p><p>end if 15:</p><p>end for 16:</p><p>Append xm to D 17:</p><p>n ← n + 1 18: end while 19: return D domain, we obtain a general TinyBERT that can be fine-tuned for downstream tasks. However, due to the significant reductions of the hidden/embedding size and the layer number, general TinyBERT performs generally worse than BERT.</p><p>Task-specific Distillation. Previous studies show that the complex models, such as finetuned BERTs, suffer from over-parametrization for domain-specific tasks (Kovaleva et al., 2019). Thus, it is possible for smaller models to achieve comparable performances to the BERTs. To this end, we propose to produce competitive fine-tuned Tiny-BERTs through the task-specific distillation. In the task-specific distillation, we re-perform the proposed Transformer distillation on an augmented task-specific dataset. Specifically, the fine-tuned BERT is used as the teacher and a data augmentation method is proposed to expand the task-specific training set. Training with more task-related examples, the generalization ability of the student model can be further improved.</p><p>Data Augmentation. We combine a pre-trained language model BERT and GloVe <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref> word embeddings to do word-level the TinyBERT primarily learn the intermediate structures of BERT at pre-training stage. From our preliminary experiments, we also found that conducting prediction-layer distillation at pre-training stage does not bring extra improvements on downstream tasks, when the Transformer-layer distillation (Attn and Hidn distillation) and Embedding-layer distillation have already been performed. replacement for data augmentation. Specifically, we use the language model to predict word replacements for single-piece words <ref type="bibr" target="#b33">(Wu et al., 2019)</ref>, and use the word embeddings to retrieve the most similar words as word replacements for multiple-pieces words 3 . Some hyper-parameters are defined to control the replacement ratio of a sentence and the amount of augmented dataset. More details of the data augmentation procedure are shown in Algorithm 1. We set p t = 0.4, N a = 20, K = 15 for all our experiments.</p><p>The above two learning stages are complementary to each other: the general distillation provides a good initialization for the task-specific distillation, while the task-specific distillation on the augmented data further improves TinyBERT by focusing on learning the task-specific knowledge. Although there is a significant reduction of model size, with the data augmentation and by performing the proposed Transformer distillation method at both the pre-training and fine-tuning stages, TinyBERT can achieve competitive performances in various NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the effectiveness and efficiency of TinyBERT on a variety of tasks with different model settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate TinyBERT on the General Language Understanding Evaluation (GLUE) <ref type="bibr" target="#b29">(Wang et al., 2018)</ref> benchmark, which consists of 2 singlesentence tasks: CoLA <ref type="bibr" target="#b31">(Warstadt et al., 2019)</ref>, SST-2 <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>, 3 sentence similarity tasks: <ref type="bibr">MRPC (Dolan and Brockett, 2005)</ref>, STS-B <ref type="bibr" target="#b1">(Cer et al., 2017)</ref>, QQP <ref type="bibr" target="#b2">(Chen et al., 2018)</ref>, and 4 natural language inference tasks: MNLI <ref type="bibr" target="#b32">(Williams et al., 2018)</ref>, QNLI <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>, RTE <ref type="bibr" target="#b0">(Bentivogli et al., 2009) and</ref><ref type="bibr">WNLI (Levesque et al., 2012)</ref>. The metrics for these tasks can be found in the GLUE paper <ref type="bibr" target="#b29">(Wang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TinyBERT Settings</head><p>We instantiate a tiny student model (the number of layers M =4, the hidden size d =312, the feedforward/filter size d i =1200 and the head number h=12) that has a total of 14.5M parameters. This model is referred to as TinyBERT 4 . The original </p><formula xml:id="formula_12">M =4, d=312, d i =1200), BERT SMALL is (M =4, d=512, d i =2048), BERT 4 -PKD and DistilBERT 4 is (M =4, d=768, d i =3072</formula><p>) and the architecture of BERT 6 -PKD, DistilBERT 6 and TinyBERT 6 is (M =6, d=768, d i =3072). All models are learned in a single-task manner. The inference speedup is evaluated on a single NVIDIA K80 GPU. † denotes that the comparison between MobileBERT TINY and TinyBERT 4 may not be fair since the former has 24 layers and is task-agnosticly distilled from IB-BERT LARGE while the later is a 4-layers model task-specifically distilled from BERT BASE .</p><p>BERT BASE (N =12, d=768, d i =3072 and h=12) is used as the teacher model that contains 109M parameters. We use g(m) = 3 × m as the layer mapping function, so TinyBERT 4 learns from every 3 layers of BERT BASE . The learning weight λ of each layer is set to 1. Besides, for a direct comparisons with baselines, we also instantiate a TinyBERT 6 (M =6, d =768, d i =3072 and h=12) with the same architecture as BERT 6 -PKD <ref type="bibr" target="#b23">(Sun et al., 2019)</ref> and DistilBERT 6 <ref type="bibr" target="#b20">(Sanh et al., 2019)</ref>.</p><p>TinyBERT learning includes the general distillation and the task-specific distillation. For the general distillation, we set the maximum sequence length to 128 and use English Wikipedia (2,500M words) as the text corpus and perform the intermediate layer distillation for 3 epochs with the supervision from a pre-trained BERT BASE and keep other hyper-parameters the same as BERT pretraining <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>. For the task-specific distillation, under the supervision of a fine-tuned BERT, we firstly perform intermediate layer distillation on the augmented data for 20 epochs 4 with batch size 32 and learning rate 5e-5, and then perform prediction layer distillation on the augmented data 5 for 3 epochs with choosing the batch size from {16, 32} and learning rate from {1e-5, 2e-5, 3e-5} on dev set. At task-specific distillation, the maximum sequence length is set to 64 for singlesentence tasks, and 128 for sequence pair tasks. <ref type="bibr">4</ref> For large datasets MNLI, QQP, and QNLI, we only perform 10 epochs of the intermediate layer distillation, and for the challenging task CoLA, we perform 50 epochs at this step. <ref type="bibr">5</ref> For regression task STS-B, the original train set is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare TinyBERT with BERT TINY , BERT SMALL 6 <ref type="bibr" target="#b26">(Turc et al., 2019)</ref> and several state-of-the-art KD baselines including BERT-PKD <ref type="bibr" target="#b23">(Sun et al., 2019)</ref>, PD <ref type="bibr" target="#b26">(Turc et al., 2019)</ref>, DistilBERT <ref type="bibr" target="#b20">(Sanh et al., 2019)</ref> and Mobile-BERT <ref type="bibr" target="#b24">(Sun et al., 2020)</ref>. BERT TINY means directly pretraining a small BERT, which has the same model architecture as TinyBERT 4 . When training BERT TINY , we follow the same learning strategy as described in the original BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>. To make a fair comparison, we use the released code to train a 4-layer BERT 4 -PKD 7 and a 4-layer DistilBERT 4 8 and fine-tuning these 4-layer baselines with suggested hyper-paramters. For 6-layer baselines, we use the reported numbers or evaluate the results on the test set of GLUE with released models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results on GLUE</head><p>We submitted our model predictions to the official GLUE evaluation server to obtain results on the test set 9 , as summarized in <ref type="table">Table 1</ref>.</p><p>The experiment results from the 4-layer student models demonstrate that: 1) There is a large performance gap between BERT TINY (or BERT SMALL ) and BERT BASE due to the dramatic reduction in model size. 2) TinyBERT 4 is consistently better than BERT TINY on all the GLUE tasks and obtains a large improvement of 6.8% on average. This indicates that the proposed KD learning framework can effectively improve the performances of small models on a variety of downstream tasks. 3) TinyBERT 4 significantly outperforms the 4-layer state-of-the-art KD baselines (i.e., BERT 4 -PKD and DistilBERT 4 ) by a margin of at least 4.4%, with ∼28% parameters and 3.1x inference speedup. 4) Compared with the teacher BERT BASE , TinyBERT 4 is 7.5x smaller and 9.4x faster in the model efficiency, while maintaining competitive performances. 5) For the challenging CoLA dataset (the task of predicting linguistic acceptability judgments), all the 4-layer distilled models have big performance gaps compared to the teacher model, while TinyBERT 4 achieves a significant improvement over the 4-layer baselines. 6) We also compare TinyBERT with the 24layer MobileBERT TINY , which is distilled from 24-layer IB-BERT LARGE . The results show that TinyBERT 4 achieves the same average score as the 24-layer model with only 38.7% FLOPs. 7) When we increase the capacity of our model to TinyBERT 6 , its performance can be further elevated and outperforms the baselines of the same architecture by a margin of 2.6% on average and achieves comparable results with the teacher. 8) Compared with the other two-stage baseline PD, which first pre-trains a small BERT, then performs distillation on a specific task with this small model, TinyBERT initialize the student in task-specific stage via general distillation. We analyze these two initialization methods in Appendix C.</p><p>In addition, BERT-PKD and DistilBERT initialize their student models with some layers of a pretrained BERT, which makes the student models have to keep the same size settings of Transformer layer (or embedding layer) as their teacher. In our two-stage distillation framework, TinyBERT is initialized through general distillation, making it more flexible in choosing model configuration.</p><p>More Comparisons. We demonstrate the effectiveness of TinyBERT by including more baselines such as Poor Man's BERT <ref type="bibr" target="#b19">(Sajjad et al., 2020)</ref>, BERT-of-Theseus <ref type="bibr">(Xu et al., 2020)</ref> and MiniLM , some of which only report results on the GLUE dev set. In addition, we evaluate TinyBERT on SQuAD v1.1 and v2.0. Due to the space limit, we present our results in the Appendix A and B.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>In this section, we conduct ablation studies to investigate the contributions of : a) different procedures of the proposed two-stage TinyBERT learning framework in <ref type="figure" target="#fig_0">Figure 1, and b</ref>) different distillation objectives in Equation 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Effects of Learning Procedure</head><p>The proposed two-stage TinyBERT learning framework consists of three key procedures: GD (General Distillation), TD (Task-specific Distillation) and DA (Data Augmentation). The performances of removing each individual learning procedure are analyzed and presented in <ref type="table" target="#tab_3">Table 2</ref>. The results indicate that all of the three procedures are crucial for the proposed method. The TD and DA has comparable effects in all the four tasks. We note that the task-specific procedures (TD and DA) are more helpful than the pre-training procedure (GD) on all of the tasks. Another interesting observation is that GD contribute more on CoLA than on MNLI and MRPC. We conjecture that the ability of linguistic generalization <ref type="bibr" target="#b31">(Warstadt et al., 2019)</ref> learned by GD plays an important role in the task of linguistic acceptability judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effects of Distillation Objective</head><p>We investigate the effects of distillation objectives on the TinyBERT learning. Several baselines are proposed including the learning without the Transformer-layer distillation (w/o Trm), the embedding-layer distillation (w/o Emb) or the prediction-layer distillation (w/o Pred) 10 respectively. The results are illustrated in <ref type="table" target="#tab_4">Table 3</ref> and show that all the proposed distillation objectives are useful. The performance w/o Trm 11 drops significantly from 75.6 to 56.3. The reason for the significant drop lies in the initialization of student model. At the pre-training stage, obtaining a good initialization is crucial for the distillation of transformer-based models, while there is no supervision signal from upper layers to update the parameters of transformer layers at this stage under the w/o Trm setting. Furthermore, we study the contributions of attention (Attn) and hidden states (Hidn) in the Transformer-layer distillation. We can find the attention based distillation has a greater impact than hidden states based distillation. Meanwhile, these two kinds of knowledge distillation are complementary to each other, which makes them the most important distillation techniques for Transformer-based model in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effects of Mapping Function</head><p>We also investigate the effects of different mapping functions n = g(m) on the TinyBERT learning. Our original TinyBERT as described in section 4.2 uses the uniform strategy, and we compare with two typical baselines including top-strategy The comparison results are presented in <ref type="table" target="#tab_6">Table 4</ref>. We find that the top-strategy performs better than the bottom-strategy on MNLI, while being worse on MRPC and CoLA, which confirms the observations that different tasks depend on the knowledge from different BERT layers. The uniform strategy covers the knowledge from bottom to top layers of BERT BASE , and it achieves better performances than the other two baselines in all the tasks. Adaptively choosing layers for a specific task is a challenging problem and we leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Pre-trained Language Models Compression Generally, pre-trained language models (PLMs) can be compressed by low-rank approximation <ref type="bibr">(Ma 10</ref> The prediction-layer distillation performs soft crossentropy as Equation 10 on the augmented data. "w/o Pred" means performing standard cross-entropy against the groundtruth of original train set. <ref type="bibr">11</ref> Under "w/o Trm" setting, we actually 1) conduct embedding-layer distillation at the pre-training stage; 2) perform embedding-layer and prediction-layer distillation at finetuning stage.   <ref type="bibr">hghani et al., 2019;</ref><ref type="bibr">Lan et al., 2020)</ref>, knowledge distillation <ref type="bibr" target="#b20">Sanh et al., 2019;</ref><ref type="bibr" target="#b26">Turc et al., 2019;</ref><ref type="bibr" target="#b24">Sun et al., 2020;</ref>, pruning <ref type="bibr" target="#b5">(Cui et al., 2019;</ref><ref type="bibr">Mc-Carley, 2019;</ref><ref type="bibr">Elbayad et al., 2020;</ref><ref type="bibr">Gordon et al., 2020;</ref><ref type="bibr">Hou et al., 2020)</ref> or quantization <ref type="bibr" target="#b21">(Shen et al., 2019;</ref><ref type="bibr" target="#b36">Zafrir et al., 2019)</ref>. In this paper, our focus is on knowledge distillation. Knowledge Distillation for PLMs There have been some works trying to distill pre-trained language models (PLMs) into smaller models. BiLSTM SOFT    <ref type="bibr" target="#b4">(Clark et al., 2020)</ref> proposes a sample-efficient task called replaced token detection to accelerate pre-training, and it also presents a 12-layer ELECTRA small that has comparable performance with TinyBERT 4 . Different from these small PLMs, TinyBERT 4 is a 4-layer model which can achieve more speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we introduced a new method for Transformer-based distillation, and further proposed a two-stage framework for TinyBERT. Extensive experiments show that TinyBERT achieves competitive performances meanwhile significantly reducing the model size and inference time of BERT BASE , which provides an effective way to deploy BERT-based NLP models on edge devices. In future work, we would study how to effectively transfer the knowledge from wider and deeper teachers (e.g., BERT LARGE ) to student TinyBERT.</p><p>Combining distillation with quantization/pruning would be another promising direction to further compress the pre-trained language models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results on SQuAD v1.1 and v2.0</head><p>We also demonstrate the effectiveness of Tiny-BERT on the question answering (QA) tasks: SQuAD v1.1 <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> and SQuAD v2.0 <ref type="bibr" target="#b16">(Rajpurkar et al., 2018)</ref>. Following the learning procedure in the previous work <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, we treat these two tasks as the problem of sequence labeling which predicts the possibility of each token as the start or end of answer span. One small difference from the GLUE tasks is that we perform the prediction-layer distillation on the original training dataset instead of the augmented dataset, which can bring better performances.</p><p>The results show that TinyBERT consistently outperforms both the 4-layer and 6-layer baselines, which indicates that the proposed framework also works for the tasks of token-level labeling. Compared with sequence-level GLUE tasks, the question answering tasks depend on more subtle knowledge to infer the correct answer, which increases the difficulty of knowledge distillation. We leave how to build a better QA-TinyBERT as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Initializing TinyBERT with BERT TINY</head><p>In the proposed two-stage learning framework, to make TinyBERT effectively work for different downstream tasks, we propose the General Distillation (GD) to capture the general domain knowledge, through which the TinyBERT learns the knowledge   from intermediate layers of teacher BERT at the pre-training stage. After that, a general TinyBERT is obtained and used as the initialization of student model for Task-specific Distillation (TD) on downstream tasks. In our preliminary experiments, we have also tried to initialize TinyBERT with the directly pretrained BERT TINY , and then conduct the TD on downstream tasks. We denote this compression method as BERT TINY (+TD). The results in <ref type="table" target="#tab_12">Table 7</ref> show that BERT TINY (+TD) performs even worse than BERT TINY on MRPC and CoLA tasks. We conjecture that if without imitating the BERT BASE 's behaviors at the pre-training stage, BERT TINY will derive mismatched distributions in intermediate representations (e.g., attention matrices and hidden states) with the BERT BASE model. The following task-specific distillation under the supervision of fine-tuned BERT BASE will further disturb the learned distribution/knowledge of BERT TINY , finally leading to poor performances on some less-data tasks. For the intensive-data task (e.g. MNLI), TD has enough training data to make BERT TINY acquire the task-specific knowledge very well, although the pre-trained distributions have already been disturbed.</p><p>From the results of <ref type="table" target="#tab_12">Table 7</ref>, we find that GD can effectively transfer the knowledge from the teacher BERT to the student TinyBERT and achieve comparable results with BERT TINY (61.1 vs. 63.9), even without performing the MLM and NSP tasks. Furthermore, the task-specific distillation boosts the performances of TinyBERT by continuing on learning the task-specific knowledge from fine-tuned teacher BERT BASE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D GLUE Details</head><p>The GLUE datasets are described as follows: MNLI. Multi-Genre Natural Language Inference is a large-scale, crowd-sourced entailment classification task <ref type="bibr" target="#b32">(Williams et al., 2018)</ref>. Given a pair of premise, hypothesis , the goal is to predict whether the hypothesis is an entailment, contradiction, or neutral with respect to the premise. QQP. Quora Question Pairs is a collection of question pairs from the website Quora. The task is to determine whether two questions are semantically equivalent <ref type="bibr" target="#b2">(Chen et al., 2018)</ref>. QNLI. Question Natural Language Inference is a version of the Stanford Question Answering Dataset which has been converted to a binary sentence pair classification task by <ref type="bibr" target="#b29">Wang et al. (2018)</ref>. Given a pair question, context . The task is to determine whether the context contains the answer to the question. SST-2. The Stanford Sentiment Treebank is a binary single-sentence classification task, where the goal is to predict the sentiment of movie reviews <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>.</p><p>CoLA. The Corpus of Linguistic Acceptability is a task to predict whether an English sentence is a grammatically correct one <ref type="bibr" target="#b31">(Warstadt et al., 2019)</ref>.</p><p>STS-B. The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and many other domains <ref type="bibr" target="#b1">(Cer et al., 2017)</ref>. The task aims to evaluate how similar two pieces of texts are by a score from 1 to 5. MRPC. Microsoft Research Paraphrase Corpus is a paraphrase identification dataset where systems aim to identify if two sentences are paraphrases of each other <ref type="bibr">(Dolan and Brockett, 2005)</ref>. RTE. Recognizing Textual Entailment is a binary entailment task with a small training dataset <ref type="bibr" target="#b0">(Bentivogli et al., 2009</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The illustration of TinyBERT learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The details of Transformer-layer distillation consisting of Attn loss (attention based distillation) and Hidn loss (hidden states based distillation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(g(m) = m + N − M ; 0 &lt; m ≤ M ) and bottomstrategy (g(m) = m; 0 &lt; m ≤ M ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1909.10351v5 [cs.CL] 16 Oct 2020</figDesc><table><row><cell></cell><cell>General</cell><cell></cell><cell>Task-specific</cell><cell></cell></row><row><cell>Text Corpus Large-scale</cell><cell>Distillation</cell><cell>TinyBERT General</cell><cell>Distillation</cell><cell>TinyBERT Fine-tuned</cell></row><row><cell></cell><cell>Task Dataset</cell><cell>Data Augmentation</cell><cell>Task Dataset Augmented</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1: Results are evaluated on the test set of GLUE official benchmark. The best results for each group of student models are in-bold. The architecture of TinyBERT 4 and BERT TINY is (</figDesc><table><row><cell>System</cell><cell cols="12">#Params #FLOPs Speedup MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg</cell></row><row><cell>BERT BASE (Teacher)</cell><cell>109M</cell><cell>22.5B</cell><cell>1.0x</cell><cell>83.9/83.4</cell><cell>71.1</cell><cell>90.9</cell><cell>93.4</cell><cell>52.8</cell><cell>85.2</cell><cell>87.5</cell><cell cols="2">67.0 79.5</cell></row><row><cell>BERT TINY</cell><cell>14.5M</cell><cell>1.2B</cell><cell>9.4x</cell><cell>75.4/74.9</cell><cell>66.5</cell><cell>84.8</cell><cell>87.6</cell><cell>19.5</cell><cell>77.1</cell><cell>83.2</cell><cell cols="2">62.6 70.2</cell></row><row><cell>BERT SMALL</cell><cell>29.2M</cell><cell>3.4B</cell><cell>5.7x</cell><cell>77.6/77.0</cell><cell>68.1</cell><cell>86.4</cell><cell>89.7</cell><cell>27.8</cell><cell>77.0</cell><cell>83.4</cell><cell cols="2">61.8 72.1</cell></row><row><cell>BERT 4 -PKD</cell><cell>52.2M</cell><cell>7.6B</cell><cell>3.0x</cell><cell>79.9/79.3</cell><cell>70.2</cell><cell>85.1</cell><cell>89.4</cell><cell>24.8</cell><cell>79.8</cell><cell>82.6</cell><cell cols="2">62.3 72.6</cell></row><row><cell>DistilBERT 4</cell><cell>52.2M</cell><cell>7.6B</cell><cell>3.0x</cell><cell>78.9/78.0</cell><cell>68.5</cell><cell>85.2</cell><cell>91.4</cell><cell>32.8</cell><cell>76.1</cell><cell>82.4</cell><cell cols="2">54.1 71.9</cell></row><row><cell>MobileBERT TINY  †</cell><cell>15.1M</cell><cell>3.1B</cell><cell>-</cell><cell>81.5/81.6</cell><cell>68.9</cell><cell>89.5</cell><cell>91.7</cell><cell>46.7</cell><cell>80.1</cell><cell>87.9</cell><cell cols="2">65.1 77.0</cell></row><row><cell>TinyBERT 4 (ours)</cell><cell>14.5M</cell><cell>1.2B</cell><cell>9.4x</cell><cell>82.5/81.8</cell><cell>71.3</cell><cell>87.7</cell><cell>92.6</cell><cell>44.1</cell><cell>80.4</cell><cell>86.4</cell><cell cols="2">66.6 77.0</cell></row><row><cell>BERT 6 -PKD</cell><cell>67.0M</cell><cell>11.3B</cell><cell>2.0x</cell><cell>81.5/81.0</cell><cell>70.7</cell><cell>89.0</cell><cell>92.0</cell><cell>-</cell><cell>-</cell><cell>85.0</cell><cell>65.5</cell><cell>-</cell></row><row><cell>PD</cell><cell>67.0M</cell><cell>11.3B</cell><cell>2.0x</cell><cell>82.8/82.2</cell><cell>70.4</cell><cell>88.9</cell><cell>91.8</cell><cell>-</cell><cell>-</cell><cell>86.8</cell><cell>65.3</cell><cell>-</cell></row><row><cell>DistilBERT 6</cell><cell>67.0M</cell><cell>11.3B</cell><cell>2.0x</cell><cell>82.6/81.3</cell><cell>70.1</cell><cell>88.9</cell><cell>92.5</cell><cell>49.0</cell><cell>81.3</cell><cell>86.9</cell><cell cols="2">58.4 76.8</cell></row><row><cell>TinyBERT 6 (ours)</cell><cell>67.0M</cell><cell>11.3B</cell><cell>2.0x</cell><cell>84.6/83.2</cell><cell>71.6</cell><cell>90.4</cell><cell>93.1</cell><cell>51.1</cell><cell>83.7</cell><cell>87.3</cell><cell cols="2">70.0 79.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies of different procedures (i.e., TD, GD, and DA) of the two-stage learning framework. The variants are validated on the dev set.</figDesc><table><row><cell>System</cell><cell cols="3">MNLI-m MNLI-mm MRPC</cell><cell>CoLA</cell><cell>Avg</cell></row><row><cell>TinyBERT 4</cell><cell>82.8</cell><cell>82.9</cell><cell>85.8</cell><cell>50.8</cell><cell>75.6</cell></row><row><cell>w/o Embd</cell><cell>82.3</cell><cell>82.3</cell><cell>85.0</cell><cell>46.7</cell><cell>74.1</cell></row><row><cell>w/o Pred</cell><cell>80.5</cell><cell>81.0</cell><cell>84.3</cell><cell>48.2</cell><cell>73.5</cell></row><row><cell>w/o Trm</cell><cell>71.7</cell><cell>72.3</cell><cell>70.1</cell><cell>11.2</cell><cell>56.3</cell></row><row><cell>w/o Attn</cell><cell>79.9</cell><cell>80.7</cell><cell>82.3</cell><cell>41.1</cell><cell>71.0</cell></row><row><cell>w/o Hidn</cell><cell>81.7</cell><cell>82.1</cell><cell>84.1</cell><cell>43.7</cell><cell>72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of different distillation objectives in the TinyBERT learning. The variants are validated on the dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results (dev) of different mapping strategies for TinyBERT 4 .</figDesc><table><row><cell>et al., 2019; Lan et al., 2020), weight sharing (De</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparisons between TinyBERT with other baselines on the dev set of GLUE tasks. Mcc refers to Matthews correlation and Pear/Spea refer to Pearson/Spearman. same model architecture as TinyBERT 6(i.e. M =6, d =768, d i =3072).The direct comparison results are shown in Table 5. We can see the TinyBERT 6 outperforms all the baselines under the same settings of architecture and evaluation methods. The effectiveness of TinyBERT is further confirmed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Results (dev) of baselines and TinyBERT on question answering tasks. The architecture of MiniLM 4 is (M =4, d=384, d i =1536) which is wider than TinyBERT 4 , and the architecture of MiniLM 6 is the same as TinyBERT 6 (M =6, d=768, d i =3072)</figDesc><table><row><cell>System</cell><cell cols="5">MNLI-m MNLI-mm MRPC CoLA Avg</cell></row><row><cell></cell><cell>(392k)</cell><cell>(392k)</cell><cell cols="2">(3.5k) (8.5k)</cell><cell></cell></row><row><cell>BERT TINY</cell><cell>75.9</cell><cell>76.9</cell><cell>83.2</cell><cell cols="2">19.5 63.9</cell></row><row><cell>BERT TINY (+TD)</cell><cell>79.2</cell><cell>79.7</cell><cell>82.9</cell><cell cols="2">12.4 63.6</cell></row><row><cell>TinyBERT (GD)</cell><cell>76.6</cell><cell>77.2</cell><cell>82.0</cell><cell>8.7</cell><cell>61.1</cell></row><row><cell>TinyBERT (GD+TD)</cell><cell>80.5</cell><cell>81.0</cell><cell>82.4</cell><cell cols="2">29.8 68.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Results of different methods at pre-training stage. TD and GD refers to Task-specific Distillation (without data augmentation) and General Distillation, respectively. The results are evaluated on dev set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In the general distillation, we do not perform predictionlayer distillation as Equation 10. Our motivation is to make</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A word is tokenized into multiple word-pieces by the tokenizer of BERT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/google-research/bert 7 https://github.com/intersun/ PKD-for-BERT-Model-Compression 8 https://github.com/huggingface/transformers/tree/ master/examples/distillation 9 https://gluebenchmark.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by NSFC NO.61832020, No.61821003, 61772216, National Science and Technology Major Project No.2017ZX01032-101, Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A More Comparisons on GLUE</head><p>Since some prior works on BERT compression only evaluate their models on the GLUE dev set, for an easy and direct comparison, we here compare our TinyBERT 6 with the reported results from these prior works. All the compared methods have the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finetune bert with sparse self-attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Universal transformers. In ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02178</idno>
		<title level="m">Fastbert: a self-distilling bert with adaptive inference time</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tensorized transformer for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pruning a bert-based question answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06360</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Poor man&apos;s bert: Smaller and faster transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03844</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05840</idno>
		<title level="m">Q-bert: Hessian based ultra low precision quantization of bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for bert model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02984</idno>
		<title level="m">Mobilebert: a compact taskagnostic bert for resource-limited devices</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12136</idno>
		<title level="m">Distilling task-specific knowledge from bert into simple neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<title level="m">Well-read students learn better: The impact of student initialization on knowledge distillation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10957</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<title level="m">Neural network acceptability judgments. TACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional bert contextual augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02925</idno>
		<title level="m">2020. Bert-of-theseus: Compressing bert by progressive module replacing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wasserblat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06188</idno>
		<title level="m">Q8bert: Quantized 8bit bert</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
