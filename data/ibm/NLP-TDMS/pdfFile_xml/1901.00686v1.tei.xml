<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GENERATING MULTIPLE OBJECTS AT SPATIALLY DISTINCT LOCATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge Technology</orgName>
								<orgName type="department" key="dep2">Department of Informatics</orgName>
								<orgName type="institution">Universität Hamburg Vogt</orgName>
								<address>
									<addrLine>Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heinrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge Technology</orgName>
								<orgName type="department" key="dep2">Department of Informatics</orgName>
								<orgName type="institution">Universität Hamburg Vogt</orgName>
								<address>
									<addrLine>Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge Technology</orgName>
								<orgName type="department" key="dep2">Department of Informatics</orgName>
								<orgName type="institution">Universität Hamburg Vogt</orgName>
								<address>
									<addrLine>Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GENERATING MULTIPLE OBJECTS AT SPATIALLY DISTINCT LOCATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Understanding how to learn powerful representations from complex distributions is the intriguing goal behind adversarial training on image data. While recent advances have enabled us to generate high-resolution images with Generative Adversarial Networks (GANs), currently most GAN models still focus on modeling images that either contain only one centralized object (e.g. faces (CelebA), objects (ImageNet), birds , flowers (Oxford-102), etc.) or on images from one specific domain (e.g. <ref type="bibr">LSUN bedrooms, LSUN churches, etc.)</ref>. This means that, overall, the variance between images used for training GANs tends to be low <ref type="bibr" target="#b14">(Raj et al., 2017)</ref>. However, many real-life images contain multiple distinct objects at different locations within the image and with different relations to each other. This is for example visible in the MS-COCO data set <ref type="bibr" target="#b11">(Lin et al., 2014)</ref>, which consists of images of different objects at different locations within one image. In order to model images with these complex relationships, we need models that can model images containing multiple objects at distinct locations. To achieve this, we need control over what kind of objects are generated (e.g. persons, animals, objects, etc.), the location, and the size of these objects. This is a much more challenging task than generating a single object in the center of an image. Current work <ref type="bibr" target="#b10">(Karacan et al., 2016;</ref><ref type="bibr" target="#b9">Johnson et al., 2018;</ref><ref type="bibr" target="#b6">Hong et al., 2018b;</ref><ref type="bibr" target="#b23">Wang et al., 2018)</ref> often approaches this challenge by using a semantic layout as additional conditional input. While this can be successful in controlling the image layout and object placement, it also places a high burden on the generating process since a complete scene layout must be obtained first. We propose a model that does not require a full semantic layout, but instead only requires the desired object locations and identities (see <ref type="figure" target="#fig_0">Figure 1</ref>). One part of our model, called the global pathway, is responsible for generating the general layout of the complete image, while a second path, the object pathway, is used to explicitly generate the features of different objects based on the relevant object label and location. 1 arXiv:1901.00686v1 [cs.CV] 3 Jan 2019</p><p>Published as a conference paper at ICLR 2019</p><p>The generator gets as input a natural language description of the scene (if existent), the locations and labels of the various objects within the scene, and a random noise vector. The global pathway uses this to create a scene layout encoding which describes high-level features and generates a global feature representation from this. The object pathway generates a feature representation of a given object at a location described by the respective bounding box and is applied iteratively over the scene at the locations specified by the individual bounding boxes. We then concatenate the feature representations of the global and the object pathway and use this to generate the final image.</p><p>The discriminator, which also consists of a global and object pathway, gets as input the image, the bounding boxes and their respective object labels, and the textual description. The global pathway is then applied to the whole image and obtains a feature representation of the global image features. In parallel, the object pathway focuses only on the areas described by the bounding boxes and the respective object labels and obtains feature representations of these specific locations. Again, the outputs of both the global and the object pathway are merged and the discriminator is trained to distinguish between real and generated images.</p><p>In contrast to previous work we do not generate a scene layout of the whole scene but only focus on relevant objects which are placed at the specified locations, while the global consistency of the image is the responsibility of the other part of our model. To summarize our model and contributions: 1) We propose a GAN model that enables us to control the layout of a scene without the use of a scene layout. 2) Through the use of an object pathway which is responsible for learning features of different object categories, we gain control over the identity and location of arbitrarily many objects within a scene. 3) The discriminator judges not only if the image is realistic and aligned to the natural language description, but also whether the specified objects are at the given locations and of the correct object category. 4) We show that the object pathway does indeed learn relevant features for the different objects, while the global pathway focuses on general image features and the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Having more control over the general image layout can lead to a higher quality of images <ref type="bibr" target="#b16">(Reed et al., 2016a;</ref><ref type="bibr" target="#b6">Hong et al., 2018b)</ref> and is also an important requirement for semantic image manipulation <ref type="bibr" target="#b5">(Hong et al., 2018a;</ref><ref type="bibr" target="#b23">Wang et al., 2018)</ref>. Approaches that try to exert some control over the image layout utilize Generative Adversarial Nets <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>, Refinement Networks (e.g. <ref type="bibr" target="#b1">Chen &amp; Koltun (2017)</ref>; ), recurrent attention-based models (e.g. <ref type="bibr" target="#b12">Mansimov et al. (2016)</ref>), autoregressive models (e.g. <ref type="bibr" target="#b18">Reed et al. (2016c)</ref>), and even memory networks supplying the image generation process with previously extracted image features .</p><p>One way to exert control over the image layout is by using natural language descriptions of the image, e.g. image captions, as shown by <ref type="bibr" target="#b17">Reed et al. (2016b)</ref>, , <ref type="bibr" target="#b20">Sharma et al. (2018)</ref>, and <ref type="bibr" target="#b25">Xu et al. (2018b)</ref>. However, these approaches are trained only with images and their respective captions and it is not possible to specifically control the layout or placement of specific objects within the image. Several approaches suggested using a semantic layout of the image, generated from the image caption, to gain more fine-grained control over the final image. <ref type="bibr" target="#b10">Karacan et al. (2016)</ref>, <ref type="bibr" target="#b9">Johnson et al. (2018)</ref>, and <ref type="bibr" target="#b23">Wang et al. (2018)</ref> use a scene layout to generate images in which given objects are drawn within their specified segments based on the generated scene layout. <ref type="bibr" target="#b6">Hong et al. (2018b)</ref> use the image caption to generate bounding boxes of specific objects within the image and predict the object's shape within each bounding box. This is further extended by <ref type="bibr" target="#b5">Hong et al. (2018a)</ref> by making it possible to manipulate images on a semantic level. While these approaches offer a more detailed control over the image layout they heavily rely on a semantic scene layout for the image generating process, often implying complex preprocessing steps in which the scene layout is constructed.</p><p>The two approaches most closely related to ours are by <ref type="bibr" target="#b16">Reed et al. (2016a)</ref> and <ref type="bibr" target="#b14">Raj et al. (2017)</ref>. <ref type="bibr" target="#b14">Raj et al. (2017)</ref> introduce a model that consists of individual "blocks" which are responsible for different object characteristics (e.g. color, shape, etc.). However, their approach was only tested on the synthetic SHAPES data set <ref type="bibr" target="#b0">(Andreas et al., 2016)</ref>, which has only comparatively low variability and no image captions. <ref type="bibr" target="#b17">Reed et al. (2016b)</ref> condition both the generator and the discriminator on either a bounding box containing the object or keypoints describing the object's shape. However, the used images are still of relatively low variability (e.g. birds <ref type="bibr" target="#b22">(Wah et al., 2011)</ref>) and only contain one object, usually located in the center of the image. In contrast, we model images with several different objects at various locations and apply our object pathway multiple times at each image, both in the  generator and in the discriminator. Additionally, we use the image caption and bounding box label to obtain individual labels for each bounding box, while <ref type="bibr" target="#b17">Reed et al. (2016b)</ref> only use the image caption as conditional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>For our approach 1 , the central goal is to generate objects at arbitrary locations within a scene while keeping the scene overall consistent. For this we make use of a generative adversarial network (GAN) <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>. A GAN consists of two networks, a generator and a discriminator, where the generator tries to reproduce the true data distribution and the discriminator tries to distinguish between generated data points and data points sampled from the true distribution. We use the conditional GAN framework, in which both the generator and the discriminator get additional information, such as labels, as input. The generator G (see <ref type="figure" target="#fig_0">Figure 1</ref>) gets as input a randomly sampled noise vector z, the location and size of the individual bounding boxes bbox i , a label for each of the bounding boxes encoded as a one-hot vector l onehoti , and, if existent, an image caption embedding ϕ obtained with a pretrained char-CNN-RNN network from <ref type="bibr" target="#b17">Reed et al. (2016b)</ref>. As a pre-processing step (A), the generator constructs labels label i for the individual bounding boxes from the image caption ϕ and the provided labels l onehoti of each bounding box. For this, we concatenate the image caption embedding ϕ and the one-hot vector of a given bounding box l onehoti and create a new label embedding label i by applying a matrix-multiplication followed by a non-linearity (i.e. a fully connected layer). The resulting label label i contains the previous label as well as additional information from the image caption, such as color or shape, and is potentially more meaningful. In case of missing image captions, we use the one-hot embedding l onehoti only.</p><p>The generator consists of two different streams which get combined later in the process. First, the global pathway (B) is responsible for creating a general layout of the global scene. It processes the previously generated local labels label i for each of the bounding boxes and replicates them spatially at the location of each bounding box. In areas where the bounding boxes overlap the label embeddings label i are summed up, while the areas with no bounding boxes remain filled with zeros. Convolutional layers are applied to this layout to obtain a high-level layout encoding which is concatenated with the noise vector z and the image caption embedding ϕ and the result is used to generate a general image layout f global .</p><p>Second, the object pathway (C) is responsible for generating features of the objects f locali within the given bounding boxes. This pathway creates a feature map of a predefined resolution using convolutional layers which receive the previously generated label label i as input. This feature map is further transformed with a Spatial Transformer Network (STN) <ref type="bibr" target="#b7">(Jaderberg et al., 2015)</ref> to fit into the Published as a conference paper at ICLR 2019 bounding box at the given location on an empty canvas. The same convolutional layers are applied to each of the provided labels, i.e. we have one object pathway that is applied several times across different labels label i and whose output feeds onto the corresponding coordinates on the empty canvas. Again, features within overlapping bounding box areas are summed up, while areas outside of any bounding box remain zero.</p><p>As a final step, the outputs of the global and object pathways f global and f locali are concatenated along the channel axis and are used to generate the image in the final resolution, using common GAN procedures. The specific changes of the generator compared to standard architectures are the object pathway that generates additional features at specific locations based on provided labels, as well as the layout encoding which is used as additional input to the global pathway. These two extensions can be added to the generator in any existing architecture with limited extra effort.</p><p>The discriminator receives as input an image (either original or generated), the location and size of the bounding boxes bbox i , the labels for the bounding boxes as one-hot vectors l onehoti , and, if existent, the image caption embedding ϕ. Similarly to the generator, the discriminator also possesses both a global (D) and an object (E) pathway respectively. The global pathway takes the image and applies multiple convolutional layers to obtain a representation f global of the whole image. The object pathway first uses a STN to extract the objects from within the given bounding boxes and then concatenates these extracted features with the spatially replicated bounding box label l onehoti . Next, convolutional layers are applied and the resulting features f locali are again added onto an empty canvas within the coordinates specified by the bounding box. Note, similarly to the generator we only use one object pathway that is applied to multiple image locations, where the outputs are then added onto the empty canvas, summing up overlapping parts and keeping areas outside of the bounding boxes set to zero. Finally, the outputs of both the object and global pathways f locali and f global are concatenated along the channel axis and we again apply convolutional layers to obtain a merged feature representation. At this point, the features are concatenated either with the spatially replicated image caption embedding ϕ (if existent) or the sum of all one-hot vectors l onehoti along the channel axis, one more convolutional layer is applied, and the output is classified as either generated or real.</p><p>For the general training, we can utilize the same procedure that is used in the GAN architecture that is modified with our proposed approach. In our work we mostly use the StackGAN  and AttnGAN <ref type="bibr" target="#b25">(Xu et al., 2018b)</ref> frameworks which use a modified objective function taking into consideration the additional conditional information and provided image captions. As such, our discriminator D and our generator G optimize the following objective function:</p><formula xml:id="formula_0">min G max D V (D, G) = E (x,c)∼pdata [logD(x, c)] + E (z)∼pz,(c)∼pdata [log(1 − D(G(z, c), c))],</formula><p>where x is an image, c is the conditional information for this image (e.g. label i , bounding boxes bbox i , or an image caption ϕ), z is a randomly sampled noise vector used as input for G, and p data is the true data distribution.  and others use an additional technique called conditioning augmentation for the image captions which helps improve the training process and the quality of the generated images. In the experiments in which we use image captions (MS-COCO) we also make use of this technique 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION AND ANALYSIS</head><p>For the evaluation, we aim to study the quality of the generated images with a particular focus on the generalization capabilities and the contribution of specific parts of our model, in both controllable and large-scale cases. Thus, in the following sections, we evaluate our approach on three different data sets: the Multi-MNIST data set, the CLEVR data set, and the MS-COCO data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MULTI-MNIST</head><p>In our first experiment, we used the Multi-MNIST data set <ref type="bibr" target="#b2">(Eslami et al., 2016)</ref> for testing the basic functionality of our proposed model. Using the implementation provided by <ref type="bibr" target="#b2">Eslami et al. (2016)</ref>, we created 50,000 images of resolution 64 × 64 px that contain exactly three normal-sized MNIST digits in non-overlapping locations on a black background.</p><p>2 More detailed information about the implementation can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Published as a conference paper at ICLR 2019  <ref type="bibr">9, 6, 1, 5, 9 5, 5, 5, 8, 3 6, 1, 8, 5, 2 6, 1, 1, 8, 0 1, 6, 7 2, 8, 4 6, 6, 9 9, 7, 9 6</ref>, 1, 0 9, 0, 0 8, 2, 2 4, 6, 2 7, 3, 9 0, 2, 9 As a first step, we tested whether our model can learn to generate digits at the specified locations and whether we can control the digit identity, the generated digit's size, and the number of generated digits per image. According to the results, we can control the location of individual digits, their identity, and their size, even though all training images contain exactly three digits in normal size. <ref type="figure" target="#fig_1">Figure 2</ref> shows that we can control how many digits are generated within an image (rows A-B, for two to five digits) and various sizes of the bounding box (row C). As a second step, we created an additional Multi-MNIST data set in which all training images contain only digits 0-4 in the top half and only digits 5-9 in the bottom half of the image. For testing digits in the opposite half, we can see that the model is indeed capable of generalizing the position (row D, left), i.e. it can generate digits 0-4 in the bottom half of the image and digits 5-9 in the top half of the image. Nevertheless, we also observed that this does not always work perfectly, as the network sometimes alters digits towards the ones it has seen during training at the respective locations, e.g. producing a "4" more similar to a "9" if in bottom half of the image, or generating a "7" more similar to a "1" if in top half of the image.</p><p>As a next step, we created a Multi-MNIST data set with images that only contain digits in the top half of the image, while the bottom half is always empty. We can see <ref type="figure" target="#fig_1">(Figure 2</ref>, row D, right) that the resulting model is not able to generate digits in the bottom half of the image (see <ref type="figure" target="#fig_5">Figure 6</ref> in the Appendix for more details on this). Controlling for the location still works, i.e. bounding boxes are filled with "something", but the digit identity is not clearly recognizable. Thus, the model is able to control both the object identity and the object location within an image and can generalize to novel object locations to some extent.</p><p>To test the impact of our model extensions, i.e. the object pathway in both the generator and the discriminator as well as the layout encoding, we performed ablation studies on the previously created Multi-MNIST data set with three digits at random locations. We first disabled the use of the layout encoding in the generator and left the rest of the model unchanged. In the results <ref type="figure" target="#fig_1">(Figure 2</ref>, row E, left), we can see that, overall, both the digit identity and the digit locations are still correct, but minor imperfections can be observed within various images. This is most likely due to the fact that the global pathway of the generator has no information about the digit identity and location until its features get merged with the object pathway. As a next test, we disabled the object pathway of the Published as a conference paper at ICLR 2019 discriminator and left the rest of the model unmodified. Again, we see (row E, right) that we can still control the digit location, although, again, minor imperfections are visible. More strikingly, we have a noticeably higher error rate in the digit identity, i.e. the wrong digit is generated at a given location, most likely due to the fact that there is not object pathway in the discriminator controlling the object identity at the various locations. In comparison, the imperfections are different when only the object pathway of the generator is disabled (row F, left). The layout encoding and the feedback of the discriminator seem to be enough to still produce the digits in the correct image location, but the digit identity is often incorrect or not recognizable at all. Finally, we tested disabling the object pathway in both the discriminator and the generator (see row F, right). This leads to a loss of control of both image location as well as identity and sometimes even results in images with more or fewer than three digits per image. This shows that only the layout encoding, without any of the object pathways, is not enough to control the digit identity and location. Overall, these results indicate that we do indeed need both the layout encoding, for a better integration of the global and object pathways, and the object pathways in both the discriminator and the generator, for optimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CLEVR</head><p>In our second experiment we used more complex images containing multiple objects of different colors and shapes. The goal of this experiment was to evaluate the generalization ability of our object pathway across different object characteristics. For this, we performed tests similar to <ref type="bibr" target="#b14">(Raj et al., 2017)</ref>, albeit on the more complex CLEVR data set <ref type="bibr" target="#b8">(Johnson et al., 2017)</ref>. In the CLEVR data set objects are characterized by multiple properties, in our case the shape, the color, and the size. Based on the implementation provided by <ref type="bibr" target="#b8">Johnson et al. (2017)</ref>, we rendered 25,000 images with a resolution of 64 × 64 pixels containing 2 − 4 objects per image. The label for a given bounding box of an object is the object shape and color (both encoded as one-hot encoding and then concatenated), while the object size is specified through the height and width of the bounding box.</p><p>Similar to the first experiment, we tested our model for controlling the object characteristics, size, and location. In the first row of <ref type="figure" target="#fig_2">Figure 3</ref> we present the results of the trained model, where the left image of each pair shows the originally rendered one, while the right image was generated by our model. We can confirm that the model can control both the location and the objects' shape and color characteristics. The model can also generate images containing an arbitrary number of objects (forth and fifths pair), even though a maximum of four objects per image was seen during training.</p><p>The CLEVR data set offers a split specifically intended to test the generalization capability of a model, in which cylinders can be either red, green, purple, or cyan and cubes can be either gray, blue, brown, or yellow during training, while spheres can have any of these colors. During testing, the colors between cylinders and cubes are reversed. Based on these restrictions, we created a second data set of 25,000 training images for testing our model. Results of the test are shown in the second row of <ref type="figure" target="#fig_2">Figure 3</ref> (again, left image of each pair shows the originally rendered one, while the right image was generated by our model). We can see that the color transfer to novel shape-color combinations takes place, but, similarly to the Multi-MNIST results, we can see some artifacts, where e.g. some cubes look a bit more like cylinders and vice versa. Overall, the CLEVR experiment confirms the indication that our model can control object characteristics (provided through labels) and object locations (provided through bounding boxes) and can generalize to novel object locations, novel amounts of objects per image, and novel object characteristic combinations within reasonable boundaries. The authors report a "best" value of 25.89 ± 0.47, but when calculating the IS with the pretrained model provided by the authors we only obtain an IS of 23.61. Other researchers on the authors' Github website report a similar value for the pretrained model. <ref type="bibr">5</ref> We use the updated source code (IS of 10.62) as our baseline model. <ref type="table">Table 1</ref>: Comparison of the Inception Score (IS) and Fréchet Inception Distance (FID) on the MS-COCO data set for different models. Note: the IS and FID values of our models are not necessarily directly comparable to the other models, since our model gets at test time, in addition to the image caption, up to three bounding boxes and their respective object labels as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MS-COCO</head><p>For our final experiment, we used the MS-COCO data set <ref type="bibr" target="#b11">(Lin et al., 2014)</ref> to evaluate our model on natural images of complex scenes. In order to keep our evaluation comparable to previous work, we used the 2014 train/test split consisting of roughly 80,000 training and 40,000 test images and rescaled the images to a resolution of 256 × 256 px. At train-time, we used the bounding boxes and object labels of the three largest objects within an image, i.e. we used zero to three bounding boxes per image. Similarly to work by <ref type="bibr" target="#b9">Johnson et al. (2018)</ref> we only considered objects that cover at least 2% of the image for the bounding boxes. To evaluate our results quantitatively, we computed both the Inception Score (IS, larger is better), which tries to evaluate how recognizable and diverse objects within images are <ref type="bibr" target="#b19">(Salimans et al., 2016)</ref>, as well as the Fréchet Inception Distance (FID, smaller is better), which compares the statistics of generated images with real images <ref type="bibr" target="#b4">(Heusel et al., 2017)</ref>. As a qualitative evaluation, we generated images that contain more than one object, and checked, whether the bounding boxes can control the object placement. We tested our approach with two commonly used architectures for text-to-image synthesis, namely the StackGAN <ref type="bibr" target="#b26">(Zhang et al., 2017)</ref> and the AttnGAN <ref type="bibr" target="#b25">(Xu et al., 2018b)</ref>, and compared the images generated by these and our models.</p><p>In the StackGAN, the training process is divided into two steps: first, it learns a generator for images with a resolution of 64 × 64 px based on the image captions, and second, it trains a second generator, which uses the smaller images (64 × 64 px) from the first generator and the image caption as input to generate images with a resolution of 256×256 px. Here, we added the object pathways and the layout encoding at the beginning of both the first generator and the second generator and used the object pathway in both discriminators. The other parts of StackGAN architecture and all hyperparameters remain the same as in the original training procedure for the MS-COCO data set. We trained the model three times from scratch and randomly sampled 3 times 30,000 image captions from the test set for each model. We then calculated the IS and FID values on each of the nine samples of 30,000 generated images and report the averaged values. As presented in <ref type="table">Table 1</ref>, our StackGAN with added object pathways outperforms the original StackGAN both on the IS and the FID, increasing the IS from 10.62 to 12.12 and decreasing the FID from 74.05 to 55.30. Note, however, that this might also be due to the additional information our model is provided with as it receives up to three bounding boxes and respective bounding box labels per image in addition to the image caption.</p><p>We also extended the AttnGAN by <ref type="bibr" target="#b25">Xu et al. (2018b)</ref>, the current state-of-the-art model on the MS-COCO data set (based on the Inception Score), with our object pathway to evaluate its impact on 7 Published as a conference paper at ICLR 2019 Three discriminators judge the output of the generator at an image resolution of 64 × 64, 128 × 128, and 256 × 256 px. Through this, the image generation process is guided at multiple levels, which helps during the training process. Additionally, the AttnGAN implements an attention technique through which the networks focus on specific areas of the image for specific words in the image caption and adds an additional loss that checks if the image depicts the content as described by the image caption. There, in the same way as for the StackGAN, we added our object pathway at the beginning of the generator as well as to the discriminator that judges the generator outputs at a resolution of 64 × 64 px. All other discriminators, the higher layers of the generator, and all other hyperparameters and training details stay unchanged. <ref type="table">Table 1</ref> shows that adding the object pathway to the AttnGAN increases the IS of our baseline model (the pretrained model provided by the authors) from 23.61 to 24.76, while the FID is roughly the same as for the baseline model.</p><p>To evaluate whether the StackGAN model equipped with an object pathway (StackGAN+OP) actually generates objects at the given positions we generated images that contain multiple objects and inspected them visually. <ref type="figure" target="#fig_3">Figure 4</ref> shows some example images, more results can be seen in the Appendix in <ref type="figure">Figures 7 and 9</ref>. We can observe that the StackGAN+OP indeed generates images in which the objects are at appropriate locations. In order to more closely inspect our global and object pathways, we can also disable them during the image generation process. <ref type="figure">Figure 5</ref> shows additional examples, in which we generate the same image with either the global or the object pathway disabled during the generation process. Row C of <ref type="figure">Figure 5</ref> shows images in which the object pathway was disabled and, indeed, we observe that the images contain mostly background information and objects at the location of the bounding boxes are either not present or of much less detail than when the object pathway is enabled. Conversely, row D of <ref type="figure">Figure 5</ref> shows images which were generated when the global pathway was disabled. As expected, areas outside of the bounding boxes are empty, but we also observe that the bounding boxes indeed contain images that resemble the appropriate objects. These results indicate, as in the previous experiments, that the global pathway does indeed model holistic image features, while the object pathway focuses on specific, individual objects.</p><p>When we add the object pathway to the AttnGAN (AttnGAN + OP) we can observe similar results 4 . Again, we are able to control the location and identity of objects through the object pathway, however, we observe that the AttnGAN+OP, as well as the AttnGAN in general, tends to place objects corresponding to specific features at many locations throughout the image. For example, if the caption contains the word "traffic light" the AttnGAN tends to place objects similar to traffic lights throughout the whole image. Since our model only focuses on generating objects at given locations, while not enforcing that these objects only occur at these locations, this behavior leads to the result that the AttnGAN+OP generates desired objects at the desired locations, but might also place the same object at other locations within the image. Note, however, that we only added the object pathway Published as a conference paper at ICLR 2019 <ref type="figure">Figure 5</ref>: Examples of images generated from the given caption from the MS-COCO data set. A) shows the original images and the respective image captions, B) shows images generated by our StackGAN+OP (with the corresponding bounding boxes for visualization) with the object pathway enabled, C) shows images generated by the our StackGAN+OP when the object pathway is disabled, and D) shows images generated by the our StackGAN+OP when the global pathway is disabled.</p><p>to the lowest generator and discriminator and that we might gain even more control over the object location by introducing object pathways to the higher generators and discriminators, too.</p><p>In order to further evaluate the quality of the generations, we ran an object detection test on the generated images using a pretrained YOLOv3 network <ref type="bibr" target="#b15">(Redmon &amp; Farhadi, 2018)</ref>. Here, the goal is to measure how often an object detection framework, which was trained on MS-COSO as well, can detect a specified object at a specified location 5 . The results confirm the previously made observations: For both the StackGAN and the AttnGAN the object pathway seems to improve the image quality, since YOLOv3 detects a given object more often correctly when the images are generated with an object pathway as opposed to images generated with the baseline models. The StackGAN generates objects at the given bounding box, resulting in an Intersection over Union (IoU) of greater than 0.3 for all tested labels and greater than 0.5 for 86.7% of the tested labels. In contrast, the AttnGAN tends to place salient object features throughout the image, which leads to an even higher detection rate by the YOLOv3 network, but a smaller average IoU (only 53.3% of the labels achieve an IoU greater than 0.3). Overall, our experiments on the MS-COCO data set indicate that it is possible to add our object pathway to pre-existing GAN models without having to change the overall model architecture or training process. Adding the object pathway provides us with more control over the image generation process and can, in some cases, increase the quality of the generated images as measured via the IS or FID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">DISCUSSION</head><p>Our experiments indicate that we do indeed get additional control over the image generation process through the introduction of object pathways in GANs. This enables us to control the identity and location of multiple objects within a given image based on bounding boxes and thereby facilitates the generation of more complex scenes. We further find that the division of work on a global and object pathway seems to improve the image quality both subjectively and based on quantitative metrics such as the Inception Score and the Fréchet Inception Distance.</p><p>The results further indicate that the focus on global image statistics by the global pathway and the more fine-grained attention to detail of specific objects by the object pathway works well. This is visualized for example in rows C and D of <ref type="figure">Figure 5</ref>. The global pathway (row C) generates features for the general image layout and background but does not provide sufficient details for individual objects. The object pathway (row D), on the other hand, focuses entirely on the individual objects and generates features specifically for a given object at a given location. While this is the desired behavior Published as a conference paper at ICLR 2019 of our model it can also lead to sub-optimal images if there are not bounding boxes for objects that should be present within the image. This can often be the case if the foreground object is too small (in our case less than 2% of the total image) and is therefore not specifically labeled. In this case, the objects are sometimes not modeled in the image at all, despite being prominent in the respective image caption, since the object pathway does not generate any features. We can observe this, for example, in images described as "many sheep are standing on the grass", where the individual sheep are too small to warrant a bounding box. In this case, our model will often only generate an image depicting grass and other background details, while not containing any sheep at all.</p><p>Another weakness is that bounding boxes that overlap too much (empirically an overlap of more than roughly 30%) also often lead to sub-optimal objects at that location. Especially in the overlapping section of bounding boxes we often observe local inconsistencies or failures. This might be the result of our merging of the different features within the object pathway since they are simply added to each other at overlapping areas. A more sophisticated merging procedure could potentially alleviate this problem.Another approach would be to additionally enhance the bounding box layout by predicting the specific object shape within each bounding box, as done for example by <ref type="bibr" target="#b6">Hong et al. (2018b)</ref>.</p><p>Finally, currently our model does not generate the bounding boxes and labels automatically. Instead, they have to be provided at test time which somewhat limits the usability for unsupervised image generation. However, even when using ground truth bounding boxes, our models still outperform other current approaches that are tested with ground truth bounding boxes (e.g. <ref type="bibr" target="#b6">Hong et al. (2018b)</ref>) based on the IS and FID. This is even without the additional need of learning to specify the shape within each bounding box as done by <ref type="bibr" target="#b6">Hong et al. (2018b)</ref>. In the future, this limitation can be avoided by extracting the relevant bounding boxes and labels directly from the image caption, as it is done for example by <ref type="bibr" target="#b6">Hong et al. (2018b)</ref>, , and <ref type="bibr" target="#b21">Tan et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>With the goal of understanding how to gain more control over the image generation process in GANs, we introduced the concept of an additional object pathway. Such a mechanism for differentiating between a scene representation and object representations allows us to control the identity, location, and size of arbitrarily many objects within an image, as long as the objects do not overlap too strongly. In parallel, a global pathway, similar to a standard GAN, focuses on the general scene layout and generates holistic image features. The object pathway, on the other hand, gets as input an object label and uses this to generate features specifically for this object which are then placed at the location given by a bounding box The object pathway is applied iteratively for each object at each given location and as such, we obtain a representation of individual objects at individual locations and of the general image layout (background, etc.) as a whole. The features generated by the object and global pathway are then concatenated and are used to generate the final image output. Our tests on synthetic and real-world data sets suggest that the object pathway is an extension that can be added to common GAN architectures without much change to the original architecture and can, along with more fine-grained control over the image layout, also lead to better image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS</head><p>Here we provide some more details about the exact implementation of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MULTI-MNIST AND CLEVR</head><p>To train our GAN approach on the Multi-MNIST (CLEVR) data set we use the Stage-I Generator and Discriminator from the StackGAN MS-COCO architecture 6 . In our following description an upsample block describes the following sequence: nearest neighbor upsampling with factor 2, a convolutional layer with X filters (filter size 3 × 3, stride 1, padding 1), batch normalization, and a ReLU activation. The bounding box labels are one-hot vectors of size [1, 10] encoding the digit identity (CLEVR: [1, 13] encoding object shape and color). Please refer to <ref type="table" target="#tab_3">Table 2</ref> for detailed information on the individual layers described in the following. For all leaky ReLU activations alpha was set to 0.2.</p><p>In the object pathway of the generator we first create a zero tensor O G which will contain the feature representations of the individual objects. We then spatially replicate each bounding box label into a 4 × 4 layout of shape (10, 4, 4) (CLEVR: <ref type="figure" target="#fig_0">(13, 4, 4)</ref>) and apply two upsampling blocks. The resulting tensor is then added to the tensor O G at the location of the bounding box using a spatial transformer network.</p><p>In the global pathway of the generator we first obtain the layout encoding. For this we create a tensor of shape (10, 16, 16) (CLEVR: <ref type="figure" target="#fig_0">(13, 16, 16)</ref>) that contains the one-hot labels at the location of the bounding boxes and is zero everywhere else. We then apply three convolutional layers, each followed by batch normalization and a leaky ReLU activation. We reshape the output to shape (1, 64) and concatenate it with the noise tensor of shape (1, 100) (sampled from a random normal distribution) to form a tensor of shape <ref type="bibr">(1,</ref><ref type="bibr">164)</ref>. This tensor is then fed into a dense layer, followed by batch normalization and a ReLU activation and the output is reshaped to <ref type="bibr">(−1, 4, 4)</ref>. We then apply two upsampling blocks to obtain a tensor of shape <ref type="bibr">(−1, 16, 16)</ref>.</p><p>At this point, the outputs of the object and the global pathway are concatenated along the channel axis to form a tensor of shape <ref type="bibr">(−1, 16, 16)</ref>. We then apply another two upsampling blocks resulting in a tensor of shape (−1, 64, 64) followed by a convolutional layer and a TanH activation to obtain the final image of shape <ref type="figure" target="#fig_0">(−1, 64, 64</ref>).</p><p>In the object pathway of the discriminator we first create a zero tensor O D which will contain the feature representations of the individual objects. We then use a spatial transfomer network to extract the image features at the locations of the bounding boxes and reshape them to a tensor of shape <ref type="bibr">(1,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref>  <ref type="figure" target="#fig_0">(CLEVR: (3, 16, 16)</ref>). The one-hot label of each bounding box are spatially replicated to a shape of (10, 16, 16) (CLEVR: <ref type="bibr">(13,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref>) and concatenated with the previously extracted features to form a tensor of shape <ref type="bibr">(11,</ref><ref type="bibr">16,</ref><ref type="bibr">16) (CLEVR: (16,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref>). We then apply a convolutional layer, batch normalization and a leaky ReLU activation to the concatenation of features and label and, again, use a spatial transformer network to resize the output to the shape of the respective bounding box before adding it to the tensor O D .</p><p>In the global pathway of the discriminator, we apply two convolutional layers, each followed by batch normalization and a leaky ReLU activation and concatenate the resulting tensor with the output of the object pathway. After this, we again apply two convolutional layers, each followed by batch normalization and a leaky ReLU activation. We concatenate the resulting tensor with the conditioning information about the image content, in this case, the sum of all one-hot vectors. To this tensor we apply another convolutional layer, batch normalization, a leaky ReLU activation, and another convolutional layer, to obtain the final output of the discriminator of shape (1).</p><p>Similarly to the procedure of StackGAN and other conditional GANs we train the discriminator to classify real images with correct labels (the sum of one-hot vectors supplied in the last step of the process) as real, while generated images with correct labels and real images with (randomly sampled) incorrect labels should be classified as fake. Published as a conference paper at ICLR 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MS-COCO</head><p>StackGAN-Stage-I For training the Stage-I generator and discriminator (images of size 64 × 64 pixels) we follow the same procedure and architecture outlined in the previous section about the training on the Multi-MNIST and CLEVR data sets. The only difference is that we now have image captions as an additional description of the image. As such, to obtain the bounding box labels we concatenate the image caption embedding 7 and the one-hot encoded bounding box label and apply a dense layer with 128 units, batch normalization, and a ReLU activation to it, to obtain a label of shape (1, 128) for each bounding box. In the final step of the discriminator when we concatenate the feature representation with the conditioning vector, we use the image encoding as conditioning vector and do not use any bounding box labels at this step. The rest of the training proceeds as described in the previous section, except that the bounding box labels now have a shape of (1, 128). All other details can be found in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>StackGAN-Stage-II In the second part of the training, we train a second generator and discriminator to generate images with a resolution of 256 × 256 pixels. The generator gets as input images with a resolution of 64 × 64 pixels (generated by the trained Stage-I generator) and the image caption and uses them to generate images with a 256 × 256 pixels resolution. A new discriminator is trained to distinguish between real and generated images.</p><p>On the Stage-II generator we perform the following modifications we use the same procedure as in the Stage-I generator to obtain the bounding box labels. To obtain an image encoding from the generated 64 × 64 image we use three convolutional layers, each followed by batch normalization and a ReLU activation to obtain a feature representation of shape <ref type="bibr">[−1, 16, 16]</ref>. Additionally, we replicate each bounding box label (obtained with the dense layer) spatially at the locations of the bounding boxes on an empty canvas of shape <ref type="bibr">[128,</ref><ref type="bibr">16,</ref><ref type="bibr">16]</ref> and then concatenate it along the channel axis with the image encoding and the spatially replicated image caption embedding. As in the standard StackGAN we then apply more convolutional layers with residual connections to obtain the final image embedding of shape <ref type="bibr">[−1, 16, 16]</ref>, which provides the input for both the object and the global pathway.</p><p>The generator's object pathway gets as input the image encoding described in the previous step. First, we create a zero tensor O G which will contain the feature representations of the individual objects. We then use a spatial transformer network to extract the features from within the bounding box and reshapes those features to <ref type="bibr">[−1, 16, 16]</ref>. After this, we apply two upsample blocks and then use a spatial transformer network to add the features to O G within the bounding box region. This is done for each of the bounding boxes within the image.</p><p>The generator's global pathway gets as input the image encoding and uses the same convolutional layers and upsampling procedures as the original StackGAN Stage-II generator. The outputs of the object and global pathway are merged at the resolution of [−1, 64, 64] by concatenating the two outputs along the channel axis. After this, we continue using the standard StackGAN architecture to generate images of shape <ref type="bibr">[3,</ref><ref type="bibr">256,</ref><ref type="bibr">256]</ref>.</p><p>The Stage-II discriminator's object pathway first creates a zero tensor O D which will contain the feature representations of the individual objects. It gets as input the image (resolution of 256 × 256 pixels) and we use a spatial transformer network to extract the features from the bounding box and reshape those features to a shape of <ref type="bibr">[3,</ref><ref type="bibr">32,</ref><ref type="bibr">32]</ref>. We spatially replicate the bounding box label (one-hot encoding) to a shape of <ref type="bibr">[−1, 32, 32]</ref> and concatenate it with the extracted features along the channel axis. This is then given to the object pathway which consists of two convolutional layers with batch normalization and a LeakyReLU activation. The output of the object pathway is again transformed to the width and height of the bounding box with a spatial transformer network and then added to O D . This procedure is performed with each of the bounding boxes within the image (maximum of three during training).</p><p>The Stage-II discriminator's global pathway consists of the standard StackGAN layers, i.e. it gets as input the image (256 × 256 pixels) and applies convolutional layers with stride 2 to it. The outputs of the object and global pathways are merged at the resolution of <ref type="bibr">[−1, 32, 32]</ref> by concatenating the two outputs along the channel axis We then apply more convolutional with stride 2 to decrease the resolution. After this, we continue in the same way as the original StackGAN.</p><p>AttnGAN On the AttnGAN 8 we only modify the training at the lower layers of the generator and the first discriminator (working on images of 64 × 64 pixels resolution). For this, we perform the same modifications as described in the StackGAN-Stage-I generator and discriminator. In the generator we obtain the bounding box labels in the same way as in the StackGAN, by concatenating the image caption embedding with the respective one-hot vector and applying a dense layer with 100 units, batch normalization, and a ReLU activation to obtain a bounding box label. In contrast to the previous architectures, we follow the AttnGAN implementation in use the gated linear unit function (GLU) as standard activation for our convolutional layers in the generator.</p><p>In the generator's object pathway we first create a zero tensor O G of shape <ref type="bibr">(192,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref> which will contain the feature representations of the individual objects. We then spatially replicate each bounding box label into a 4 × 4 layout of shape (100, 4, 4) and apply two upsampling blocks with 768 and 384 filters (filter size=3, stride=1, padding=1). The resulting tensor is then added to the tensor O G at the location of the bounding box using a spatial transformer network.</p><p>In the global pathway of the generator we first obtain the layout encoding in the same way as in the StackGAN-I generator, except that the three convolutional layers of the layout encoding now have 50, 25, and 12 filters respectively (filter size=3, stride=2, padding=1). We concatenate it with the noise tensor of shape (1, 100) (sampled from a random normal distribution) and the image caption embedding to form a tensor of shape <ref type="bibr">(1,</ref><ref type="bibr">248)</ref>. This tensor is then fed into a dense layer with 24,576 units, followed by batch normalization and a ReLU activation and the output is reshaped to <ref type="bibr">(768,</ref><ref type="bibr">4,</ref><ref type="bibr">4)</ref>. We then apply two upsampling blocks with 768 and 384 filters to obtain a tensor of shape <ref type="bibr">(192,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref>.</p><p>At this point the outputs of the object and the global pathways are concatenated along the channel axis to form a tensor of shape <ref type="bibr">(384,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref>. We then apply another two upsampling blocks with 192 and 96 filters, resulting in a tensor of shape <ref type="bibr">(48,</ref><ref type="bibr">64,</ref><ref type="bibr">64)</ref>. This feature representation is then used by the following layers of the AttnGAN generator in the same way as detailed in the original paper and implementation.</p><p>In the object pathway of the discriminator we first create a zero tensor O D which will contain the feature representations of the individual objects. We then use a spatial transfomer network to extract the image features at the locations of the bounding boxes and reshape them to a tensor of shape <ref type="bibr">(3,</ref><ref type="bibr">16,</ref><ref type="bibr">16)</ref>. The one-hot label of each bounding box is spatially replicated to a shape of (−1, 16, 16) and concatenated with the previously extracted features. We then apply a convolutional layer with 192 filters (filter size=4, stride=1, padding=1), batch normalization and a leaky ReLU activation to the concatenation of features and label and, again, use a spatial transformer network to resize the output to the shape of the respective bounding box before adding it to the tensor O D .</p><p>In the global pathway of the discriminator we apply two convolutional layers with 96 and 192 filters (filter size=4, stride=2, padding=1), each followed by batch normalization and a leaky ReLU activation and concatenate the resulting tensor with the output of the object pathway. After this, we again apply two convolutional layers with 384 and 768 filters (filter size=4, stride=2, padding=1), each followed by batch normalization and a leaky ReLU activation. We concatenate the resulting tensor with the spatially replicated image caption embedding. To this tensor we apply another convolutional layer with 768 filters (filter size=3, stride=1, padding=1), batch normalization, a leaky ReLU activation, and another convolutional layer with one filter (filter size=4, stride=4, padding=0), to obtain the final output of the discriminator of shape (1). The rest of the training and all other hyperparameters and architectural values are left the same as in the original implementation.</p><p>Published as a conference paper at ICLR 2019</p><p>Multi-MNIST CLEVR MS-COCO-I MS-COCO-II  <ref type="bibr">1, 3, 7 6, 4, 9 9, 3, 5 5, 9, 5 3, 4, 6 6, 3, 5 6, 9, 9 1, 2, 1 8, 4, 6 9, 4, 1 5, 9, 1 8, 0, 9 4, 5, 5 4, 4, 6 7, 2, 6 9, 0, 0 6, 7, 1 3, 4, 2 0, 3, 7 2, 7, 8 9, 7, 2 1, 6, 1 3, 7, 9 4, 1, 5 5, 5, 3 9, 2, 1 7, 6, 4 4, 2, 1 4, 8, 3 2, 2, 4 6, 5, 4 5, 9, 6 9,4, 5 3, 0, 1 1, 0, 7 3, 8, 2 7, 3, 0 4, 3, 9 4, 8, 1 9, 6, 6</ref>  As a result our model only generates the background, without the appropriate foreground object, even though the foreground object is very clearly described in the image caption. <ref type="figure">Figure 9</ref> provides similar results but for random bounding box positions.  <ref type="figure" target="#fig_0">Figure 10</ref> shows images generated by our AttnGAN where we randomly change the location of the various bounding boxes. Again, the last three examples show failure cases where we put the locations of the bounding boxes at "uncommon" positions. In the image depicting the sandwiches we put the location of the plate in the top half of the image, in the image with the dogs we put the dogs' location in the top half, and in the image with the motorbike we put the human in the left half and the motorbike in the right half of the image.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E OBJECT DETECTION ON MS-COCO IMAGES</head><p>To further inspect the quality of the location and recognizability of the generated objects within an image, we ran a test on object detection using a YOLOv3 network <ref type="bibr" target="#b15">Redmon &amp; Farhadi (2018)</ref> that was also pretrained on the MS-COCO data set 9 . We use the Pytorch implementation from https://github.com/ayooshkathuria/pytorch-yolo-v3 to get the bounding box and label predictions for our images. We follow the standard guidelines and keep all hyperparameters for the YOLOv3 network as in the implementation. We picked the 30 most common training labels (based on how many captions contain these labels) and evaluate the models on these labels, see <ref type="table" target="#tab_7">Table 3</ref>.</p><p>In the following, we evaluate how often the pretrained YOLOv3 network recognizes a specific object within a generated image that should contain this object based on the image caption. For example, we expect an image generated from the caption "a young woman taking a picture with her phone" to contain a person somewhere in the image and we check whether the YOLOv3 network actually recognizes a person in the generated image. Since the baseline StackGAN and AttnGAN only receive the image caption as input (no bounding boxes and no bounding box labels) we decided to only use captions that clearly imply the presence of the given label (see <ref type="table" target="#tab_7">Table 3</ref>). We chose this strategy in order to allow for a fair comparison of the resulting presence or absence of a given object. Specifically, for a given label we choose all image captions from the test set that contain one of the associated words for this label (associated words were chosen manually, see <ref type="table" target="#tab_7">Table 3</ref>) and then generated three images for each caption with each model. Finally, we counted the number of images in which the given object was detected by the YOLOv3 network. <ref type="table" target="#tab_9">Table 4</ref> shows the ratio of images for each label and each model in which the given object was detected at any location within the image.</p><p>Additionally, for our models that also receive the bounding boxes as input, we calculated the Intersection over Union (IoU) between the ground truth bounding box (the bounding box supplied to the model) and the bounding box predicted by the YOLOv3 network for the recognized object. <ref type="table" target="#tab_9">Table 4</ref> presents the average IoU (for the models that have an object pathway) for each object in the images in which YOLOv3 detected the given object. For each image in which YOLOv3 detected the given object, we calculated the IoU between the predicted bounding box and the ground truth bounding box for the given object. In the cases in which either an image contains multiple instances of the given object (i.e. multiple different bounding boxes for this object were given to the generator) or YOLOv3 detects the given object multiple times we used the maximum IoU between all predicted and ground truth bounding boxes for our statistics.   <ref type="table" target="#tab_9">Table 4</ref> summarizes the results with the 30 tested labels. We can observe that the StackGAN with object pathway outperforms the original StackGAN when comparing the recall of the YOLOv3 network, i.e. in how many images with a given label the YOLOv3 network actually detected the given object. The recall of the original StackGAN is higher than 10% for 26.7% of the labels, while our StackGAN with object pathway results in a recall greater than 10% for 60% of the labels. The IoU is greater than 0.3 for every label, while 86.7% of the labels result an IoU of greater than 0.5 (original images: 100%) and 30% have an IoU of greater than 0.7 (original images: 96.7%). This indicates that we can indeed control the location and identity of various objects within the generated images.</p><p>Compared to the StackGAN, the AttnGAN achieves a much greater recall, with 80% and 83.3% of the labels having a recall of greater than 10% for the original AttnGAN and the AttnGAN with object pathway respectively. The difference in recall values between the original AttnGAN and the AttnGAN with object pathway is also smaller, with our AttnGAN having a higher (lower) recall than the original AttnGAN (we only count cases where the difference is at least 5%) in 26.7% (13.3%) of the labels. The average IoU, on the other hand, is a lot smaller for the AttnGAN than for the StackGAN. We only achieve an IoU greater than 0.3 (0.5, 0.7) for 53.3% (3.3%, 0%) of the labels. As mentioned in the discussion (subsection 4.4), we attribute this to the observation that the AttnGAN tends to place seemingly recognizable features of salient objects at arbitrary locations throughout the image. This might attribute to the overall higher recall but may negatively affect the IoU.</p><p>Overall, these results further confirm our previous experiments and highlight that the addition of the object pathway to the different models does not only enable the direct control of object location and identity but can also help to increase the image quality. The increase in image quality is supported by a higher Inception Score, lower Fréchet Inception Distance (for StackGAN) and a higher performance of the YOLOv3 network in detecting objects within generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>Published as a conference paper at ICLR 2019  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Both the generator and the discriminator of our model consist of a global and an object pathway. The global pathway focuses on global image characteristics, such as the background, while the object pathway is responsible for modeling individual objects at their specified location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Multi-MNIST images generated by the model. Training included only images with three individual normal-sized digits. Highlighted bounding boxes and yellow ground truth for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Images from the CLEVR data set. The left image of each pair shows the rendered image according to specific attributes. The right image of each pair is the image generated by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples of images generated from the given caption from the MS-COCO data set. A) shows the original images and the respective image captions, B) shows images generated by our StackGAN+OP (with the corresponding bounding boxes for visualization), and C) shows images generated by the original StackGAN (Zhang et al., 2017) 3 a different model. As opposed to the StackGAN, the AttnGAN consists of only one model which is trained end-to-end on the image captions by making use of multiple, intermediate, discriminators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6</head><label></label><figDesc>https://github.com/hanzhanggit/StackGAN-Pytorch 13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Systematic test of digits over vertically different regions. Training set included three normal-sized digits only in the top half of the image. Highlighted bounding boxes and yellow ground truth for visualization. We can see that the model fails to generate recognizable digits once their location is too far in the bottom half of the image, as this location was never observed during training. C ADDITIONAL EXAMPLES OF MS-COCO RESULTS: STACKGAN Figure 7 shows results of text-to-image synthesis on the MS-COCO data set with the StackGAN architecture. Rows A show the original image and image caption, rows B show the images generated by our StackGAN + Object Pathway and the given bounding boxes for visualization, and rows C show images generated by the original StackGAN (pretrained model obtained from https: //github.com/hanzhanggit/StackGAN-Pytorch). The last block of examples (last row) show typical failure cases of our model, where there is no bounding box for the foreground object present.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>AttnGAN examples with random locations -refer to page 17 for more information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Distribution of recall and IoU values in the YOLOv3 object detection test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11</head><label>11</label><figDesc>visualizes how the IoU and recall values are distributed for the different models, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>StackGAN-V2 Zhang et al. (2018a)   256 × 256 8.30 ± 0.10 81.59StackGAN Zhang et al. (2018a)   256 × 256 8.45 ± 0.03 1 74.05PPGN Nguyen et al. (2017)   227 × 227 9.58 ± 0.21 ChatPainter (StackGAN)<ref type="bibr" target="#b20">Sharma et al. (2018)</ref> 256 × 256 9.74 ± 0.02 Semantic Layout<ref type="bibr" target="#b6">Hong et al. (2018b)</ref> 128 × 128 11.46 ± 0.09 2 HDGan<ref type="bibr" target="#b29">Zhang et al. (2018c)</ref> 256 × 256 11.86 ± 0.18 71.27 ± 0.12 3 AttnGAN<ref type="bibr" target="#b25">Xu et al. (2018b)</ref> 256 × 256 23.61 ± 0.21 4 33.10 ± 0.11 3 Recently updated to 10.62 ± 0.19 in its source code. 2 When using the ground truth bounding boxes at test time (as we do) the IS increases to 11.94 ± 0.09. 3 FID score was calculated with samples generated with the pretrained model provided by the authors.</figDesc><table><row><cell>Published as a conference paper at ICLR 2019</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Resolution IS ↑</cell><cell>FID ↓</cell></row><row><cell>GAN-INT-CLS Reed et al. (2016b)</cell><cell>64 × 64</cell><cell>7.88 ± 0.07</cell><cell>60.62</cell></row><row><cell>StackGAN + Object Pathways (Ours) 5</cell><cell cols="3">256 × 256 12.12 ± 0.31 55.30 ± 1.78</cell></row><row><cell>AttnGAN + Object Pathways (Ours)</cell><cell cols="3">256 × 256 24.76 ± 0.43 33.35 ± 1.15</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell></row></table><note>14</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overview of the individual layers used in our networks to generate images of resolution 64 × 64 / 256 × 256 pixels. Values in brackets (C, H, W ) represent the tensor's shape. Numbers in the columns after convolutional, residual, or dense layers describe the number of filters / units in that layer. (fs=x, s=y, p=z) describes filter size, stride, and padding for that convolutional / residual layer. 16Published as a conference paper at ICLR 2019B ADDITIONAL EXAMPLES OF MULTI-MNIST RESULTS: TRAINING AND TEST SET OVER COMPLEMENTARY REGIONS</figDesc><table><row><cell>Optimizer</cell><cell></cell><cell cols="2">Adam (beta 1 = 0.5, beta 2 = 0.999)</cell><cell></cell></row><row><cell>Learning Rate</cell><cell>0.0002</cell><cell>0.0002</cell><cell>0.0002</cell><cell>0.0002</cell></row><row><cell>Schedule: halve every x epochs</cell><cell>10</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>Training Epochs</cell><cell>20</cell><cell>40</cell><cell>120</cell><cell>110</cell></row><row><cell>Batch Size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>40</cell></row><row><cell>Weight Initialization</cell><cell>N (0, 0.02)</cell><cell>N (0, 0.02)</cell><cell>N (0, 0.02)</cell><cell>N (0, 0.02)</cell></row><row><cell>Z-Dim / Img-Caption-Dim</cell><cell>100 / 10</cell><cell>100 / 13</cell><cell>100 / 128</cell><cell>100 / 128</cell></row><row><cell>Generator</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv (fs=3, s=1, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>192</cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>384</cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>768</cell></row><row><cell>Concat with image caption and bbox labels</cell><cell></cell><cell></cell><cell cols="2">(1024, 16, 16)</cell></row><row><cell>Conv (fs=3, str=1, pad=1)</cell><cell></cell><cell></cell><cell></cell><cell>768</cell></row><row><cell>4 × Res. (fs=3, s=1, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>768</cell></row><row><cell>Object Pathway</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>O G Shape</cell><cell cols="3">(256, 16, 16) (192, 16, 16) (384, 16, 16)</cell><cell>(192, 64, 64)</cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>512</cell><cell>384</cell><cell>768</cell><cell>384</cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>256</cell><cell>192</cell><cell>384</cell><cell>192</cell></row><row><cell>Output Shape</cell><cell cols="3">(256, 16, 16) (192, 16, 16) (384, 16, 16)</cell><cell>(192, 64, 64)</cell></row><row><cell>Global Pathway</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layout Encoding</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv (fs=3, s=2, p=1)</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell></cell></row><row><cell>Conv (fs=3, s=2, p=1)</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell></cell></row><row><cell>Conv (fs=3, s=2, p=1)</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell></cell></row><row><cell>Dense Layer Units</cell><cell>16,384</cell><cell>12,288</cell><cell>24,576</cell><cell></cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>512</cell><cell>384</cell><cell>768</cell><cell>384</cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>256</cell><cell>192</cell><cell>384</cell><cell>192</cell></row><row><cell>Output Shape</cell><cell>(256, 16, 16)</cell><cell cols="2">192, 16, 16) (384, 16, 16)</cell><cell>(192, 64, 64)</cell></row><row><cell>Concat outputs of object and global pathways</cell><cell cols="3">(512, 16, 16) (384, 16, 16) (768, 16, 16)</cell><cell>(384, 64, 64)</cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>128</cell><cell>96</cell><cell>192</cell><cell>96</cell></row><row><cell>Upsample (fs=3, s=1, p=1)</cell><cell>64</cell><cell>48</cell><cell>96</cell><cell>48</cell></row><row><cell>Conv (fs=3, s=1, p=1)</cell><cell>1</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>Generator Output</cell><cell>(1, 64, 64)</cell><cell>(3, 64, 64)</cell><cell>(3, 64, 64)</cell><cell>(3, 256, 256)</cell></row><row><cell>Discriminator</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Object Pathway</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>O D Shape</cell><cell>(128, 16, 16)</cell><cell>(96, 16, 16)</cell><cell>(192, 16, 16)</cell><cell>(192, 32, 32)</cell></row><row><cell>Conv (fs=4, s=1, p=1)</cell><cell>128</cell><cell>96</cell><cell>192</cell><cell>192</cell></row><row><cell>Conv (fs=4, s=1, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>192</cell></row><row><cell>Output Shape</cell><cell>(128, 16, 16)</cell><cell>(96, 16, 16)</cell><cell>(192, 16, 16)</cell><cell>(192, 32, 32)</cell></row><row><cell>Global Pathway</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell>64</cell><cell>48</cell><cell>96</cell><cell>96</cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell>128</cell><cell>96</cell><cell>192</cell><cell>192</cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>384</cell></row><row><cell>Output Shape</cell><cell>(128, 16, 16)</cell><cell>(96, 16, 16)</cell><cell>(192, 16, 16)</cell><cell>(384, 32, 32)</cell></row><row><cell>Concat outputs of object and global pathways</cell><cell cols="3">(256, 16, 16) (192, 16, 16) (384, 16, 16)</cell><cell>(576, 32, 32)</cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell>256</cell><cell>192</cell><cell>384</cell><cell>768</cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell>512</cell><cell>384</cell><cell>768</cell><cell>1,536</cell></row><row><cell>Conv (fs=4, s=2, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>3,072</cell></row><row><cell>Conv (fs=3, s=1, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>1,536</cell></row><row><cell>Conv (fs=3, s=1, p=1)</cell><cell></cell><cell></cell><cell></cell><cell>768</cell></row><row><cell>Concat with conditioning vector</cell><cell>(522, 4, 4)</cell><cell>(397, 4, 4)</cell><cell>(896, 4, 4)</cell><cell>(896, 4, 4)</cell></row><row><cell>Conv (fs=3, s=1, p=1)</cell><cell>512</cell><cell>384</cell><cell>768</cell><cell>768</cell></row><row><cell>Conv (fs=4, s=4, p=0)</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The first six examples show images generated by our StackGAN where we changed the location and size of the respective bounding boxes. The last three examples show failure cases in which we changed the location of the bounding boxes to "unusual" locations. For the image with the child on the bike, we put the bounding box of the bike somewhere in the top half of the image and the bounding box for the child somewhere in the bottom part. Similarly, for the man sitting on a bench, we put the bench in the top and the man in the bottom half of the image. Finally, for the image depicting a pizza on a plate, we put the plate location in the top half of the image and the pizza in the bottom half. D ADDITIONAL EXAMPLES OF MS-COCO RESULTS: ATTNGAN Figure 8 shows results of text-to-image synthesis on the MS-COCO data set with the AttnGAN architecture. Rows A show the original image and image caption, rows B show the images generated by our AttnGAN + Object Pathway and the given bounding boxes for visualization, and rows C show images generated by the original AttnGAN (pretrained model obtained from https: //github.com/taoxugit/AttnGAN). The last block of examples (last row) show typical failure cases, in which the model does generate the appropriate object within the bounding box, but also places the same object at multiple other locations within the image. Similarly as for StackGAN,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>StackGAN examples with random locations -refer to page 17 for more information.</figDesc><table><row><cell></cell><cell>Adjacent computer</cell><cell>A young child holding</cell><cell>A sandwich with</cell></row><row><cell>A</cell><cell>screens near the keyboard show</cell><cell>onto a kite while standing on a green</cell><cell>meat, vegetables and dressing is</cell></row><row><cell></cell><cell>different displays</cell><cell>grass covered field</cell><cell>sitting on a plate</cell></row><row><cell>B</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>A group of people</cell><cell>A red double decker</cell><cell>A herd of zebra</cell></row><row><cell>A</cell><cell>standing on top of a</cell><cell>bus on the street</cell><cell>running around</cell></row><row><cell></cell><cell>snow covered slope</cell><cell>next to a car</cell><cell>a dirt field</cell></row><row><cell>B</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>A little boy riding</cell><cell>A young man</cell><cell>A big slice of</cell></row><row><cell>A</cell><cell>his bike and</cell><cell>sitting on top of</cell><cell>cheese pizza</cell></row><row><cell></cell><cell>wearing a helmet</cell><cell>a white bench</cell><cell>on a white plate</cell></row><row><cell>B</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B A</cell><cell>Figure 9: A desk with several A man with a nametag in a suit and tie and two women holding glasses on each side of him</cell><cell>A group of sheep A busy road in London shows several red busses and smaller cars as pedes-trians walk next to them</cell><cell>An open lap top A man kneeling down in the snow next to his small son on skis</cell></row><row><cell>A</cell><cell>monitors under it and two computers and a</cell><cell>walking down a path with a few stopping to</cell><cell>computer on a wooden desk and two note</cell></row><row><cell></cell><cell>laptop on top of the desk</cell><cell>eat grass along the side</cell><cell>pads also on the desk</cell></row><row><cell>B</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>Two sandwiches on whole wheat bread filled with meat cheddar cheese slices alfalfa sprouts and green leafy lettuce</cell><cell>A man on a path with a child on his back walking two dogs with other people in the background</cell><cell>A man a motorcycle that is on a road that has grass fields on both sides and a stop sign</cell></row><row><cell>B</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Words that were used to identify given labels in the image caption for the YOLOv3 object detection test.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.101 .786 ± .004 .444 .660 ± .054 .395 ± .016 Horse .933 .842 .129 .330 ± .048 .585 ± .039 .532 .619 ± .027 .300 ± .006 Giraffe .972 .857 .173 .467 ± .035 .606 ± .030 .472 .650 ± .084 .365 ± .030 Toilet .898 .826 .005 .122 ± .021 .690 ± .010 .201 .220 ± .021 .224 ± .011 Bear .381 .859 .015 .120 ± .018 .720 ± .036 .319 .303 ± .028 .357 ± .010 Bench .828 .798 .001 .030 ± .008 .627 ± .034 .094 .094 ± .031 .308 ± .018 Umbrella .912 .762 .001 .023 ± .009 .578 ± .030 .060 .063 ± .017 .154 ± .053 Elephant .940 .867 .060 .414 ± .069 .688 ± .033 .350 .500 ± .141 .353 ± .006 Chair .757 .755 .014 .039 ± .004 .488 ± .039 .070 .093 ± .005 .225 ± .001 Zebra .972 .875 .732 .781 ± .023 .686 ± .017 .870 .766 ± .063 .315 ± .022 Boat .795 .709 .077 .010 ± .011 .594 ± .021 .168 .202 ± .027 .206 ± .020 Bird .837 .781 .059 .097 ± .027 .500 ± .066 .322 .357 ± .042 .250 ± .020 Aeroplane .912 .812 .125 .223 ± .043 .667 ± .026 .499 .415 ± .010 .320 ± .035 Bicycle .825 .760 .007 .053 ± .020 .558 ± .052 .170 .191 ± .013 .233 ± .024 Surfboard .873 .780 .030 .067 ± .019 .459 ± .056 .104 .110 ± .025 .143 ± .016 Kite .772 .633 .029 .057 ± .028 .426 ± .086 .260 .162 ± .068 .120 ± .018 Truck .887 .832 .082 .243 ± .062 .717 ± .022 .378 .367 ± .027 .393 ± .019 Stop Sign .527 .874 .001 .261 ± .057 .780 ± .011 .070 .124 ± .048 .101 ± .014 TV Monitor .818 .833 .037 .264 ± .005 .765 ± .016 .529 .435 ± .314 .243 ± .066 Sofa .878 .794 .012 .087 ± .024 .628 ± .044 .170 .191 ± .057 .329 ± .028 Sandwich .792 .796 .045 .139 ± .049 .628 ± .014 .340 .370 ± .054 .318 ± .031 Sheep .943 .727 .004 .091 ± .006 .460 ± .011 .250 .304 ± .037 .116 ± .022</figDesc><table><row><cell>Label</cell><cell cols="2">Orig. Img. StackGAN Recall IoU Recall</cell><cell>StackGAN + OP Recall IoU</cell><cell>AttnGAN Recall</cell><cell>AttnGAN + OP Recall IoU</cell></row><row><cell>Person</cell><cell>.943 .824</cell><cell>.355</cell><cell>.451 ± .019 .624 ± .012</cell><cell>.598</cell><cell>.610 ± .008 .276 ± .006</cell></row><row><cell cols="2">Dining table .355 .774</cell><cell>.007</cell><cell>.022 ± .004 .734 ± .011</cell><cell>.069</cell><cell>.045 ± .022 .490 ± .018</cell></row><row><cell>Car</cell><cell>.433 .792</cell><cell>.012</cell><cell>.047 ± .007 .622 ± .020</cell><cell>.006</cell><cell>.063 ± .010 .144 ± .043</cell></row><row><cell>Cat</cell><cell>.715 .821</cell><cell>.021</cell><cell>.104 ± .100 .622 ± .008</cell><cell>.423</cell><cell>.430 ± .066 .350 ± .012</cell></row><row><cell>Dog</cell><cell>.703 .819</cell><cell>.068</cell><cell>.150 ± .007 .601 ± .004</cell><cell>.450</cell><cell>.488 ± .048 .311 ± .007</cell></row><row><cell>Bus</cell><cell>.747 .877</cell><cell>.161</cell><cell>.393 ± .031 .794 ± .009</cell><cell>.352</cell><cell>.416 ± .032 .374 ± .006</cell></row><row><cell>Train</cell><cell>.900 .835</cell><cell>.133</cell><cell>.310 ± .033 .700 ± .007</cell><cell>.393</cell><cell>.438 ± .110 .355 ± .036</cell></row><row><cell>Bed</cell><cell>.775 .789</cell><cell>.032</cell><cell>.141 ± .018 .701 ± .001</cell><cell>.539</cell><cell>.552 ± .030 .505 ± .002</cell></row><row><cell>Pizza</cell><cell>.912 .842</cell><cell>.119</cell><cell>.485 ±</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results of YOLOv3 detections on generated and original images. Recall provides the fraction of images in which YOLOv3 detected the given object. IoU (Intersection over Union) measures the maximum IoU per image in which the given object was detected.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code can be found here: https://github.com/tohinz/multiple-objects-gan</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Generated with the model from: https://github.com/hanzhanggit/StackGAN-Pytorch 4 Examples of images generated by the AttnGAN+OP can be seen in the Appendix in Figures 8 and 10.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">See Appendix for more details on the procedure and the exact results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Downloaded from https://github.com/reedscot/icml2016</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/taoxugit/AttnGAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2019Figure 7: Additional StackGAN examples -refer to page 17 for information about the figure.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Pretrained weights from the author, acquired via: https://pjreddie.com/darknet/yolo/21Published as a conference paper at ICLR 2019</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors gratefully acknowledge partial support from the German Research Foundation DFG under project CML (TRR 169) and the European Union under project SECURE (No 642667). We also thank the NVIDIA Corporation for their support through the GPU Grant Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3225" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hierarchical semantic image manipulation through structured representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2712" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to generate images of outdoor scenes from attributes and semantic layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00215</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating images from captions with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compositional generation of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cusuh</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems ViGIL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Aäron van den Oord, Nal Kalchbrenner, Victor Bapst, Matt Botvinick, and Nando de Freitas. Generating interpretable images with controllable structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chatpainter: Improving text to image generation using dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dendi</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01110</idno>
		<title level="m">Generating abstract scenes from textual descriptions</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep structured generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stackgan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text-to-image synthesis via visual-memory creative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Photographic text-to-image synthesis with a hierarchicallynested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6199" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
