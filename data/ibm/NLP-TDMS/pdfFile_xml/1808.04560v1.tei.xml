<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WEI, WANG, YANG, LIU: DEEP RETINEX DECOMPOSITION Deep Retinex Decomposition for Low-Light Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
							<email>weichen582@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
							<email>yangwenhan@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
							<email>liujiaying@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WEI, WANG, YANG, LIU: DEEP RETINEX DECOMPOSITION Deep Retinex Decomposition for Low-Light Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deep Retinex-Net learned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjustment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.</p><p>* Indicates equal contributions. â€  Corresponding author.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Insufficient lighting in image capturing can significantly degrade the visibility of images. The lost details and low contrast not only cause unpleasant subjective feelings, but also hurt the performance of many computer vision systems which are designed for normal-light images. There are a lot of causes for insufficient lighting, such as low-light environment, limited performance of photography equipment, and inappropriate configurations for the equipment.</p><p>To make the buried details visible, improve the subjective experience and usability of current computer vision systems, low-light image enhancement is demanded.</p><p>In the past decades, many researchers have devoted their attention to solving the problem of low-light image enhancement. Many techniques have been developed to improve the subjective and objective quality of low-light images. Histogram equalization (HE) <ref type="bibr" target="#b19">[20]</ref> and its variants restrain the histograms of the output images to meet some constraints. Dehazing based method <ref type="bibr" target="#b4">[5]</ref> utilizes the inverse connection between the images with insufficient illumination and those in hazy environments.</p><p>Another category of low-light enhancement methods is built on Retinex theory <ref type="bibr" target="#b11">[12]</ref>, which assumes the observed color image can be decomposed into reflectance and illumination. Single-scale Retinex (SSR) <ref type="bibr" target="#b10">[11]</ref> constrains the illumination map to be smooth by Gaussian filter as the early attempt. Multi-scale Retinex (MSRCR) <ref type="bibr" target="#b9">[10]</ref> extends SSR with multi-scale Gaussian filters and color restoration. <ref type="bibr" target="#b22">[23]</ref> proposes a method to preserve naturalness of illumination with lightness-order-error measure. Fu et al. <ref type="bibr" target="#b6">[7]</ref> proposed to fuse multiple derivations of the initially illumination map. SRIE <ref type="bibr" target="#b6">[7]</ref> estimates reflectance and illumination simultaneously using a weighted variational model. After manipulating the illumination, the target result can be restored. LIME <ref type="bibr" target="#b8">[9]</ref>, on the other hand, only estimates illumination with structure prior and uses reflection as the final enhanced results. There are also Retinex-based methods for joint low-light enhancement and noise removal <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Although these methods may produce promising results in some cases, they still suffer from the limitation in model capacity of the decomposition for reflectance and illumination. It is difficult to design well-working constraints for image decomposition that can be applied in various scenes. Besides, the manipulations on illumination map are also hand-crafted and the performance of these methods usually relies on careful parameter tuning.</p><p>With the rapid development of deep neural network, CNN has been widely used in lowlevel image processing, including super-resolution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, rain removal <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> et al. Lore et al. <ref type="bibr" target="#b16">[17]</ref> uses stacked sparse denoising auto-encoder for simultaneous low-light enhancement and noise reduction (LLNet), however the nature of low-light pictures is not taken into account.</p><p>To overcome these difficulties, we propose a data-driven Retinex decomposition method. A deep network, called as Retinex-Net, that integrates image decomposition and the successive enhancement operations is built. First, a subnetwork, Decom-Net is used to split the observed image into lighting-independent reflectance and structure-aware smooth illumination. The Decom-Net is learned with two constraints. First, low/normal-light images share the same reflectance. Second, the illumination map should be smooth but retain main structures, which is obtained by a structure-aware total variation loss. Then, another Enhance-Net adjusts the illumination map to maintain consistency at large regions while tailor local distributions by multi-scale concatenation. Since noise is often louder in dark regions and even amplified by the enhancement process, denoising on reflectance is introduced. For training such a network, we build a dataset of low/normal-light image pairs from real photography and synthetic images from RAW datasets. Extensive experiments demonstrate that our method not only achieves pleasing visual quality in low-light enhancement but also provides a good representation of image decomposition. The contributions of our work are summarized as follows:</p><p>â€¢ We build a large scale dataset with paired low/normal-light images captured in real scenes. As far as we know, it is the first attempt in the low-light enhancement field.</p><p>â€¢ We construct a deep-learning image decomposition based on Retinex model. The de-  <ref type="figure">Figure 1</ref>: The proposed framework for Retinex-Net. The enhancement process is divided into three steps: decomposition, adjustment and reconstruction. In the decomposition step, a subnetwork Decom-Net decomposes the input image into reflectance and illumination. In the following adjustment step, an encoder-decoder based Enhance-Net brightens up the illumination. Multi-scale concatenation is introduced to adjust the illumination from multi-scale perspectives. Noise on the reflectance is also removed at this step. Finally, we reconstruct the adjusted illumination and reflectance to get the enhanced result.</p><p>composition network is end-to-end trained with the successive low-light enhancement network, thus the framework is by nature good at light condition adjustment.</p><p>â€¢ We propose a structure-aware total variation constraint for deep image decomposition.</p><p>By mitigating the effect of total variation at the places where gradients are strong, the constraint successfully smooths the illumination map and retains the main structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Retinex-Net for Low-Light Enhancement</head><p>The classic Retinex theory models the human color perception. It assumes that the observed images can be decomposed into two components, reflectance and illumination. Let S represent the source image, then it can be denoted by</p><formula xml:id="formula_0">S = R â€¢ I,<label>(1)</label></formula><p>where R represents reflectance, I represents illumination and â€¢ represents element-wise multiplication. Reflectance describes the intrinsic property of captured objects, which is considered to be consistent under any lightness conditions. The illumination represents the various lightness on objects. On low-light images, it usually suffers from darkness and unbalanced illumination distributions. Motivated by Retinex theory, we design a deep Retinex-Net to perform the reflectance /illumination decomposition and low-light enhancement jointly. The network consists of three steps: decomposition, adjustment, and reconstruction. At the decomposition step, Retinex-Net decomposes the input image into R and I by a Decom-Net. It takes in pairs of low/normal-light images at the training stage, while only low-light images as input at the testing stage. With the constraints that the low/normal-light images share the same reflectance and the smoothness of illumination, Decom-Net learns to extract the consistent R between variously illuminated images in a data-driven way. At the adjustment step, an Enhance-Net is used to brighten up the illumination map. The Enhance-Net takes an overall framework of encoder-decoder. A multi-scale concatenation is used to maintain the global consistency of illumination with context information in large regions while tuning the local distributions with focused attention. Furthermore, the amplified noise, which often occurs in low-light conditions, is removed from reflectance if needed. Then, we combine the adjusted illumination and reflectance by element-wise multiplication at the reconstruction stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data-Driven Image Decomposition</head><p>One way to decompose the observed image is estimating reflectance and illumination directly on the low-light input image with elaborately hand-crafted constraints. Since Eq.(1) is highly ill-posed, it is not easy to design a proper constraint function adaptive to various scenes. Therefore, we try to address this problem in a data-driven way.</p><p>During the training stage, Decom-Net takes in paired low/normal-light images each time and learns the decomposition for both low-light and its corresponding normal-light image under the guidance that the low-light image and normal-light image share the same reflectance. Note that although the decomposition is trained with paired data, it can decompose the lowlight input individually in the testing phase. During training, there is no need to provide the ground truth of the reflectance and illumination. Only requisite knowledge including the consistency of reflectance and the smoothness of illumination map is embedded into the network as loss functions. Thus, the decomposition of our network is automatically learned from paired low/normal-light images, and by nature suitable for depicting the light variation among the images under different light conditions.</p><p>One thing to note is that although this problem may be similar to intrinsic image decomposition in form, they are different essentially. In our task, we do not need to obtain the actual intrinsic image accurately, but a good representation for light adjustment. Thus, we let the network learn to find the consistent component between low-light image and its corresponding enhanced result.</p><p>As illustrated in <ref type="figure">Fig. 1</ref>, Decom-Net takes the low-light image S low and the normal-light one S normal as input, then estimates the reflectance R low and the illumination I low for S low , as well as R normal and I normal for S normal , respectively. It first uses a 3 Ã— 3 convolutional layer to extract features from the input image. Then, several 3 Ã— 3 convolutional layers with Rectified Linear Unit (ReLU) as the activation function are followed to map the RGB image into reflectance and illumination. A 3 Ã— 3 convolutional layer projects R and I from feature space, and sigmoid function is used to constrain both R and I in the range of [0, 1].</p><p>The loss L consists of three terms: reconstruction loss L recon , invariable reflectance loss L ir , and illumination smoothness loss L is :</p><formula xml:id="formula_1">L = L recon + Î» ir L ir + Î» is L is ,<label>(2)</label></formula><p>where Î» ir and Î» is denote the coefficients to balance the consistency of reflectance and the smoothness of illumination.</p><p>Based on the assumption that both R low and R high can reconstruct the image with the corresponding illumination map, the reconstruction loss L recon is formulated as: Invariable reflectance loss L ir is introduced to constrain the consistency of reflectance:</p><formula xml:id="formula_2">L recon = âˆ‘ i=low,normal âˆ‘ j=low,normal Î» i j ||R i â€¢ I j âˆ’ S j || 1 . (3) (a) Input (b) Reflectance (c) Illumination (d) (f) (e) (g) (h)</formula><formula xml:id="formula_3">L ir = ||R low âˆ’ R normal || 1 .<label>(4)</label></formula><p>Illumination smoothness loss L is is described in detail in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structure-Aware Smoothness Loss</head><p>One basic assumption for illumination map is the local consistency and the structure-awareness, as mentioned by <ref type="bibr" target="#b8">[9]</ref>. In other words, a good solution for illumination map should be smooth in textural details while can still preserve the overall structure boundary. Total variation minimization (TV) <ref type="bibr" target="#b1">[2]</ref>, which minimizes the gradient of the whole image, is often used as smoothness prior for various image restoration tasks. However, directly using TV as loss function fails at regions where the image has strong structures or where lightness changes drastically. It is due to the uniform reduction for gradient of illumination map regardless of whether the region is of textual details or strong boundaries. In other words, TV loss is structure-blindness. The illumination is blurred and strong black edges are left on reflectance, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>To make the loss aware of the image structure, the original TV function is weighted with the gradient of reflectance map. The final L is is formulated as:</p><formula xml:id="formula_4">L is = âˆ‘ i=low,normal ||âˆ‡I i â€¢ exp(âˆ’Î» g âˆ‡R i )||,<label>(5)</label></formula><p>where âˆ‡ denotes the gradient including âˆ‡ h (horizontal) and âˆ‡ v (vertical), and Î» g denotes the coefficient balancing the strength of structure-awareness. With the weight exp(âˆ’Î» g âˆ‡R i ), L is loosens the constraint for smoothness where the gradient of reflectance is steep, in other words, where image structures locate and where the illumination should be discontinuous.</p><p>Although LIME <ref type="bibr" target="#b8">[9]</ref> also considers to keep image structures in illumination map with weighted TV constraint, we argue that the two methods are different. For LIME, the total variation constraint is weighted by an initial illumination map, which is the maximum intensity of each pixel in R, G and B channels. Our structure-aware smoothness loss instead is weighted by reflectance. The static initial estimation used in LIME may not depict the image structure as well as reflectance does, since reflectance is assumed as the physical property of an image. Since our Decom-Net is trained off-line with large-scale of data, the illumination and weight (the reflectance) can be updated simultaneously in training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Scale Illumination Adjustment</head><p>The illumination enhancement network takes an overall framework of an encoder-decoder architecture. To adjust the illumination from hierarchical perspectives, we introduce a multiscale concatenation, as shown in <ref type="figure">Fig. 1</ref>.</p><p>An encoder-decoder architecture obtains context information in large regions. The input image is successively down-sampled to a small scale, at which the network can have a perspective of the large-scale illumination distribution. This brings network the ability of adaptive adjustment. With large-scale illumination information, up-sampling blocks reconstruct local illumination distribution. Skip connections are introduced from a down-sampling block to its corresponding mirrored up-sampling block by element-wise summation, which enforces the network to learn residuals.</p><p>To adjust the illumination hierarchically, which means to maintain the consistency of global illumination while tailor the diverse local illumination distribution, a multi-scale concatenation is introduced. If there are M progressively up-sampling blocks, each of which extracts a C channel feature map, we resize these features at different scales by nearestneighbor interpolation to the final scale and concatenate them to a C Ã— M channel feature map. Then, by a 1 Ã— 1 convolutional layer, the concatenated features are reduced to C channels. A 3 Ã— 3 convolutional layer is followed to reconstruct the illumination mapÄ¨.</p><p>A down-sampling block consists of a convolutional layer with stride 2 and a ReLU. In the up-sampling block, a resize-convolutional layer is used. As demonstrated in <ref type="bibr" target="#b18">[19]</ref>, it can avoid checkerboard pattern of artifacts. Resize-convolutional layer consists of a nearest-neighbor interpolation operation, a convolutional layer with stride 1, and a ReLU.</p><p>The loss function L for Enhance-Net consists of the reconstruction loss L recon and the illumination smoothness loss L is . L recon means to produce a normal-lightÅœ, which is</p><formula xml:id="formula_5">L recon = ||R low â€¢ÃŽ âˆ’ S normal || 1 .<label>(6)</label></formula><p>L is is the same as Eq.(5) except thatÃŽ is weighted by gradient map of R low .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Denoising on Reflectance</head><p>In the decomposition step, several constraints are imposed to the network, one of which is the structure-aware smoothness of illumination map. When the estimated illumination map is smooth, details are all retained on the reflectance, including boosted noise. Therefore, we can operate denoising method on reflectance before reconstructing the output image with illumination map. Given that noise in dark regions is amplified according to the lightness intensity during the decomposition, we should use illumination-related denoising method. Our implementation is described in Sec. 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal Low</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>Although the low-light enhancement problem has been studied for decades, to the best of our knowledge, current publicly available datasets provide no paired low/normal-light images captured in real scenes. Several low-light enhancement works use datasets of High Dynamic Range (HDR) as an alternative, such as MEF dataset <ref type="bibr" target="#b17">[18]</ref>. However, these datasets are in small scale and contain limited scenes. Thus, they cannot be used to train a deep network.</p><p>To make it tractable to learn a low-light enhancement network from a large-scale dataset, we construct a new one consisting of two categories: real photography pairs and synthetic pairs from raw images. The first one captures the degradation features and properties in real cases.</p><p>The second plays a role in data augmentation, diversifying scenes and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Captured in Real Scenes</head><p>Our dataset, named LOw Light paired dataset (LOL), contains 500 low/normal-light image pairs. To the best of our knowledge, LOL is the first dataset containing image pairs taken from real scenes for low-light enhancement. Most low-light images are collected by changing exposure time and ISO, while other configurations of the cameras are fixed. We capture images from a variety of scenes, e.g., houses, campuses, clubs, streets. <ref type="figure" target="#fig_1">Fig. 3</ref> shows a subset of the scenes.</p><p>Since camera shaking, object movement, and lightness changing may cause misalignment between the image pairs, inspired by <ref type="bibr" target="#b0">[1]</ref>, a three-step method is used to eliminate such misalignments between the image pairs in our dataset. The implementation details can be found in the supplementary file. These raw images are resized to 400 Ã— 600 and converted to Portable Network Graphics format. The dataset will be available publicly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synthetic Image Pairs from Raw Images</head><p>To make synthetic images match the property of real dark photography, we analyze the illumination distribution of low-light images. We collect 270 low-light images from public MEF <ref type="bibr" target="#b17">[18]</ref>, NPE <ref type="bibr" target="#b22">[23]</ref>, LIME <ref type="bibr" target="#b8">[9]</ref>, DICM <ref type="bibr" target="#b12">[13]</ref>, VV 1 , and Fusion <ref type="bibr" target="#b2">[3]</ref> dataset, transform the images into YCbCr channel and calculate the histogram of Y channel. We also collect 1000 raw images from RAISE <ref type="bibr" target="#b3">[4]</ref> as normal-light images and calculate the histogram of Y channel in YCbCr. <ref type="figure">Fig. 4</ref> shows the result. Raw images contain more information than the converted results. When operating on raw images, all calculations used to generate pixel values are performed in one step on the base data, making the result more accurate. 1000 raw images in RAISE <ref type="bibr" target="#b3">[4]</ref> are used to synthesize low-light images. Interface provided by Adobe Lightroom is used and we try different kinds of parameters to make the histogram of Y channel fit the result in low-light images. Final parameter configuration can be found in the supplementary material. As shown in <ref type="figure">Fig. 4</ref>, the illumination distribution of synthetic images matches that of low-light images. Finally we resize these raw images to 400 Ã— 600 and convert them to Portable Network Graphics format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our LOL dataset mentioned in Sec. 3 with 500 image pairs is divided into 485 pairs for training and another 15 ones for evaluation. So the net-work is trained on 485 real-case image pairs as well as 1000 synthetic ones. The whole network is light-weighted since we empirically find it already enough for our purpose. The Decom-Net takes 5 convolutional layers with a ReLU activation between 2 conv-layers without ReLU. The Enhance-Net consists of 3 down-sampling blocks and 3 up-sampling ones. We first train the Decom-Net and the Enhance-Net, then fine-tune the network end-to-end using stochastic gradient descent (SGD) with back-propagation. Batch size is set to be 16 and patch-size to be 96 Ã— 96. Î» ir , Î» is and Î» g are set to 0.001, 0.1 and 10 respectively. When i = j, Î» i j is set to 0.001, and when i = j, Î» i j is set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decomposition Results</head><p>In <ref type="figure">Fig. 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We evaluate our approach on real-scene images from public LIME <ref type="bibr" target="#b8">[9]</ref>, MEF <ref type="bibr" target="#b17">[18]</ref>, and DICM <ref type="bibr" target="#b12">[13]</ref> dataset. LIME contains 10 testing images. MEF contains 17 image sequences with multiple exposure levels. DICM collected 69 images with commercial digital cameras. We compare our Retinex-Net with four state-of-the-art methods, including de-hazing based method (DeHz) <ref type="bibr" target="#b4">[5]</ref>, naturalness preserved enhancement algorithm (NPE) <ref type="bibr" target="#b22">[23]</ref>, simultaneous reflectance and illumination estimation algorithm (SRIE) <ref type="bibr" target="#b7">[8]</ref>, and illumination map estimation based (LIME) <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure" target="#fig_3">Fig. 6</ref> shows visual comparison on three natural images. More results can be found in the supplementary file. As shown in every red rectangle, our method brightens up the objects buried in dark lightness enough without overexposure, which benefits from the learningbased image decomposition method and the multi-scale tailored illumination map. Compared with LIME, our results are not partially over-exposed (see the leaves in Still lives and the outside leaves in Room). The objects have no dark edges, compared with DeHz, which benefits from the weighted TV loss term (see edges on the houses in Street).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Joint Low-Light Enhancement and Denoising</head><p>Considering the comprehensive performance, BM3D <ref type="bibr" target="#b2">[3]</ref> is used as the denoising operation in Retinex-Net. As noise is unevenly amplified on reflectance, we use a illumination relative strategy (see supplementary material). We compare our joint-denoising Retinex-Net with two methods, one is LIME with denoising post-processing, the other is JED <ref type="bibr" target="#b21">[22]</ref>, a recent joint low-light enhancement and denoising method. As shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, details are better preserved by Retinex-Net while LIME and JED blur the edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, a deep Retinex decomposition method is proposed, which can learn to decompose the observed image into reflectance and illumination in a data-driven way without the ground truth of decomposed reflectance and illumination. Subsequent light enhancement on illumination and denoising operations on reflectance are introduced. The decomposition network and low-light enhancement network are trained end-to-end. Experimental results show that our method produces visually pleasing enhancement results as well as a good representation of image decomposition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration for effectiveness of our reflectance gradient-weighted TV loss for illumination smoothness. The first row displays the input image (a), reflectance (b) and illumination (c) with weighted TV loss from left to right. The second row displays a zoom-in region where (d) is for input image, (e) and (f) are for R and I with weighted TV loss, (g) and (h) are for R and I with original TV loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Several examples for low/normal-light image pairs in LOL dataset. Objects and scenes captured in this dataset are diverse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Fitting results based on the histogram of Y channel in YCbCr. For clarity, the histogram in depicted in the form of curve graphs and the vertical axis is scaled in logarithmic domain. The horizontal axis represents the pixel value, noticing that Y channel ranges from 16 to 240.Low (b) R by LIME (c) I by LIME (a) Input (d) R by Retinex-Net (e) I by Retinex-Net Normal The decomposition results using our Decom-Net and LIME on Bookshelf in LOL dataset. In our results, the reflectance of the low-light image resembles the reflectance of the normal-light image except for the amplified noise in dark regions occurred in real scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The results using different methods on natural images: (top-to-bottom) Street from LIME dataset, Still lives from LIME dataset, and Room from MEF dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The joint denoising results using different methods on Wardrobe in LOL Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>we illustrate a low/normal-light image pairs in the evaluation set of our LOL dataset, as well as the reflectance and illumination map decomposed by Decom-Net and LIME. More examples are provided in the supplementary file. It is shown that our Decom-Net can extract underlying consistent reflectance from a pair of images under quite different light conditions in both textual and smooth regions. The reflectance of the low-light image resembles the reflectance of the normal-light image except for the amplified noise in dark regions occurred in real scenes. The illumination maps, on the other hand, portray the lightness and shadow on the image. Compared with our result, LIME has much illumination information left on the reflectance (see the shadow on the shelf).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/vonikakis/datasets</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">RenoirÄºc a dataset for real low-light image noise reductionÄ…Ã®</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication &amp; Image Representation</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="144" to="154" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An augmented lagrangian method for total variation video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramsin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshabeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kristofor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truong</forename><forename type="middle">Q</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3097" to="3111" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising with blockmatching and 3d filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6064</biblScope>
			<biblScope unit="page" from="354" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Raise: a raw images dataset for digital image forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Duc Tien Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Pasquini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Conotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Systems Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast efficient algorithm for enhancement of low lighting video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blind visual quality assessment for image super-resolution by convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fusion-based enhancing method for weakly illuminated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xiao Ping Zhang, and Xinghao Ding. A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2782" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">A</forename><surname>Zia-Ur. Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="76" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">A</forename><surname>Zia-Ur. Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="965" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint denoising and enhancement for low-light images via retinex model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mading</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Forum on Digital TV and Wireless Multimedia Communications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structure-revealing lowlight image enhancement via robust retinex model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mading</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2828" to="2841" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Erase or fill? deep joint recurrent rain removal and reconstruction in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adedotun</forename><surname>Kin Gwn Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumik</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment for multiexposure image fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3345</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deconvolution and checkerboard artifacts. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://distill.pub/2016/deconv-checkerboard/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive histogram equalization and its variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Philip</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Amburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trey</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ter Haar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Romeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision Graphics &amp; Image Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="368" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attentive generative adversarial network for raindrop removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint enhancement and denoising method via sequential decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mading</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Circuits and Systems (ISCAS)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Miao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3538" to="3586" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep edge guided recurrent residual learning for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5895" to="5907" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1357" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video super-resolution based on spatial-temporal recurrent residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reference guided deep super-resolution via manifold localized external compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifeng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
