<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Style Normalization and Restitution for Generalizable Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.comchenzhibo@ustc.edu.cnlz@robots.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Style Normalization and Restitution for Generalizable Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing fully-supervised person re-identification (ReID) methods usually suffer from poor generalization capability caused by domain gaps. The key to solving this problem lies in filtering out identity-irrelevant interference and learning domain-invariant person representations. In this paper, we aim to design a generalizable person ReID framework which trains a model on source domains yet is able to generalize/perform well on target domains. To achieve this goal, we propose a simple yet effective Style Normalization and Restitution (SNR) module. Specifically, we filter out style variations (e.g., illumination, color contrast) by Instance Normalization <ref type="figure">(IN)</ref>. However, such a process inevitably removes discriminative information. We propose to distill identity-relevant feature from the removed information and restitute it to the network to ensure high discrimination. For better disentanglement, we enforce a dual causality loss constraint in SNR to encourage the separation of identity-relevant features and identity-irrelevant features. Extensive experiments demonstrate the strong generalization capability of our framework. Our models empowered by the SNR modules significantly outperform the state-of-the-art domain generalization approaches on multiple widely-used person ReID benchmarks, and also show superiority on unsupervised domain adaptation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (ReID) aims at matching/identifying a specific person across cameras, times, and locations. It facilitates many applications and has attracted a lot of attention.</p><p>Abundant approaches have been proposed for supervised person ReID, where a model is trained and tested on different splits of the same dataset <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20]</ref>. They typically focus on addressing the challenge of geometric misalignment among images caused by diversity of poses/viewpoints. In general, they perform well on the  <ref type="figure">Figure 1</ref>: Illustration of motivation and our idea. Person images captured from different cameras and environments present style variations which result in domain gaps. We use style normalization (with Instance Normalization) to alleviate style variations. However, this also results in the loss of some discriminative (identity-relevant) information. We propose to further restitute such information from the residual of the original information and the normalized information for generalizable and discriminative person ReID. trained dataset but suffer from signicant performance degradation (poor generalization capability) when testing on a previously unseen dataset. There are usually style discrepancies across domains/datasets which hinder the achievement of high generalization capability. <ref type="figure">Figure 1</ref> shows some example images 1 from different ReID datasets. The person images are captured by different cameras under different environments (e.g., lighting, seasons). They present a large style discrepancy in terms of illumination, hue, color contrast and saturation, quality/resolution, etc. For a ReID system, we expect it to be able to identify the same person even captured in different environments, and distinguish between different people even if their appearance are similar. Both generalization and discrimination capabilities, although seemly conflicting with each other, are very important for robust ReID.</p><p>Considering the existence of domain gaps and poor generalization capability, fully-supervised approaches or settings are not practical for real-world widespread ReID system deployment, where the onsite manual annotation on the target domain data is expensive and hardly feasible. In recent years, some unsupervised domain adaptation (UDA) methods have been studied to adapt a ReID model from source to target domain <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b59">60]</ref>. UDA models update using unlabeled target domain data, emancipating the labelling efforts. However, data collection and model update are still required, adding additional cost.</p><p>We mainly focus on the more economical and practical domain generalizable person ReID. Domain generalization (DG) aims to design models that are generalizable to previously unseen domains <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45]</ref>, without having to access the target domain data and labels, and without requiring model updating. Most DG methods assume that the source and target domains have the same label space <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> and they are not applicable to ReID since the target domains for ReID typically have a different label space from the source domains. Generalizable person ReID is challenging which aims to achieve high discrimination capability on unseen target domain that may have large domain discrepancy. The study on domain generalizable ReID is rare <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19]</ref> and remains an open problem. Jia et al. <ref type="bibr" target="#b18">[19]</ref> and Zhou et al. <ref type="bibr">[75]</ref> integrate Instance Normalization <ref type="bibr">(IN)</ref> in the networks to alleviate the domain discrepancy due to appearance style variations. However, IN inevitably results in the loss of some discriminative features <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref>, hindering the achievement of high efficiency ReID.</p><p>In this paper, we aim to design a generalizable ReID framework which achieves both high generalization capability and discrimination capability. The key is to find a way to disentangle the identity-relevant features and the identityirrelevant features (e.g., image styles). <ref type="figure">Figure 1</ref> illustrates our main idea. Considering the domain gaps among image samples, we perform style normalization by means of IN to eliminate style variations. However, the normalization inevitably discards some discriminative information and thus may hamper the ReID performance. From the residual information (which is the difference between the original information and the normalized information), we further distill the identity-relevant information as a compensation to the normalized information. <ref type="figure">Figure 2</ref> shows our framework with the proposed Style Normalization and Restitution (SNR) modules embedded. To better disentangle the identity-relevant features from the residual, a dual causality loss constraint is added by ensuring the features after restitution of identity-relevant features to be more discriminative, and the features after compensation of identityirrelevant features to be less discriminative.</p><p>We summarize our main contributions as follows: • We propose a practical domain generalizable person ReID framework that generalizes well on previously unseen domains/datasets. Particularly, we design a Style Normalization and Restitution (SNR) module. SNR is simple yet effective and can be used as a plug-and-play module for existing ReID architectures to enhance their generalization capabilities. • To facilitate the restitution of identity-relevant features from those discarded in the style normalization phase, we introduce a dual causality loss constraint in SNR for better feature disentanglement. We validate the effectiveness of the proposed SNR module on multiple widely-used benchmarks and settings. Our models significantly outperform the state-of-the-art domain generalizable person ReID approaches and can also boost the performance of unsupervised domain adaptation for ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised Person ReID. In the last decade, fullysupervised person ReID has achieved great progress, especially for deep learning based approaches <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b66">67]</ref>. These methods usually perform well on the testing set of the source datasets but generalize poorly to previously unseen domains/datasets due to the style discrepancy across domains. This is problematic especially in practical applications, where the target scenes typically have different styles from the source domains and there is no readily available target domain data or annotation for training. Unsupervised Domain Adaptation (UDA) for Person ReID. When the target domain data is accessible, even without annotations, it can be explored for the domain adaptation for enhancing the ReID performance. This requires target domain data collection and model updating. UDAbased ReID methods can be roughly divided into three categories: style transfer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b34">35]</ref>, attribute recognition <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b41">42]</ref>, and target-domain pseudo label estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b63">64]</ref>. For pseudo label estimation, recently, Yu et al. propose a method called multilabel reference learning (MAR) which evaluates the similarity of a pair of images by comparing them to a set of known reference persons to mine hard negative samples <ref type="bibr" target="#b63">[64]</ref>.</p><p>Our proposed domain generalizable SNR module can also be combined with the UDA methods (e.g., by plugging into the UDA backbone) to further enhance the ReID performance. We will demonstrate its effectiveness by combining it with the UDA approach of MAR in Subsection 4.5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalization (DG). Domain Generalization is</head><formula xml:id="formula_0">෨ + = ෨ + + ෨ + ෨ ෨ + = ෨ + + ෨ − = ෨ + − Conv Block 3 Conv Block 2 ෨ + ෨ Dual</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causality Loss</head><p>Dual Causality Loss Recently, a strong baseline for domain generalizable person ReID is proposed by simply combing multiple source datasets and training a single CNN <ref type="bibr" target="#b23">[24]</ref>. Song et al. <ref type="bibr" target="#b44">[45]</ref> propose a generalizable person ReID framework by using a meta-learning pipeline to make the model domain invariant. To overcome the inconsistency of label spaces among different datasets, it maintains a training datasets shared memory bank. Instance Normalization (IN) has been widely used in image style transfer <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b51">52]</ref> and proved that it actually performs a kind of style normalization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17]</ref>. Jia et al. <ref type="bibr" target="#b18">[19]</ref> and Zhou et al. <ref type="bibr">[75]</ref> apply this idea to ReID to alleviate the domain discrepancy and boost the generalization capability. However, IN inevitably discards some discriminative information. In this paper, we study how to design a generalibale ReID framework that can exploit the merit of IN while avoiding the loss of discriminative information.</p><formula xml:id="formula_1">(a) (b) (c) − ෨ − = ෨ + −</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Generalizable Person ReID</head><p>We aim at designing a generalizable and robust person ReID framework. During the training, we have access to one or several annotated source datasets. The trained model will be deployed directly to unseen domains/datasets and is expected to work well with high generalization capability. <ref type="figure">Figure 2</ref> shows the overall flowchat of our framework. Particularly, we propose a Style Normalization and Restitution (SNR) module to boost the generalization and discrimination capability of ReID models especially on unseen domains. SNR can be used as a plug-and-play module for existing ReID networks. Taking the widely used ReID network of ResNet-50 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37]</ref> as an example (see <ref type="figure">Figure</ref> 2(a)), SNR module is added after each convolutional block. In the SNR module, we first eliminate style discrepancy among samples by Instance Normalization <ref type="figure">(IN)</ref>. Then, a dedicated restitution step is proposed to distill identityrelevant (discriminative) features from those previsouly discarded by IN, and add them to the normalized features. Moreover, for the SNR module, we design a dual causality loss constraint to facilitate the distillation of identityrelevant features from the information discarded by IN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Style Normalization and Restitution (SNR)</head><p>Person images for ReID could be captured by different cameras under different scenes and environments (e.g., indoor/outdoors, shopping malls, street, sunny/cloudy). As shown in <ref type="figure">Figure 1</ref>, they present style discrepancies (e.g., in illumination, hue, contrast, saturation, quality), especially for samples from two different datasets/domains. Domain discrepancy between the source and target domain generally hinders the generalization capability of ReID models.</p><p>A learning-theoretic analysis shows that reducing dissimilarity improves the generalization ability on new domains <ref type="bibr" target="#b39">[40]</ref>. Instance Normalization (IN) performs some kinds of style normalization which reduces the discrepancy/dissimilarity among instances/samples <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref>, so it can enhance the generalization ability of networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">75]</ref>. However, IN inevitably removes some discriminative information and results in weaker discrimination capability <ref type="bibr" target="#b40">[41]</ref>. To address this problem, we propose to restitute the task-specific discriminative features from the IN removed information, by disentangling it into identity-relevant features and identity-irrelevant features with a dual causality loss constraint (see <ref type="figure">Figure 2</ref>(b)). We elaborate on the designed SNR module hereafter.</p><p>For an SNR module, we denote the input (which is a feature map) by F ∈ R h×w×c and the output by F + ∈ R h×w×c , where h, w, c denote the height, width, and number of channels, respectively. Style Normalization Phase. In SNR, we first try to reduce the domain discrepancy on the input features by performing Instance Normalization <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17]</ref> as</p><formula xml:id="formula_2">F = IN(F ) = γ( F − µ(F ) σ(F ) ) + β,<label>(1)</label></formula><p>where µ(·) and σ(·) denote the mean and standard deviation computed across spatial dimensions independently for each channel and each sample/instance, γ, β ∈ R c are parameters learned from data. IN could filter out some instancespecific style information from the content. With IN taking place in the feature space, Huang et al. <ref type="bibr" target="#b16">[17]</ref> have argued and experimentally shown that IN has more profound impacts than a simple contrast normalization and it performs a form of style normalization by normalizing feature statistics. Style Restitution Phase. IN reduces style discrepancy and boosts the generalization capability. However, with the mathematical operations being deterministic and taskirrelevant, it inevitably discards some discriminative (taskrelevant) information for ReID. We propose to restitute the identity-relevant feature to the network by distilling it from the residual feature R. R is defined as</p><formula xml:id="formula_3">R = F − F ,<label>(2)</label></formula><p>which denotes the difference between the original input feature F and the style normalized feature F . Given R, we further disentangle it into two parts: identity-relevant feature R + ∈ R h×w×c and identityirrelevant feature R − ∈ R h×w×c through masking R by a learned channel attention vector a = [a 1 , a 2 , · · · , a c ] ∈ R c : </p><formula xml:id="formula_4">R</formula><p>where R(:, :, k) ∈ R h×w denotes the k th channel of feature map R, k = 1, 2, · · · , c. We expect the channel attention vector a to enable the adaptive distillation of the identityrelevant features for restitution, and derive it by SE-like <ref type="bibr" target="#b15">[16]</ref> channel attention as</p><formula xml:id="formula_6">a = g(R) = σ(W 2 δ(W 1 pool(R))),<label>(4)</label></formula><p>which consists of a global average pooling layer followed by two FC layers that are parameterized by W 2 ∈ R (c/r)×c and W 1 ∈ R c×(c/r) which are followed by ReLU activation function δ(·) and sigmoid activation function σ(·), respectively. To reduce the number of parameters, a dimension reduction ratio r is used and is set to 16.</p><p>By adding the distilled identity-relevant feature R + to the style normalized feature F , we obtain the output feature F + of the SNR module as</p><formula xml:id="formula_7">F + = F + R + .<label>(5)</label></formula><p>Dual Causality Loss Constraint. In order to facilitate the disentanglement of identity-relevant feature and identityirrelevant feature, we design a dual causality loss constraint by comparing the discrimination capability of features before and after the restitution. As illustrated in <ref type="figure">Figure 2</ref>(c), the main idea is that: after restituting the identity-relevant feature R + to the normalized feature F , the feature becomes more discriminative; On the other hand, after restituting the identity-irrelevant feature R − to the normalized feature F , the feature should become less discriminative. We achieve this by defining a dual causality loss L SN R which consists of clarification loss L + SN R and destruction loss L − SN R , i.e., L SN R = L + SN R + L − SN R . Within a mini-batch, we sample three images, i.e., an anchor sample a, a positive sample p that has the same identity as the anchor sample, and a negative sample n that has a different identity from the anchor sample. For simplicity, we differentiate the three samples by subscript. For example, the style normalized feature of sample a is denoted by F a .</p><p>Intuitively, adding the identity-relevant feature R + to the normalized feature F , which we refer to as enhanced feature F + = F + R + , results in better discrimination capability -the sample features with same identities are closer and those with different identities are farther apart. We calculate the distances between samples on a spatially average pooled feature to avoid the distraction caused by spatial misalignment among samples (e.g., due to different poses/viewpoints). We denote the spatially average pooled feature of F and F + as f = pool( F ), f + = pool( F + ), respectively. The clarification loss is thus defined as</p><formula xml:id="formula_8">L + SN R = Sof tplus(d( f + a , f + p ) − d( f a , f p )) + Sof tplus(d( f a , f n ) − d( f + a , f + n )),<label>(6)</label></formula><p>where d(x, y) denotes the distance between x and y which is defined as d(x, y) = 0.5 − x T y/(2 x y ). Sof tplus(·) = ln(1 + exp(·)) is a monotonically increasing function that aims to reduce the optimization difficulty by avoiding negative loss values.</p><p>On the other hand, we expect that the adding of the identity-irrelevant feature R − to the normalized feature F , which we refer to as contaminated feature F − = F + R − , could decrease the discrimination capability. In comparison with the normalized feature F before the compensation, we expect that adding R − would push the sample features with same identities farther apart and pull those with different identities closer. We denote the spatially average pooled feature of F − as f − = pool( F − ). The destruction loss is:</p><formula xml:id="formula_9">L − SN R = Sof tplus(d( f a , f p ) − d( f − a , f − p )) + Sof tplus(d( f − a , f − n ) − d( f a , f n )).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Training</head><p>We use the commonly used ResNet-50 as a base ReID network and insert the proposed SNR module after each convolution block (in total four convolution blocks/stages)(see <ref type="figure">Figure 2</ref>(a)). We train the entire network in an end-to-end manner. The overall loss is</p><formula xml:id="formula_10">L = L ReID + 4 b=1 λ b L b SN R ,<label>(8)</label></formula><p>where L b SN R denotes the dual causality loss for the b th SNR module. L ReID denotes the widely-used ReID Loss (classification loss <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b8">9]</ref>, and triplet loss with batch hard mining <ref type="bibr" target="#b13">[14]</ref>) on the ReID feature vectors. λ b is a weight which controls the relative importance of the regularization at stage b. In considering that the features of stage 3 and 4 are more relevant to the task (high-level semantics), we experimentally set λ 3 , λ 4 to 0.5, and λ 1 ,λ 2 to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the datasets and evaluation metrics in Subsection 4.1. Then, for generalizable ReID, we validate the effectiveness of SNR in Subsection 4.2 and study its design choices in Subsection 4.3. We conduct visualization analysis in Subsection 4.4. Subsection 4.5 shows the comparisons of our schemes with the stateof-the-art approaches for both generalizable person ReID and unsupervised domain adapation ReID, respectively. In Subsection 4.6, we further validate the effectiveness of applying the SNR modules to another backbone network and to cross modality (Infrared-RGB) person ReID.</p><p>We use ResNet-50 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b36">37]</ref> as our base network for both baselines and our schemes. We build a strong baseline Baseline with some commonly used tricks integrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>To evaluate the generalization ability of our approach and to be consistent with what were done in prior works for performance comparisons, we conduct extensive experiments on commonly used public ReID datasets, including Market1501 <ref type="bibr" target="#b68">[69]</ref>, DukeMTMC-reID <ref type="bibr" target="#b70">[71]</ref>, CUHK03 <ref type="bibr" target="#b27">[28]</ref>, the large-scale MSMT17 <ref type="bibr" target="#b55">[56]</ref>, and four small-scale ReID datasets of PRID <ref type="bibr" target="#b14">[15]</ref>, GRID <ref type="bibr" target="#b35">[36]</ref>, VIPeR <ref type="bibr" target="#b10">[11]</ref>, and i-LIDS <ref type="bibr" target="#b56">[57]</ref>. We denote Market1501 by M, DukeMTMC-reID by Duke or D, and CUHK03 by C for simplicity.</p><p>We follow common practices and use the cumulative matching characteristics (CMC) at Rank-1, and mean average precision (mAP) to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We perform comprehensive ablation studies to demonstrate the effectiveness of the SNR module and its dual causality loss constraint. We mimic the real-world scenario for generalizable person ReID, where a model is trained on some source dataset(s) A while tested on previously unseen dataset B. We denote this as A→B. We have several experimental settings to evaluate the generalization capability, e.g., Market1501→Duke and others, Duke→Market1501 and others, M+D+C+MSMT17→others. Our settings cover both single source dataset for training and multiple source datasets for training. Effectiveness of Our SNR. Here we compare several schemes. Baseline: a strong baseline based on ResNet-50. Baseline-A-IN: a naive model where we replace all the Batch Normalization(BN) <ref type="bibr" target="#b17">[18]</ref>   <ref type="figure">Figure 2</ref>(a)). We also refer to it as SNR for simplicity. <ref type="table" target="#tab_9">Table 5</ref> shows the results. We have the following observations/conclusions: 1) Baseline-A-IN improves Baseline by 4.3% in mAP for Market1501→Duke, and 4.7% in mAP for Duke→Market1501. Other IN-related baselines also bring gains, which demonstrates the effectiveness of IN for improving the generalization capability for ReID. But, IN also inevitably discards some discriminative (identity-relevant) information and we can see it clearly decreases the performance of Baseline-A-IN, Baseline-IBN and Baseline-IN for the same-domain ReID (e.g., Market1501→Market1501). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Design Choices of SNR</head><p>Which Stage to Add SNR? We compare the cases of adding a single SNR module to a different convolutional block/stage, and to all the four stages (i.e., stage-1 ∼ 4) of the ResNet-50 (see <ref type="figure">Figure 2</ref>(a)). The module is added after the last layer of a convolutional block/stage. As <ref type="table" target="#tab_4">Table 2b</ref> shows, in comparison with Baseline, the improvement from adding SNR is significant on stage-3 and stage-4 and is a little smaller on stage-1 and stage-2. When SNR is added to all the four stages, we achieve the best performance. Influence of Disentanglement Design. In our SNR module, as described in (3)(4) of Subsection 3.1, we use g(·), and its complementary one 1 − g(·) as masks to extract identity-relevant feature R + and identity-irrelevant feature R − from the residual feature R. Here, we study the influence of different disentanglement designs within SNR. SNR conv : we disentangle the residual feature R through 1×1 convolutional layer followed by non-liner ReLU activation, i.e., R + = ReLU (W + R), R − = ReLU (W − R). SNR g(·) 2 : we use two unshared gates g(·) + , g(·) − to obtain R + and R − respectively. <ref type="table" target="#tab_4">Table 2c</ref> shows the results. We observe that (1) ours outperforms SNR conv by 3.9% and 4.5% in mAP for M→D and D→M, respectively, demonstrating the benefit of content-adaptive design; (2) ours outperforms SNR g(·) 2 by 2.4%/2.9% in mAP on the unseen target Duke/Market1501, demonstrating the benefit of the design which encourages interaction between R + and R − .    <ref type="bibr" target="#b69">70]</ref>, we get each activation map by summarizing the feature maps along channels followed by a spatial 2 normalization. <ref type="figure" target="#fig_5">Figure 6</ref>(a) shows the activation maps of normalized feature F , enhanced feature F + = F + R + , and contaminated feature F − = F + R − , respectively. We see that after adding the identity-irrelevant feature R − , the contaminated feature F − has high response mainly on background. In contrast, the enhanced feature F + with the restitution of identity-relevant feature R + has high responses on regions of the human body, better capturing discriminative regions.</p><p>Moreover, in <ref type="figure" target="#fig_5">Figure 6</ref>(b), we further compare the activation maps F + of our scheme and those of the strong baseline scheme Baseline by varying the styles of input images (e.g., contrast, illumination, saturation). We can see that, for the images with different styles, the activation maps of our scheme are more consistent/invariant than those of Baseline. In contrast, the activation maps of Baseline are more disorganized and are easily affected by style variants. These indicate our scheme is more robust to style variations. Visualization of Feature Distributions. In <ref type="figure" target="#fig_4">Figure 4</ref>, we visualize the distribution of the features from the 3 rd SNR module of our network using t-SNE <ref type="bibr" target="#b38">[39]</ref>. They denote the distributions of features for (a) input F , (b) style normalized feature F , and (c) output F + of the SNR module. We observe that, (a) before SNR, the extracted features from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-Arts</head><p>Thanks to the capability of reducing style discrepancy and restitution of identity-relevant features, our proposed SNR module can enhance the generalization ability and maintain the discrimintive ability of ReID networks. It can be used for generalizable person ReID, i.e., domain generalization (DG), and can also be used to build the backbone networks for unsupervised domain adaptation (UDA) for person ReID. We evaluate the effectiveness of SNR on both DG-ReID and UDA-ReID by comparing with the state-ofthe-art approaches in <ref type="table">Table 6</ref>.</p><p>Domain generalizable person ReID is very attractive in practical applications, which supports "train once and run everywhere". However, there are very few works in this field <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">75,</ref><ref type="bibr" target="#b23">24]</ref>. Thanks to the exploration of the style normalization and restitution, our scheme SNR(Ours) significantly outperforms the second best method OSNet-IBN [75] by 6.9% and 7.8% for Market1501→Duke and Duke→Market1501 in mAP, respectively. OSNet-IBN adds Instance Normalization (IN) to the lower layers of their proposed OSNet following <ref type="bibr" target="#b40">[41]</ref>. However, this does not overcome the intrinsic shortcoming of IN and is not optimal.</p><p>Song et al. <ref type="bibr" target="#b44">[45]</ref> also explore domain generalizable person ReID and propose a Domain-Invariant Mapping Network (DIMN) to learn the mapping between a person image and its identity classifier with a meta-learning pipeline. We follow <ref type="bibr" target="#b44">[45]</ref> and train SNR on the same five datasets (M+D+C+CUHK02 <ref type="bibr" target="#b26">[27]</ref>+CUHK-SYSU <ref type="bibr" target="#b58">[59]</ref>). SNR outperforms DIMN by 14.6%/6.6%/1.2%/11.5% in mAP and 12.9%/10.9%/1.7%/13.9% in Rank-1 on the PRID/GRID/VIPeR/i-LIDS. Unsupervised domain adaptation for ReID has been extensively studied where the unlabeled target data is also used for training. We follow the most commonly-used source→target setting <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">75,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b59">60]</ref> for comparison. We take SNR (see <ref type="figure">Figure 2</ref>(a)) as the backbone followed by a domain adaptation strategy MAR <ref type="bibr" target="#b63">[64]</ref> for domain adaptation, which we denote as SNR(Ours)+MAR <ref type="bibr" target="#b63">[64]</ref>. For comparison, we take our strong Baseline as the backbone followed by MAR, which we denote as Baseline+MAR, to evaluate the effectiveness of the proposed SNR modules. We can see that SNR(Ours)+MAR <ref type="bibr" target="#b63">[64]</ref> significantly outperforms the second-best UDA ReID method by 3.8%, 3.4% in mAP for Market1501+Duke(U)→Duke and Duke+Market1501(U)→Market1501, respectively.</p><p>In addition, SNR(Ours)+MAR outperforms Baseline+MAR by 22.9%, 24.5% in mAP. Similar trends can be found for MSMT17+Duke(U)→Duke and MSMT17+Market1501(U)→Market1501.</p><p>In general, as a plug-and-play module, SNR clearly enhances the generalization capability of ReID networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Extension</head><p>Performance on Other Backbone. We add SNR into the recently proposed lightweight ReID network OSNet <ref type="bibr">[75]</ref> and observe that by simply inserting SNR modules between the OS-Blocks, the new scheme OSNet-SNR outperforms their model OSNet-IBN by 5.0% and 5.5% in mAP for M→D and D→M, respectively (see <ref type="figure">Supplementary)</ref>. RGB-Infrared Cross-Modality Person ReID. To further demonstrate the capability of SNR in handling images with large style variations, we conduct experiment on a more challenging RGB-Infrared cross-modality person ReID task on benchmark dataset SYSU-MM01 <ref type="bibr" target="#b57">[58]</ref>. Our scheme which integrates SNR to Baseline outperforms Baseline significantly by 8.4%, 8.2%, 11.0%, and 11.5% in mAP under 4 different settings, and also achieves the state-of-the-art performance (see Supplementary for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a generalizable person ReID framework to enable effective ReID. A Style Normalization and Restitution (SNR) module is introduced to exploit the merit of Instance Normalization (IN) that filters out the interference from style variations, and restitute the identityrelevant features that are discarded by IN. To efficiently disentangle the identity-relevant and -irrelevant features, we further design a dual causality loss constraint in SNR. Extensive experiments on several benchmarks/settings demonstrate the effectiveness of SNR. Our framework with SNR embedded achieves the best performance on both domain generalization and unsupervised domain adaptation ReID. Moreover, we have also verified SNR's effectiveness on RGB-Infrared ReID task, and on another backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This work was supported in part by NSFC under Grant U1908209, 61632001 and the National Key Research and Development Program of China 2018AAA0101400. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Implementation Details</head><p>Network Details. We use ResNet-50 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b36">37]</ref> as our base network for both baselines and our schemes. We build a strong baseline Baseline with some commonly used tricks integrated. Similar to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b36">37]</ref>, the last spatial downsample operation in the last Conv block is removed. The proposed SNR module is added after the last layer of each convolutional block/stage of the first four stages. The input image resolution is 256×128.</p><p>Data Augmentation. We use the commonly used data augmentation strategies of random cropping <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b66">67]</ref>, horizontal flipping, and label smoothing regularization <ref type="bibr" target="#b48">[49]</ref>. To enhance the generalization ability, we further incorporate some useful data augmentation tricks, such as color jittering and disabling random erasing (REA) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr">75]</ref>. REA hurts models in cross-domain ReID task <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b23">24]</ref>, because REA which masks the regions of training images makes the model learn more knowledge in the training source domain. It causes the model to perform worse in the unseen target domain.</p><p>Training Details for Domain Generalization. Following <ref type="bibr" target="#b13">[14]</ref>, a batch is formed by first randomly sampling P identities. For each identity, we sample K images. Then the batch size is B = P × K. We set P = 24 and K = 4 (i.e., batch size B = P × K = 96.</p><p>We use the Adam optimizer <ref type="bibr" target="#b22">[23]</ref> for model optimization. Similar to <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b66">67]</ref>, we first warm up the model for 20 epochs with a linear growth learning rate from 8×10 −6 to 8×10 −4 . Then we set the initial learning rate as 8×10 −4 and optimize the Adam optimizer with a weight decay of 5×10 −4 . The learning rate is decayed by a factor of 0.5 for every 40 epochs. Our model (here we use ResNet-50 as our backbone) with SNR converges well after training of 280 epochs and we use it for evaluating the generalization performance on target datasets. All our models are implemented on PyTorch and trained on a single 32G NVIDIA-V100 GPU.</p><p>Training Details for Domain Adaptation. For unsupervised domain adaptation person ReID, we combine our network with the unsupervised ReID approach MAR <ref type="bibr" target="#b63">[64]</ref> for fine-tuning on the unlabelled target domain data. MAR <ref type="bibr" target="#b63">[64]</ref> plays the role of assigning psudeo labels by hard negative mining, which facilitates the fine-tuning of base network. Similar to <ref type="bibr" target="#b63">[64]</ref>, during the fine-tuning, both source labeled data and target unlabelled data are jointly used for effective joint training. Specifically, during fine-tuning, a training batch of size 96 is composed of 1) labeled source data (size B 1 = P × K = 48, where P = 12, K = 4), and 2) unlabeled target data (size B 2 = 48). For the labeled source data, we optimize the network with the ReID loss L ReID and the proposed dual causality loss L SN R . For the unlabeled target data, we follow the adaptation strategy of MAR <ref type="bibr" target="#b63">[64]</ref> to assign a pseudo soft multilabel for each sample and utilize these pseudo labels to perform soft multilabel-guided hard negative mining for training. We fine-tune the network also with the Adam optimizer <ref type="bibr" target="#b22">[23]</ref> with a initial learning rate of 1×10 −5 for 200 epochs. We optimize the Adam optimizer with a weight decay of 5×10 −4 . The learning rate is decayed by a factor of 0.5 at 50, 100 and 150 epochs. Why do we perform disentanglement only on channel level? We perform feature disentanglement only on channel level for two reasons: 1) Those identity-irrelevant style factors (e.g., illumination, contrast, saturation) are typically regarded as spatially consistent, which are hard to disentangle by spatial-attention. 2) In our SNR, "disentanglement" aims at better "restitution" of the lost discriminative information due to Instance Normalization (IN). IN reduces style discrepancy of input features by performing normalization across spatial dimensions independently for each channel, where the normalization parameters are the same across different spatial positions. To be consistent with IN, we disentangle the features and restitute the identity-relevant ones to the normalized features on channel level.  <ref type="bibr" target="#b14">[15]</ref> 385 1134 2 outdoor GRID <ref type="bibr" target="#b35">[36]</ref> 250 500 2 indoor i-LIDS <ref type="bibr" target="#b56">[57]</ref> 119 476 N/A indoor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Details of Datasets</head><p>In <ref type="table" target="#tab_8">Table 4</ref>, we present the detailed information about the related person ReID datasets. Market1501 <ref type="bibr" target="#b68">[69]</ref>, DukeMTMC-reID <ref type="bibr" target="#b70">[71]</ref>, CUHK03 <ref type="bibr" target="#b27">[28]</ref>, and large-scale MSMT17 <ref type="bibr" target="#b55">[56]</ref> are the most commonly used datasets for fully supervised ReID <ref type="bibr" target="#b66">[67,</ref><ref type="bibr">75]</ref> and unsupervised domain adaption ReID <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b7">8]</ref>. VIPeR <ref type="bibr" target="#b10">[11]</ref>, PRID2011 <ref type="bibr" target="#b14">[15]</ref>, GRID <ref type="bibr" target="#b35">[36]</ref>, and i-LIDS <ref type="bibr" target="#b56">[57]</ref> are small ReID datasets which could be used for evaluating cross-domain/generalizable person ReID <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Market1501 <ref type="bibr" target="#b68">[69]</ref> and DukeMTMC-reID <ref type="bibr" target="#b70">[71]</ref> have pre-established test probe and test gallery splits which we use for our training and crosstest (i.e., M→D, D→M). For the smaller datasets (VIPeR, PRID2011, GRID, and i-LIDS), we use the standard 10 ran-  <ref type="figure">Figure 5</ref>: Person images from different ReID datasets: Market-1501 <ref type="bibr" target="#b68">[69]</ref>, DukeMTMC-reID <ref type="bibr" target="#b70">[71]</ref>, CUHK03 <ref type="bibr" target="#b27">[28]</ref>, MSMT17 <ref type="bibr" target="#b55">[56]</ref>, and the four small-scale ReID datasets of PRID <ref type="bibr" target="#b14">[15]</ref>, GRID <ref type="bibr" target="#b35">[36]</ref>, VIPeR <ref type="bibr" target="#b10">[11]</ref>, and i-LIDS <ref type="bibr" target="#b56">[57]</ref>. All images have been re-sized to 256×128 for easier comparison. We observe there are obvious domain gaps/style discrepancies across different datasets, especially for PRID <ref type="bibr" target="#b14">[15]</ref> and GRID <ref type="bibr" target="#b35">[36]</ref>.</p><p>dom splits as in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref> for testing (the four small datasets are not involved in training). CUHK03 <ref type="bibr" target="#b27">[28]</ref> and MSMT17 <ref type="bibr" target="#b55">[56]</ref> are used for training.</p><p>We randomly pick up 10 identities from each ReID dataset and show them in <ref type="figure">Figure 5</ref>. We observe that: 1) there is style discrepancy across datasets, which is rather obvious for PRID and GRID; 2) MSMT17 has large style variants within the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">More Ablation Study Results</head><p>We show more comparisons of our scheme and others to demonstrate the effectiveness of our SNR module for generalizable person ReID in <ref type="table" target="#tab_9">Table 5</ref>.</p><p>We have observations consistent with those in our paper. 1) IN-related baselines bring generalization ability improvement but decrease the performance for the samedomain. 2) Our Baseline-SNR achieves superior generalization capability thanks to the restitution of identity-relevant information by the SNR modules.</p><p>3) The generalization performance on unseen target domain increases consistently as the number of source datasets increases.</p><p>In <ref type="table" target="#tab_9">Table 5</ref>, we also present the total number of source training images as marked by data num. N. For the single source dataset settings, MSMT17 is the largest dataset, which contains 126k images while Market1501 or Duke has about 33K images. For the target testing datasets VIPeR and iLIDs, the performance of Baseline trained by this large scale dataset MSMT17 is 3.8% to 12.5% higher than those trained by Market1501 or Duke in mAP. Generally, the increase of training data could improve the performance. However, the performance of Baseline trained by MSMT17 has a rather low mAP accuracy of 9.8% on the target dataset GRID, being even poorer than that trained on Market1501 (25.8%) or Duke (14.5%). For the target dataset PRID, similarly, MSMT17 does not provide clear superiority. These indicate that it is not always true that a larger amount of training data results in better performance. The domain gap between MSMT17 and GRID is larger than that between Market1510/Duke and GRID. To validate this, we analyze the feature divergence (FD, detailed descriptions can be found in Section 4 below) between GRID and MSMT17, Market1501, Duke, respectively. We find that the divergence (here we calculate the feature divergence of the third convolutional block/stage within our Baseline-SNR trained by combining all the four datasets) of Market1501 vs. GRID, Duke vs. GRID, MSMT17 vs. GRID are 2.17, 3.49, and 4.51, respectively. Note that the larger the FD value, the larger the feature discrepancy between the two domains. The domain gap between MSMT17 and GRID is larger than that between Market1501 (or Duke) and GRID. For the similar reason, we find that additionally adding MSMT17 as the source training data does not bring further performance improvement on GRID and PRID target datasets in our scheme Baseline-SNR in comparison with the model trained by M+D+C source datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">More Visualization Analysis</head><p>More Feature Map Visualization. In our paper, we compare the activation maps F + of our scheme and those of the strong baseline scheme Baseline by varying the styles of input images (e.g., contrast, illumination, saturation). Here, <ref type="figure" target="#fig_5">Figure 6</ref>(a) shows more visualization and <ref type="figure" target="#fig_5">Figure 6</ref>(b) shows visualization results on real images. We have the similar observations that the activation maps of our scheme are more consistent/invariant to style variants.</p><p>Feature Divergence Analysis. We analyze the feature di-  vergence between two datasets on three schemes: Baseline, Baseline-IN, and ours SNR, respectively. Following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29]</ref>, we use the symmetric KL divergence of features between domain A and B as the metric to measure feature divergence of the two domains. We train the models using Market1501 training dataset and evaluate the feature divergences between the test set of Market1501 and Duke (500 samples are randomly selected from each set). We calculate the feature divergence of the four convolutional blocks/stages respectively and show the results in <ref type="figure" target="#fig_6">Figure 7</ref>. We observe that the feature divergence (FD) is large for Baseline. The introduction of IN as in scheme Baseline-IN significantly reduces the FD on all the four stages. The FD of Stage-4 is higher than that in Stage-3. That is likely because Stage-4 is more related to high-level discriminative semantic features for distinguishing different identities. The discrimination may increase the feature divergence. Due to the introduction of the SNR modules, the FD on all convolutional blocks/stages is also significantly reduced in our scheme in comparison with Baseline. It is higher than that of the scheme Baseline-IN which is probably because the restitution of some identity-relevant features increases the discrimination capability and thus increases the FD. Visualization of ReID Feature Vector Distributions. In <ref type="table">Table 6</ref>: Performance (%) comparisons with the state-of-the-art approaches for the Domain Generalizable Person ReID (top rows) and Unsupervised Domain Adaptation for Person ReID (bottom rows), respectively. "(U)" denotes "unlabeled". We mask the schemes of our Baseline and our Baseline with SNR modules (i.e., SNR(Ours)) by gray, with fair comparison between each pair to validate the effectiveness of SNR modules.     (i.e., Market1501→Duke). In comparison with Baseline, the feature distribution of the same identity (same color) becomes more compact while those of the different identities are pushed away in our scheme. It is easier to distinguish between different identities by our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison with State-of-the-Arts (Complete version)</head><p>To save space, we only present the latest approaches in the paper and here we show comparisons with more approaches in <ref type="table">Table 6</ref>. Besides the description in Introduction and Related Work sections of our paper, we illustrate the difference between domain generalization and domain adaptation for person ReID in <ref type="table" target="#tab_13">Table 8</ref>.</p><p>Moreover, in <ref type="table" target="#tab_12">Table 7</ref>, we further compare our SNR with the latest generalizable ReID method Domain-Invariant Mapping Network (DIMN) <ref type="bibr" target="#b44">[45]</ref> under the same experimental setting, i.e., training on the same five datasets, Mar-ket1501 [69] + DukeMTMC-reID [71] + CUHK02 [27] + CUHK03 <ref type="bibr" target="#b27">[28]</ref> + CUHK-SYSU <ref type="bibr" target="#b58">[59]</ref>. We observe that SNR not only outperforms the Baseline by a large margin (up to 22.7% in mAP on PRID), but also significantly outperforms DIMN <ref type="bibr" target="#b44">[45]</ref> by 14.6%/6.6%/1.2%/11.5% in mAP on PRID/GRID/VIPeR/i-LIDS, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Performance on Another Backbone</head><p>Our SNR is a plug-and-play module which can be added to available ReID networks. We integrate it into the recently proposed lightweight ReID network OSNet [75] and <ref type="table" target="#tab_3">Table  10</ref> shows the results. We can see that by simply inserting SNR modules between the OS-Blocks, the new scheme OSNet-SNR outperforms their best model OSNet-IBN by 5.0% and 5.5% in mAP for M→D and D→M, respectively. Note that, for fair comparison, we use the official released weights and codes 2 of OSNet [75] to conduct these experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RGB-Infrared Cross-Modality Person ReID</head><p>To further demonstrate the generalization capability of the proposed SNR module, we conduct experiment on a more challenging RGB-Infrared cross-modality person ReID task, where there is a large style discrepancy between RGB images and Infrared images.</p><p>We evaluate our models on the standard benchmark dataset SYSU-MM01 <ref type="bibr" target="#b57">[58]</ref>. Following <ref type="bibr" target="#b57">[58]</ref>, we conduct evaluation using the released official code based on the average of 10 repeated random split of gallery and probe sets. As shown in <ref type="table" target="#tab_14">Table 9</ref>, in comparison with Baseline, our scheme which integrates the proposed SNR module on Baseline achieves a significant gain of 8.4%, 8.2%, 11.0%, and 11.5% in terms of mAP under 4 different experimental settings, and achieves the state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>+ (:, :, k) =a k R(:, :, k), R − (:, :, k) =(1 − a k )R(:, :, k),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Activation maps of different features within an SNR module (SNR 3). They show SNR can disentangle the identity-relevant/irrelevant features well. (b) Activation maps of our scheme (bottom) and the strong baseline Baseline (top) corresponding to images of varied styles. Our maps are more consistent/invariant to style variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of distributions of intermediate features before/within/after the SNR module using the tool of t-SNE [39]. 'Red'/'green' nodes: samples from source dataset Market1501/unseen target dataset Duke. two datasets ('red': source training dataset Market1501; 'green': unseen target dataset Duke) are largely separately distributed and have an obvious domain gap. (b) Within the SNR module, after IN, this domain gap has been eliminated.But the samples of the same identity ('yellow' and 'purple' colored nodes denote two identities respectively) become dispersive. (c) After the restitution of identity-relevant features, not only has the domain gap of feature distributions been shrunk, but also the feature distribution of samples with same identity become more compact than that in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Activation maps of our scheme (bottom) and the strong baseline Baseline (top) corresponding to images of varied styles. The maps of our method are more consistent/invariant to style variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Analysis of the feature divergence between two different domains, Market1501 and Duke.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of the final ReID feature vector distribution for Baseline and Ours on the unseen target dataset Duke. Different identities are denoted by different colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 ,</head><label>8</label><figDesc>we further visualize the distribution of the final ReID feature vectors using t-SNE<ref type="bibr" target="#b38">[39]</ref> for Baseline scheme and our final scheme on the unseen target dataset Duke</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>2 https://github.com/KaiyangZhou/deep-person-reid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>layers in Baseline by Instance Normalization(IN). Baseline-IBN: Similar to IBN-Net (IBN-b) [41] and OSNet [75], we add IN only to the last layers of Conv1 and Conv2 blocks of Baseline respectively. Baseline-A-SN: a model where we replace all the BN layers in Baseline by Switchable Normalization (SN). SN [38] can be regarded as an adaptive ensemble version of normalization techniques of IN, BN, and LN (Layer Normalization) [2]. Baseline-IN: four IN layers are added after the first four convolutional blocks/stages of Baseline respectively. Baseline-SNR: our final scheme where four SNR modules are added after the first four convolutional blocks/stages of Baseline respectively (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance (%) comparisons of our scheme and others to demonstrate the effectiveness of our SNR module for generalizable person ReID. The rows denote source dataset(s) for training and the columns correspond to different target datasets for testing. We mask the results of supervised ReID by gray where the testing domain has been seen in training. Due to space limitation, we only show a portion of the results here and more comparisons can be found in Supplementary.Baseline-A-SN learns the combination weights of IN, BN, and LN in the training dataset and thus has superior performance in the same domain, but it does not have dedicated design for boosting the generalization capability.2) Thanks to the compensation of the identity-relevant information through the proposed restitution step, our final scheme Baseline-SNR achieves superior generalization capability, which significantly outperforms all the baseline schemes. In particular, Baseline-SNR outperforms Baseline-IN by 8.5%, 6.7%, 15.0% in mAP for M→D, D→M, and D→GRID, respectively.3) The generalization performance on previously unseen target domain increases consistently as the number of source datasets increases. When all the four source datasets are used (the large-scale MSMT17<ref type="bibr" target="#b55">[56]</ref> also included), we have a very strong baseline (i.e., 52.1% in mAP on VIPeR dataset vs. 37.6% when Market1501 alone is used as source). Interestingly, our method still significantly outperforms the strong baseline Baseline, even by 21.0% in mAP on PRID dataset, demonstrating SNR's effectiveness. 4) The performance of different schemes with respects to PRID/GRID varies greatly and the mAPs are all relatively low, which is caused by the large style discrepancy between PRID/GRID and other datasets. For such challenging cases, our scheme still outperforms Baseline-IN significantly by 7.2% and 4.9% in mAP for M→PRID and D→PRID, respectively. 5) For supervised ReID (masked by gray), our scheme also clearly outperforms Baseline by 1.9% and 1.7% in mAP for M→M and D→D, respectively. That is because there is also style discrepancy within the source domain. Influence of Dual Causality Loss Constraint. We study the effectiveness of the proposed dual causality loss L SN R SN R ) by 7.5% and 4.7% in mAP for M→D and D→M, respectively. Such constraints facilitate the disentanglement of identityrelevant/identity-irrelevant features. In addition, both the clarification loss L + SN R and the destruction loss L − SN R , are vital to SNR and they are complementary and jointly contribute to a superior performance. Complexity. The model size of our final scheme SNR is very similar to that of Baseline (24.74 M vs. 24.56 M).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Target: Market1501</cell><cell cols="2">Target: Duke</cell><cell cols="2">Target: PRID</cell><cell cols="2">Target: GRID</cell><cell cols="2">Target: VIPeR</cell><cell cols="2">Target: iLIDs</cell></row><row><cell>Source</cell><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell></cell><cell>Baseline</cell><cell>82.8</cell><cell>93.2</cell><cell>19.8</cell><cell>35.3</cell><cell>13.7</cell><cell>6.0</cell><cell>25.8</cell><cell>16.0</cell><cell>37.6</cell><cell>28.5</cell><cell>61.5</cell><cell>53.3</cell></row><row><cell></cell><cell>Baseline-A-IN</cell><cell>75.3</cell><cell>89.8</cell><cell>24.1</cell><cell>42.7</cell><cell>33.9</cell><cell>21.0</cell><cell>35.6</cell><cell>27.2</cell><cell>38.1</cell><cell>29.1</cell><cell>64.2</cell><cell>55.0</cell></row><row><cell></cell><cell>Baseline-IBN</cell><cell>81.1</cell><cell>92.2</cell><cell>21.5</cell><cell>39.2</cell><cell>19.1</cell><cell>12.0</cell><cell>27.5</cell><cell>19.2</cell><cell>32.1</cell><cell>23.4</cell><cell>58.3</cell><cell>48.3</cell></row><row><cell>Market1501 (M)</cell><cell>Baseline-A-SN</cell><cell>83.2</cell><cell>93.9</cell><cell>20.1</cell><cell>38.0</cell><cell>35.4</cell><cell>25.0</cell><cell>29.0</cell><cell>22.0</cell><cell>32.2</cell><cell>23.4</cell><cell>53.4</cell><cell>43.3</cell></row><row><cell></cell><cell>Baseline-IN</cell><cell>79.5</cell><cell>90.9</cell><cell>25.1</cell><cell>44.9</cell><cell>35.0</cell><cell>25.0</cell><cell>35.7</cell><cell>27.8</cell><cell>35.1</cell><cell>27.5</cell><cell>64.0</cell><cell>54.2</cell></row><row><cell></cell><cell>Baseline-SNR (Ours)</cell><cell>84.7</cell><cell>94.4</cell><cell>33.6</cell><cell>55.1</cell><cell>42.2</cell><cell>30.0</cell><cell>36.7</cell><cell>29.0</cell><cell>42.3</cell><cell>32.3</cell><cell>65.6</cell><cell>56.7</cell></row><row><cell></cell><cell>Baseline</cell><cell>21.8</cell><cell>48.3</cell><cell>71.2</cell><cell>83.4</cell><cell>15.7</cell><cell>11.0</cell><cell>14.5</cell><cell>8.8</cell><cell>37.0</cell><cell>26.9</cell><cell>68.3</cell><cell>58.3</cell></row><row><cell></cell><cell>Baseline-A-IN</cell><cell>26.5</cell><cell>56.0</cell><cell>64.5</cell><cell>78.9</cell><cell>38.6</cell><cell>29.0</cell><cell>19.6</cell><cell>13.6</cell><cell>35.1</cell><cell>27.2</cell><cell>67.4</cell><cell>56.7</cell></row><row><cell></cell><cell>Baseline-IBN</cell><cell>24.6</cell><cell>52.5</cell><cell>69.5</cell><cell>81.4</cell><cell>27.4</cell><cell>19.0</cell><cell>19.9</cell><cell>12.0</cell><cell>32.8</cell><cell>23.4</cell><cell>63.5</cell><cell>61.7</cell></row><row><cell>Duke (D)</cell><cell>Baseline-A-SN</cell><cell>25.3</cell><cell>55.0</cell><cell>73.0</cell><cell>85.9</cell><cell>41.4</cell><cell>32.0</cell><cell>18.8</cell><cell>12.8</cell><cell>31.3</cell><cell>24.1</cell><cell>64.8</cell><cell>63.3</cell></row><row><cell></cell><cell>Baseline-IN</cell><cell>27.2</cell><cell>58.5</cell><cell>68.9</cell><cell>80.4</cell><cell>40.5</cell><cell>27.0</cell><cell>20.3</cell><cell>13.2</cell><cell>34.6</cell><cell>26.3</cell><cell>70.6</cell><cell>65.0</cell></row><row><cell></cell><cell>Baseline-SNR (Ours)</cell><cell>33.9</cell><cell>66.7</cell><cell>72.9</cell><cell>84.4</cell><cell>45.4</cell><cell>35.0</cell><cell>35.3</cell><cell>26.0</cell><cell>41.2</cell><cell>32.6</cell><cell>79.3</cell><cell>68.7</cell></row><row><cell>M + D + CUHK03</cell><cell>Baseline</cell><cell>72.4</cell><cell>88.7</cell><cell>70.1</cell><cell>83.8</cell><cell>39.0</cell><cell>28.0</cell><cell>29.6</cell><cell>20.8</cell><cell>52.1</cell><cell>41.5</cell><cell>89.0</cell><cell>85.0</cell></row><row><cell>+ MSMT17</cell><cell>Baseline-SNR (Ours)</cell><cell>82.3</cell><cell>93.4</cell><cell>73.2</cell><cell>85.5</cell><cell>60.0</cell><cell>49.0</cell><cell>41.3</cell><cell>30.4</cell><cell>65.0</cell><cell>55.1</cell><cell>91.9</cell><cell>87.0</cell></row></table><note>which consists of clarification loss L + SN R and destruction loss L − SN R . Table 2a shows the results. Our final scheme SNR with the dual causality loss L SN R outperforms that without such constraints (i.e., scheme SNR w/o L</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Effectiveness of dual causality loss constraint (a), and study on design choices of SNR (b) and (c).(a) Study on the dual causality loss constraint.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Study on which stage to add SNR.</cell><cell cols="3">(c) Disentanglement designs in SNR.</cell></row><row><cell>Method</cell><cell cols="4">M−→D mAP Rank-1 mAP Rank-1 D−→M</cell><cell>Method</cell><cell cols="4">M−→D mAP Rank-1 mAP Rank-1 D−→M</cell><cell>Method</cell><cell cols="2">M−→D mAP Rank-1 mAP Rank-1 D−→M</cell></row><row><cell cols="2">Baseline SNR w/o L SN R 26.1 19.8 SNR w/o L + SN R 28.8 SNR w/o L − SN R 28.0 SNR 33.6</cell><cell>35.3 45.0 48.9 48.1 55.1</cell><cell>21.8 29.2 30.2 30.3 33.9</cell><cell>48.3 57.4 59.8 59.1 66.7</cell><cell cols="2">Baseline 19.8 stage-1 23.7 stage-2 24.0 stage-3 26.4 stage-4 26.2 stages-all 33.6</cell><cell>35.3 42.8 44.4 46.3 45.8 55.1</cell><cell>21.8 27.6 28.6 29.5 29.4 33.9</cell><cell>48.3 57.7 58.8 60.7 59.7 66.7</cell><cell cols="2">Baseline SNRconv 29.7 19.8 SNR g(·) 2 31.2 SNR 33.6</cell><cell>35.3 51.1 52.9 55.1</cell><cell>21.8 29.4 31.0 33.9</cell><cell>48.3 61.7 63.8 66.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance (%) comparisons with the state-of-the-art approaches for the Domain Generalizable Person ReID (top rows) and the Unsupervised Domain Adaptation Person ReID (bottom rows), respectively. "(U)" denotes "unlabeled". We mask the schemes that use our Baseline and those that use our SNR modules by gray, which provides fair comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Target: Duke</cell><cell></cell><cell cols="2">Taeget: Market1501</cell></row><row><cell></cell><cell>Method</cell><cell>Venue</cell><cell>Source</cell><cell>mAP</cell><cell>Rank-1</cell><cell>Source</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell></cell><cell>OSNet-IBN [75]</cell><cell>ICCV'19</cell><cell>Market1501</cell><cell>26.7</cell><cell>48.5</cell><cell>Duke</cell><cell>26.1</cell><cell>57.7</cell></row><row><cell></cell><cell>Baseline</cell><cell>This work</cell><cell>Market1501</cell><cell>19.8</cell><cell>35.3</cell><cell>Duke</cell><cell>21.8</cell><cell>48.3</cell></row><row><cell>Domain Generalization</cell><cell>Baseline-IBN [19] SNR(Ours)</cell><cell>BMVC'19 This work</cell><cell>Market1501 Market1501</cell><cell>21.5 33.6</cell><cell>39.2 55.1</cell><cell>Duke Duke</cell><cell>24.6 33.9</cell><cell>52.5 66.7</cell></row><row><cell>(w/o using</cell><cell>StrongBaseline [24]</cell><cell>ArXiv'19</cell><cell>MSMT17</cell><cell>43.3</cell><cell>64.5</cell><cell>MSMT17</cell><cell>36.6</cell><cell>64.8</cell></row><row><cell>target data)</cell><cell>OSNet-IBN [75]</cell><cell>ICCV'19</cell><cell>MSMT17</cell><cell>45.6</cell><cell>67.4</cell><cell>MSMT17</cell><cell>37.2</cell><cell>66.5</cell></row><row><cell></cell><cell>Baseline</cell><cell>This work</cell><cell>MSMT17</cell><cell>39.1</cell><cell>60.4</cell><cell>MSMT17</cell><cell>33.8</cell><cell>59.9</cell></row><row><cell></cell><cell>SNR(Ours)</cell><cell>This work</cell><cell>MSMT17</cell><cell>50.0</cell><cell>69.2</cell><cell>MSMT17</cell><cell>41.4</cell><cell>70.1</cell></row><row><cell></cell><cell>ATNet [35]</cell><cell>CVPR'19</cell><cell>Market1501 + Duke (U)</cell><cell>24.9</cell><cell>45.1</cell><cell>Duke + Market1501 (U)</cell><cell>25.6</cell><cell>55.7</cell></row><row><cell></cell><cell>CamStyle [74]</cell><cell>TIP'19</cell><cell>Market1501 + Duke (U)</cell><cell>25.1</cell><cell>48.4</cell><cell>Duke + Market1501 (U)</cell><cell>27.4</cell><cell>58.8</cell></row><row><cell></cell><cell>ARN [30]</cell><cell>CVPRW'19</cell><cell>Market1501 + Duke (U)</cell><cell>33.4</cell><cell>60.2</cell><cell>Duke + Market1501 (U)</cell><cell>39.4</cell><cell>70.3</cell></row><row><cell>Unsupervised Domain Adaptation (using unlabeled target data)</cell><cell>ECN [73] PAST [66] SSG [8] Baseline+MAR [64] SNR(Ours)+MAR [64]</cell><cell>CVPR'19 ICCV'19 ICCV'19 This work This work</cell><cell>Market1501 + Duke (U) Market1501 + Duke (U) Market1501 + Duke (U) Market1501 + Duke (U) Market1501 + Duke (U)</cell><cell>40.4 54.3 53.4 35.2 58.1</cell><cell>63.3 72.4 73.0 56.5 76.3</cell><cell>Duke + Market1501 (U) Duke + Market1501 (U) Duke + Market1501 (U) Duke + Market1501 (U) Duke + Market1501 (U)</cell><cell>43.0 54.6 58.3 37.2 61.7</cell><cell>75.1 78.4 80.0 62.4 82.8</cell></row><row><cell></cell><cell>MAR [64]</cell><cell>CVPR'19</cell><cell>MSMT17 + Duke (U)</cell><cell>48.0</cell><cell>67.1</cell><cell>MSMT17 + Market1501 (U)</cell><cell>40.0</cell><cell>67.7</cell></row><row><cell></cell><cell>PAUL [60]</cell><cell>CVPR'19</cell><cell>MSMT17 + Duke (U)</cell><cell>53.2</cell><cell>72.0</cell><cell>MSMT17 + Market1501 (U)</cell><cell>40.1</cell><cell>68.5</cell></row><row><cell></cell><cell>Baseline+MAR [64]</cell><cell>This work</cell><cell>MSMT17 + Duke (U)</cell><cell>46.2</cell><cell>66.3</cell><cell>MSMT17 + Market1501 (U)</cell><cell>39.4</cell><cell>66.9</cell></row><row><cell></cell><cell>SNR(Ours) + MAR [64]</cell><cell>This work</cell><cell>MSMT17 + Duke (U)</cell><cell>61.6</cell><cell>78.2</cell><cell>MSMT17 + Market1501 (U)</cell><cell>65.9</cell><cell>85.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>[ 74 ]</head><label>74</label><figDesc>Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, and Yi Yang. Camstyle: A novel data augmentation method for person re-identification. TIP, 2018. [75] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, et al. Omni-scale feature learning for person re-identification. ICCV, 2019.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Details about the ReID datasets.</figDesc><table><row><cell>Datasets</cell><cell>Identities</cell><cell>Images</cell><cell>Cameras</cell><cell>Scene</cell></row><row><cell>Market1501 [69]</cell><cell>1501</cell><cell>32668</cell><cell>6</cell><cell>outdoor</cell></row><row><cell>DukeMTMC-reID [71]</cell><cell>1404</cell><cell>32948</cell><cell>8</cell><cell>outdoor</cell></row><row><cell>CUHK03 [28]</cell><cell>1467</cell><cell>28192</cell><cell>2</cell><cell>indoor</cell></row><row><cell>MSMT17 [56]</cell><cell>4101</cell><cell>126142</cell><cell>15</cell><cell>outdoor, indoor</cell></row><row><cell>VIPeR [11]</cell><cell>632</cell><cell>1264</cell><cell>2</cell><cell>outdoor</cell></row><row><cell>PRID2011</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performance (%) comparisons of our scheme and others to demonstrate the effectiveness of our SNR module for generalizable person ReID. The rows denote source dataset(s) for training and the columns correspond to different target datasets for testing. We mask the results of supervised ReID by gray where the testing domain has been seen in training. Note that we show the total number of source training images by data num..</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Target: Market1501</cell><cell cols="2">Target: Duke</cell><cell cols="2">Target: PRID</cell><cell cols="2">Target: GRID</cell><cell cols="2">Target: VIPeR</cell><cell cols="2">Target: iLIDs</cell></row><row><cell>Source</cell><cell>Method</cell><cell></cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>82.8</cell><cell>93.2</cell><cell>19.8</cell><cell>35.3</cell><cell>13.7</cell><cell>6.0</cell><cell>25.8</cell><cell>16.0</cell><cell>37.6</cell><cell>28.5</cell><cell>61.5</cell><cell>53.3</cell></row><row><cell></cell><cell>Baseline-A-IN</cell><cell></cell><cell>75.3</cell><cell>89.8</cell><cell>24.1</cell><cell>42.7</cell><cell>33.9</cell><cell>21.0</cell><cell>35.6</cell><cell>27.2</cell><cell>38.1</cell><cell>29.1</cell><cell>64.2</cell><cell>55.0</cell></row><row><cell>Market1501 (M) data num. 32.6k</cell><cell>Baseline-IBN Baseline-A-SN Baseline-IN</cell><cell></cell><cell>81.1 83.2 79.5</cell><cell>92.2 93.9 90.9</cell><cell>21.5 20.1 25.1</cell><cell>39.2 38.0 44.9</cell><cell>19.1 35.4 35.0</cell><cell>12.0 25.0 25.0</cell><cell>27.5 29.0 35.7</cell><cell>19.2 22.0 27.8</cell><cell>32.1 32.2 35.1</cell><cell>23.4 23.4 27.5</cell><cell>58.3 53.4 64.0</cell><cell>48.3 43.3 54.2</cell></row><row><cell></cell><cell cols="2">Baseline-SNR (Ours)</cell><cell>84.7</cell><cell>94.4</cell><cell>33.6</cell><cell>55.1</cell><cell>42.2</cell><cell>30.0</cell><cell>36.7</cell><cell>29.0</cell><cell>42.3</cell><cell>32.3</cell><cell>65.6</cell><cell>56.7</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>21.8</cell><cell>48.3</cell><cell>71.2</cell><cell>83.4</cell><cell>15.7</cell><cell>11.0</cell><cell>14.5</cell><cell>8.8</cell><cell>37.0</cell><cell>26.9</cell><cell>68.3</cell><cell>58.3</cell></row><row><cell></cell><cell>Baseline-A-IN</cell><cell></cell><cell>26.5</cell><cell>56.0</cell><cell>64.5</cell><cell>78.9</cell><cell>38.6</cell><cell>29.0</cell><cell>19.6</cell><cell>13.6</cell><cell>35.1</cell><cell>27.2</cell><cell>67.4</cell><cell>56.7</cell></row><row><cell>Duke (D) data num. 32.9k</cell><cell>Baseline-IBN Baseline-A-SN Baseline-IN</cell><cell></cell><cell>24.6 25.3 27.2</cell><cell>52.5 55.0 58.5</cell><cell>69.5 73.0 68.9</cell><cell>81.4 85.9 80.4</cell><cell>27.4 41.4 40.5</cell><cell>19.0 32.0 27.0</cell><cell>19.9 18.8 20.3</cell><cell>12.0 12.8 13.2</cell><cell>32.8 31.3 34.6</cell><cell>23.4 24.1 26.3</cell><cell>63.5 64.8 70.6</cell><cell>61.7 63.3 65</cell></row><row><cell></cell><cell cols="2">Baseline-SNR (Ours)</cell><cell>33.9</cell><cell>66.7</cell><cell>72.9</cell><cell>84.4</cell><cell>45.4</cell><cell>35.0</cell><cell>35.3</cell><cell>26.0</cell><cell>41.2</cell><cell>32.6</cell><cell>79.3</cell><cell>68.7</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>72.6</cell><cell>88.2</cell><cell>60.0</cell><cell>77.8</cell><cell>14.8</cell><cell>9.0</cell><cell>23.1</cell><cell>15.2</cell><cell>39.4</cell><cell>30.4</cell><cell>74.3</cell><cell>65.0</cell></row><row><cell></cell><cell>Baseline-A-IN</cell><cell></cell><cell>76.5</cell><cell>91.4</cell><cell>62.2</cell><cell>80.1</cell><cell>45.0</cell><cell>30.0</cell><cell>36.7</cell><cell>28.0</cell><cell>37.3</cell><cell>28.2</cell><cell>73.6</cell><cell>65.2</cell></row><row><cell>Market1501 (M)</cell><cell>Baseline-IBN</cell><cell></cell><cell>74.6</cell><cell>90.4</cell><cell>62.3</cell><cell>80.1</cell><cell>43.7</cell><cell>32.0</cell><cell>32.6</cell><cell>24.0</cell><cell>42.8</cell><cell>33.2</cell><cell>73.8</cell><cell>65.0</cell></row><row><cell>+ Duke (D)</cell><cell>Baseline-A-SN</cell><cell></cell><cell>73.1</cell><cell>89.8</cell><cell>61.7</cell><cell>79.0</cell><cell>47.9</cell><cell>37.0</cell><cell>28.0</cell><cell>21.6</cell><cell>38.0</cell><cell>28.8</cell><cell>68.1</cell><cell>61.7</cell></row><row><cell>data num. 65.5k</cell><cell>Baseline-IN</cell><cell></cell><cell>77.5</cell><cell>91.6</cell><cell>63.9</cell><cell>81.5</cell><cell>48.1</cell><cell>36.0</cell><cell>39.2</cell><cell>31.2</cell><cell>43.8</cell><cell>33.9</cell><cell>73.2</cell><cell>64.3</cell></row><row><cell></cell><cell cols="2">Baseline-SNR (Ours)</cell><cell>80.3</cell><cell>92.9</cell><cell>67.2</cell><cell>83.1</cell><cell>57.9</cell><cell>50.0</cell><cell>41.3</cell><cell>34.4</cell><cell>46.7</cell><cell>37.7</cell><cell>85.2</cell><cell>80.0</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>76.4</cell><cell>89.8</cell><cell>63.6</cell><cell>79.0</cell><cell>27.0</cell><cell>19.0</cell><cell>25.7</cell><cell>18.4</cell><cell>46.3</cell><cell>36.4</cell><cell>77.1</cell><cell>66.3</cell></row><row><cell>Market1501 (M) + Duke (D) + CUHK03 (C) data num. 93.7k</cell><cell cols="2">Baseline-A-IN Baseline-IBN Baseline-A-SN Baseline-IN Baseline-SNR (Ours)</cell><cell>76.8 76.2 71.1 77.8 81.2</cell><cell>90.7 91.3 89.3 91.3 93.3</cell><cell>63.0 62.8 62.0 64.4 68.4</cell><cell>81.3 80.5 78.8 81.6 84.2</cell><cell>55.6 56.6 55.4 56.4 60.9</cell><cell>44.0 48.0 46.0 47.0 52.0</cell><cell>40.8 40.9 34.1 41.0 45.2</cell><cell>33.6 31.2 26.4 31.8 36.8</cell><cell>50.9 48.4 50.3 49.3 52.3</cell><cell>41.8 38.9 39.8 39.9 42.4</cell><cell>77.7 76.9 79.6 80.9 91.0</cell><cell>70.0 68.3 71.7 74.7 86.7</cell></row><row><cell>MSMT17 (MT)</cell><cell>Baseline</cell><cell></cell><cell>23.1</cell><cell>48.2</cell><cell>29.2</cell><cell>47.6</cell><cell>16.4</cell><cell>11.0</cell><cell>9.8</cell><cell>5.6</cell><cell>40.8</cell><cell>30.1</cell><cell>74.0</cell><cell>66.7</cell></row><row><cell>data num. 126k</cell><cell cols="2">Baseline-SNR (Ours)</cell><cell>40.9</cell><cell>69.5</cell><cell>49.9</cell><cell>69.2</cell><cell>48.4</cell><cell>39.0</cell><cell>30.3</cell><cell>24.0</cell><cell>57.2</cell><cell>47.5</cell><cell>87.7</cell><cell>81.9</cell></row><row><cell>M + D + C + MT</cell><cell>Baseline</cell><cell></cell><cell>72.4</cell><cell>88.7</cell><cell>70.1</cell><cell>83.8</cell><cell>39.0</cell><cell>28.0</cell><cell>29.6</cell><cell>20.8</cell><cell>52.1</cell><cell>41.5</cell><cell>89.0</cell><cell>85.0</cell></row><row><cell>data num. 220k</cell><cell cols="2">Baseline-SNR (Ours)</cell><cell>82.3</cell><cell>93.4</cell><cell>73.2</cell><cell>85.5</cell><cell>60.0</cell><cell>49.0</cell><cell>41.3</cell><cell>30.4</cell><cell>65.0</cell><cell>55.1</cell><cell>91.9</cell><cell>87.0</cell></row><row><cell cols="2">Market1501 (IDs: 1501) DukeMTMC-reID (IDs: 1404)</cell><cell cols="2">CUHK03 (IDs: 1467)</cell><cell cols="2">MSMT17 (IDs: 4101)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PRID (IDs: 385)</cell><cell>GRID (IDs: 250)</cell><cell cols="2">VIPeR (IDs: 632)</cell><cell cols="2">iLIDs (IDs: 119)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Performance (%) comparison with the latest domain generalizable ReID method Domain-Invariant Mapping Network (DIMN)<ref type="bibr" target="#b44">[45]</ref> under the same experimental setting (i.e., training on the same five datasets, Market1501<ref type="bibr" target="#b68">[69]</ref>+DukeMTMC-reID<ref type="bibr" target="#b70">[71]</ref>+CUHK02<ref type="bibr" target="#b26">[27]</ref>+CUHK03<ref type="bibr" target="#b27">[28]</ref>+CUHK-SYSU<ref type="bibr" target="#b58">[59]</ref>).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Target: PRID</cell><cell cols="2">Target: GRID</cell><cell cols="2">Target: VIPeR</cell><cell cols="2">Target: iLIDs</cell></row><row><cell>Source</cell><cell>Method</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell></cell><cell>DIMN [45] CVPR'19</cell><cell>51.9</cell><cell>39.2</cell><cell>41.1</cell><cell>29.3</cell><cell>60.1</cell><cell>51.2</cell><cell>78.4</cell><cell>70.2</cell></row><row><cell>Market + Duke + CUHK02 + CUHK03 + CUHK-SYSU</cell><cell>Baseline SNR (Ours)</cell><cell>43.8 66.5</cell><cell>35.0 52.1</cell><cell>37.7 47.7</cell><cell>28.0 40.2</cell><cell>54.6 61.3</cell><cell>45.6 52.9</cell><cell>75.3 89.9</cell><cell>65.0 84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Differences between settings of supervised, domain adaptive, and domain generalizable ReID.</figDesc><table><row><cell>Setting</cell><cell>Use target domain data?</cell><cell>Use target domain label?</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell></row><row><cell>Domain adaptation</cell><cell></cell><cell></cell></row><row><cell>Domain generalization</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Performance (%) comparisons with the state-of-the-art RGB-IR ReID approaches on SYSU-MM01 dataset. R1, R10, R20 denote Rank-1, Rank-10 and Rank-20 accuracy, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">All Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Indoor-Search</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Veneue</cell><cell></cell><cell cols="2">Single-Shot</cell><cell></cell><cell></cell><cell cols="2">Multi-shot</cell><cell></cell><cell></cell><cell cols="2">Single-Shot</cell><cell></cell><cell></cell><cell cols="2">Multi-Shot</cell><cell></cell></row><row><cell></cell><cell></cell><cell>mAP</cell><cell>R1</cell><cell>R10</cell><cell>R20</cell><cell>mAP</cell><cell>R1</cell><cell>R10</cell><cell>R20</cell><cell>mAP</cell><cell>R1</cell><cell>R10</cell><cell>R20</cell><cell>mAP</cell><cell>R1</cell><cell>R10</cell><cell>R20</cell></row><row><cell>HOG [4]</cell><cell>CVPR'05</cell><cell>4.24</cell><cell>2.76</cell><cell>18.3</cell><cell>32.0</cell><cell>2.16</cell><cell>3.82</cell><cell>22.8</cell><cell>37.7</cell><cell>7.25</cell><cell>3.22</cell><cell>24.7</cell><cell>44.6</cell><cell>3.51</cell><cell>4.75</cell><cell>29.1</cell><cell>49.4</cell></row><row><cell>MLBP [32]</cell><cell>ICCV'15</cell><cell>3.86</cell><cell>2.12</cell><cell>16.2</cell><cell>28.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LOMO [31]</cell><cell>CVPR'15</cell><cell>4.53</cell><cell>3.64</cell><cell>23.2</cell><cell>37.3</cell><cell>2.28</cell><cell>4.70</cell><cell>28.3</cell><cell>43.1</cell><cell>10.2</cell><cell>5.75</cell><cell>34.4</cell><cell>54.9</cell><cell>5.64</cell><cell>7.36</cell><cell>40.4</cell><cell>60.4</cell></row><row><cell>GSM [33]</cell><cell>TPAMI'17</cell><cell>8.00</cell><cell>5.29</cell><cell>33.7</cell><cell>53.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>One-stream [58]</cell><cell>ICCV'17</cell><cell>13.7</cell><cell>12.1</cell><cell>49.7</cell><cell>66.8</cell><cell>8.59</cell><cell>16.3</cell><cell>58.2</cell><cell>75.1</cell><cell>56.0</cell><cell>17.0</cell><cell>63.6</cell><cell>82.1</cell><cell>15.1</cell><cell>22.7</cell><cell>71.8</cell><cell>87.9</cell></row><row><cell>Two-stream [58]</cell><cell>ICCV'17</cell><cell>12.9</cell><cell>11.7</cell><cell>48.0</cell><cell>65.5</cell><cell>8.03</cell><cell>16.4</cell><cell>58.4</cell><cell>74.5</cell><cell>21.5</cell><cell>15.6</cell><cell>61.2</cell><cell>81.1</cell><cell>14.0</cell><cell>22.5</cell><cell>72.3</cell><cell>88.7</cell></row><row><cell>Zero-Padding [58]</cell><cell>ICCV'17</cell><cell>16.0</cell><cell>14.8</cell><cell>52.2</cell><cell>71.4</cell><cell>10.9</cell><cell>19.2</cell><cell>61.4</cell><cell>78.5</cell><cell>27.0</cell><cell>20.6</cell><cell>68.4</cell><cell>85.8</cell><cell>18.7</cell><cell>24.5</cell><cell>75.9</cell><cell>91.4</cell></row><row><cell>TONE [61]</cell><cell>AAAI'18</cell><cell>14.4</cell><cell>12.5</cell><cell>50.7</cell><cell>68.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HCML [61]</cell><cell>AAAI'18</cell><cell>16.2</cell><cell>14.3</cell><cell>53.2</cell><cell>69.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BCTR [62]</cell><cell>IJCAI'18</cell><cell>19.2</cell><cell>16.2</cell><cell>54.9</cell><cell>71.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BDTR [62]</cell><cell>IJCAI'18</cell><cell>19.7</cell><cell>17.1</cell><cell>55.5</cell><cell>72.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>D-HSME [12]</cell><cell>AAAI'19</cell><cell>23.2</cell><cell>20.7</cell><cell>62.8</cell><cell>78.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>cmGAN [3]</cell><cell>IJCAI'18</cell><cell>27.8</cell><cell>27.0</cell><cell>67.5</cell><cell>80.6</cell><cell>22.3</cell><cell>31.5</cell><cell>72.7</cell><cell>85.0</cell><cell>42.2</cell><cell>31.7</cell><cell>77.2</cell><cell>89.2</cell><cell>32.8</cell><cell>37.0</cell><cell>80.9</cell><cell>92.3</cell></row><row><cell>D 2 RL [55]</cell><cell>CVPR'19</cell><cell>29.2</cell><cell>28.9</cell><cell>70.6</cell><cell>82.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>This work</cell><cell>25.5</cell><cell>26.3</cell><cell>66.7</cell><cell>80.2</cell><cell>19.2</cell><cell>32.7</cell><cell>73.5</cell><cell>86.8</cell><cell>39.4</cell><cell>30.8</cell><cell>75.1</cell><cell>86.8</cell><cell>29.0</cell><cell>40.1</cell><cell>83.1</cell><cell>93.6</cell></row><row><cell>Ours</cell><cell>This work</cell><cell>33.9</cell><cell>34.6</cell><cell>75.9</cell><cell>86.6</cell><cell>27.4</cell><cell>41.7</cell><cell>83.3</cell><cell>92.3</cell><cell>50.4</cell><cell>40.9</cell><cell>83.8</cell><cell>91.8</cell><cell>40.5</cell><cell>50.0</cell><cell>91.4</cell><cell>96.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Evaluation of the generalization capability of proposed SNR modules on OSNet [75]. We use the official released weights and codes of OSNet for the experiments.</figDesc><table><row><cell></cell><cell cols="2">M−→D</cell><cell cols="2">D−→M</cell></row><row><cell>Method</cell><cell cols="4">mAP Rank-1 mAP Rank-1</cell></row><row><cell>Baseline (ResNet50)</cell><cell>19.8</cell><cell>35.3</cell><cell>21.8</cell><cell>48.3</cell></row><row><cell>OSNet [75]</cell><cell>19.3</cell><cell>35.2</cell><cell>21.7</cell><cell>49.9</cell></row><row><cell>OSNet-IBN [75]</cell><cell>26.7</cell><cell>48.5</cell><cell>26.1</cell><cell>57.7</cell></row><row><cell>OSNet-SNR</cell><cell>31.7</cell><cell>53.6</cell><cell>31.6</cell><cell>62.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All faces in the images are masked for anonymization. arXiv:2005.11037v1 [cs.CV] 22 May 2020</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Re-id done right: towards good practices for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojana</forename><surname>Gajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naila</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05339</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification with generative adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="677" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1811.10144</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fd-gan: Pose-guided feature distilling gan for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hsme: Hypersphere manifold embedding for visible thermal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8385" to="8392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Frustratingly easy person re-identification: Generalizing person re-id in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqi</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-shot knowledge distillation for image-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantics-aligned representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fairest of them all: Establishing a strong baseline for cross-domain person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devinder</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthipan</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Marchwica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12016</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3594" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptation and reidentification network: An unsupervised deep transfer learning approach to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3685" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-domain visual matching via generalized similarity measure and feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1089" to="1102" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Chichung</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive transfer network for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Timedelayed correlation analysis for multi-camera activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen Change Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="129" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Differentiable learning-to-normalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanglin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A novel unsupervised camera-aware domain adaptation framework for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Posenormalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalizable person reidentification by domain-invariant mapping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification with iterative self-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Resource aware person re-identification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequn</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to reduce dual-level discrepancy for infrared-visible person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Person transfer GAN to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wei-Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Shaogang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rgb-infrared cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5380" to="5389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">End-to-end deep learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Patch-based discriminative feature learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchical discriminative learning for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visible thermal person re-identification via dual-constrained top-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyuan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1092" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Crossview asymmetric metric learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Person reidentification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
