<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
							<email>laynehuang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<email>chzhang@cis.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<email>kai.han@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
							<email>jinge.yao@microsoft.com</email>
						</author>
						<title level="a" type="main">Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification is a challenging task due to various complex factors. Recent studies have attempted to integrate human parsing results or externally defined attributes to help capture human parts or important object regions. On the other hand, there still exist many useful contextual cues that do not fall into the scope of predefined human parts or attributes. In this paper, we address the missed contextual cues by exploiting both the accurate human parts and the coarse non-human parts. In our implementation, we apply a human parsing model to extract the binary human part masks and a self-attention mechanism to capture the soft latent (non-human) part masks. We verify the effectiveness of our approach with new state-of-the-art performances on three challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03. Our implementation is available at https://github.com/ggjy/P2Net.pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification has attracted increasing attention from both the academia and the industry in the past decade due to its significant role in video surveillance. Given an image for a particular person captured by one camera, the goal is to re-identify this person from images captured by different cameras from various viewpoints.</p><p>The task of person re-identification is inherently challenging because of the significant visual appearance changes caused by various factors such as human pose variations, lighting conditions, part occlusions, background cluttering and distinct camera viewpoints. All these factors make the misalignment problem become one of the most important problems in person re-identification task. With the surge of interest in deep representation learning, various approaches have been developed to address the misalignment problem, which could be roughly summarized as the following streams: <ref type="bibr" target="#b0">(1)</ref> Hand-crafted partitioning, * Corresponding author. † Equal contribution. which relies on manually designed splits such as grid cells <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b56">57]</ref> or horizontal stripes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref> of the input image or feature maps, based on the assumption that human parts are well-aligned in the RGB color space. (2) The attention mechanism, which tries to learn an attention map over the last output feature map and constructs the aligned part features accordingly <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b44">45]</ref>. (3) Predicting a set of predefined attributes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36]</ref> as useful features to guide the matching process. (4) Injecting human pose estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b26">27]</ref> or human parsing result <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref> to extract the human part aligned features based on the predicted human key points or semantic human part regions, while the success of such approaches heavily count on the accuracy of human parsing models or pose estimators. Most of the previous studies mainly focus on learning more accurate human part representations, while neglecting the influence of potentially useful contextual cues that could be addressed as "non-human" parts.</p><p>Existing human parsing based approaches <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b54">55]</ref> utilize an off-the-shelf semantic segmentation model to divide the input image into K predefined human parts, according to a predefined label set. <ref type="bibr" target="#b0">1</ref> Beyond these predefined part categories, there still exist many objects or parts which could be critical for person re-identification, but tend to be recognized as background by the pre-trained human parsing models. For example, we illustrate some failure cases from human parsing results on the Market-1501 dataset in <ref type="figure">Figure 1</ref>. We can find that the objects belonging to undefined categories such as backpack, reticule and umbrella are in fact helpful and sometimes crucial for person re-identification. The existing human parsing datasets are mainly focused on parsing human regions, and most of these datasets fail to include all possible identifiable objects that could help person re-identification. Especially, most of the previous attention Query Image Gallery Image <ref type="figure">Figure 1</ref>: Failure cases of the human parsing model: The first row illustrates the query images, the second row illustrates the gallery images from Market-1501 and each column consists of two images belonging to the same identity. All of the regions marked with red circle are mis-classified as background (marked with black color) due to the limited label set while their ground-truth labels should be backpack, reticule and umbrella. It can be seen that these mis-classified regions are crucial for the person re-identification.</p><p>based approaches are mainly focused on extracting the human part attention maps.</p><p>Explicitly capturing useful information beyond predefined human parts or attributes has not been well studied in the previous literature. Inspired by the recently popular self-attention scheme <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48]</ref>, we attempt to address the above problem by learning latent part masks from the raw data, according to the appearance similarities among pixels, which provide a coarse estimation of both human parts and the non-human parts, with the latter largely overlooked from the previous approaches based on human parsing.</p><p>Moreover, we propose a dual part-aligned representation scheme to combine the complementary information from both the accurate human parts and the coarse nonhuman parts. In our implementation, we apply a human parsing model to extract the human part masks and compute the human part-aligned representations for the features from low-levels to high-levels. For the non-human part information, we apply self-attention mechanism to learn to group all pixels belonging to the same latent part together. We also extract the latent non-human part information on the feature maps from the low-levels to the high-levels. Through combining the advantages of both the accurate human part information and the coarse non-human part information, our approach learns to augment the representation of each pixel with the representation of the part (human parts or nonhuman parts) that it belongs to.</p><p>Our main contributions are summarized as below:</p><p>• We propose the dual part-aligned representation to update the representation by exploiting the complementary information from both the accurate human parts and the coarse non-human parts. • We introduce the P 2 -Net and show that our P 2 -Net achieves new state-of-the-art performance on three benchmarks including Market-1501, DukeMTMC-reID and CUHK03.</p><p>• We analyze the contributions from both the human part representation and the latent part (non-human part) representation and discuss their complementary strengths in our ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The part misalignment problem is one of the key challenges for person re-identification, a host of methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8]</ref> have been proposed to mainly exploit the human parts to handle the body part misalignment problem, we briefly summarize the existing methods as below: Hand-crafted Splitting for ReID. In previous studies, there are methods proposed to divide the input image or the feature map into small patches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref> or stripes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref> and then extract region features from the local patches or stripes. For instance, PCB <ref type="bibr" target="#b40">[41]</ref> adopts a uniform partition and further refines every stripe with a novel mechanism. The hand-crafted approaches depend on the strong assumption that the spatial distributions of human bodies and human poses are exactly matching. Semantic Segmentation for ReID. Different from the hand-crafted splitting approaches, <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b9">10]</ref> apply a human part detector or a human parsing model to capture more accurate human parts. For example, SPReID <ref type="bibr" target="#b9">[10]</ref> utilizes a parsing model to generate 5 different predefined human part masks to compute more reliable part representations, which achieves promising results on various person re-identification benchmarks. Poses/Keypoints for ReID. Similar to the semantic segmentation approaches, poses or keypoints estimation can also be used for accurate/reliable human part localization. For example, there are approaches exploring both the human poses and the human part masks <ref type="bibr" target="#b8">[9]</ref>, or generating human part masks via exploting the connectivity of the keypoints <ref type="bibr" target="#b49">[50]</ref>. There are some other studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">55]</ref> that also exploit the pose cues to extract the part-aligned features. Attention for ReID. Attention mechanisms have been used to capture human part information in recent work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>. Typically, the predicted attention maps distribute most of the attention weights on human parts that may help improve the results. To the best of our knowledge, we find that most of the previous attention approaches are limited to capturing the human parts only. Attributes for ReID. Semantic attributes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref> have been exploited as feature representations for person reidentification tasks. Previous work <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b57">58]</ref> leverages the attribute labels provided by original dataset to generate attribute-aware feature representation. Different from previous work, our latent part branch can attend to important visual cues without relying on detailed supervision signals from the limited predefined attributes. Our Approach. To the best of our knowledge, we are the first to explore and define the (non-human) contextual cues. We empirically demonstrate the effectiveness of combining separately crafted components for the well-defined, accurate human parts and all other potentially useful (but coarse) contextual regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>First, we present our key contribution: dual partaligned representation, which learns to combine both the accurate human part information and the coarse latent part information to augment the representation of each pixel (Sec. 3.1). Second, we present the network architecture and the detailed implementation of P 2 -Net (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dual Part-Aligned Representation</head><p>Our approach consists of two branches: a human part branch and a latent part branch. Given an input feature map X of size N × C, where N = H × W , H and W are the height and width of the feature map, C is the number of channels, we apply the human part branch to extract accurate human part masks and compute the human partaligned representation X Human accordingly. We also use a latent part branch to learn to capture both the coarse nonhuman part masks and the coarse human part masks based on the appearance similarities between different pixels, then we compute the latent part-aligned representation X Latent according to the coarse part masks. Last, we augment the original representation with both the human part-aligned representation and the latent part-aligned representation. Human Part-Aligned Representation. The main idea of the human part-aligned representation is to represent each pixel with the human part representation that the pixel belongs to, which is the aggregation of the pixel-wise representations weighted by a set of confidence maps. Each confidence map is used to surrogate a semantic human part.</p><p>We illustrate how to compute the human part-aligned representation in this section. Assuming there are K−1 predefined human part categories in total from a human parsing model, we treat all the rest proportion of regions in the image as background according to the human parsing result. In summary, we need to estimate K confidence maps for the human part branch.</p><p>We apply the state-of-the-art human parsing framework CE2P <ref type="bibr" target="#b22">[23]</ref> to predict the semantic human part masks for all the images in all three benchmarks in advance, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(b). We denote the predicted label map of image I as L. We re-scale the label map L to be of the same size as the feature map X (x i is the representation of pixel i, essentially the i th row of X) before using it. We use l i to represent the human part category of pixel i within the rescaled label map, and l i is of K different values including K − 1 human part categories and one background category.</p><p>We denote the K confidence maps as P 1 , P 2 , · · · , P K , where each confidence map P k is associated with a human part category (or the background category). According to the predicted label map L, we set p ki = 1 (p ki is the i th element of P k ) if l i ≡ k and p ki = 0 otherwise. Then we apply L1 normalization on each confidence map and compute the human part representation as below,</p><formula xml:id="formula_0">h k = g( N i=1p ki x i ),<label>(1)</label></formula><p>where h k is the representation of the k th human part, g function is used to learn better representation andp ki is the confidence score after L1 normalization. Then we generate the human part-aligned feature map X Human of the same size as the input feature map X, and each element of X Human is set as</p><formula xml:id="formula_1">x Human i = K k=1 1[l i ≡ k]h k ,<label>(2)</label></formula><p>where 1[l i ≡ k] is an indicator function and each x Human i is essentially the part representation of the semantic human part that it belongs to. For the pixels predicted as the background, we choose to aggregate the representations of all pixels that are predicted as the background and use it to augment their original representations. Latent Part-Aligned Representation. We explain how to estimate the latent part representation in this section. Since we can not predict accurate masks for non-human cues based on the existing approaches, we adapt the selfattention mechanism <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48]</ref> to enhance our framework by learning to capture some coarse latent parts automatically from data based on the semantic similarities between each pixel and other pixels. The latent part is expected to capture details that are weakly utilized in the human part branch.  ResNet-50 backbone consists of a stem, four stages (e.g., Res-1, Res-2, Res-3 and Res-4), global average pooling (GAP) and a classifier. We insert a DPB after every stage within the ResNet backbone. (b) The DPB consists of a human part branch and a latent part branch. For the human part branch, we employ the CE2P <ref type="bibr" target="#b22">[23]</ref> to predict the human part label maps and generate the human part masks accordingly. For the latent part branch, we employ the self-attention scheme to predict the latent part masks. We compute the human part-aligned representation and latent part-aligned representation within the two branches separately. Last, we add the outputs from these two branches to the input feature map as the final output feature map.</p><p>We are particularly interested in the contribution from the coarse non-human part masks on the important cues that are missed by the predefined human parts or attributes. In our implementation, the latent part branch learns to predict N coarse confidence maps Q 1 , Q 2 , · · · , Q N for all N pixels, each confidence map Q i learns to pay more attention to the pixels that belong to the same latent part category as the i th pixel.</p><p>We illustrate how to compute the confidence map for the pixel i as below,</p><formula xml:id="formula_2">q ij = 1 Z i exp(θ(x j ) φ(x i )),<label>(3)</label></formula><p>where q ij is the j th element of Q i , x i and x j are the representations of the pixels i and j respectively. θ(·) and φ(·) are two transform functions to learn better similarities and are implemented as 1 × 1 convolution, following the self-attention mechanism <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48]</ref>. The normalization factor Z i is a sum of all the similarities associated with pixel i:</p><formula xml:id="formula_3">Z i = N j=1 exp(θ(x j ) φ(x i ))</formula><p>. Then we estimate the latent part-aligned feature map X Latent as below,</p><formula xml:id="formula_4">x Latent i = N j=1 q ij ψ(x j ),<label>(4)</label></formula><p>where x Latent i is the i th element of X Latent . We estimate the latent part-aligned representation for pixel i by aggregating the representations of all the other pixels according to their similarities with pixel i. ψ is a function used to learn better representation, which is implemented with 1 × 1 convolution + BN + ReLU.</p><p>For the latent part-aligned representation, we expect each pixel can pay more attention to the part that it belongs to, which is similar with the recent work <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b53">54]</ref>. The selfattention is a suitable mechanism to group the pixels with similar appearance together. We empirically study the influence of the coarse human part information and the coarse non-human part information to verify the effectiveness is mainly attributed to the coarse non-human parts (Sec. 4.3).</p><p>Last, we fuse the human part-aligned representation and the latent part-aligned representation as below,</p><formula xml:id="formula_5">Z = X + X Human + X Latent ,<label>(5)</label></formula><p>where Z is the final representation of our DPB block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">P 2 -Net</head><p>Backbone. We use ResNet-50 pre-trained on ImageNet as the backbone following the previous PCB <ref type="bibr" target="#b40">[41]</ref>. For the latent part branch, we employ the self-attention mechanism on the output feature map from each stage directly. Network Architecture. The ResNet backbone takes an image I as input and outputs feature map X after the Res-4 stage. We feed the feature map X into the global average pooling layer and employ the classifier at last. We insert the DPB after every stage to update the representation before feeding the feature map into the next stage. We could achieve better performance through applying more DPBs.</p><p>The overall pipeline is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(a). Loss Function. All of our baseline experiments only employ the softmax loss to ensure the fairness of the comparison and for ease of ablation study. To compare with the state-of-the-art approaches, we further employ the triplet loss (details in appendix) following the previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>Market-1501. Market-1501 dataset <ref type="bibr" target="#b58">[59]</ref> consists of 1501 identities captured by 6 cameras, where the train set consists of 12, 936 images of 751 identities, the test set is divided into a query set that contains 3, 368 images and a gallery set that contains 16, 364 images.</p><p>DukeMTMC-reID. DukeMTMC-reID dataset <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b59">60]</ref> consists of 36, 411 images of 1, 404 identities captured by 8 cameras, where the train set contains 16, 522 images, the query set consists of 2, 228 images and the gallery set consists of 17, 661 images.</p><p>CUHK03. CUHK03 dataset <ref type="bibr" target="#b14">[15]</ref> contains 14, 096 images of 1, 467 identities captured by 6 cameras. CUHK03 provides two types of data, hand-labeled ("labeled") and DPMdetected ("detected") bounding boxes, the latter type is more challenging due to severe bounding box misalignment and cluttered background. We conduct experiments on both "labeled" and "detected" types of data. We split the dataset following the training/testing split protocol proposed in <ref type="bibr" target="#b60">[61]</ref>, where the train/query/gallery set consists of 7, 368/1, 400/5, 328 images respectively. We employ two kinds of evaluation metrics including the cumulative matching characteristics (CMC) and mean average precision (mAP). Especially, all of our experiments employ the single-query setting without any other postprocessing techniques such as re-ranking <ref type="bibr" target="#b60">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We choose ResNet-50 pre-trained on ImageNet as our backbone. After getting the feature map from the last residual block, we use a global average pooling and a linear layer (FC+BN+ReLU) to compute a 256-D feature embedding. We use ResNet-50 trained with softmax loss as our baseline model, and set the stride of the last stage in ResNet from 2 to 1 following <ref type="bibr" target="#b40">[41]</ref>. We also use triplet loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b55">56]</ref> to improve the performance.</p><p>We use the state-of-the-art human parsing model CE2P <ref type="bibr" target="#b22">[23]</ref> to predict the human part label maps for all the images in the three benchmark in advance. The CE2P model is trained on the Look Into Person <ref type="bibr" target="#b17">[18]</ref> (LIP) dataset, which consists of ∼30, 000 finely annotated images with 20 semantic labels (19 human parts and 1 background). We divide the 20 semantic categories into K groups 2 , and train the CE2P model with the grouped labels. We adopt the training strategies as described in CE2P <ref type="bibr" target="#b22">[23]</ref>.</p><p>All of our implementations are based on PyTorch framework <ref type="bibr" target="#b25">[26]</ref>. We resize all the training images to 384 × 128 and then augment them by horizontal flip and random erasing <ref type="bibr" target="#b61">[62]</ref>. We set the batch size as 64 and train the model with base learning rate starts from 0.05 and decays to 0.005 after 40 epochs, the training is finished at 60 epochs. We set momentum µ = 0.9 and the weight decay as 0.0005. All of the experiments are conducted on a single NVIDIA TITAN XP GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>The core idea of DPB lies on the human part branch and the latent part branch. We perform comprehensive ablation studies of them in follows. Influence of the part numbers for human part branch. As we can divide the input image into different number of parts in different levels. we study the impact of the number of different semantic parts (i.e., K = 1, K = 2, K = 5) on the Market-1501 benchmark. We summarize all of the results in <ref type="table">Table 1</ref>. The 1 st row reports the results of baseline model and the the 2 nd row to 4 th report the performances that only apply the human part branch with different choices of K. When K = 1, there is no extra parsing information added to the network and the performances keep almost the <ref type="table">Table 1</ref>: Ablation study of the DPB on Market-1501. K is the number of human parts within the human part branch, We insert the DPB after the stage-k (Res-k), where k = 1, 2, 3, 4. We employ HP-p to represent the human part branch choosing K = p. DPB (HP-p) represents using the human part branch only while DPB (Latent) represents using the latent part branch only.    same with the baseline model. When K = 2, the human part branch introduces the foreground and the background contextual information to help extract more reliable human context information. we can observe obvious improvements in R-1 and mAP compared to the previous two results. The performance improves with larger K, which indicates that accurately aggregating contextual information from pixels belonging to same semantic human part is crucial for person re-identification. We set K = 5 as the default setting for human part branch if not specified. Non-human part in latent part branch. The choice of self-attention for latent part branch is mainly inspired by that self-attention can learn to group the similar pixels together without extra supervision (also shown useful in segmentation <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b11">12]</ref>). Considering that latent part branch is in fact the mixture of the coarse human and non-human part information, we empirically verify that the performance  gain from latent part branch is mainly attributed to capturing non-human parts, as shown in <ref type="table" target="#tab_3">Table 2</ref>. We use binary masks predicted by human parsing model (K = 2) to control the influence of human or non-human regions within latent part branch. Here we study two kinds of settings: (1) only use non-human part information within latent part branch. We apply binary human masks (1 for non-human pixels and 0 for human pixels) to remove the influence of pixels predicted as human parts, which is called as Latent w/o HP. <ref type="bibr" target="#b1">(2)</ref> only use human part information within latent part branch. We also apply binary human masks (1 for human pixels and 0 for non-human pixels) to remove the influence of pixels predicted as non-human parts, which is called as Latent w/o NHP. It can be seen that the gain of latent part branch mainly comes from the help of non-human part information, Latent w/o HP outperforms Latent w/o NHP and is very close to the original latent part branch.</p><p>Besides, we study the contribution of latent branch when applying human part branch (HP-5). We choose DPB (HP-5) inserted after Res-2 as our baseline and add latent part branch that applies self-attention on either the human regions only (Latent w/o NHP in <ref type="figure" target="#fig_2">Figure 3</ref>) or non-human regions only (Latent w/o HP in <ref type="figure" target="#fig_2">Figure 3</ref>). It can be seen that DPB (HP-5 + Latent w/o HP) largely outperforms DPB (HP-5 + Latent w/o NHP) and is close to DPB (HP-5 + Latent), which further verifies the effectiveness of latent part branch is mainly attributed to exploiting the non-human parts. Complementarity of two branches. Dual part-aligned block (DPB) consists of two branches: human part branch and latent part branch. The human part branch helps improve the performance by eliminating the influence of noisy background context information, and the latent part branch introduces latent part masks to surrogate various non-human parts. We empirically show that the two branches are complementary through the experimental results on the 6 th row of <ref type="table">Table 1</ref>. It can be seen that combining both the human part-aligned representation and the latent part-aligned representation boosts the performance for all stages. We can draw the following conclusions from <ref type="table" target="#tab_4">Table 3 and Table 4</ref>: (i) Although the latent part masks are learned from scratch, DPB (latent) achieves comparable results with the human part branch in general, which carries stronger prior information of human parts knowledge, showing the importance of the non-human part context. (ii) Human part branch and latent part branch are complementary to each other. In comparison to the results only using a single branch, inserting 5× DPB attains 1% and 3% gain in terms of R-1 and mAP on Market-1501, 1.6% and 1% gain in terms of R-1 and mAP on CUHK03, respectively.</p><p>We visualize the predicted human part masks to illustrate how it helps improve the performance in <ref type="figure" target="#fig_3">Figure 4</ref>. For all query images above, baseline method fails to return the correct images of the same identity while we can find out the correct images by employing human part masks. In summary, we can see that the context information of the non-informative background influences the final results and the human part masks eliminate the influence of these noisy  Net (w/ Human Part Masks). There exist some important nonhuman parts in all of the four query images. The P 2 -Net (w/ Human Part Masks) categorizes these crucial non-human parts as background and fails to return the correct image at Recall@1. The P 2 -Net (w/ Latent Part Masks) predicts the latent part mask associated with these non-human parts, which successfully returns the correct image at Recall@1. It can be seen that the predicted latent part masks serves as reliable surrogate for the non-human part.</p><p>context information.</p><p>There also exist large amounts of scenarios that nonhuman part context information is the key factor. We illustrate some typical examples in <ref type="figure" target="#fig_5">Figure 5</ref>, and we mark the non-human but informative parts with red circles. For example, the 1 st and 4 th row illustrate that mis-classifying the bag as background causes the failure of the human part masks based method. Our approach addresses these failed cases through learning the latent part masks and it can be seen that the predicted latent part masks within latent part branch well surrogate the non-human but informative parts. In summary, the human part branch benefits from the latent part branch through dealing with crucial non-human part information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of DPB.</head><p>To study the influence of the numbers of DPB (with human part representation only, with latent part representation only and with both human and latent part representations), we add 1 block (to Res-2), 3 blocks (2 to Res-2, and 1 to Res-3) and 5 blocks (2 to Res-2, and 3 to Res-3) within the backbone network. As shown in <ref type="table" target="#tab_4">Table 3</ref>, more DPB blocks lead to better performance. We achieve the best performance with 5 DPB blocks, which boosts the R-1 accuracy and mAP by 5.6% and 11.9%, respectively. We set the number of DPB block as 5 in all of our state-ofthe-art experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with state-of-the-art</head><p>We empirically verify the effectiveness of our approach with a series of state-of-the-art (SOTA) results on all of the three benchmarks. We illustrate more details as following.</p><p>We illustrate the comparisons of our P 2 -Net with the previous state-of-the-art methods on Market-1501 in Table 5. Our P 2 -Net outperforms all the previous methods by a large margin. We achieve a new SOTA performance such as R-1=95.2% and mAP=85.6% respectively. Especially, our P 2 -Net outperforms the previous PCB by 1.8% in mAP without using multiple softmax losses for training. When equiped with the triplet loss, our P 2 -Net still outperforms the PCB by 1.4% and 4.0% in terms of R-1 and mAP, respectively. Besides, our proposed P 2 -Net also outperforms the SPReID [10] by 2.7% measured by R-1 accuracy.</p><p>We summarize the comparisons on DukeMTMC-reID in <ref type="table" target="#tab_7">Table 6</ref>. It can be seen that P 2 -Net surpasses all previous SOTA methods. SPReID <ref type="bibr" target="#b9">[10]</ref> is the method has the closest performance with us in R-1 accuracy. Notably, SPReID train their model with more than 10 extra datasets to improve the performance while we only use the original dataset as training set.</p><p>Last, we evaluate our P 2 -Net on CUHK03 dataset. We follow the training/testing protocol proposed by <ref type="bibr" target="#b60">[61]</ref>. As illustrated in <ref type="table" target="#tab_8">Table 7</ref>, our P 2 -Net outperforms previous SOTA method MGCAM <ref type="bibr" target="#b33">[34]</ref> by 28.2% measured by R-1 accuracy and 23.4% measured by mAP. For the CUHK03detected dataset, our P 2 -Net still outperforms previous SOTA method PCB+RPP <ref type="bibr" target="#b40">[41]</ref> by 11.2% measured by R-1 accuracy and 11.4% measured by mAP.</p><p>In conclusion, our P 2 -Net outperforms all the previous approaches by a large margin and achieves new state-of-  the-art performances on all the three challenging person reidentification benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel dual part-aligned representation scheme to address the non-human part misalignment problem for person re-identification. It consists of a human part branch and a latent part branch to tackle both human part misalignment and non-human part misalignment problem. The human part branch adopts off-theshelf human parsing model to inject structural prior information by capturing the predefined semantic human parts for a person. The latent part branch adopts a self-attention mechanism to help capture the detailed part categories beyond the injected prior information. Based on our dual partaligned representation, we achieve new state-of-the-art performances on all of the three benchmarks including Market-1501, DukeMTMC-reID and CUHK03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>In this supplementary material, we provide the complexity analysis of our method, details about triplet loss, and show more typical experiment results and cases on DukeMTMC-ReID and CUHK03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Triplet loss</head><p>As mentioned in Sec 3.2, we use triplet loss to improve the performance in final results. The details are as folows: <ref type="bibr" target="#b0">(1)</ref> We prepare each mini-batch by randomly sampling 16 classes (identities) and 4 images for each class. <ref type="bibr" target="#b1">(2)</ref> We set the weight rate as 1:1 on all three datasets. (3) Given a minibatch of 64 samples, we construct a triplet for each sample by choosing the hardest positive sample and the hardest negative sample measured by their Euclidean distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Strategies for inserting DPB.</head><p>We do the ablation study to find the results of adding DPB after different Res-k residual blocks. As shown in the <ref type="table">Table 1</ref> of main paper, we can find that all types of blocks (DPB/Human Part Branch/Latent Part Branch) achieve better performances when they are inserted after the Res-2 and Res-3 stages, compared to Res-1 and Res-4 stages. Specifically, the DPB improves the Rank-1 accuracy and mAP by 4.4% and 9.5% when inserted after res-2 stage, 3.4% and 7.3% when inserted after res-3 stage, respectively. One possible explanation is that the feature map from Res-1 has more precise localization information but less semantic information, and the deeper feature map from Res-4 is insufficient to provide precise spatial information. In conclusion, Res-2 and Res-3 can benefit more from the proposed DPB. So the 5×DPB in all experiments means that we add 2 DPB blocks to Res-2 and 3 DPB blocks to Res-3, if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Complexity analysis</head><p>We compare the proposed model with ResNet-50 and ResNet-101 in model size and computation complexity, measured by the number of parameers and FLOPs during inference on CUHK03. And we test the inference time of each forward pass on a single GTX 1080Ti GPU with CUDA8.0 given an input image of size 3 × 384 × 128. <ref type="table" target="#tab_9">Table 8</ref> shows that our method outperforms ResNet-101 with smaller model size, less computation amount and faster inference speed, the improvement of P 2 -Net is not just because the added depth to the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Experiments on DukeMTMC-reID</head><p>To further verify that the latent part branch and the human part branch are complementary, we also conduct the controlled experiments on both DukeMTMC-ReID and CUHK03.</p><p>We present the results on DukeMTMC-reID in <ref type="table" target="#tab_10">Table 9</ref>. It can be seen that DPB achieves better performance than   either only employing the latnet part branch or only employing the human part branch. e.g., "1 × DPB" improves the mAP of "1 × DPB (HP-5)" from 66.99 to 67.93. "5 × DPB" improves the mAP of "5 × HPP (HP-5)" from 68.64 to 70.84.</p><p>We present the advantages of human part branch in <ref type="figure">Figure 6</ref> . The results with human part branch perform more robust compared with the results of baseline and the results with the latent part branch. For example, the query image on the 1 st line carries the misleading information caused by the part of a car. Both the baseline method and the method with latent part branch return the images carrying parts of the car, and the method with human part branch returns the correct result by removing the influence of the car.</p><p>We also present the benefits of latent part branch in <ref type="figure">Figure 7</ref>. The failed cases in both the baseline and the method with human part branch are solved by using the latent part masks generated by latent part branch. It can be seen that these latent part masks capture some non-human but im- There exist some important non-human parts within all these two query images. The DPB (w/ Human Part Masks) categorizes these important parts to background and fails to return the correct image. The DPB (w/ Latent Part Masks) predicts the latent part mask associated with these parts, which helps to find the correct image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the overall structure of P 2 -Net and the dual part-aligned block (DPB). (a) Given an input image, we employ a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Latent w/o NHP vs. Latent w/o HP: Latent w/o NHP only applies self-attention on the human part regions while Latent w/o HP only applies self-attention on the non-human part regions. The human/non-human part regions are based on the human parsing prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of Baseline and P 2 -Net that only employs the human part branch. For all of the four query images, Recall@3 of the Baseline method is 0, while the Recall@1 of the P 2 -Net (w/ Human Part Masks) is 1. The 1 st and 2 nd rows illustrate the cases that the bag is visible in one viewpoint but invisible in other viewpoints, human part masks eliminate the influence of the bags as the bags are categorized as background. The 3 rd and 4 th rows illustrate cases that the area of person only occupies small proportions of the whole images and the background context information leads to poor performance, human part masks can eliminate the influence of background regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of P 2 -Net (w/ Latent Part Masks) and P 2 -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Comparison of Baseline, DPB (w/ Latent Part Masks) and DPB (w/ Human Part Masks) on DukeMTMC-ReID. We denote P 2 -Net that only employs human part branch as the method w/ Human Part Masks. Both these two query images suffer from the problem of occlusions and contain useless or misleading background information. Both the baseline and DPB (w/ Latent Part Masks) fail to return the correct results within the top 3 positions while DPB (w/ Human Part Masks) returns the correct result at top 1 position. Comparison of Baseline, DPB (w/ Human Part Masks) and DPB (w/ Latent Part Masks) on DukeMTMC-ReID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Dual Part-Aligned Representation. In our implementation, we employ the dual part-aligned block (DPB) after Res-1, Res-2, Res-3 and Res-4 stages. Assuming that the input image is of size 384 × 128, the output feature map from Res-1/Res-2/Res-3/Res-4 stage is of size 96 × 32/48 × 16/ 24 × 8/24 × 8 respectively. We have conducted detailed ablation study about DPB in Section 4.3. For the human part branch, we employ the CE2P<ref type="bibr" target="#b22">[23]</ref> model to extract the human part label maps of size 128 × 64, then we resize the label maps to be of the size 96×32/48×16/24×8/24×8 for the four stages respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">Ablation study of the human-part (Latent w/o NHP) and</cell></row><row><cell cols="6">non-human part (Latent w/o HP) in the latent part branch.</cell><cell></cell></row><row><cell>HP-5</cell><cell cols="2">Latent w/o NHP w/o HP Latent</cell><cell cols="2">Market Res-2 R-1 mAP</cell><cell cols="2">Market Res-3 R-1 mAP</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.36</cell><cell>71.48</cell><cell>88.36</cell><cell>71.48</cell></row><row><cell>×</cell><cell></cell><cell>×</cell><cell>91.19</cell><cell>77.22</cell><cell>91.12</cell><cell>77.10</cell></row><row><cell>×</cell><cell>×</cell><cell></cell><cell>91.55</cell><cell>78.25</cell><cell>91.35</cell><cell>77.23</cell></row><row><cell>×</cell><cell></cell><cell></cell><cell cols="2">91.73 78.48</cell><cell>91.47</cell><cell>77.80</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Market Res-2</cell><cell cols="2">CUHK (detected)</cell></row><row><cell></cell><cell>×</cell><cell>×</cell><cell>91.83</cell><cell>78.72</cell><cell>67.57</cell><cell>60.02</cell></row><row><cell></cell><cell></cell><cell>×</cell><cell>91.97</cell><cell>79.31</cell><cell>68.46</cell><cell>61.98</cell></row><row><cell></cell><cell>×</cell><cell></cell><cell cols="2">92.56 80.60</cell><cell>69.61</cell><cell>62.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">HP-5 Latent</cell><cell>R-1</cell><cell>R-5</cell><cell>R-10</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>88.36</cell><cell>95.39</cell><cell>97.06</cell><cell>71.48</cell></row><row><cell>+ 1 × DPB</cell><cell></cell><cell>×</cell><cell>91.83</cell><cell>96.89</cell><cell>97.95</cell><cell>78.72</cell></row><row><cell>+ 3 × DPB</cell><cell></cell><cell>×</cell><cell>92.01</cell><cell>97.15</cell><cell>98.16</cell><cell>78.87</cell></row><row><cell>+ 5 × DPB</cell><cell></cell><cell>×</cell><cell>92.26</cell><cell>97.26</cell><cell>98.20</cell><cell>79.28</cell></row><row><cell>+ 1 × DPB</cell><cell>×</cell><cell></cell><cell>91.73</cell><cell>96.86</cell><cell>98.10</cell><cell>78.48</cell></row><row><cell>+ 3 × DPB</cell><cell>×</cell><cell></cell><cell>92.12</cell><cell>97.32</cell><cell>98.28</cell><cell>80.15</cell></row><row><cell>+ 5 × DPB</cell><cell>×</cell><cell></cell><cell>92.79</cell><cell>97.65</cell><cell>98.52</cell><cell>80.49</cell></row><row><cell>+ 1 × DPB</cell><cell></cell><cell></cell><cell>92.75</cell><cell>97.45</cell><cell>98.22</cell><cell>80.98</cell></row><row><cell>+ 3 × DPB</cell><cell></cell><cell></cell><cell>93.28</cell><cell>97.79</cell><cell>98.61</cell><cell>82.08</cell></row><row><cell>+ 5 × DPB</cell><cell></cell><cell></cell><cell>93.96</cell><cell>97.98</cell><cell cols="2">98.81 83.40</cell></row></table><note>Comparison of using 1, 3 and 5 DPBs on the Market- 1501. DPB consists of both the human part branch and the latent part branch here.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the two branches of DPB on CUHK03.</figDesc><table><row><cell>Method</cell><cell cols="2">HP-5 Latent</cell><cell>R-1</cell><cell>R-5</cell><cell>R-10</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>60.29</cell><cell>78.21</cell><cell>84.86</cell><cell>54.79</cell></row><row><cell></cell><cell></cell><cell>×</cell><cell>69.93</cell><cell>83.86</cell><cell>88.90</cell><cell>63.34</cell></row><row><cell>+ 5 × DPB</cell><cell>×</cell><cell></cell><cell>69.84</cell><cell>83.50</cell><cell>89.83</cell><cell>63.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.55</cell><cell>85.71</cell><cell cols="2">90.80 64.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the SOTA on Market-1501.</figDesc><table><row><cell>Method</cell><cell>R-1</cell><cell>R-5</cell><cell cols="2">R-10 mAP</cell></row><row><cell>Spindle [55]</cell><cell>76.9</cell><cell>91.5</cell><cell>94.6</cell><cell>-</cell></row><row><cell>MGCAM [34]</cell><cell>83.8</cell><cell>-</cell><cell>-</cell><cell>74.3</cell></row><row><cell>PDC [35]</cell><cell>84.1</cell><cell>92.7</cell><cell>94.9</cell><cell>63.4</cell></row><row><cell>AACN [50]</cell><cell>85.9</cell><cell>-</cell><cell>-</cell><cell>66.9</cell></row><row><cell>PSE [29]</cell><cell>87.7</cell><cell>94.5</cell><cell>96.8</cell><cell>69.0</cell></row><row><cell>PABR [39]</cell><cell>90.2</cell><cell>96.1</cell><cell>97.4</cell><cell>76.0</cell></row><row><cell>SPReID [10]</cell><cell>92.5</cell><cell>97.2</cell><cell>98.1</cell><cell>81.3</cell></row><row><cell>MSCAN [14]</cell><cell>80.3</cell><cell>-</cell><cell>-</cell><cell>57.5</cell></row><row><cell>DLPAR [56]</cell><cell>81.0</cell><cell>92.0</cell><cell>94.7</cell><cell>63.4</cell></row><row><cell>SVDNet [40]</cell><cell>82.3</cell><cell>92.3</cell><cell>95.2</cell><cell>62.1</cell></row><row><cell>DaF [52]</cell><cell>82.3</cell><cell>-</cell><cell>-</cell><cell>72.4</cell></row><row><cell>JLML [16]</cell><cell>85.1</cell><cell>-</cell><cell>-</cell><cell>65.5</cell></row><row><cell>DPFL [3]</cell><cell>88.9</cell><cell>-</cell><cell>-</cell><cell>73.1</cell></row><row><cell>HA-CNN [17]</cell><cell>91.2</cell><cell>-</cell><cell>-</cell><cell>75.7</cell></row><row><cell>SGGNN [32]</cell><cell>92.3</cell><cell>96.1</cell><cell>97.4</cell><cell>82.8</cell></row><row><cell>GSRW [31]</cell><cell>92.7</cell><cell>96.9</cell><cell>98.1</cell><cell>82.5</cell></row><row><cell>PCB + RPP [41]</cell><cell>93.8</cell><cell>97.5</cell><cell>98.5</cell><cell>81.6</cell></row><row><cell>P 2 -Net</cell><cell>94.0</cell><cell>98.0</cell><cell>98.8</cell><cell>83.4</cell></row><row><cell cols="5">P 2 -Net (+ triplet loss) 95.2 98.2 99.1 85.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison with the SOTA on DukeMTMC-reID.</figDesc><table><row><cell>Method</cell><cell>R-1</cell><cell>R-5</cell><cell cols="2">R-10 mAP</cell></row><row><cell>AACN [50]</cell><cell>76.8</cell><cell>-</cell><cell>-</cell><cell>59.3</cell></row><row><cell>PSE [29]</cell><cell>79.8</cell><cell>89.7</cell><cell>92.2</cell><cell>62.0</cell></row><row><cell>PABR [39]</cell><cell>82.1</cell><cell>90.2</cell><cell>92.7</cell><cell>64.2</cell></row><row><cell>SPReID [10]</cell><cell>84.4</cell><cell>91.9</cell><cell>93.7</cell><cell>71.0</cell></row><row><cell>SBAL [24]</cell><cell>71.3</cell><cell>-</cell><cell>-</cell><cell>52.4</cell></row><row><cell>ACRN [30]</cell><cell>72.6</cell><cell>84.8</cell><cell>88.9</cell><cell>52.0</cell></row><row><cell>SVDNet [40]</cell><cell>76.7</cell><cell>86.4</cell><cell>89.9</cell><cell>56.8</cell></row><row><cell>DPFL [3]</cell><cell>79.2</cell><cell>-</cell><cell>-</cell><cell>60.6</cell></row><row><cell>SVDEra [61]</cell><cell>79.3</cell><cell>-</cell><cell>-</cell><cell>62.4</cell></row><row><cell>HA-CNN [17]</cell><cell>80.5</cell><cell>-</cell><cell>-</cell><cell>63.8</cell></row><row><cell>GSRW [31]</cell><cell>80.7</cell><cell>88.5</cell><cell>90.8</cell><cell>66.4</cell></row><row><cell>SGGNN [32]</cell><cell>81.1</cell><cell>88.4</cell><cell>91.2</cell><cell>68.2</cell></row><row><cell>PCB + RPP [41]</cell><cell>83.3</cell><cell>90.5</cell><cell>92.5</cell><cell>69.2</cell></row><row><cell>P 2 -Net</cell><cell>84.9</cell><cell>92.1</cell><cell>94.5</cell><cell>70.8</cell></row><row><cell cols="5">P 2 -Net (+ triplet loss) 86.5 93.1 95.0 73.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison with the SOTA on CUHK03.</figDesc><table><row><cell>Method</cell><cell cols="2">labeled R-1 mAP</cell><cell cols="2">detected R-1 mAP</cell></row><row><cell>DaF [52]</cell><cell>27.5</cell><cell cols="3">31.5 26.4 30.0</cell></row><row><cell>SVDNet [40]</cell><cell>40.9</cell><cell cols="3">37.8 41.5 37.3</cell></row><row><cell>DPFL [3]</cell><cell>43.0</cell><cell cols="3">40.5 40.7 37.0</cell></row><row><cell>HA-CNN [17]</cell><cell>44.4</cell><cell cols="3">41.0 41.7 38.6</cell></row><row><cell>SVDEra [61]</cell><cell>49.4</cell><cell cols="3">45.1 48.7 43.5</cell></row><row><cell>MGCAM [34]</cell><cell>50.1</cell><cell cols="3">50.2 46.7 46.9</cell></row><row><cell>PCB + RPP [41]</cell><cell>-</cell><cell>-</cell><cell cols="2">63.7 57.5</cell></row><row><cell>P 2 -Net</cell><cell>75.8</cell><cell>69.2</cell><cell>71.6</cell><cell>64.2</cell></row><row><cell cols="5">P 2 -Net (+ triplet loss) 78.3 73.6 74.9 68.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Complexity comparison of DPB/Baseline on CUHK03.</figDesc><table><row><cell>Method</cell><cell>5×DPB</cell><cell>Params</cell><cell>FLOPs</cell><cell>Time</cell><cell>R-1</cell><cell>mAP</cell></row><row><cell>R-50</cell><cell>×</cell><cell>24.2M</cell><cell>14.9G</cell><cell>19ms</cell><cell>60.29</cell><cell>54.79</cell></row><row><cell>R-101</cell><cell>×</cell><cell>43.2M</cell><cell>22.1G</cell><cell>32ms</cell><cell>68.14</cell><cell>63.45</cell></row><row><cell>R-50</cell><cell></cell><cell>31.6M</cell><cell>18.6G</cell><cell>27ms</cell><cell>71.55</cell><cell>64.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison experiments on DukeMTMC-ReID. DPB (HP-5) only uses the human part branch and sets K = 5. DPB (Latent) only uses the latent part branch. DPB uses both the human part branch and the latent part branch. DPB (Latent) 82.20 90.33 92.69 65.09 1 × DPB 83.80 91.38 93.58 67.93 5 × DPB (HP-5) 84.08 91.82 94.10 68.64 5 × DPB (Latent) 84.45 91.97 94.25 69.07 5 × DPB 84.91 92.08 94.45 70.84</figDesc><table><row><cell>Method</cell><cell>R-1</cell><cell>R-5</cell><cell>R-10</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell cols="4">79.85 89.81 92.19 62.57</cell></row><row><cell>1 × DPB (HP-5)</cell><cell cols="4">83.04 91.18 93.22 66.99</cell></row><row><cell>1 ×</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparison experiments on CUHK03. DPB (HP-5) only uses the human part branch and sets K = 5. DPB (Latent) only uses the latent part branch. DPB uses both the human part branch and the latent part branch. DPB (HP-5) 67.57 81.32 87.36 60.02 1 × DPB (Latent) 68.59 83.14 87.96 61.75 1 × DPB 70.43 84.50 89.64 63.93 5 × DPB (HP-5) 69.93 83.86 88.90 63.34 5 × DPB (Latent) 69.84 83.50 89.83 63.25 5 × DPB 71.55 85.71 90.80 64.23</figDesc><table><row><cell>Method</cell><cell>R-1</cell><cell>R-5</cell><cell>R-10</cell><cell>mAP</cell></row><row><cell>Baseline</cell><cell cols="4">60.29 78.21 84.86 54.79</cell></row><row><cell>1 ×</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">E.g. the label set in<ref type="bibr" target="#b17">[18]</ref>: background, hat, hair, glove, sunglasses, upper-clothes, dress, coat, socks, pants, jumpsuits, scarf, skirt, face, rightarm, left-arm, right-leg, left-leg, right-shoe and left-shoe.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">When K = 5, each group represents background, head, upper-torso, lower-torso and shoe; when K = 2, it represents background and foreground; when K = 1, it treats the whole image as a single part.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shaogang Gong, et al. Person reidentification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving person reidentification via pose-aware multi-shot matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeong-Jun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attribute-aware attention model for fine-grained representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attribute aware pooling for pedestrian attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanjian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11837</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person reidentification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards unified human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pose-aware person recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Interlaced sparse self-attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Jianyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jingdong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12273</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Attributes-based re-identification. In Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person reidentification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Look into person: Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video-based person re-identification via 3d convolutional networks and non-local attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouwang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pose transferrable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Devil in the details: Towards accurate single and multiple human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05996</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised bayesian attribute learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NIPSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Posenormalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Person reidentification by deep learning attribute-complementary information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep group-shuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-task learning with low rank attribute embedding for multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">Steven</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep neural networks with inexact matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arulkumar</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moitreya</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiat-Pin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharmili</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attribute recognition by joint recurrent learning of context and correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Person re-identification with cascaded pairwise convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Divide and fuse: A re-ranking approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jingdong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jingdong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Person reidentification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Attribute-driven feature disentangling and temporal aggregation for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">And we can find similar performance improvements by combining the human part branch and the latent part branch</title>
	</analytic>
	<monogr>
		<title level="m">We report the results on CUHK03 (detected) in Table 10</title>
		<imprint/>
	</monogr>
	<note>e.g., &quot;1 × DPB&quot; improves the mAP of &quot;1 × DPB (HP-5)&quot; from 60.02 to 63.93. &quot;5 × DPB&quot; improves the mAP of &quot;5</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Our approach boosts the performance of baseline model by a large margin, especially on CUHK03 dataset, the probable reasons are (i) the quality is better (less blurring effects, higher image resolutions: 266×90 in CUHK03, 128×64 in Market-1501), thus the DBP can estimate more accurate human parsing and latent attention results. (ii) the background across different images is more noisy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">×</forename><surname>Dpb</surname></persName>
		</author>
		<idno>HP-5)&quot; from 63.34 to 64.23</idno>
		<imprint/>
	</monogr>
	<note>DPB can remove the influence of the background</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
