<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Answer Networks for Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com‡kevinduh@cs.jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Answer Networks for Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We utilize a stochastic answer network (SAN) to explore multi-step inference strategies in Natural Language Inference. Rather than directly predicting the results given the inputs, the model maintains a state and iteratively refines its predictions. This can potentially model more complex inferences than the existing single-step inference methods. Our experiments show that SAN achieves state-of-the-art results on four benchmarks: Stanford Natural Language Inference (SNLI), MultiGenre Natural Language Inference (MultiNLI), SciTail, and Quora Question Pairs datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>The natural language inference task, also known as recognizing textual entailment (RTE), is to infer the relation between a pair of sentences (e.g., premise and hypothesis). This task is challenging, since it requires a model to fully understand the sentence meaning, (i.e., lexical and compositional semantics). For instance, the following example from MultiNLI dataset  illustrates the need for a form of multi-step synthesis of information between premise: "If you need this book, it is probably too late unless you are about to take an SAT or GRE.", and hypothesis: "It's never too late, unless you're about to take a test." To predict the correct relation between these two sentences, the model needs to first infer that "SAT or GRE" is a "test", and then pick the correct relation, e.g., contradiction.</p><p>This kind of iterative process can be viewed as a form of multi-step inference. To best of our knowledge, all of works on NLI use a single step inference. Inspired by the recent success of multi-step inference on Machine Reading Comprehension (MRC) <ref type="bibr" target="#b7">(Hill et al., 2016;</ref><ref type="bibr" target="#b3">Dhingra et al., 2016;</ref><ref type="bibr" target="#b20">Sordoni et al., 2016;</ref><ref type="bibr" target="#b10">Kumar et al., 2015;</ref><ref type="bibr" target="#b19">Shen et al., 2017;</ref><ref type="bibr" target="#b26">Xu et al., 2018)</ref>, we explore the multi-step inference strategies on NLI. Rather than directly predicting the results given the inputs, the model maintains a state and iteratively refines its predictions. We show that our model outperforms single-step inference and further achieves the state-of-the-art on SNLI, MultiNLI, SciTail, and Quora Question Pairs datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-step inference with SAN</head><p>The natural language inference task as defined here involves a premise P = {p 0 , p 1 , ..., p m−1 } of m words and a hypothesis H = {h 0 , h 1 , ..., h n−1 } of n words, and aims to find a logic relationship R between P and H, which is one of labels in a close set: entailment, neutral and contradiction. The goal is to learn a model f (P, H) → R.</p><p>In a single-step inference architecture, the model directly predicts R given P and H as input. In our multi-step inference architecture, we additionally incorporate a recurrent state s t ; the model processes multiple passes through P and H, iteratively refining the state s t , before finally generating the output at step t = T , where T is an a priori chosen limit on the number of inference steps. <ref type="figure">Figure 1</ref> describes in detail the architecture of the stochastic answer network (SAN) used in this study; this model is adapted from the MRC multistep inference literature . Compared to the original SAN for MRC, in the SAN for NLI we simplify the bottom layers and Selfattention layers since the length of the premise and hypothesis is short). We also modify the answer module from prediction a text span to an NLI classification label. Overall, it contains four different layers: 1) the lexicon encoding layer computes word representations; 2) the contextual encoding layer modifies these representations in context; 3) the memory generation layer gathers all information from the premise and hypothesis and forms a Premise Lexicon Encoding Layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis</head><p>Word Embedding Character Embedding He saw Jon was asleep. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information Gathering Layer s t-1 s t s t+1 s t-1 s t s t+1</head><p>Memory <ref type="figure">Figure 1</ref>: Architecture of the Stochastic Answer Network (SAN) for Natural Language Inference.</p><p>"working memory" for the final answer module; 4) the final answer module, a type of multi-step network, predicts the relation between the premise and hypothesis. Lexicon Encoding Layer. First we concatenate word embeddings and character embeddings to handle the out-of-vocabulary words 1 . Following , we use two separate twolayer position-wise feedforward network <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> to obtain the final lexicon embedings, E p ∈ R d×m and E h ∈ R d×n , for the tokens in P and H, respectively. Here, d is the hidden size. Contextual Encoding Layer. Two stacked BiL-STM layers are used on the lexicon encoding layer to encode the context information for each word in both P and H. Due to the bidirectional layer, it doubles the hidden size. We use a maxout layer (Goodfellow et al., 2013) on the BiLSTM to shrink its output into its original hidden size. By a concatenation of the outputs of two BiLSTM layers, we obtain C p ∈ R 2d×m and C h ∈ R 2d×n as representation of P and H, respectively. <ref type="bibr">1</ref> We omit POS Tagging and Name Entity Features for simplicity Memory Layer. We construct our working memory via an attention mechanism. First, a dotproduct attention is adopted like in <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> to measure the similarity between the tokens in P and H. Instead of using a scalar to normalize the scores as in <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>, we use a layer projection to transform the contextual information of both C p and C h :</p><formula xml:id="formula_0">A = dropout(f attention (Ĉ p ,Ĉ h )) ∈ R m×n (1)</formula><p>where A is an attention matrix, and dropout is applied for smoothing. Note thatĈ p andĈ h is transformed from C p and C h by one layer neural network ReLU (W 3 x), respectively. Next, we gather all the information on premise and hypothesis by:</p><formula xml:id="formula_1">U p = C p ; C h A ∈ R 4d×m and U h = C h ; C p A ∈ R 4d×n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The semicolon</head><p>; indicates vector/matrix concatenation; A is the transpose of A. Last, the working memory of the premise and hypothesis is generated by using a BiLSTM based on all the information gathered:</p><formula xml:id="formula_2">M p = BiLST M ([U p ; C p ]) and M h = BiLST M ([U h ; C h ]). Answer module.</formula><p>Formally, our answer module will compute over T memory steps and output the relation label. At the beginning, the initial state s 0 is the summary of the M h :</p><formula xml:id="formula_3">s 0 = j α j M h j , where α j = exp(θ 2 ·M h j ) j exp(θ 2 ·M h j ) . At time step t in the range of {1, 2, ..., T − 1}, the state is defined by s t = GRU (s t−1 , x t ).</formula><p>Here, x t is computed from the previous state s t−1 and memory M p :</p><formula xml:id="formula_4">x t = j β j M p j and β j = sof tmax(s t−1 θ 3 M p )</formula><p>. Following <ref type="bibr" target="#b13">(Mou et al., 2015)</ref>, one layer classifier is used to determine the relation at each step t ∈ {0, 1, . . . , T − 1}.</p><formula xml:id="formula_5">P r t = sof tmax(θ 4 [s t ; x t ; |s t − x t |; s t · x t ]). (2)</formula><p>At last, we utilize all of the T outputs by averaging the scores:</p><formula xml:id="formula_6">P r = avg([P r 0 , P r 1 , ..., P r T −1 ]).<label>(3)</label></formula><p>Each P r t is a probability distribution over all the relations, {1, . . . , |R|}. During training, we apply stochastic prediction dropout before the above averaging operation. During decoding, we average all outputs to improve robustness.</p><p>This stochastic prediction dropout is similar in motivation to the dropout introduced by (Srivastava et al., 2014). The difference is that theirs is dropout at the intermediate node-level, whereas ours is dropout at the final layer-level. Dropout at the node-level prevents correlation between features. Dropout at the final layer level, where randomness is introduced to the averaging of predictions, prevents our model from relying exclusively on a particular step to generate correct output. Note that it only contains two types of labels, so is a binary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation details</head><p>The spaCy tool 2 is used to tokenize all the dataset and PyTorch is used to implement our models. We fix word embedding with 300-dimensional GloVe word vectors <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref>. For the character encoding, we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1, 3, 5 and the hidden size 50, 100, 150. 3 So lexicon embeddings are d=600-dimensions. The embedding for the out-of-vocabulary is zeroed. The hidden size of LSTM in the contextual encoding layer, memory generation layer is set to 128, thus the input size of output layer is 1024 (128 * 2 * 4) as Eq 2. The projection size in the attention layer is set to 256. To speed up training, we use weight normalization <ref type="bibr" target="#b18">(Salimans and Kingma, 2016)</ref>. The dropout rate is 0.2, and the dropout mask is fixed through time steps <ref type="bibr" target="#b4">(Gal and Ghahramani, 2016)</ref> in LSTM. The mini-batch size is set to 32. Our optimizer is Adamax <ref type="bibr" target="#b9">(Kingma and Ba, 2014)</ref> and its learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>One main question which we would like to address is whether the multi-step inference help on NLI. We fixed the lower layer and only compare different architectures for the output layer:</p><p>1. Single-step: Predict the relation using Eq 2 based on s 0 and x 0 . Here,</p><formula xml:id="formula_7">x 0 = j α j M p j , where α j = exp(w·M p j ) j exp(w·M p j ) 4 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SAN:</head><p>The multi-step inference model. We use 5-steps with the prediction dropout rate 0.2 on the all experiments. <ref type="table">Table 1</ref> shows that our multi-step model consistently outperforms the single-step model on the dev set of all four datasets in terms of accuracy. For example, on SciTail dataset, SAN outperforms the single-step model by <ref type="bibr">+3.89 (85.46 vs 89.35)</ref>.</p><p>We compare our results with the state-of-the-art in  <ref type="bibr" target="#b17">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> use a large amount of external knowledge or a large scale pretrained contextual embeddings. However, SAN is still competitive these models. On SciTail dataset, SAN even outperforms GPT. Due to the space limitation, we only list two top models. <ref type="bibr">5</ref> We further utilize BERT as a feature extractor 6 and use the SAN answer module on top of it. Comparing with Single-step baseline, the proposed model obtains +2.8 improvement on the Sc-iTail test set (94.0 vs 91.2) and +2.1 improvement on the SciTail dev set (96.1 vs 93.9). This shows the generalization of the proposed model which can be easily adapted on other models 7 . Analysis: How many steps it needs? We search the number of steps t from 1 to 10. We observe that when t increases, our model obtains a better improvement (e.g., 86.7 (t = 2)); however when t = 5 or t = 6, it achieves best results (89.4) on SciTail dev set and then begins to downgrade 4 For direct comparison, this has the same three lower layers as <ref type="figure">Fig. 1 and only</ref>   <ref type="bibr">6</ref> We run BERT (the base model) to extract embeddings of both premise and hypothesis and then feed it to answer models for a fair comparison. 7 Due to highly time consumption and space limitation, we omit the results using BERT on SNLI/MNLI/Quora Question dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MultiNLI Test Matched Mismatched DIIN <ref type="bibr" target="#b5">(Gong et al., 2017)</ref> 78.8 77.8 BERT <ref type="bibr">(Devlin et al., 2018) 86.7</ref> 85.9 SAN 79.3 78.7 SNLI Dataset (Accuracy%) ESIM+ELMo 88.7 GPT <ref type="bibr" target="#b17">(Radford et al., 2018)</ref> 89.9 SAN 88.7</p><p>Quora Question Dataset (Accuracy%) <ref type="bibr" target="#b22">(Tomar et al., 2017)</ref> 88.4 <ref type="bibr" target="#b5">(Gong et al., 2017)</ref> 89.1 SAN 89.4 SciTail Dataset (Accuracy%) <ref type="bibr" target="#b8">(Khot et al., 2018)</ref> 77.3 GPT <ref type="bibr" target="#b17">(Radford et al., 2018)</ref> 88.3 SAN 88.4  the performance. Thus, we set t = 5 in all our experiments.</p><p>We also looked internals of our answer module by dumping predictions of each step (the max step is set to 5). Here is an example 8 from MutiNLI dev set. Our model produces total 5 labels (contradiction, neutral, neutral, neutral, and neutral) at each step and makes the final decision by voting neutral. Surprising, we found that human annotators also gave different 5 labels: contradiction, neutral, neutral, neutral, neutral. It shows robustness of our model which uses collective wise.</p><p>Finally, we analyze our model on the annotated subset 9 of development set of MultiNLI. It contains 1,000 examples, each tagged by categories shown in <ref type="table" target="#tab_5">Table 3</ref>. Our model outperforms the best system in RepEval 2017 <ref type="bibr" target="#b1">(Chen et al., 2017)</ref> in most cases, except on "Conditional" and "Tense Difference" categories. We also find that SAN works extremely well on "Active/Passive" and "Paraphrase" categories. Comparing with Chen's model, the biggest improvement of SAN (50% vs 77% and 58% vs 85% on Matched and Mismatched settings respectively) is on the "Antonym" category. In particular, on the most challenging "Long Sentence" and "Quantity/Time" categories, SAN's result is substantially better than previous systems. This demonstrates the robustness of multi-step inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We explored the use of multi-step inference in natural language inference by proposing a stochastic answer network (SAN). Rather than directly predicting the results (e.g. relation R such as entailment or not) given the input premise P and hypothesis H, SAN maintains a state s t , which it iteratively refines over multiple passes on P and H in order to make a prediction. Our state-of-theart results on four benchmarks (SNLI, MultiNLI, SciTail, Quora Question Pairs) show the effectiveness of this multi-step inference architecture. In future, we would like to incorporate the pertrained contextual embedding, e.g., <ref type="bibr">ELMo (Peters et al., 2018)</ref> and GPT <ref type="bibr" target="#b17">(Radford et al., 2018)</ref> into our model and multi-task learning <ref type="bibr" target="#b11">(Liu et al., 2019)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Here, we evaluate our model in terms of accuracy on four benchmark datasets. SNLI (Bowman et al., 2015) contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus, and hypothesis are manually annotated. MultiNLI contains 433k sentence pairs, which are collected similarly as SNLI. However, the premises are collected from a broad range of genre of American English.</figDesc><table><row><cell></cell><cell cols="2">Single-step SAN</cell></row><row><cell>MultiNLI matched</cell><cell>78.69</cell><cell>79.88</cell></row><row><cell>MultiNLI mismatched</cell><cell>78.83</cell><cell>79.91</cell></row><row><cell>SNLI</cell><cell>88.32</cell><cell>88.73</cell></row><row><cell>Quora</cell><cell>89.67</cell><cell>90.70</cell></row><row><cell>SciTail</cell><cell>85.46</cell><cell>89.35</cell></row><row><cell cols="3">Table 1: Comparison of single and multi-step infer-</cell></row><row><cell cols="3">ence strategies on MultiNLI, SNLI, Quora Question</cell></row><row><cell>and SciTail dev sets.</cell><cell></cell><cell></cell></row><row><cell cols="3">400k question pairs, and each question pair is an-</cell></row><row><cell cols="3">notated with a binary value indicating whether</cell></row><row><cell cols="3">the two questions are paraphrase of each other.</cell></row><row><cell cols="3">SciTail dataset is created from a science ques-</cell></row><row><cell cols="3">tion answering (SciQ) dataset. It contains 1,834</cell></row><row><cell cols="3">questions with 10,101 entailments examples and</cell></row><row><cell>16,925 neutral examples.</cell><cell></cell><cell></cell></row><row><cell>3 Experiments</cell><cell></cell><cell></cell></row><row><cell>3.1 Dataset</cell><cell></cell><cell></cell></row><row><cell>The test and development</cell><cell></cell><cell></cell></row><row><cell>sets are further divided into in-domain (matched)</cell><cell></cell><cell></cell></row><row><cell>and cross-domain (mismatched) sets. The Quora</cell><cell></cell><cell></cell></row><row><cell>Question Pairs dataset (Wang et al., 2017) is pro-</cell><cell></cell><cell></cell></row><row><cell>posed for paraphrase identification. It contains</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. Our model achieves the best perfor-</cell></row><row><cell>mance on SciTai and Quora Question tasks. For</cell></row><row><cell>instance, SAN obtains 89.4 (vs 89.1) and 88.4</cell></row><row><cell>(88.3) on the Quora Question and SciTail test set,</cell></row><row><cell>respectively and set the new state-of-the-art. On</cell></row><row><cell>SNLI and MultiNLI dataset, ESIM+ELMo (Peters</cell></row><row><cell>et al., 2018), GPT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art on MultiNLI, SNLI and Quora Question test sets.</figDesc><table><row><cell>Tag</cell><cell cols="3">Matched Chen 1 SAN Chen 1 SAN Mismatched</cell></row><row><cell>Conditional</cell><cell cols="3">100% 65% 100% 81%</cell></row><row><cell>Word overlap</cell><cell cols="2">63 % 86% 76%</cell><cell>92%</cell></row><row><cell>Negation</cell><cell>75%</cell><cell>80% 72%</cell><cell>79%</cell></row><row><cell>Antonym</cell><cell>50%</cell><cell>77% 58%</cell><cell>85%</cell></row><row><cell>Long Sentence</cell><cell>67%</cell><cell>84% 67%</cell><cell>79%</cell></row><row><cell cols="2">Tense Difference 86%</cell><cell>75% 89%</cell><cell>83%</cell></row><row><cell>Active/Passive</cell><cell cols="3">88% 100% 91% 100%</cell></row><row><cell>Paraphrase</cell><cell>78%</cell><cell>92% 89%</cell><cell>92%</cell></row><row><cell>Quantity/Time</cell><cell>33%</cell><cell>53% 46%</cell><cell>51%</cell></row><row><cell>Coreference</cell><cell>83%</cell><cell>73% 80%</cell><cell>84%</cell></row><row><cell>Quantifier</cell><cell>74%</cell><cell>81% 77%</cell><cell>80%</cell></row><row><cell>Modal</cell><cell>75%</cell><cell>79% 76%</cell><cell>82%</cell></row><row><cell>Belief</cell><cell>73%</cell><cell>77% 74%</cell><cell>78%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Error analysis on MultiNLI. See for reference.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://spacy.io3  We limit the maximum length of a word by 20 characters. The character embedding size is set to 20.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Its ID is id 144185n with premise (And he said, What's going on?) and hypothesis (I told him to mind his own business.)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://www.nyu.edu/projects/bowman/multinli/multinli1.0annotations" />
	</analytic>
	<monogr>
		<title level="j">Natural Language Inference with External Knowledge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Natural Language Inference over Interaction Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SciTail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1506.07285</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic answer networks for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08422</idno>
		<title level="m">Natural language inference by tree-based convolution and heuristic matching</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bowman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shared Task: Multi-Genre Natural Language Inference with Sentence Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An empirical analysis of multiple-turn reasoning strategies in reading comprehension tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03230</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02245</idno>
		<title level="m">Iterative alternating neural attention for machine reading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thyago</forename><surname>Gaurav Singh Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04565</idno>
		<title level="m">Neural paraphrase identification of questions with noisy pretraining</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multi-task learning for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06963</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
