<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anonymous Walk Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
						</author>
						<title level="a" type="main">Anonymous Walk Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of representing entire graphs has seen a surge of prominent results, mainly due to learning convolutional neural networks (CNNs) on graphstructured data. While CNNs demonstrate stateof-the-art performance in graph classification task, such methods are supervised and therefore steer away from the original problem of network representation in task-agnostic manner. Here, we coherently propose an approach for embedding entire graphs and show that our feature representations with SVM classifier increase classification accuracy of CNN algorithms and traditional graph kernels. For this we describe a recently discovered graph object, anonymous walk, on which we design task-independent algorithms for learning graph representations in explicit and distributed way. Overall, our work represents a new scalable unsupervised learning of state-of-the-art representations of entire graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A wide range of real world applications deal with network analysis and classification tasks. An ease of representing data with graphs makes them very valuable asset in any data mining toolbox; however, the complexity of working with graphs led researchers to seek for new ways of representing and analyzing graphs, of which network embeddings have become broadly popular due to their success in several machine learning areas such as graph classification <ref type="bibr" target="#b4">(Cai et al., 2017)</ref>, visualization <ref type="bibr" target="#b5">(Cao et al., 2016)</ref>, and pattern recognition <ref type="bibr" target="#b15">(Monti et al., 2017)</ref>.</p><p>Essentially, network embeddings are vector representations of graphs that capture local and global traits and, as a consequence, are more suitable for standard machine learning techniques such as SVM that works on numerical vectors Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s). rather than graph structures. Ideally, a practitioner would like to have a polynomial-time algorithm that can convert different graphs into different feature vectors. However, such algorithm would be capable of deciding whether two graphs are isomorphic <ref type="bibr" target="#b6">(Gärtner et al., 2003)</ref>, for which currently only quasipolynomial-time algorithm exists <ref type="bibr" target="#b1">(Babai, 2016)</ref>. Hence, there are fundamental challenges in the design of polynomial-time algorithm for network-to-vector conversion. Instead, a lot of research was devoted to the question of designing network embedding models that are computationally efficient and preserve similarity between graphs.</p><p>Broadly speaking, network embeddings come from one of the two buckets, either based on engineered graph features or driven by training on graph data. Feature-based methods traditionally appeared in graph kernel setting <ref type="bibr" target="#b26">(Vishwanathan et al., 2010)</ref>, where each graph is decomposed into discrete components, distribution of which is used as a vector representation of a graph <ref type="bibr" target="#b9">(Haussler, 1999)</ref>. Importantly, general concept of feature-based methods implies ad-hoc knowledge about the data at hand. For example, Random Walk kernel <ref type="bibr" target="#b26">(Vishwanathan et al., 2010)</ref> assumes that graph realization originates from the types of random walks a graph has, whereas for Weisfeiler-Lehman (WL) kernel <ref type="bibr" target="#b21">(Shervashidze et al., 2011)</ref> the insight is in subtree patterns of a graph. For high-dimensional graph embeddings feature-based methods produce sparse solution as only few substructures are common across graphs. This is known as diagonal dominance <ref type="bibr" target="#b27">(Yanardag &amp; Vishwanathan, 2015)</ref>, a situation when a graph representation is only similar to itself, but not to any other graph.</p><p>On the other hand, data-driven approach learns network embeddings by optimizing some form of objective function defined on graph data. Deep Graph Kernels (DGK) <ref type="bibr" target="#b27">(Yanardag &amp; Vishwanathan, 2015)</ref>, for example, learns a positive semidefinite matrix that weights the relationship between graph substructures, while Patchy-San (PSCN) <ref type="bibr" target="#b17">(Niepert et al., 2016)</ref> constructs locally connected neighborhoods for training a convolutional neural network on. Data-driven approach implies learning distributed graph representations that have demonstrated promising classification results <ref type="bibr" target="#b17">(Niepert et al., 2016;</ref><ref type="bibr" target="#b23">Tixier et al., 2017)</ref>.</p><p>Our approach. We propose to use a natural graph object arXiv:1805.11921v3 [cs.</p><p>LG] 8 Jun 2018 named anonymous walk as a base for learning feature-based and data-driven network embeddings. Recent discovery <ref type="bibr" target="#b12">(Micali &amp; Allen Zhu, 2016)</ref> has shown that anonymous walks provide characteristic graph traits and are capable to reconstruct network proximity of a node exactly. In particular, distribution of anonymous walks starting at node u is sufficient for reconstruction of a subgraph induced by all vertices within a fixed distance from u; and such distribution uniquely determines underlying Markov processes from u, i.e. no two different subgraphs exist having the same distribution of anonymous walks. This implies that two graphs with similar distributions of anonymous walks should be topologically similar. We therefore define feature-based network embeddings on distribution of anonymous walks and show an efficient sampling approach that approximates distributions for large networks.</p><p>To overcome sparsity of feature-based methods, we design a data-driven approach that learns distributed representations on the generated corpus of anonymous walks via backpropagation, in the same vein as neural models in NLP <ref type="bibr" target="#b11">(Le &amp; Mikolov, 2014;</ref><ref type="bibr" target="#b2">Bengio et al., 2003)</ref>. Considering anonymous walks for the same source node as co-occurring words in the sentence and graph as a collection of such sentences, the hope is that by predicting a target word in a given context of words and a document, the proposed algorithm learns semantic meaning of words and a document.</p><p>To the best of our knowledge, we are the first to introduce anonymous walks in the context of learning network representations and we highlight the following contributions:</p><p>• Based on the notion of anonymous walk, we propose feature-based network embeddings, for which we describe an efficient sampling procedure to alleviate time complexity of exact computation.</p><p>• By maximizing the likelihood of preserving network proximity of anonymous walks, we propose a scalable algorithm to learn data-driven network embeddings.</p><p>• On widely-used real datasets, we demonstrate that our network embeddings achieve state-of-the-art performance in comparison with other graph kernels and neural networks in graph classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Anonymous Walks</head><p>Random walks are the sequences of nodes, where each new node is selected independently from the set of neighbors of the last node in the sequence. Normally states in a random walk correspond to a label or a global name of a node; however, for reasons described below such states could be unavailable. Yet, recently it has been shown that anonymized version of a random walk can provide a flexible way to reconstruct a network even when global names are absent <ref type="bibr" target="#b12">(Micali &amp; Allen Zhu, 2016)</ref>. We next define a notion of anonymous walk.</p><p>Definition 1. Let s = (u 1 , u 2 , . . . , u k ) be an ordered list of elements u i ∈ V . We define the positional function pos: (s, u i ) → q such that for any ordered list s = (u 1 , u 2 , . . . , u k ) and an element u i ∈ V it returns a list q = (p 1 , p 2 , . . . , p l ) of all positions p j ∈ N of u i occurrences in a list s.</p><p>For example, if s = (a, b, c, b, c), then pos(s, a) = (1) as element a appears only on the first position and pos(s, b) = (2, 4).</p><formula xml:id="formula_0">Definition 2 (Anonymous Walk). If w = (v 1 , v 2 , . . . , v k )</formula><p>is a random walk, then its corresponding anonymous walk is the sequence of integers</p><formula xml:id="formula_1">a = (f (v 1 ), f (v 2 ), . . . , f (v k )), where integer f (v i ) = min pj ∈pos(w,vi) pos(w, v i ).</formula><p>We denote mapping of a random walk w to anonymous walk a by w → a. <ref type="figure">Figure 1</ref>. An example demonstrating the concept of anonymous walk. Two different random walks 1 and 2 of the graph correspond to the same anonymous walk 1. A random walk 3 corresponds to another anonymous walk 2.</p><p>For instance, in the graph of <ref type="figure">Fig. 1</ref> </p><formula xml:id="formula_2">a random walk a → b → c → b → c matches anonymous walk 1 → 2 → 3 → 2 → 3. Likewise, another random walk c → d → b → d → b also corresponds to anonymous walk 1 → 2 → 3 → 2 → 3. Conversely, another random walk a → b → a → b → d corresponds to a different anonymous walk 1 → 2 → 1 → 2 → 3.</formula><p>Intuitively, states in anonymous walk correspond to the first position of the node in a random walk and their total number equals to the number of distinct nodes in a random walk. Particular name of the state does not matter (so, for example, anonymous walk 1 → 2 → 3 would be the same as anonymous walk 3 → 1 → 2); however, by agreement, anonymous walks start from 1 and continue to name new states by incrementing the current maximum state in an anonymous walk.</p><p>Rationale. From the perspective of a single node, in the position of an observer, global topology of the network may be hidden deliberately (e.g. social networks often restrict outsiders to examine your friendships) or otherwise (e.g. newly created links in the world wide web may be yet unknown to the search engine). Nevertheless, an observer can, on his own, experiment with the network by starting a random walk from itself, passing the process to its neighbors and recording the observed states in a random walk. As global names of the nodes are not available to an observer, one way to record the states anonymously is by describing them by the first occurrence of a node in a random walk. Not only are such records succinct, but it is common to have privacy constraints <ref type="bibr" target="#b0">(Abraham, 2012)</ref> that would not allow to record a full description of nodes.</p><p>Somewhat remarkably, <ref type="bibr" target="#b12">(Micali &amp; Allen Zhu, 2016)</ref> show that for a single node u in a graph G, a known distribution D l over anonymous walks of length l is sufficient to reconstruct topology of the ball B(u, r) with the center at u and radius r, i.e. the subgraph of graph G induced by all vertices distanced at most r hops from u. For the task of learning embeddings, the topology of network is available and thus distribution of anonymous walks D l can be computed precisely. As no two different subgraphs can have the same distribution D l , it is useful to generalize distribution of anonymous walks from a single node to the whole network and use it as a feature representation of a graph. This idea paves the way to our feature-based network embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithms</head><p>We start from discussion of leveraging anonymous walks for learning network embeddings in a feature-based manner. Inspired by empirical results we train an objective function on local neighborhoods of anonymous walks, which further improves results of classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">AWE: Feature-Based model</head><formula xml:id="formula_3">By definition, a weighted directed graph is a tuple G = (V, E, Ω), where V = {v 1 , v 2 , . . . , v n } is a set of n vertices, E ⊆ V × V is a set of edges,</formula><p>and Ω ⊂ R is a set of edge weights. Given graph G we construct a random walk graph R = (V, E, P ) such that every edge e = (u, v) has a weight p e equals to ω e / v∈Nout(u)</p><formula xml:id="formula_4">ω (u,v) , where N out (u)</formula><p>is the set of out-neighbors of u and ω e ∈ Ω. A random walk w with length l on graph R is a sequence of nodes u 1 , u 2 , . . . , u l+1 , where u i ∈ V , such that a pair (u i , u i+1 ) is selected with a probability p <ref type="bibr">(ui,ui+1)</ref> in a random walk graph R. A probability p(w) of having a random walk w is the total probability of choosing the edges in a random walk, i.e. p(w) = e∈w p e .</p><p>According to the Definition 1, anonymous walk is a random walk, where each state is recorded by its first occurrence index in the random walk. The number of all possible anonymous walks of length l in an arbitrary graph grows exponentially with l ( <ref type="figure" target="#fig_0">Figure 2</ref>). Consider an initial node u and a set of all different random walks W u l that start from u and have length l. These random walks correspond to a set of η different anonymous walks A u l = (a u 1 , a u 2 , . . . , a u η ). A probability of seeing anonymous walk a u i of length l for a node u is p(a u i ) = w∈W u l w →ai p(w). Aggregating probabilities across all vertices in a graph and normalizing them by the total number of nodes N , we get the probability of choosing anonymous walk a i in graph G: We are now ready to define network embeddings that we name feature-based anonymous walk embeddings (AWE).</p><formula xml:id="formula_5">p(a i ) = 1 N u∈G p(a u i ) = 1 N u∈G w∈W u l w →ai p(w).</formula><p>Definition 3 (feature-based AWE). Let A l = (a 1 , a 2 , . . . , a η ) be the set of all possible anonymous walks of length l. Anonymous walk embedding of a graph G is the vector f G of size η, whose i-th component corresponds to a probability p(a i ), of having anonymous walk a i in a graph G: f G = (p(a 1 ), p(a 2 ), . . . , p(a η )).</p><p>(1)</p><p>Direct computation of AWE relies on the enumeration of all different random walks in graph G, which is shown below to grow exponentially with the number of steps l.</p><formula xml:id="formula_6">Theorem 1. The running time of Anonymous Walk Em- beddings (eq. 1) is O(nl(d max in (v) · d max out (v)) l/2 ), where d max</formula><p>in/out is the maximum in/out degree in graph G with n vertices.</p><p>Proof. Let k l be the number of random walks of length l in a directed graph. According to <ref type="bibr" target="#b24">(Tubig, 2012)</ref> k l can be bounded by the powers of in-and out-degrees of nodes in G:</p><formula xml:id="formula_7">k 2 l ≤ ( v∈G d l in (v))( v∈G d l out (v)).</formula><p>Hence, the number of random walks in a graph is at most</p><formula xml:id="formula_8">n(d max in (v) · d max out (v)) l/2 , where d max</formula><p>in/out is the maximum in/out degree. As it requires O(l) operations to map one random walk of length l to anonymous walk, the theorem follows.</p><p>Sampling. As complete counting of all anonymous walks in a large graph may be infeasible, we describe a sampling approach to approximate the true distribution. In this fashion, we draw independently a set of m random walks and calculate its corresponding empirical distribution of anonymous walks. To guarantee that empirical and actual distributions are close with a given confidence, we set the number m of random walks sufficiently large.</p><p>More formally, let A l = (a 1 , a 2 , . . . , a η ) be the set of all possible anonymous walks of length l. For two discrete probability distributions P and Q on set A l , define L 1 distance as:</p><formula xml:id="formula_9">P − Q 1 = ai∈A |P (a i ) − Q(a i )|</formula><p>For a graph G let D l be the actual distribution of anonymous walks A l of length l and let X m = (X 1 , X 2 , . . . , X m ) be i.i.d. random variables drawn from D l . The empirical distribution D m of the original distribution D l is defined as:</p><formula xml:id="formula_10">D m (i) = 1 m Xj ∈X m [[X j = a i ]], where [[x]] = 1 if x is true and 0 otherwise.</formula><p>Then, for all ε &gt; 0 and δ ∈ [0, 1] the number of samples m to satisfy P { D m − D 1 ≥ ε} ≤ δ equals to (from <ref type="bibr" target="#b20">(Shervashidze et al., 2009)</ref>):</p><formula xml:id="formula_11">m = 2 ε 2 (log(2 η − 2) − log(δ)) .<label>(2)</label></formula><p>For example, there are η = 877 possible anonymous walks with length l = 7 ( <ref type="figure" target="#fig_0">Figure 2</ref>). If we set ε = 0.5 and δ = 0.05, then m = 4888. If we decrease ε = 0.1 and δ = 0.01, then the number of samples will increase to 122500.</p><p>As transition probabilities for random walks can be preprocessed, sampling of a node in a random walk of length l can be done in O(1) via alias method. Hence, the overall running time of sampling approach to compute feature-based anonymous walk embeddings is O(ml).</p><p>Our experimental study shows state-of-the-art classification accuracy of feature-based AWE on real datasets. We continue to design data-driven approach that eliminates the sparsity of feature-based embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">AWE: data-driven model</head><p>Our approach for learning network embeddings is analogous to methods for learning paragraph vectors in a text corpus <ref type="bibr" target="#b11">(Le &amp; Mikolov, 2014)</ref>. In our case, an anonymous walk is a word, a randomly sampled set of anonymous walks starting from the same node is a set of co-occurring words, and a graph is a document.</p><p>Neighborhoods of anonymous walks. To leverage the analogy from NLP, we first need to generate a corpus of cooccurring anonymous walks in a graph G. We define a neighborhood between two anonymous walks of length l if they share the same source node. This is similar to other methods such as shortest-paths co-occurrence in DGK <ref type="bibr" target="#b27">(Yanardag &amp; Vishwanathan, 2015)</ref> and rooted subgraphs neighborhood in graph2vec <ref type="bibr" target="#b16">(Narayanan et al., 2017)</ref>, which proved to be successful in empirical studies. Therefore, we iterate over each vertex u in a graph G, sampling T random walks (w u 1 , w u 2 , . . . , w u T ) that start at node u and map to a sequence of co-occurred anonymous walks s u = (a u 1 , a u 2 , . . . , a u T ), i.e. w u i → a u i . A collection of all s u for all vertices u ∈ G is a corpus of co-occurred anonymous walks in a graph and is analogous to a collection of sentences in a document.</p><p>Training. In this framework, we learn representation vector d of a graph and anonymous walks matrix W (see <ref type="figure">Figure  3</ref>). Vector d has 1 × d g size, where d g is embedding size of a graph. Matrix W has η × d a size, where η is the number of all possible anonymous walks of length l and d a is embedding size of anonymous walk. For convenience, we call d as a document vector and W as a word matrix. Each graph corresponds to its vector d and an anonymous walk corresponds to a row in a matrix W. The model tries to predict a target anonymous walk given co-occurring context anonymous walks and a graph.</p><p>Formally, a sequence of co-occurred anonymous walks s = (a 1 , a 2 , . . . , a T ) corresponds to vectors w 1 , w 2 , . . . , w T of matrix W, and a graph G corresponds to vector d. We aim to maximize the average log probability:</p><formula xml:id="formula_12">1 T T −∆ t=∆ log p(w t |w t−∆ , . . . , w t+∆ , d),<label>(3)</label></formula><p>where ∆ is a window size, i.e. number of context words for each target word. Probability in objective <ref type="formula" target="#formula_12">(3)</ref> is defined via softmax function:</p><formula xml:id="formula_13">p(w t |w t−∆ , . . . , w t+∆ , d) = e y(wt) η i=1 e y(wi)<label>(4)</label></formula><p>Each y(w t ) is unnormalized log probability for output word i:</p><formula xml:id="formula_14">y(w t ) = b + U h(w t−∆ , . . . , w t+∆ , d)</formula><p>where b ∈ R and U ∈ R da+dg are softmax parameters. Vector h is constructed by first averaging walk vectors w t−∆ , . . . , w t+∆ and then concatenating with a graph vector d. The reason is that since anonymous walks are randomly sampled, we average vectors w t−∆ , . . . , w t+∆ to compensate for the lack of knowledge on the order of walks; and at the same time, the graph vector d is shared among multiple (context, target) pairs.</p><p>To avoid computation of the sum in softmax equation <ref type="formula" target="#formula_13">(4)</ref>, which becomes impractical for large sets of anonymous walks, one can use Hierarchical softmax <ref type="bibr" target="#b14">(Mikolov et al., 2013b)</ref> or NCE loss functions <ref type="bibr" target="#b7">(Gutmann &amp; Hyvärinen, 2010)</ref> to speed up training. In our work, we use sampled softmax <ref type="bibr" target="#b10">(Jean et al., 2015)</ref> that for each training example picks only a fraction of vocabulary according to a chosen sampling function. One can measure distribution of anonymous walks in a graph via means of definition 1 and decide on a corresponding sampling function.</p><p>At every step of the model, we sample context and target anonymous walks from a graph and compute the gradient error from prediction of target walk and update vectors of context walks and a graph via gradient backpropagation. When given several networks to embed, one can reuse word matrix W across graphs, thereby sharing previously learned embeddings of walks.</p><p>Summarizing, after initialization of matrix W for all anonymous walks of length l and a graph vector d, the model repeats the following two steps for all nodes in a graph: 1) for sampled co-occurred anonymous walks the model calculates a loss (Eq. 3) of predicting a target walk (one of the sampled anonymous walks) by considering all context walks and a graph; 2) the model updates the vectors of context walks in matrix W and graph vector d via gradient backpropagation. One step of the model is depicted in <ref type="figure">Figure 3</ref>. After using up all sampled corpus, a learned graph vector d is called anonymous walk embedding. Definition 4 (data-driven AWE). Anonymous walk embedding of a graph G is a vector representation d learned on a corpus of sampled anonymous walks from a graph G.</p><p>So despite the fact that graph and walk vectors are initialized randomly, as an indirect result of predicting a walk in the <ref type="figure">Figure 3</ref>. A framework for learning data-driven anonymous walk embeddings. Graph is represented by a vector d and anonymous walks are represented by rows of matrix W. All co-occurring anonymous walks start from the same node in a graph. The goal is to predict a target walk w4 by its surrounding context walks (w1, w2, w3) and a graph vector d. We average embeddings of context walks and then concatenate with a graph vector to predict a target vector. Vectors are updated using stochastic gradient descent on a corpus of sampled anonymous walks.</p><p>context of other walks and a graph the model also learns feature representations of networks. Intuitively, a graph vector can be thought as a word with a special meaning: it serves as an overall summary for all anonymous walks in the graph.</p><p>In our experiments, we show how anonymous walk network embeddings can be used in graph classification problem, demonstrating state-of-the-art performance in classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Graph Classification</head><p>Graph classification is a task to predict a class label of a whole graph and it has found applications in bioinformatics  and malware detection <ref type="bibr" target="#b16">(Narayanan et al., 2017)</ref>. In this task, given a series of N graphs {G i } N i=1 and their corresponding labels {L i } N i=1 , we are asked to train a model m: G → L that would efficiently classify new graphs. Two typical approaches to graph classification problem are (1) supervised learning classification algorithms such as PSCN algorithm <ref type="bibr" target="#b17">(Niepert et al., 2016)</ref> and <ref type="formula" target="#formula_11">(2)</ref> graph kernel methods such as WL kernel <ref type="bibr">(Sher-vashidze et al., 2011)</ref>. As we are interested in designing task-agnostic network embeddings that do not require labeled data during training, we show how to use anonymous walk embeddings in conjunction with kernel methods to perform classification of new graphs. For this we define a kernel function on two graphs.</p><p>Definition 5 (Kernel function). Kernel function is a symmetric, positive semidefinite function k: X × X → R n defined for a non-empty set X.</p><p>When X ⊆ R n , several popular choices of kernel exist <ref type="bibr" target="#b19">(Schölkopf &amp; Smola, 2002)</ref>:</p><formula xml:id="formula_15">• Inner product k(x, y) = x, y , ∀x, y ∈ R n , • Polynomial k(x, y) = ( x, y + c) d , ∀x, y ∈ R n , • RBF k(x, y) = exp(− x − y 2 2 2σ 2 ), ∀x, y ∈ R n .</formula><p>With network embeddings, it is then easy to define a kernel function on two graphs:</p><formula xml:id="formula_16">K(G 1 , G 2 ) = k(f (G 1 ), f (G 2 )),<label>(5)</label></formula><p>where f (G i ) is an embedding of a graph G i and k: (x, y) → R n is a kernel function.</p><p>To train a graph classifier m one can then construct a square kernel matrix K for training data G 1 , G 2 , . . . , G N and feed this matrix to a kernelized algorithm such as SVM. Every element of kernel matrix equals to:</p><formula xml:id="formula_17">K ij = K(G i , G j ).</formula><p>For classifying new test instance G τ , one would first compute graph kernels with training instances (K(G 1 , G τ ), K(G 2 , G τ ), . . . , K(G N , G τ )) and provide it to a trained classifier m.</p><p>In our experiments, we use anonymous walk embeddings to compute kernel matrices and show that kernelized SVM classifier achieves top performance comparing to more complex state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our embeddings on the task of graph classification for variety of widely-used datasets.</p><p>Datasets. We evaluate performance on two sets of graphs. One set contains unlabeled graph data and is related to social networks <ref type="bibr" target="#b27">(Yanardag &amp; Vishwanathan, 2015)</ref>. Another set contains graphs with labels on node and/or edges and originates from bioinformatics <ref type="bibr" target="#b21">(Shervashidze et al., 2011)</ref>. Statistics of these ten graph datasets presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Evaluation. We train a multiclass SVM classifier with onevs-one scheme. We perform a 10-fold cross-validation and for each fold we estimate SVM parameter C from the range [0.001, 0.01, 0.1, 1, 10] using validation set. This process is repeated 10 times and an average accuracy is reported, i.e. the average number of correctly classified test graphs. Competitors. PSCN is a convolutional neural network algorithm <ref type="bibr" target="#b17">(Niepert et al., 2016)</ref> with size of receptive field equals to 10. PSCN is the state-of-the-art instance of neural network algorithms, which has achieved strong classification accuracy in many datasets, and we use the best reported accuracy for these algorithms. GK is a graphlet kernel <ref type="bibr" target="#b20">(Shervashidze et al., 2009</ref>) and DGK is a deep graphlet kernel <ref type="bibr" target="#b27">(Yanardag &amp; Vishwanathan, 2015)</ref> with graphlet size equals to 7. WL is Weisfeiler-Lehman graph kernel algorithm <ref type="bibr" target="#b21">(Shervashidze et al., 2011)</ref> with height of subtree pattern equals to 7. WL proved consistenly strong results comparing to other graph kernels and supervised algorithms. ER is exponential random walk kernel <ref type="bibr" target="#b6">(Gärtner et al., 2003)</ref> with exponent equals to 0.5 and kR is k-step random walk kernel with k = 3 <ref type="bibr" target="#b22">(Sugiyama &amp; Borgwardt, 2015)</ref>.</p><p>Setup. For feature-based anonymous walk embeddings (Def. 1), we choose length l of walks from the range [2, 3, . . . , 10] and approximate actual distribution of anonymous walks using sampling equation <ref type="formula" target="#formula_11">(2)</ref> with ε = 0.1 and δ = 0.05.</p><p>For data-driven anonymous walk embeddings (Def. 4), we set length of walks l = 10 to generate a corpus of cooccurred anonymous walks. We run gradient descent with 100 iterations for 100 epochs with batch size that we vary from the range [100, 500, 1000, 5000, 10000]. Context walks are drawn from a window, which size varies in the range <ref type="bibr">[2,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16]</ref>. The embedding size of walks and graphs d a and d g equals to 128. Finally, candidate sampling function for softmax equation (4) chooses uniform or loguniform distribution of sampled classes.</p><p>To perform classification, we compute a kernel matrix, where Inner product, Polynomial, and RBF kernels are tested. For RBF kernel function we choose parameter σ from the range [10 −5 , 10 −4 , . . . , 1, 10]; for Polynomial function we set c = 0 and d = 2. We run the experiments on a machine with Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz and 32GB RAM 1 . We refer to our algorithms as AWE (DD) and AWE (FB) for data-driven and feature-based approaches correspondingly.</p><p>Classification results. <ref type="table" target="#tab_1">Table 2</ref> presents results on classification accuracy for Social unlabeled datasets. AWE approaches are consistently at the top, sharing top-2 results for all six social datasets, despite being unsupervised approach unlike PSCN. At the same time, <ref type="table">Table 4</ref> shows accuracy results for labeled bio datasets. Note that AWE are learned using only topology of the network and not node/edge labels. In this setting, embeddings obtained by AWE (FB) approach achieves competitive performance for the labeled datasets.</p><p>Overall observations.</p><p>• <ref type="table" target="#tab_1">Tables 2 and 4</ref> demonstrate that AWE is competitive to supervised state-of-the-art solutions in graph classification task. Importantly, even with simple classifiers such as SVM, AWE increases classification accuracy comparing to other more complex neural network models. Likewise, just comparing graph kernels, we can see that anonymous walks is at the top with tranditional graph objects such as graphlets (GK kernel) or subtree patterns (WL kernel).</p><p>• While feature-based and data-driven approaches are different in nature, the resulted classification accuracy is close across many datasets. As such, only on RE-B dataset data-driven approach has more than 5% increase in the accuracy. In practice, we found that using feature-based approach for small length l (e.g. ≤ 10) produces competitive results, while data-driven approach works best for large number of iterations and length l.</p><p>• Polynomial and RBF kernel functions bring nonlinearity to the classification algorithm and are able to learn more complex classification boundaries. <ref type="table" target="#tab_2">Table 3</ref> shows that RBF and Polynomial kernels are well suited for feature-based and data-driven models respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability.</head><p>To test for scalability, we learn network representations using AWE (DD) algorithm for Erdos-Renyi graphs with increasing sizes from [10, 10 1 , 10 2 , 10 3 , 10 4 , 3 · 10 4 ]. For each size we construct 10 Erdos-Renyi graphs with µ = np ∈ <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5]</ref>, where n is the number of nodes and p is the probability of having an edge between two arbitrary nodes. In that case, a graph has m ∝ µn edges.</p><p>1 Code can be found at https://github.com/nd7141/ AWE We average time to train AWE (DD) embeddings across 10 graphs for every n and µ. Our setup: size of embeddings equals to 128, batch size equals to 100, window size equals to 100. We run AWE (DD) model for 100 iterations in one epoch. In <ref type="figure" target="#fig_1">Figure 4</ref>, we empirically observe that the model to learn AWE (DD) network representations scales to networks with tens of thousands of nodes and edges and requires no more than a few seconds to map a graph to a vector. Intuition behind performance. There is a couple of factors that leads anonymous walk embeddings to state-of-theart performance in graph classification task. First, the use of anonymous walks is backed up by a recent discovery that, under certain condition, distribution of anonymous walks of a single node is sufficient to reconstruct a topology of the ball around a node. Hence, at least on a level of a single node, distribution of anonymous walk serves as a unique representation of subgraphs in a network. Second, data-driven approach reuses hitherto learned embeddings matrix W in previous iterations for learning embeddings of new graph instances. Therefore one can think of anonymous walks as words that have semantic meaning unified across all graphs. While learning graph embeddings, we simultaneously learn the meaning of different anonymous walks, which provides extra information for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Network representations were first studied in the context of graph kernels <ref type="bibr" target="#b6">(Gärtner et al., 2003)</ref> and then have become a separate topic that found numerous applications beyond graph classification <ref type="bibr" target="#b4">(Cai et al., 2017)</ref>. Our feature-based embeddings originate from learning distribution on anonymous walks in a graph and is alike to the approach of graph kernels. Embeddings based on graph kernels include Ran-  dom Walk <ref type="bibr" target="#b6">(Gärtner et al., 2003)</ref>, Graphlet <ref type="bibr" target="#b20">(Shervashidze et al., 2009</ref>), Weisfeiler-Lehman <ref type="bibr" target="#b21">(Shervashidze et al., 2011)</ref>, Shortest-Path <ref type="bibr" target="#b3">(Borgwardt &amp; Kriegel, 2005)</ref> decompositions and all can be summarized as an instance of R-convolution framework <ref type="bibr" target="#b9">(Haussler, 1999)</ref>.</p><p>Distributed representations have become trendy after significant achievements in NLP applications <ref type="bibr" target="#b13">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b19">b)</ref>. Our data-driven network embeddings stem from paragraph-vector distributed-memory model <ref type="bibr" target="#b11">(Le &amp; Mikolov, 2014</ref>) that has become successful in learning document representations. Other related approaches include Deep Graph Kernel <ref type="bibr" target="#b27">(Yanardag &amp; Vishwanathan, 2015)</ref> that learns a matrix for graph kernel that encodes relationship between substructures; PSCN <ref type="bibr" target="#b17">(Niepert et al., 2016)</ref> and 2D CNN <ref type="bibr" target="#b23">(Tixier et al., 2017)</ref> algorithms that learn convolutional neural networks on graphs; graph2vec <ref type="bibr" target="#b16">(Narayanan et al., 2017)</ref> learns network embeddings by extracting rooted subgraphs and training on skipgram negative sampling model <ref type="bibr" target="#b14">(Mikolov et al., 2013b)</ref>; FGSD <ref type="bibr" target="#b25">(Verma &amp; Zhang, 2017</ref>) that con-structs feature vector from the histogram of the multiset of node pairwise distances. <ref type="bibr" target="#b4">(Cai et al., 2017)</ref> provides a more comprehensive list of graph embeddings. Besides this, there is a list of aggregation techniques of node embeddings for the purpose of graph classification <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We described two unsupervised algorithms to compute network vector representations using anonymous walks. In the first approach, we use distribution of anonymous walks as a network embedding. As the exact calculation of network embeddings can be expensive we demonstrate how one can sample walks in a graph to approximate actual distribution with a given confidence. Next, we show how one can learn distributed graph representations in a data-driven manner, similar to learning paragraph vectors in NLP.</p><p>In our experiments, we show that our network embeddings even with simple SVM classifier achieve increase in classification accuracy comparing to state-of-the-art supervised neural network methods and graph kernels. This demonstrates that representation of your data can be more promising subject to study than the type and architecture of your predictive model.</p><p>Although the focus of this work was in representation of networks, AWE algorithm can be used to learn node, edge, or any subgraph representations by replacing graph vector with a corresponding subgraph vector. In all graph and subgraph representations, we expect data-driven approach to be a strong alternative to feature-based methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Y -axis is in log scale. The number of different anonymous walks increases exponentially with length of walks l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Average running time to generate anonymous walk embedding for Erdos-Renyi graphs, with µ = np ∈ [2, 3, 4, 5] where n is the number of nodes and p is probability parameter of Erdos-Renyi model. X-axis is in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This work was supported by the Ministry of Education and Science of the Russian Federation (Grant no. 14.756.31.0001) and by the Skoltech NGP Program No. 1-NGP-1567 Simulation and Transfer Learning for Deep 3D Geometric Data Analysis (a Skoltech-MIT joint project).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Graph datasets used in classification experiments.</figDesc><table><row><cell>The</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of classification accuracy (mean ± std., %) in Social datasets. Top-2 results are in bold. OOM is out-of-memory. DD) 51.54 ± 3.61 74.45 ± 5.83 73.93 ± 1.94 87.89 ± 2.53 50.46 ± 1.91 39.20 ± 2.09 PSCN 45.23 ± 2.84 71.00 ± 2.29 72.60 ± 2.15 86.30 ± 1.58 49.10 ± 0.70 41.32 ± 0.32 DGK 44.55 ± 0.52 66.96 ± 0.56 73.09 ± 0.25 78.04 ± 0.39 41.27 ± 0.18 32.22 ± 0.10 FB AWE (FB) 51.58 ± 4.66 73.13 ± 3.28 70.99 ± 1.49 82.97 ± 2.86 54.74 ± 2.93 41.51 ± 1.98 WL 49.33 ± 4.75 73.4 ± 4.63 79.02 ± 1.77 81.1 ± 1.9 49.44 ± 2.36 38.18 ± 1.3 GK 43.89 ± 0.38 65.87 ± 0.98 72.84 ± 0.28 65.87 ± 0.98 41.01 ± 0.17 31.82 ± 0.08</figDesc><table><row><cell></cell><cell>Algorithm</cell><cell>IMDB-M</cell><cell>IMDB-B</cell><cell>COLLAB</cell><cell>RE-B</cell><cell>RE-M5K</cell><cell>RE-M12K</cell></row><row><cell>DD</cell><cell>AWE (ER</cell><cell>OOM</cell><cell>64.00 ± 4.93</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell></cell><cell>kR</cell><cell cols="2">34.47 ± 2.42 45.8 ± 3.45</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Kernel function comparison in classification task (%).</figDesc><table><row><cell cols="2">Algorithm</cell><cell cols="3">IMDB-M COLLAB RE-B</cell></row><row><cell cols="2">AWE (DD) RBF</cell><cell>50.73</cell><cell>73.93</cell><cell>87.89</cell></row><row><cell cols="2">AWE (DD) Inner</cell><cell>51.54</cell><cell>73.77</cell><cell>84.82</cell></row><row><cell cols="2">AWE (DD) Poly</cell><cell>45.32</cell><cell>70.45</cell><cell>79.35</cell></row><row><cell cols="2">AWE (FB) RBF</cell><cell>51.58</cell><cell>70.99</cell><cell>82.97</cell></row><row><cell cols="2">AWE (FB) Inner</cell><cell>46.45</cell><cell>69.60</cell><cell>76.83</cell></row><row><cell cols="2">AWE (FB) Poly</cell><cell>46.57</cell><cell>64.3</cell><cell>67.22</cell></row><row><cell cols="5">Table 4. Classification accuracy (%) in labeled Bio datasets.</cell></row><row><cell>Algorithm</cell><cell cols="2">Enzymes</cell><cell>DD</cell><cell>Mutag</cell></row><row><cell>AWE</cell><cell cols="4">35.77 ± 5.93 71.51 ± 4.02 87.87 ± 9.76</cell></row><row><cell>PSCN</cell><cell>−</cell><cell cols="3">77.12 ± 2.41 92.63 ± 4.21</cell></row><row><cell>DGK</cell><cell cols="2">27.08 ± 0.79</cell><cell>−</cell><cell>82.66 ± 1.45</cell></row><row><cell>WL</cell><cell cols="4">53.15 ± 1.14 77.95 ± 0.70 80.72 ± 3.00</cell></row><row><cell>GK</cell><cell cols="4">32.70 ± 1.20 78.45 ± 0.26 81.58 ± 2.11</cell></row><row><cell>ER</cell><cell cols="2">14.97 ± 0.28</cell><cell>OOM</cell><cell>71.89 ± 0.66</cell></row><row><cell>kR</cell><cell cols="2">30.01 ± 1.01</cell><cell>OOM</cell><cell>80.05 ± 1.64</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Skolkovo Institute of Science and Technology, Moscow, Russia 2 Criteo Research, Paris, France. Correspondence to: Sergey Ivanov &lt;sergei.ivanov@skolkovotech.ru&gt;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computational Social Networks: Security and Privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph isomorphism in quasipolynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016</title>
		<meeting>the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2016<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="697" />
		</imprint>
	</monogr>
	<note>extended abstract</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janvin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005)</title>
		<meeting>the 5th IEEE International Conference on Data Mining (ICDM 2005)<address><addrLine>Houston, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11-30" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A comprehensive survey of graph embedding: Problems, techniques and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<idno>abs/1709.07604</idno>
		<ptr target="http://arxiv.org/abs/1709.07604" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory and Kernel Machines, 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-24" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reconstructing markov processes from independent and anonymous experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page" from="108" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning distributed representations of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Mining and Learning with Graphs (MLG)</title>
		<meeting>the 13th International Workshop on Mining and Learning with Graphs (MLG)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning with Kernels: support vector machines, regularization, optimization, and beyond. Adaptive computation and machine learning series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelfth International Conference on Artificial Intelligence and Statistics<address><addrLine>Clearwater Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04-16" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Halting in random walk kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1639" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Classifying graphs as images with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno>abs/1708.02218</idno>
		<ptr target="http://arxiv.org/abs/1708.02218" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The number of walks and degree powers in directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Rutgers University, TUM-I123 ; TU Munich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
