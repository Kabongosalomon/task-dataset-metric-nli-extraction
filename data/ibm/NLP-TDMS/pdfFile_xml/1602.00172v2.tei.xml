<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning For Smile Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-26">July 26, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">O</forename><surname>Glauner</surname></persName>
							<email>patrick.glauner@uni.lusnt.uni.lu</email>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Centre for Security, Reliability and Trust</orgName>
								<orgName type="institution">University of Luxembourg</orgName>
								<address>
									<postCode>2721</postCode>
									<settlement>Luxembourg</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning For Smile Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-07-26">July 26, 2017</date>
						</imprint>
					</monogr>
					<note>1:47 WSPC -Proceedings Trim Size: 9in x 6in paper 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Vision</term>
					<term>Deep Learning</term>
					<term>Facial expression recognition</term>
					<term>GPU acceleration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by recent successes of deep learning in computer vision, we propose a novel application of deep convolutional neural networks to facial expression recognition, in particular smile recognition. A smile recognition test accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action (DISFA) database, significantly outperforming existing approaches based on hand-crafted features with accuracies ranging from 65.55% to 79.67%. The novelty of this approach includes a comprehensive model selection of the architecture parameters, allowing to find an appropriate architecture for each expression such as smile. This is feasible because all experiments were run on a Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations on a CPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks are celebrating a comeback under the term "deep learning" for the last ten years by training many hidden layers allowing to selflearn complex feature hierarchies. This makes them of particular interest for computer vision, in which feature description is a long-standing issue. Many advances have been reported in this period, including new training methods and a paradigm shift of training from CPUs to GPUs. As a result, those advances allow to train more reliable models much faster. This has for example resulted in breakthroughs 3 in signal processing. Nonetheless, deep neural networks are not a magic bullet and successful training is still heavily based on experimentation.</p><p>The Facial Action Coding System (FACS) 1 is a system to taxonomize any facial expression of a human being by their appearance on the face. Action units describe muscles or muscle groups in the face, are set or un-set and the activation may be on different intensity levels. State-of-the art approaches in this field mostly rely on hand-crafted features leaving a lot of potential for higher accuracies. In contrast to other fields such as face or gesture recognition, only very few works on deep learning applied to facial expression recognition have been reported so far 2 in which the architecture parameters are fixed. We are not aware of publications in which the architecture of a deep neural network for facial expression recognition is subject to extensive model selection. This allows to learn appropriate architectures per action unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Deep neural networks</head><p>Training neural networks is difficult, as their cost functions have many local minima. The more hidden layers, the more difficult the training of a neural network. Hence, training tends to converge to a local minimum, resulting in poor generalization of the network. In order to overcome these issues, a variety of new concepts have been proposed in the literature, of which only a few can be named in this chapter. Unsupervised pre-training methods, such as autoencoders 8 allow to initialize the weights well in order for backpropagation to quickly optimize them. The Rectified Linear Unit (ReLU) 7 and dropout 10 are new regularization methods. The new training methods and other new concepts can also lead to significant improvements of shallow neural networks with just a few hidden layers. Convolutional neural networks (CNNs) were initially proposed by LeCun 5 for the recognition of hand-written digits. A CNN consists of two layers: a convolutional layer, followed by a subsampling layer. Inspired by biological processes and exploiting the fact that nearby pixels are strongly correlated, CNNs are relatively insensitive to small translations or rotations of the image input.</p><p>Training deep neural networks is slow due to the number of parameters in the model. As the training can be described in a vectorized form, it is possible to massively parallelize it. Modern GPUs have thousands of cores and are therefore an ideal candidate for the execution of the training of neural networks. Significant speedups of factor 10 or higher 9 have been reported. A difficulty is to write GPU code. In the last few years, more abstract libraries have been released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DISFA database</head><p>The Denver Intensity of Spontaneous Facial Action (DISFA) 6 database consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all. <ref type="figure" target="#fig_0">Fig. 1</ref> contains a sample image of DISFA. In the original paper on DISFA 6 multi-class SVMs were trained for the different levels 0-5 of action unit intensity. Test accuracies for the individual levels and for the binary action unit recognition problem are reported for three different hand-crafted feature description techniques. In those three cases, accuracies of 65.55%, 72.94% and 79.67% for smile recognition are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Smile recognition</head><p>In the following experiments, an aligned version of DISFA is used. In this aligned version, the faces have been cropped and annotated with facial landmark points. Facial landmark points allow to compute a bounding box to fit the mouth in all images. In the experiments, two inputs are used: the mouth and face, downscaled to 85 × 69 and 128 × 104 pixels, respectively. Both inputs are used to assess if the mouth alone is as expressive as or even more expressive than the entire face for smile recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model</head><p>The architecture of the network is as follows: The input images are fed into a convolution comprising a convolutional and a subsampling layer. That convolution may be followed by more convolutions to become gradually more invariant to distortions in the input. In the second stage, a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions. The output layer consists of two units for smile or no smile. The novelty of this approach is that the exact number of convolutions, number of hidden layers and size of hidden layers are not fixed but subject to extensive model selection in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment setting</head><p>Due to training time constraints, some parameters have been fixed to reasonable and empirical values, such as the size of convolutions (5 × 5 pixels, 32 feature maps) and the size of subsamplings (2 × 2 pixels using max pooling). All layers use ReLU units, except of softmax being used in the output layer. The learning rate is fixed to α = 0.01 and not subject to model selection as it would significantly prolong the model selection. The same considerations apply to the momentum, which is fixed to µ = 0.9.</p><p>The entire database has been randomly split into a 60%/20%/20% training/validation/test ratio. Training neural networks comes with uncertainties, mostly due to the random initialization of the weights, but also due to that random split of the data. Evaluations have shown that for 10 similar experiments carried out, the standard deviation of the test accuracy is 0.041725%. Because of this low standard deviation, performing each experiment exactly once has only a very low bias and is therefore relatively safe to do for reasons of faster training time. Throughout the experiments, the classification rate is used as the accuracy measure.</p><p>The model is implemented using Lasagne 4 and the generated CUDA code is executed on a Tesla K40c 9 as training on a GPU allows to perform a comprehensive model selection in a feasible amount of time. Stochastic gradient descent with a batch size of 500 is used. <ref type="table" target="#tab_0">Table 1</ref> contains the four parameters to be optimized: the number of convolutions, the number of hidden layers, the number of units per hidden layer and the dropout factor. Each parameter was optimized independently due to training time constraints. This may not lead to an optimal model, but has proven to work empirically well. Each model was trained for 50 epochs in the model selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter optimization</head><p>For both inputs, <ref type="table" target="#tab_1">Table 2</ref> contains the final models selected. For the mouth input, there is a preference to more convolutions and more hidden layers. This is the case because slight translations or rotations in the mouth input have stronger consequences on the classification result. In the entire face, that sort of distortions may be less of a problem because other parts of the face such as the cheeks contribute to smile recognition, too. Training time per epoch are 82 seconds and 41 seconds for the mouth and face input models, respectively. Experiments have shown that the training time mostly depends on the number of convolutions. Using the Tesla K40c GPU has allowed to speed up the training time by factor ten over the use of a CPU to execute the CPU code generated by the library. This clearly demonstrates the importance of training on a GPU to do a comprehensive model selection in a feasible amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>Deep learning is an umbrella term for training neural networks with potentially many hidden layers using new training methods allowing to learn complex feature hierarchies from data. Applied to action unit recognition and smile recognition in particular, a deep convolutional neural network model with an overall accuracy of 99.45% significantly outperforms existing approaches. The underlying extensive model selection allows to find for each action unit an appropriate architecture in order to maximize test accuracies. In the future, we will extend the model to images from multiple databases and to make predictions in image sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Different input parts: a) mouth, b) face. 6 (Not at actual input size/proportions.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Parameters and possible values used in model selection.</figDesc><table><row><cell>Parameter</cell><cell>Values</cell><cell>Default value</cell></row><row><cell>#Convolutions</cell><cell>1, 2, 3</cell><cell>1</cell></row><row><cell>#Hidden layers</cell><cell>1, 2, 3</cell><cell>1</cell></row><row><cell cols="2">#Units / hidden layer 100, 200, 300, 400</cell><cell>100</cell></row><row><cell>Dropout</cell><cell>0, 0.1, 0.5, 0.7</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Selected parameter values for mouth and face input.Both final models were trained for 1000 epochs. The test accuracies of both models started to converge after about 300 epochs. For the mouth and face inputs, the best accuracies were achieved after 700 and 1000 epochs with 99.45% and 99.34%, respectively. Both models significantly outperform the state-of-the-art SVM baselines reported in Sec. 3 ranging from 65.55% to 79.67%. Overall, there is no strong preference for either the mouth or face input. Further experiments with a reduced dataset containing only 70% of the images that have no action unit(s) set at all support this hypothesis. Concretely, the test accuracies for the mouth and face input reduced to 99.24% and 99.26%, respectively. Thus, the difference between the two models has been further reduced and this time giving a very low preference for the face input. Nonetheless, this difference is not representative as it is within the experiment error standard deviation reported in Sec. 4.2.</figDesc><table><row><cell>Input</cell><cell cols="4">#Convs #Hidden layers #Units / hidden layer Dropout</cell></row><row><cell>Mouth</cell><cell>2</cell><cell>2</cell><cell>400</cell><cell>0.1</cell></row><row><cell>Face</cell><cell>1</cell><cell>1</cell><cell>400</cell><cell>0</cell></row><row><cell cols="3">4.4. Results and discussion</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Facial Action Coding System: A Technique for the Measurement of Facial Movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>Palo Alto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Learning based FACS Action Unit Occurrence and Intensity Estimation. Vicarious Perception Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gudi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="http://github.com/Lasagne/Lasagne.Retrieved" />
		<title level="m">Lasagne</title>
		<imprint>
			<date type="published" when="2015-07-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/lenet/.Retrieved" />
		<title level="m">LeNet-5, convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015-04-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rectified Linear Units Improve Restricted Boltzmann</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://deeplearning.stanford.edu/tutorial/.Retrieved" />
		<title level="m">Deep Learning Tutorial</title>
		<imprint>
			<date type="published" when="2015-02-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia: Tesla</surname></persName>
		</author>
		<ptr target="http://www.nvidia.com/object/tesla-servers.html" />
		<imprint>
			<date type="published" when="2015-08-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
