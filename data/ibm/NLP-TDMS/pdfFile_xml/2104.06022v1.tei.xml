<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lessons on Parameter Sharing across Layers in Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
							<email>sho.takase@nlp.c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
							<email>shun.kiyono@riken.jp</email>
							<affiliation key="aff1">
								<orgName type="department">RIKEN / Tohoku University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lessons on Parameter Sharing across Layers in Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a parameter sharing method for Transformers <ref type="bibr" target="#b13">(Vaswani et al., 2017)</ref>. The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers <ref type="bibr" target="#b2">(Dehghani et al., 2019)</ref>, to increase the efficiency in the computational time. We propose three strategies: SEQUENCE, CYCLE, and CY-CLE (REV) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based methods have achieved notable performance in various NLP tasks <ref type="bibr" target="#b13">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b3">Devlin et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>. In particular, <ref type="bibr">Brown et al. (2020)</ref> indicated that the larger parameter size we prepare, the better performance the model achieves. However, the model which is composed of many parameters occupies a large part of a GPU memory capacity. Thus, it is important to explore a parameter efficient way, which achieves better performance than a basic model with the same parameter size.</p><p>Parameter sharing is a widely used technique as a parameter efficient way <ref type="bibr" target="#b2">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b1">Dabre and Fujita, 2019;</ref>. <ref type="bibr" target="#b2">Dehghani et al. (2019)</ref> proposed Universal Transformer which consists of parameters for only one layer of a Transformer-based encoder-decoder, and uses the parameters N times for an N -layered encoder-decoder. <ref type="bibr" target="#b1">Dabre and Fujita (2019)</ref> and  also used such parameter sharing across layers for their Transformers. <ref type="bibr" target="#b2">Dehghani et al. (2019)</ref> reported that Universal Transformer achieved better performance than the vanilla Transformer in machine translation if they consist of the same number of parameters. However, when we prepare the same number of parameters for Universal Transformer and basic Transformer, Universal Transformer requires much more computational time because weight matrices for each layer in Universal Transformer are much larger. For example, we demonstrate that Universal Transformer requires twice as much training time as the basic Transformer in WMT En-De, which is a widely used machine translation dataset.</p><p>In this paper, we propose a new parameter sharing method which is faster than using the same parameters for all layers such as Universal Transformer. Instead of preparing parameters for only one layer, we prepare parameters for M layers to construct an N -layered encoder-decoder, where 1 ≤ M ≤ N . In other words, the proposed method relaxes the parameter sharing strategy used in previous studies <ref type="bibr" target="#b2">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b1">Dabre and Fujita, 2019;</ref>. For the parameter assignment to each layer, we provide several strategies and compare them empirically. Experimental results show that the proposed method achieves comparable scores to the method assigning parameters of one layer to all layers with smaller computational time. </p><formula xml:id="formula_0">else if i ≤ (M * ( N/M − 1)) then 18: enc i ← enc ((i−1) mod M )+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>else 20: </p><formula xml:id="formula_1">enc i ← enc M −((i−1) mod M )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>As described in Section 1, we use parameters for M layers in the construction of an N -layered Transformer-based encoder-decoder. For the parameter assignment, we provide three strategies: SEQUENCE, CYCLE, and CYCLE (REV). We describe these strategies in this section. <ref type="figure" target="#fig_0">Figure 1</ref> shows examples of three parameter assignment strategies for an encoder-side when we set M = 3 and N = 6. Let enc i be the i-th layer of an encoder. <ref type="figure" target="#fig_1">Figure 2</ref> describes the algorithm to assign each parameter to each layer for the encoder. For the decoder-side, we assign each parameter with the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SEQUENCE</head><p>The simplest strategy is to assign the same parameters to sequential N/M layers. We name this strategy SEQUENCE. For example, when we set M = 3 and N = 6, sequential 2 layers share their parameters as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CYCLE</head><p>In CYCLE, we stack M layers whose parameters are independent from each other. Then, we repeat stacking the M layers with the identical order to the first M layers until the total number of layers reaches N . When we set M = 3 and N = 6, we stack 3 layers twice as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CYCLE (REV)</head><p>Liu et al. <ref type="bibr">(2020)</ref> reported that higher decoder layers obtain larger gradient norms when we use the post layer normalization setting, which is originally used in <ref type="bibr" target="#b13">Vaswani et al. (2017)</ref> and widely used in machine translation. Their report implies that higher layers require more degrees of freedom than lower layers for their expressiveness. In other words, lower layers probably have redundant parameters in comparison with higher layers. Thus, we propose the strategy CYCLE (REV) reusing parameters of lower layers in higher layers.</p><p>In this strategy, we repeat stacking M layers in the same as CYCLE until M * ( N/M − 1) layers. For the rest of the layers, we stack M layers in reverse order. When we set M = 3 and N = 6, we stack 3 layers and then stack the 3 layers in reverse order as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Thus, the lowest layer and highest layer share their parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Layer Normalization in Transformers</head><p>For layer normalizations in Transformers, most recent studies used the pre layer normalization setting (Pre-LN) when they stacked many layers <ref type="bibr" target="#b14">(Wang et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref> because Pre-LN makes a training more stable than the post layer normalization setting (Post-LN) (Nguyen and Salazar, 2019; <ref type="bibr" target="#b15">Xiong et al., 2020)</ref>. However, Transformers with Post-LN achieve better performance if we succeed in their training <ref type="bibr" target="#b8">(Nguyen and Salazar, 2019;</ref>. To make a training Transformers with Post-LN stable,  proposed Admin which smooths the impacts of each parameter in the early stage of training. In this study, we also use Admin for the stable training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We focus on the English-to-German translation task in the same as previous studies <ref type="bibr">(Vaswani et</ref>  2017; <ref type="bibr" target="#b2">Dehghani et al., 2019;</ref><ref type="bibr" target="#b5">Kiyono et al., 2020)</ref>.</p><p>We conduct experiments on two types of Englishto-German translation tasks. The difference is the amount of training data. <ref type="table" target="#tab_1">Table 1</ref> summarizes the number of training data in each configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Standard Setting</head><p>Datasets We used WMT 2016 training dataset, which is widely used in previous studies <ref type="bibr" target="#b13">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b9">Ott et al., 2018)</ref>. The dataset contains 4.5M English-German sentence pairs. Following the previous studies, we constructed a vocabulary set with BPE <ref type="bibr" target="#b12">(Sennrich et al., 2016b)</ref> in the same manner. We set the number of BPE merge operations at 32K and shared the vocabulary between both the source and target languages. We measured case-sensitive detokenized BLEU with Sacre-BLEU (Post, 2018) 1 .</p><p>Methods For the proposed parameter assignment strategies, we fixed M = 6 and set N = 12, 18 based on the Transformer (base) setting in <ref type="bibr" target="#b13">Vaswani et al. (2017)</ref>. We compare the proposed strategies with the following baselines.</p><p>Original and Original (big): These are the original Transformer (base) and (big) settings in <ref type="bibr" target="#b13">Vaswani et al. (2017)</ref>, respectively. Admin: We applied Admin  to the Transformer (base) setting. Universal: As the parameter sharing strategy in previous studies such as Universal Transformers <ref type="bibr" target="#b2">(Dehghani et al., 2019)</ref>, we set M = 1 2 . In this setting, we increased the dimension of each layer for a fair comparison in terms of the number of parameters. We used the Universal Transformer 1 The BLEU score computed by SacreBLEU is often lower than the score by the procedure of <ref type="bibr" target="#b13">Vaswani et al. (2017)</ref> as reported in <ref type="bibr" target="#b9">Ott et al. (2018)</ref>. In fact, when we used the same procedure as <ref type="bibr" target="#b13">Vaswani et al. (2017)</ref>, the best model in <ref type="table">Table 3</ref> achieved 35.14 in the averaged BLEU score in newstest2014. However, Post (2018) encouraged using SacreBLEU for the compatibility of WMT results. <ref type="bibr">2</ref> The original Universal Transformers <ref type="bibr" target="#b2">(Dehghani et al., 2019)</ref> use the sinusoidal positional encoding for each layer and adaptive computation time technique (Graves, 2017) but we omitted them in this study to focus on the difference among parameter sharing strategies. base setting in <ref type="bibr" target="#b2">Dehghani et al. (2019)</ref>.</p><p>Results <ref type="table" target="#tab_3">Table 2</ref> shows BLEU scores on newstest2010-2016 for each method. We trained three models with different random seeds, and reported the averaged scores. In addition, <ref type="table" target="#tab_3">Table 2</ref> shows the total number of parameters and computational speeds 3 . The computational speeds are based on the speed of Original.</p><p>In the comparison between Universal and Original, Universal achieved better scores although their parameter sizes are almost the same. This result is consistent with the report in <ref type="bibr" target="#b2">Dehghani et al. (2019)</ref>. Moreover, Universal outperformed Original (big) in the averaged score even though the parameter size of Universal is much smaller than the one of Original (big). On the other hand, the proposed strategies (SEQUENCE, CYCLE, and CYCLE (REV)) were faster and achieved slightly better scores than Universal when we set M = 6 and N = 12. Since Admin did not have positive influence on the BLEU scores as shown in <ref type="table" target="#tab_3">Table 2</ref>, our strategies caused the improvements. Thus, our proposed parameter sharing strategies are more efficient in terms of the parameter size and computational time.</p><p>In M = 6 and N = 12, SEQUENCE, CYCLE, and CYCLE (REV) achieved almost the same scores. In contrast, the scores of SEQUENCE were lower than other two strategies in M = 6 and N = 18. This result indicates that CYCLE and CYCLE (REV) are better strategies when we construct a deep Transformer with small M . In M = 6 and N = 18, CYCLE (REV) improved 0.41 from Universal in the averaged BLEU score even though their computational speeds were almost the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">High Resource Setting</head><p>Datasets In the high resource setting, we constructed 44.2M translation sentence pairs as a training dataset with the procedures of <ref type="bibr" target="#b5">Kiyono et al. (2020)</ref> which achieved the best result in the WMT 2020 news translation task. In addition, we augmented the training data by using the backtranslation technique <ref type="bibr" target="#b11">(Sennrich et al., 2016a)</ref> in the same manner as <ref type="bibr" target="#b5">Kiyono et al. (2020)</ref>. We obtained 284.3M pairs as the synthetic training data.</p><p>Methods We used the original Transformer (big) setting <ref type="bibr" target="#b13">(Vaswani et al., 2017)</ref> as our baseline in using genuine training data. For the proposed strategies, we used N = 12 and M = 6.   <ref type="table">Table 3</ref>: BLEU scores on newstest2014, 2018, and 2019. We focused these test sets to compare the top system on WMT 2020 <ref type="bibr" target="#b5">(Kiyono et al., 2020)</ref>.</p><p>In using both of the genuine and synthetic (backtranslated) dataset, we applied CYCLE (REV) to the BASE setting in <ref type="bibr" target="#b5">Kiyono et al. (2020)</ref> because CYCLE (REV) achieved the best BLEU scores on most test sets in <ref type="table" target="#tab_3">Table 2</ref>. We also used N = 12 and M = 6 in this configuration. We compare the reported score of the best model described in <ref type="bibr" target="#b5">Kiyono et al. (2020)</ref>. Their model is composed of 9 layers (N = 9 and M = 1, in other words), and thus, it contains much more parameters than ours.</p><p>Results <ref type="table">Table 3</ref> shows BLEU scores on new-stest2014, 2018, and 2019. We focused on these test data to compare <ref type="bibr" target="#b5">Kiyono et al. (2020)</ref>. In the same as experiments in Section 4.1, we reported the averaged scores of three morels trained with different random seeds. <ref type="table">Table 3</ref> also shows the total number of parameters 4 . <ref type="table">Table 3</ref> shows that our proposed strategies achieved better BLEU scores than Original (big) when we prepared the same number of parameters. This result indicates that our proposed strategies are also useful in the high resource setting.</p><p>When we used additional synthetic data for the training in the same manner as <ref type="bibr" target="#b5">Kiyono et al. (2020)</ref>, CYCLE (REV) achieved comparable BLEU scores to the best system of <ref type="bibr" target="#b5">Kiyono et al. (2020)</ref> except for newstest2019 5 even though the parameter size of CYCLE (REV) were smaller than theirs. This result indicates that CYCLE (REV) is also efficient in the construction of models for the recent competitive tasks. In addition, this result implies that our proposed strategies can be used in the configuration where we train many parameters with a tremendous amount of data such as recent pre-trained language models, e.g., GPT series <ref type="bibr">(Brown et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed three parameter sharing strategies: SE-QUENCE, CYCLE, and CYCLE (REV), for internal layers in Transformers. In contrast to the previous strategy, which prepares parameters for only one layer and shares them across layers such as Universal Transformers <ref type="bibr" target="#b2">(Dehghani et al., 2019)</ref>, our proposed strategies prepare parameters for M layers to construct N layers. Experimental results show that the proposed strategies achieved comparable BLEU scores to ones of Universal with small computational time when we prepare almost the same parameters for each method. In other words, the proposed strategies are efficient in the parameter size and computational time. Moreover, CYCLE and CYCLE (REV) are appropriate to the construction of a deep model. Through experiments on the high resource setting, we demonstrated that CYCLE (REV) can achieve the comparable scores to the top system of WMT 2020 news translation task (Kiyono et al., 2020) with smaller parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of three parameter assignment strategies proposed by this study when we set M = 3 and N = 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed parameter assignment strategies for an encoder construction. CreateNewLayer is a function which creates a new encoder layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2104.06022v1 [cs.CL] 13 Apr 2021 Algorithm Encoder Construction Input: the total number of layers N , the number of independent layers M , the sharing strategy TYPE ∈ {SEQUENCE, CYCLE, CYCLE (REV)} Output: enc 1 , ..., enc N 1: for i in [1, ..., N ] do</figDesc><table><row><cell>2:</cell><cell>if i == 1 then</cell></row><row><cell>3:</cell><cell>enc i ← CreateNewLayer</cell></row><row><cell>4:</cell><cell>else if TYPE == SEQUENCE then</cell></row><row><cell>5:</cell><cell>if (i − 1) mod N/M == 0 then</cell></row><row><cell>6:</cell><cell>enc i ← CreateNewLayer</cell></row><row><cell>7:</cell><cell>else</cell></row><row><cell>8:</cell><cell>enc i ← enc i−1</cell></row><row><cell>9:</cell><cell>else if TYPE == CYCLE then</cell></row><row><cell>10:</cell><cell>if i ≤ M then</cell></row><row><cell>11:</cell><cell>enc i ← CreateNewLayer</cell></row><row><cell>12:</cell><cell>else</cell></row><row><cell>13:</cell><cell>enc i ← enc ((i−1) mod M )+1</cell></row><row><cell>14:</cell><cell>else if TYPE == CYCLE (REV) then</cell></row><row><cell>15:</cell><cell>if i ≤ M then</cell></row><row><cell>16:</cell><cell>enc i ← CreateNewLayer</cell></row><row><cell>17:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The number of translation sentence pairs in training datasets for each experiment.</figDesc><table><row><cell>al.,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>×0.63 24.65 22.32 22.83 26.98 27.88 30.27 34.99  27.13    </figDesc><table><row><cell>Method</cell><cell cols="3">M N #Params Speed 2010</cell><cell>2011</cell><cell>2012</cell><cell>2013</cell><cell>2014</cell><cell>2015</cell><cell>2016 Average</cell></row><row><cell>Original</cell><cell>6</cell><cell>6</cell><cell cols="6">61M ×1.00 24.16 22.01 22.33 26.13 27.13 29.83 34.41</cell><cell>26.57</cell></row><row><cell>Admin</cell><cell>6</cell><cell>6</cell><cell cols="6">61M ×0.97 24.14 21.93 22.25 26.14 27.05 29.59 34.23</cell><cell>26.48</cell></row><row><cell>Universal</cell><cell>1</cell><cell>6</cell><cell cols="6">63M ×0.48 24.37 22.33 22.70 26.40 27.65 30.24 34.60</cell><cell>26.90</cell></row><row><cell>Original (big)</cell><cell>6</cell><cell>6</cell><cell cols="6">210M ×0.39 24.31 22.21 22.75 26.39 28.28 30.35 33.40</cell><cell>26.81</cell></row><row><cell cols="9">SEQUENCE 61M CYCLE 6 12 6 12 61M ×0.63 24.51 22.43 22.69 26.61 27.91 30.37 34.77</cell><cell>27.04</cell></row><row><cell>CYCLE (REV)</cell><cell>6</cell><cell>12</cell><cell cols="6">61M ×0.63 24.66 22.47 22.87 26.68 27.72 30.37 34.81</cell><cell>27.08</cell></row><row><cell>SEQUENCE</cell><cell>6</cell><cell>18</cell><cell cols="6">61M ×0.47 24.53 22.44 22.73 26.59 27.73 30.30 34.80</cell><cell>27.02</cell></row><row><cell>CYCLE</cell><cell>6</cell><cell>18</cell><cell cols="6">61M ×0.47 24.74 22.60 23.04 26.89 28.14 30.54 34.79</cell><cell>27.25</cell></row><row><cell>CYCLE (REV)</cell><cell>6</cell><cell>18</cell><cell cols="6">61M ×0.47 24.93 22.77 23.09 26.88 28.09 30.60 34.84</cell><cell>27.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The number of layers, the number of parameters, computational speeds based on the original Transformer, BLEU scores on newstest2010-2016, and averaged scores when we trained each method on widely used WMT 2016 training dataset. Scores in bold denote the best result for each set.</figDesc><table><row><cell>Method</cell><cell cols="2">#Params 2014</cell><cell>2018</cell><cell>2019</cell></row><row><cell cols="3">Genuine training data</cell><cell></cell><cell></cell></row><row><cell>Original (big)</cell><cell>242M</cell><cell cols="3">31.40 47.11 42.80</cell></row><row><cell>SEQUENCE</cell><cell>242M</cell><cell cols="3">31.90 48.15 43.12</cell></row><row><cell>CYCLE</cell><cell>242M</cell><cell cols="3">32.10 48.11 43.19</cell></row><row><cell>CYCLE (REV)</cell><cell>242M</cell><cell cols="3">32.06 48.34 43.43</cell></row><row><cell cols="4">+ Synthetic (back-translated) data</cell><cell></cell></row><row><cell>Kiyono et al. (2020)</cell><cell>514M</cell><cell>33.1</cell><cell>49.6</cell><cell>42.7</cell></row><row><cell>CYCLE (REV)</cell><cell>343M</cell><cell cols="3">33.54 49.55 42.18</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We regard processed tokens per second during the training as the conputational speed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The parameter sizes of Original (big) inTables 2 and 3are different from each other due to the difference of sharing embeddings. Following<ref type="bibr" target="#b5">Kiyono et al. (2020)</ref>, we did not share embeddings in the high resource setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For newstest2019, synthetic data might harm the quality of a model because models trained with only genuine data outperformed models trained with both data.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33 (NeurIPS)</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent stacking of layers for compact neural machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6292" to="6299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tohoku-AIP-NTT at WMT 2020 news translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuto</forename><surname>Konno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation (WMT)</title>
		<meeting>the Fifth Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
	<note>Makoto Morishita, and</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ALBERT: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5747" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salazar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Spoken Language Translation (IWSLT)</title>
		<meeting>the 16th International Conference on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
		<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
