<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Positional Encoding to Control Output Sequence Length</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
							<email>sho.takase@nlp.c</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
							<email>okazaki@c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Positional Encoding to Control Output Sequence Length</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural encoder-decoder models have been successful in natural language generation tasks. However, real applications of abstractive summarization must consider additional constraint that a generated summary should not exceed a desired length. In this paper, we propose a simple but effective extension of a sinusoidal positional encoding <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref> to enable neural encoder-decoder model to preserves the length constraint. Unlike in previous studies where that learn embeddings representing each length, the proposed method can generate a text of any length even if the target length is not present in training data. The experimental results show that the proposed method can not only control the generation length but also improve the ROUGE scores.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural encoder-decoder models have been successfully applied to various natural language generation tasks including machine translation <ref type="bibr" target="#b12">(Sutskever et al., 2014)</ref>, summarization <ref type="bibr" target="#b10">(Rush et al., 2015)</ref>, and caption generation <ref type="bibr" target="#b16">(Vinyals et al., 2015)</ref>. Still, it is necessary to control the output length for abstractive summarization, which generates a summary for a given text while satisfying a space constraint. In fact, <ref type="figure">Figure 1</ref> shows a large variance in output sequences produced by a widely used encoder-decoder model <ref type="bibr" target="#b7">(Luong et al., 2015)</ref>, which has no mechanism for controlling the length of the output sequences. <ref type="bibr" target="#b0">Fan et al. (2018)</ref> trained embeddings that correspond to each output length to control the output sequence length. Since the embeddings for different lengths are independent, it is hard to generate a sequence of the length that is infrequent in training data. Thus, a method that can model any lengths continuously is required. <ref type="figure">Figure 1</ref>: Difference in number of characters between correct headlines and outputs of a widely used LSTM encoder-decoder <ref type="bibr" target="#b7">(Luong et al., 2015)</ref> which is trained on sentence-headline pairs created by <ref type="bibr" target="#b10">Rush et al. (2015)</ref> from the annotated English Gigaword corpus. The difference was investigated for 3,000 sentence-headline pairs randomly sampled from the test splits. <ref type="bibr" target="#b2">Kikuchi et al. (2016)</ref> proposed two learning based methods for an LSTM encoder-decoder: LenEmb and LenInit. LenEmb inputs an embedding representing the remaining length in each decoding step. Since this approach also prepares embeddings for each length independently, it suffers from the same problem as that in <ref type="bibr" target="#b0">Fan et al. (2018)</ref>.</p><p>On the other hand, LenInit can handle arbitrary lengths because it combines the scalar value of a desired length with a trainable embedding. LenInit initializes the LSTM cell of the decoder with the embedding depending on the scalar value of the desired length. <ref type="bibr" target="#b6">Liu et al. (2018)</ref> incorporated such scalar values into the initial state of the decoder in a CNN encoder-decoder. These approaches deal with any length but it is reasonable to incorporate the distance to the desired terminal position into each decoding step such as in LenEmb.</p><p>In this study, we focused on Transformer <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref>, which recently achieved the state-of-the-art score on the machine translation task. We extend the sinusoidal positional encoding, which represents a position of each token in Transformer <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref>, to represent a distance from a terminal position on the decoder side. In this way, the proposed method considers the remaining length explicitly at each decoding step. Moreover, the proposed method can handle any desired length regardless of its appearance in a training corpus because it uses the same continuous space for any length.</p><p>We conduct experiments on the headline generation task. The experimental results show that our proposed method is able to not only control the output length but also improve the ROUGE scores from the baselines. Our code and constructed test data are publicly available at: https://github.com/takase/control-length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Positional Encoding</head><p>Transformer <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref>  </p><p>In short, each dimension of the positional encoding corresponds to a sinusoid whose period is 10000 2i/d × 2π. Since this function returns an identical value at the same position pos, the above positional encoding can be interpreted as representing the absolute position of each input token.</p><p>In this paper, we extend Equations (1) and (2) to depend on the given output length and the distance from the terminal position. We propose two extensions: length-difference positional encoding (LDP E) and length-ratio positional encoding (LRP E). Then we replace Equations (1) and (2) with (3) and (4) (or (5) and (6)) on the decoder side to control the output sequence length. We define LDP E and LRP E as follows: </p><p>where len presents the given length constraint. LDP E returns an identical value at the position where the remaining length to the terminal position is the same. LRP E returns a similar value at the positions where the ratio of the remaining length to the terminal position is similar. Let us consider the d-th dimension as the simplest example.</p><p>Since we obtain sin(pos/len) (or cos(pos/len)) at this dimension, the equations yield the same value when the remaining length ratio is the same, e.g., pos = 5, len = 10 and pos = 10, len = 20. We add LDP E (or LRP E) to the input layer of Transformer in the same manner as in <ref type="bibr" target="#b15">Vaswani et al. (2017)</ref>. In the training step, we assign the length of the correct output to len. In the test phase, we control the output length by assigning the desired length to len.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct experiments on the headline generation task on Japanese and English datasets. The purpose of the experiments is to evaluate the ability of the proposed method to generate a summary of good quality within a specified length. We used JAMUL corpus as the Japanese test set <ref type="bibr" target="#b1">(Hitomi et al., 2019)</ref>. This test set contains three kinds of headlines for 1,181 1 news articles written by professional editors under the different upper bounds of headline lengths. The upper bounds are 10, 13, and 26 characters <ref type="bibr">(len = 10, 13, 26)</ref>. This test set is suitable for simulating the real process of news production because it is constructed by a Japanese media company.</p><p>In contrast, we have no English test sets that contain headlines of multiple lengths. Thus, we randomly extracted 3,000 sentence-headline pairs that satisfy a length constraint from the test set constructed from annotated English Gigaword <ref type="bibr" target="#b8">(Napoles et al., 2012)</ref> by pre-processing scripts of <ref type="bibr" target="#b10">Rush et al. (2015)</ref> 2 . We set three configurations for the number of characters as the length constraint: 0 to 30 characters (len = 30), 30 to 50 characters (len = 50), and 50 to 75 characters (len = 75). Moreover, we also evaluate the proposed method on the DUC-2004 task 1 <ref type="bibr" target="#b9">(Over et al., 2007)</ref> for comparison with published scores in previous studies.</p><p>Unfortunately, we have no large supervision data with multiple headlines of different lengths associated with each news article in both languages. Thus, we trained the proposed method on pairs with a one-to-one correspondences between the source articles and headlines. In the training step, we regarded the length of the target headline as the desired length len. For Japanese, we used the JNC corpus, which contains a pair of the lead three sentences of a news article and its headline <ref type="bibr" target="#b1">(Hitomi et al., 2019)</ref>. The training set contains about 1.6M pairs 3 . For English, we used sentence-headline pairs extracted from the annotated English Gigaword with the same pre-processing script used in the construction of the test set. The training set contains about 3.8M pairs.</p><p>In this paper, we used a character-level decoder to control the number of characters. On the encoder side, we used subword units to construct the vocabulary <ref type="bibr" target="#b11">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b3">Kudo, 2018)</ref>. We set the hyper-parameter to fit the vocabulary size to about 8k for Japanese and 16k for English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We implemented two methods proposed by previous studies to control the output length and handle arbitrary lengths. We employed them and Transformer as baselines.</p><p>LenInit <ref type="bibr" target="#b2">Kikuchi et al. (2016)</ref> proposed LenInit, which controls the output length by initializing the LSTM cell m of the decoder as follows:</p><formula xml:id="formula_2">m = len × b,<label>(7)</label></formula><p>where b is a trainable vector. We incorporated this method with a widely used LSTM encoderdecoder model <ref type="bibr" target="#b7">(Luong et al., 2015)</ref> 4 . For a fair comparison, we set the same hyper-parameters as in <ref type="bibr" target="#b14">Takase et al. (2018)</ref> because they indicated that the LSTM encoder-decoder model trained with the hyper-parameters achieved a similar performance to the state-of-the-art on the headline generation.</p><p>Length Control (LC) <ref type="bibr" target="#b6">Liu et al. (2018)</ref> proposed a length control method that multiplies the desired length by input token embeddings. We trained the model with their hyper-parameters.</p><p>Transformer Our proposed method is based on Transformer <ref type="bibr">(Vaswani et al., 2017) 5</ref> . We trained Transformer with the equal hyper-parameters as in the base model in <ref type="bibr" target="#b15">Vaswani et al. (2017)</ref>. <ref type="table">Table 1</ref> shows the recall-oriented ROUGE-1 (R-1), 2 (R-2), and L (R-L) scores of each method on the Japanese test set 6 . This table indicates that Transformer with the proposed method (Transformer+LDP E and Transformer+LRP E) outperformed the baselines for all given constraints (len = 10, 13, 26). Transformer+LRP E performed slightly better than Transformer+LDP E. Moreover, we improved the performance by incorporating the standard sinusoidal positional encoding (+P E) on len = 10 and 26. The results imply that the absolute position also helps to generate better headlines while controlling the output length. <ref type="table" target="#tab_1">Table 2</ref> shows the recall-oriented ROUGE scores on the English Gigaword test set. This table indicates that LDP E and LRP E significantly improved the performance on len = 75. Moreover, the absolute position (P E) also improved the performance in this test set. In particular, P E was very effective in the setting of very short headlines (len = 30). However, the proposed method slightly lowered ROUGE-2 scores from the bare Transformer on len = 30, 50. We infer that the bare Transformer can generate headlines whose lengths are close to 30 and 50 because the majority of the training set consists of headlines whose lengths are less than or equal to 50. However, most of the generated headlines breached the length constraints, as explained in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>To investigate whether the proposed method can generate good headlines for unseen lengths, we excluded headlines whose lengths are equal to the desired length (len) from the training data. The len = 10 len = 13 len = 26  32.85 11.78 28.52 Previous studies for controlling output length <ref type="bibr" target="#b2">Kikuchi et al. (2016)</ref> 26.73 8.39 23.88 <ref type="bibr" target="#b0">Fan et al. (2018)</ref> 30.00 10.27 26.43 Other previous studies <ref type="bibr" target="#b10">Rush et al. (2015)</ref> 28.18 8.49 23.81 <ref type="bibr">Nagata (2017) 32.28 10.54 27.80 Zhou et al. (2017)</ref> 29.21 9.56 25.51 <ref type="bibr" target="#b5">Li et al. (2017)</ref> 31.79 10.75 27.48 <ref type="bibr" target="#b4">Li et al. (2018)</ref> 29.33 10.24 25.24  <ref type="table" target="#tab_2">Table 3</ref> shows the recall-oriented ROUGE scores on the DUC-2004 test set. Following the evaluation protocol <ref type="bibr" target="#b9">(Over et al., 2007)</ref>, we truncated characters over 75 bytes. The table indicates that LDP E and LRP E significantly improved the performance compared to the bare Transformer, and achieved better performance than the baselines except for R-2 of LenInit. This table also shows the scores reported in the previous studies. The proposed method outperformed the previous methods that control the output length and achieved the competitive score to the state-of-the-art scores.</p><formula xml:id="formula_3">Model R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L</formula><p>Since the proposed method consists of a character-based decoder, it sometimes generated  words unrelated to a source sentence. Thus, we applied a simple re-ranking to each n-best headlines generated by the proposed method (n = 20 in this experiment) based on the contained words. Our re-ranking strategy selects a headline that contains source-side words the most. <ref type="table" target="#tab_2">Table 3</ref> shows that Transformer+LRP E+P E with this reranking (+Re-ranking) achieved better scores than the state-of-the-art <ref type="bibr" target="#b13">(Suzuki and Nagata, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis of Output Length</head><p>Following <ref type="bibr" target="#b6">Liu et al. (2018)</ref>, we used the variance of the generated summary lengths against the desired lengths as an indicator of the preciseness of the output lengths. We calculated variance (var) for n generated summaries as follows 7 :</p><formula xml:id="formula_4">var = 1 n n i=1 |l i − len| 2 ,<label>(8)</label></formula><p>where len is the desired length and l i is the length of the generated summary. <ref type="table" target="#tab_4">Table 4</ref> shows the values of Equation <ref type="formula" target="#formula_4">(8)</ref> computed for each method and the desired lengths. This table indicates that LDP E could control the length of headlines precisely. In particular, LDP E could generate headlines with the identical length to the desired one in comparison with LenInit and LC. LRP E also generated headlines with a precise length but its variance is larger than those of previous studies in very short lengths, i.e., len = 10 and 13 in Japanese. However, we consider LRP E is enough for real applications because the averaged difference between its output and the desired length is small, e.g., 0.1 for len = 10.  <ref type="table" target="#tab_4">Table 4</ref> shows the variances of the proposed method trained on the modified training data that does not contain headlines whose lengths are equal to the desired length, similar to the lower parts of <ref type="table">Table 1</ref> and 2. The variances for this part are comparable to the ones obtained when we trained the proposed method with whole training dataset. This fact indicates that the proposed method can generate an output that satisfies the constraint of the desired length even if the training data does not contain instances of such a length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The lower part of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed length-dependent positional encodings, LDP E and LRP E, that can control the output sequence length in Transformer. The experimental results demonstrate that the proposed method can generate a headline with the desired length even if the desired length is not present in the training data. Moreover, the proposed method significantly improved the quality of headlines on the Japanese headline generation task while preserving the given length constraint. For English, the proposed method also generated headlines with the desired length precisely and achieved the top ROUGE scores on the DUC-2004 test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>uses a sinusoidal positional encoding to represent the position of an input. Transformer feeds the sum of the positional encoding and token embedding to the input layer of its encoder and decoder. Let pos be the position and d be the embedding size. Then, the ith dimension of the sinusoidal positional encoding P E (pos,i) is as follows: P E (pos,2i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7</head><label></label><figDesc><ref type="bibr" target="#b6">Liu et al. (2018)</ref> multiplies Equation (8) by 0.001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Baselines LenInit 38.08 17.72 36.84 41.83 19.53 39.22 47.07 22.02 38.36 LC 35.88 15.73 34.80 40.28 18.86 38.16 42.62 19.38 35.61 Transformer 34.63 15.48 33.02 43.94 21.35 40.77 46.43 23.03 38.10 Proposed method Transformer+LDP E 42.84 21.07 41.31 46.51 22.83 43.76 50.89 24.18 40.82 +P E 42.85 20.67 41.47 46.72 22.70 43.75 51.32 25.15 41.48 Transformer+LRP E 42.70 21.62 41.35 47.05 23.70 44.13 50.68 24.70 41.23 +P E 43.36 21.63 41.93 46.39 23.09 43.49 51.21 25.03 41.43 Proposed method trained on the dataset without headlines consisting of target lengths Transformer+LDP E 41.91 20.01 40.69 45.88 22.61 43.16 50.90 24.37 40.48 +P E 42.33 20.46 40.88 44.78 22.33 42.27 50.87 24.54 40.89 Transformer+LRP E 41.91 20.10 40.52 46.01 22.87 43.47 50.33 24.37 41.00 +P E 42.59 20.76 41.16 46.52 23.65 43.81 50.73 24.64 41.01 Table 1: Recall-oriented ROUGE scores for each length on Japanese test set. This test set contains three kinds of headlines, i.e., len = 10, 13, 26, tied to a single article. 47.26 26.98 45.77 50.21 26.13 47.15 53.99 27.78 49.24 +P E 48.13 27.18 46.43 50.29 25.97 47.17 53.65 27.65 49.06 Transformer+LRP E 48.79 28.77 47.17 50.09 26.08 46.91 53.91 27.82 49.15 +P E 49.23 29.26 47.68 50.41 26.37 47.39 54.21 27.84 49.38 Proposed method trained on the dataset without headlines consisting of the target lengths Transformer+LDP E 47.35 26.76 45.70 50.46 25.96 47.30 53.69 27.61 49.04 +P E 47.44 27.42 45.99 50.67 26.07 47.57 53.76 27.53 49.03 Transformer+LRP E 48.54 28.89 47.06 50.65 26.19 47.34 53.94 27.88 49.11 +P E 49.08 29.09 47.58 50.78 26.64 47.60 53.77 27.68 48.93</figDesc><table><row><cell></cell><cell></cell><cell>len = 30</cell><cell></cell><cell></cell><cell>len = 50</cell><cell></cell><cell></cell><cell>len = 75</cell><cell></cell></row><row><cell>Model</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LenInit</cell><cell cols="9">44.58 25.90 43.34 48.42 25.47 45.56 50.78 25.74 46.42</cell></row><row><cell>LC</cell><cell cols="9">45.17 26.73 44.09 46.56 24.55 44.10 48.67 24.83 44.98</cell></row><row><cell>Transformer</cell><cell cols="9">47.48 29.77 46.17 50.02 28.04 47.29 47.31 24.83 43.75</cell></row><row><cell>Proposed method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer+LDP E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Recall-oriented ROUGE scores for each length on test data extracted from annotated English Gigaword.</figDesc><table><row><cell>Model</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LenInit</cell><cell cols="3">29.78 11.05 26.49</cell></row><row><cell>LC</cell><cell cols="3">28.68 10.79 25.72</cell></row><row><cell>Transformer</cell><cell>26.15</cell><cell cols="2">9.14 23.19</cell></row><row><cell>Proposed method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer+LDP E</cell><cell cols="3">30.95 10.53 26.79</cell></row><row><cell>+P E</cell><cell cols="3">31.00 10.78 27.02</cell></row><row><cell>+Re-ranking</cell><cell cols="3">31.65 11.25 27.46</cell></row><row><cell>Transformer+LRP E</cell><cell cols="3">30.74 10.83 26.69</cell></row><row><cell>+P E</cell><cell cols="3">31.10 11.05 27.25</cell></row><row><cell>+Re-ranking</cell><cell cols="3">32.29 11.49 28.03</cell></row><row><cell>+Ensemble (5 models)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Recall-oriented ROUGE scores in DUC-2004.    </figDesc><table><row><cell>lower parts of Table 1 and 2 show ROUGE scores</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>len = 13 len = 26 len = 30 len = 50 len = 75</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Variance</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Japanese dataset</cell><cell></cell><cell cols="2">English Gigaword</cell><cell></cell></row><row><cell cols="2">Model len = 10 Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LenInit</cell><cell>0.047</cell><cell>0.144</cell><cell>0.058</cell><cell>0.114</cell><cell>0.112</cell><cell>0.091</cell></row><row><cell>LC</cell><cell>0.021</cell><cell>0.028</cell><cell>0.040</cell><cell>0.445</cell><cell>0.521</cell><cell>0.871</cell></row><row><cell>Transformer</cell><cell>181.261</cell><cell>115.431</cell><cell>38.169</cell><cell>193.119</cell><cell>138.566</cell><cell>620.887</cell></row><row><cell>Proposed method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer+LDP E</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.015</cell><cell>0.012</cell><cell>0.013</cell></row><row><cell>+P E</cell><cell>0.003</cell><cell>0.001</cell><cell>0.001</cell><cell>0.016</cell><cell>0.009</cell><cell>0.007</cell></row><row><cell>Transformer+LRP E</cell><cell>0.121</cell><cell>0.210</cell><cell>0.047</cell><cell>0.082</cell><cell>0.071</cell><cell>0.187</cell></row><row><cell>+P E</cell><cell>0.119</cell><cell>0.144</cell><cell>0.058</cell><cell>0.142</cell><cell>0.110</cell><cell>0.173</cell></row><row><cell cols="6">Proposed method trained on the dataset without headlines consisting of the target lengths</cell><cell></cell></row><row><cell>Transformer+LDP E</cell><cell>0.000</cell><cell>0.002</cell><cell>0.000</cell><cell>0.018</cell><cell>0.009</cell><cell>0.009</cell></row><row><cell>+P E</cell><cell>0.021</cell><cell>0.001</cell><cell>0.003</cell><cell>0.021</cell><cell>0.013</cell><cell>0.010</cell></row><row><cell>Transformer+LRP E</cell><cell>0.191</cell><cell>0.362</cell><cell>0.043</cell><cell>0.120</cell><cell>0.058</cell><cell>0.133</cell></row><row><cell>+P E</cell><cell>0.183</cell><cell>0.406</cell><cell>0.052</cell><cell>0.138</cell><cell>0.081</cell><cell>0.154</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Variances of generated headlines.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We obtained this test set by applying the pre-processing script at https://github.com/asahi-research/Gingo to the original JAMUL corpus.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookarchive/NAMAS 3 We obtained this training set by applying the preprocessing script at https://github.com/asahi-research/Gingo.4  We used an implementation at https://github.com/mlpnlp/mlpnlp-nmt.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/pytorch/fairseq 6 To calculate ROUGE scores on the Japanese dataset, we used https://github.com/asahi-research/Gingo.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research results have been achieved by "Research and Development of Deep Learning Technology for Advanced Multilingual Speech Translation", the Commissioned Research of National Institute of Information and Communications Technology (NICT), Japan.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation (WMT 2018)</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation (WMT 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A large-scale multi-length headline corpus for improving length-constrained headline generation model evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Hitomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Taguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Tamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Kikuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inui</forename><surname>Kentaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensure the correctness of the summary: Incorporate entailment knowledge into abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018)</title>
		<meeting>the 27th International Conference on Computational Linguistics (COLING 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep recurrent generative decoder for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Controlling length in abstractive summarization using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4110" to="4119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Annotated Gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX &apos;12</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBC-WEKEX &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<title level="m">Duc in context. Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS 2014)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cutting-off redundant repeating generations for neural abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Direct output connection for a high-rank language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4599" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
