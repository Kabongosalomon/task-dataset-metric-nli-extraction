<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DHOG: Deep Hierarchical Object Grouping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">Nicholas</forename><surname>Darlow</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton St</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N Ed Ac</forename><surname>Darlow@sms</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton St</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Uk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton St</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton St</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Ac</forename><surname>Uk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton St</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DHOG: Deep Hierarchical Object Grouping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Representation Learning</term>
					<term>Unsupervised Learning</term>
					<term>Mutual Information</term>
					<term>Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, a number of competitive methods have tackled unsupervised representation learning by maximising the mutual information between the representations produced from augmentations. The resulting representations are then invariant to stochastic augmentation strategies, and can be used for downstream tasks such as clustering or classification. Yet data augmentations preserve many properties of an image and so there is potential for a suboptimal choice of representation that relies on matching easy-to-find features in the data. We demonstrate that greedy or local methods of maximising mutual information (such as stochastic gradient optimisation) discover local optima of the mutual information criterion; the resulting representations are also lessideally suited to complex downstream tasks. Earlier work has not specifically identified or addressed this issue. We introduce deep hierarchical object grouping (DHOG) that computes a number of distinct discrete representations of images in a hierarchical order, eventually generating representations that better optimise the mutual information objective. We also find that these representations align better with the downstream task of grouping into underlying object classes. We tested DHOG on unsupervised clustering, which is a natural downstream test as the target representation is a discrete labelling of the data. We achieved new stateof-the-art results on the three main benchmarks without any prefiltering or Sobel-edge detection that proved necessary for many previous methods to work. We obtain accuracy improvements of: 4.3% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is very expensive to label a dataset with respect to a particular task. Consider the alternative where a user, instead of labelling a dataset, specifies a simple set of class-preserving transformations or 'augmentations'. For example, lighting changes will not change a dog into a cat. Is it possible to learn a model that arXiv:2003.08821v1 [cs.CV] 13 Mar 2020 produces a useful representation by leveraging a set of such augmentations? Such a representation would need to be good at capturing salient information about the image, and enable downstream tasks to be done efficiently. If the representation were a discrete labelling which groups the dataset into clusters, an obvious choice of downstream task is unsupervised clustering that ideally should match the clusters that would be obtained by direct labelling, without ever having been learnt on explicitly labelled data.</p><p>Using data augmentations to drive unsupervised representation learning has been explored by a number of authors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref>. These approaches typically involve learning neural networks that map augmentations of the same image to similar representations. This is a reasonable approach to take as the variances across many common image augmentations often align with the invariances we would require a method to have.</p><p>In particular, a number of earlier works target maximising mutual information (MI) between augmentations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. By targeting high MI between representations computed from distinct augmentations of images, useful representations can be learned that capture the invariances induced by the augmentations. We are particularly interested in a form of representation that is a discrete labelling of the data, as this is particularly parsimonious. This labelling can be seen as a clustering <ref type="bibr" target="#b15">[16]</ref> procedure, where MI can be computed and assessment can be done directly using the learned labelling, as opposed to via an auxiliary network trained posthoc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Suboptimal mutual information maximisation</head><p>We argue and show that in many cases the MI objective is not maximised effectively in existing work due to the combination of:</p><p>1. Greedy optimisation algorithms used to train neural networks, such as stochastic gradient descent (SGD) that potentially target local optima; and 2. A limited set of data augmentations that can result in the existence of multiple local optima to the MI maximisation objective.</p><p>SGD is greedy in the sense that early-found high-gradient features can dominate and so networks will tend to learn easier-to-compute locally-optimal representations (for example, one that can be computed using fewer neural network layers) over those that depend on complex features.</p><p>By way of example, in natural images, average colour is an easy-to-compute characteristic, whereas object type is not. If the augmentation strategy preserves average colour, then a reasonable mapping need only compute average colour information, and high MI between images representations will be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Dealing with greedy solutions</head><p>A number of heuristic solutions, such as as Sobel edge-detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> as a preprocessing step, have been suggested to remove/alter the features in images that may cause trivial representations to be learned. However, this is a symptomatic treatment and not a solution. In the work presented herein, we acknowledge that greedy SGD can get stuck in local optima of the MI maximisation objective because of limited data augmentations. Instead of trying to prevent a greedy solution, we let our DHOG model learn this representation, but also require it to learn a second distinct representation. Specifically, we minimise the MI between these two representations so that the latter cannot rely on the same features used by an earlier head. We extend this idea by adding additional representations, each time requiring the latest to be distinct from all previous representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Contributions</head><p>Learning a set of representations by encouraging them to have low MI, while still maximising the original augmentation-driven MI objective for each representation, is the core idea behind Deep Hierarchical Object Grouping (DHOG). We define a mechanism to produce a set of hierarchically-ordered solutions (in the sense of easy-to-hard orderings, not tree structures). DHOG is able to better maximise the original MI objective between augmentations since each representation must correspond to a unique local optima. Our contributions are:</p><p>1. We identify the suboptimal MI maximisation problem: maximising MI between data augmentations using neural networks and stochastic gradient descent (SGD) produces a substantially suboptimal solution. <ref type="bibr" target="#b0">1</ref> We reason that the SGD learning process settles on easy-to-compute solutions early in learning and optimises for these as opposed to leveraging the capacity of deep and flexible networks. We give plausible explanations and demonstrations for why this is a case, and show, with improved performance on a clustering task, that we can explicitly avoid these suboptimal solutions. 2. We mitigate for this problem, introducing DHOG: the first robust neural network image grouping method to learn diverse and hierarchically arranged sets of discrete image labellings (Section 3) by explicitly modelling, accounting for, and avoiding spurious local optima, requiring only simple data augmentations, and needing no Sobel edge detection. 3. We show a marked improvement over the current state-of-the-art for standard benchmarks in image clustering for CIFAR-10 (4.3% improvement), CIFAR-100-20 (a 20-way class grouping of CIFAR-100, 1.5% improvement), and SVHN (7.2% improvement); we set a new accuracy benchmarks on CINIC-10; and show the utility of our method on STL-10 (Section 4).</p><p>To be clear, DHOG still learns to map data augmentations to similar representations as this is imperative to the learning process. The difference is that the DHOG framework enables a number of intentionally distinct data labellings to be learned, arranged hierarchically in terms of source feature complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Assessment task: clustering</head><p>For this work, our focus is on finding higher MI representations; we then assess the downstream capability on the ground truth task of image classification, meaning that we can either (1) learn a representation that must be 'decoded' via an additional learning step, or (2) produce a discrete labelling that requires no additional learning. Clustering methods offer a direct comparison and require no labels for learning a mapping from the learned representation to class labels. Instead, labels are only required to appropriately assign groups to appropriate classes and no learning is done using these. Therefore, our comparisons are with respect to state-of-the-art clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The idea of MI maximisation for representation learning is called the infoMAX principle <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. Contrastive predictive coding <ref type="bibr" target="#b22">[23]</ref> (CPC) models a 2D latent space using an autoregressive model and defines a predictive setup to maximise MI between distinct spatial locations. Deep InfoMAX <ref type="bibr" target="#b14">[15]</ref> (DIM) does not maximise MI across a set of data augmentations, but instead uses mutual information neural estimation <ref type="bibr" target="#b1">[2]</ref> and negative sampling to balance maximising MI between global representations and local representations. Local MI maximisation encourages compression of spurious elements, such as noise, that are inconsistent across blocks of an image. Augmented multiscale Deep InfoMAX <ref type="bibr" target="#b0">[1]</ref> (AMDIM) incorporates MI maximisation across data augmentations and multiscale comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Clustering</head><p>Clustering approaches are more directly applicable for comparison with DHOG because they explicitly learn a discrete labelling. The authors of deep embedding for clustering (DEC) <ref type="bibr" target="#b26">[27]</ref> focused their attention on jointly learning an embedding suited to clustering and a clustering itself. They argued that the notion of distance in the feature space is crucial to a clustering objective. Joint unsupervised learning of deep representations and image clusters (JULE) <ref type="bibr" target="#b27">[28]</ref> provided supervisory signal for representation learning. Some methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> employ autoencoder architectures along with careful regularisation of cluster assignments to (1) ensure sufficient information retention, and (2) avoid cluster degeneracy (i.e., mapping all images to the same class).</p><p>Deep adaptive clustering <ref type="bibr" target="#b3">[4]</ref> (DAC) recasts the clustering problem as binary pairwise classification, pre-selecting comparison samples via feature cosine distances. A constraint on the DAC system allows for a one-hot encoding that avoids cluster degeneracy. Another mechanism for dealing with degeneracy is to use a standard clustering algorithm, such as K-means to iteratively group on learned features. This approach is used by DeepCluster <ref type="bibr" target="#b2">[3]</ref>.</p><p>Associative deep clustering (ADC) <ref type="bibr" target="#b12">[13]</ref> uses the idea that associations in the embedding space are useful for learning. Clustering was facilitated by learning a network to associate data with (pseudo-labelled) centroids. They leveraged augmentations by encouraging samples to output similar cluster probabilities.</p><p>Deep comprehensive correlation mining <ref type="bibr" target="#b25">[26]</ref> (DCCM) constructs a sample correlation graph for pseudo-labels and maximises the MI between augmentations, and the MI between local and global features for each augmentation. While many of the aforementioned methods estimate MI in some manner, invariant information clustering <ref type="bibr" target="#b15">[16]</ref> (IIC) directly defines the MI using the c-way softmax output (i.e., probability of belong to class c), and maximises this over data augmentations to learn clusters. They effectively avoid degenerate solutions because MI maximisation implicitly targets marginal entropy. We use the same formulation for MI -details can be found in Section 3. <ref type="figure" target="#fig_0">Fig. 1</ref>: DHOG architecture. The skeleton is a ResNet18 <ref type="bibr" target="#b13">[14]</ref>. The final ResNet block is repeated k − 3 times (k = 8 here). 1 Augmentations of each image, x a...d , are separately processed by the network. 2 Each shallow ResNet block (1 . . . 3) constitutes shared computation for deeper blocks, while also computing separate probability vectors, z 1 . . . z 3 . Each z i is viewed as the probability for each outcome of the random variable c i that makes a discrete labelling choice. <ref type="bibr" target="#b2">3</ref> The deepest ResNet blocks compute further z &gt;3 . <ref type="bibr" target="#b3">4</ref> The network is trained by maximising the MI between allocations c i from all data augmentations, and 5 separately for each node i, minimising the MI between c i and c &lt;i for the same data augmentation. 6 This is implemented as a global optimisation by stopping gradients such that they are not back-propagated for later computation paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><formula xml:id="formula_0">2 6 MLP MLP MLP | | | | | | 4 5 ( | , | ) … … Minimise ( | , | ) … Maximise: − ∂ ( , ) / ∂ 2 &gt; 2 2 − ∂ ( , ) / ∂ &lt; 2 2 2 − ∂ ( , ) / ∂ 3 &gt; 3 3 − ∂ ( , ) / ∂ &lt; 3 3 3 − ∂ ( , ) / ∂ 1 &gt; 1 1 Batched input image augmentations 1 MLP MLP MLP MLP MLP − ∂ ( , ) / ∂ 4 &gt; 4 4 − ∂ ( , ) / ∂ &lt; 4 4 4 − ∂ ( , ) / ∂ 5 &gt; 5 5 − ∂ ( , ) / ∂ &lt; 5 5 5 − ∂ ( , ) / ∂ 6 &gt; 6 6 − ∂ ( , ) / ∂ &lt; 6 6 6 − ∂ ( , ) / ∂ 7 &gt; 7 7 − ∂ ( , ) / ∂ &lt; 7 7 7 − ∂ ( , ) / ∂ &lt; 8 8 8 3 back-propagation ⋯</formula><p>DHOG is an approach for obtaining jointly trained multi-level representations as discrete labellings, arranged in a simple-to-complex hierarchy. Later representations in the hierarchy have low MI between earlier representations.</p><p>Each discrete labelling is computed by a separate 'head' within a neural network - <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture. A head is an unit that computes a multivariate class probability vector. By requiring independence amongst heads, a diversity of solutions to the MI maximisation objective can be found. The head that best maximises MI between augmentations typically aligns better with a ground truth task that also relies on complex features (e.g., classification). <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the DHOG architecture and training principles. There are shared model weights ( 2 : ResNet blocks 1, 2, and 3) and head-specific weights (the MLP layers and 3 : ResNet blocks 4 to 8). For the sake of brevity, we abuse notation and use MI(z, z ) between labelling probability vectors as an overloaded shorthand for the mutual information MI(c, c ) between the labelling random variables c and c that have probability vectors z and z respectively.</p><p>Any branch of the DHOG architecture ( 1 to any z i ) can be regarded as a single neural network. These are trained to maximise the MI between the label variables at each head for different augmentations; i.e. between label variables with probability vectors z i (x) and z i (x ) for augmentations x and x . Four augmentations are shown at 1 . The MI is maximised pairwise between all pairs, at 4 . This process can be considered pulling the mapped representations together.</p><p>Following IIC <ref type="bibr" target="#b15">[16]</ref>, we compute the MI directly from the label probability vectors within a minibatch. Let z i , z i denote the random probability vectors at head i associated with sampling a data item and its augmentations, and passing those through the network. Then we can compute the mutual information between labels associated with each augmentation using</p><formula xml:id="formula_1">MI aug (c i , c i ) = Tr(E[z i (z i ) T ] T log(E[z i (z i ) T ])) − E[z T i ] log E[z i ] − E[(z i ) T ] log E[(z i )]<label>(1)</label></formula><p>where Tr is the matrix trace, logarithms are computed element-wise, and expectations are over data samples and augmentations of each sample. In practice we compute an empirical estimate of this MI based on samples from a minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Distinct heads</head><p>What makes DHOG different from other methods is that each head is encouraged to compute unique solutions via cross-head MI minimisation. For a minibatch of images, the particular labelling afforded by any head is trained to have low MI with other heads -5 in <ref type="figure" target="#fig_0">Figure 1</ref>. We assume multiple viable groupings/clusterings because of natural patterns in the data. By encouraging low MI between heads, these must capture different patterns in the data. Concepts such as brightness, average colours, low-level patterns, etc., are axes of variation that are reasonable to group by, and which maximise the MI objective to some degree. Complex ideas, such as shape, typically require more processing. Greedy optimisation may not discover these groupings without explicit encouragement. Unfortunately, the groupings that tend to rely on complex features are most directly informative of likely class boundaries. In other words, without a mechanism to explore viable patterns in the data (like our cross-head MI minimisation, Section 3.2), greedy optimisation will avoid finding them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-head MI minimisation</head><p>Our approach to addressing suboptimal MI maximisation is to encourage unique solutions at sequential heads (z 1 . . . z 8 in <ref type="figure" target="#fig_0">Figure 1)</ref>, which rely on different features in the data. We can compute and minimise the MI across heads using:</p><formula xml:id="formula_2">MI head (c i , c j ) = Tr(E[z i z T j ] T log(E[z i z T j ])) − E[z T i ] log(E[z i ]) − E[(z j ) T ] log(E[z j ]).<label>(2)</label></formula><p>Logarithms are element-wise, and expectations are over the data and augmentations. Note z i and z j are each computed from the same data augmentation. We estimate this from each minibatch sample. This process can be thought of as pushing the heads apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aligning assignments</head><p>When computing and subsequently minimising Equation 2, a degenerate solution must be accounted for: two different heads can effectively minimise this form of MI computation while producing identical groupings of the data because of the way MI is computed in practice. This can be done simply by ensuring the order of the labels is permuted for each head, but consistently so across the data. We use the Hungarian Method <ref type="bibr" target="#b17">[18]</ref> to choose the best match between heads, effectively mitigating spurious MI computation. This step can be computationally expensive when c is large, but is imperative to the success of DHOG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical arrangement</head><p>Requiring k heads (where k = 8 here) to produce unique representations is not necessarily the optimal method to account for suboptimal MI maximisation. Instead, what we do is encourage a simple-to-complex hierarchy structure to the heads, defined according to cross-head comparisons made using Equation 2. The hierarchy enables a reference mechanism through which later representations can be encouraged toward relying on complex and abstract features in the data. <ref type="figure" target="#fig_0">Figure 1</ref> shows 8 heads, three of which are computed from representations owing to early residual blocks of the network. The hierarchical arrangement is created by only updating head-specific weights according to comparisons made with earlier heads. In practice this is done by stopping the appropriate gradients during training -6 in the figure. For example, when computing the MI between assignments using z i=6 and those using z i =6 , gradient back-propagation is allowed when i &lt; 6 but not when i &gt; 6. In other words, when learning to produce z i=6 , the network is encouraged to produce a head that is distinct from heads 'lower' on the hierarchy. Those 'higher' on the hierarchy do not affect the optimisation, however. Extending this concept for i = 1 . . . 8 gives rise to the idea of the hierarchical arrangement.</p><p>Initial experiments showed clearly that if this hierarchical complexity routine was ignored, the gains owing to cross-head MI minimisation were reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Objective</head><p>The part of the objective producing high MI representations by 'pulling' together discrete labellings from augmentations is Equation 1 normalised over k heads:</p><formula xml:id="formula_3">MI pull = 1 k k i=0 M I aug (c i , c i ).<label>(3)</label></formula><p>The quantity used to 'push' heads apart is Equation 2 normalised per head:</p><formula xml:id="formula_4">MI push = k i=1 i j=1 j =i M I head (c i , c j ) i ,<label>(4)</label></formula><p>where each cross-head MI term is scaled by the head index, i, since that directly tracks the number of comparisons made for each head. i scales up the hierarchy, such that the total MI head associated with any head is scaled according to the number of comparisons. Scaling ensures that head-specific weight updates are all equally important. The final optimisation objective is:</p><formula xml:id="formula_5">θ * = arg max θ MI pull − αMI push ,<label>(5)</label></formula><p>where θ are the network parameters implicitly included in the MI computations, α is a hyper-parameter we call the cross-head MI-minimization coefficient. We ran experiments in Section 4 with α = 0 as an ablation study. Although the parallel repetition of entire block structures for each head is cumbersome, our earlier experiments showed that this was an important model flexibility. We used four data augmentation repeats with a batch size of 220. DHOG maximises MI between discrete labellings from different data augmentations. This is equivalent to a clustering and is similar to IIC. There are, however, key differences. In our experiments:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Design and training choices</head><p>-We train for 1000 epochs with a cosine annealing learning rate schedule, as opposed to a fixed learning rate for 2000 epochs. -We do not use sobel edge-detection or any other arbitrary preprocessing as a fixed processing step. -We make use of the fast auto-augment CIFAR-10 data augmentation strategy (for all tested datasets) found by <ref type="bibr" target="#b18">[19]</ref>. We then randomly apply (with p = 0.5) grayscale after these augmentations, and take random square crops of sizes 64 and 20 pixels for STL-10 and all other datasets, respectively.</p><p>The choice of data augmentation is important, and we acknowledge that for a fair comparison to IIC the same augmentation strategy must be used. The ablation of any DHOG-specific loss (when α = 0) largely recreates the IIC approach but with augmentations, network and head structure matched to DHOG; this enables a fair comparison between an IIC and DHOG approach.</p><p>Since STL-10 has much more unlabelled data of a similar but broader distribution than the training data, the idea of 'overclustering' was used by <ref type="bibr" target="#b15">[16]</ref>; they used more clusters than the number of classes (70 versus 10 in this case). We repeat each head with an overclustering head that does not play a part in the cross-head MI minimisation. The filter widths are doubled for STL-10. We interspersed the training data evenly and regularly through the minibatches.</p><p>To determine the DHOG cross-head MI-minimisation coefficient, α, we carried out a non-exhaustive hyper parameter search using only CIFAR-10 images (without the labels), assessing performance on a held out validation set sampled from the training data. This did not use the evaluation data in any way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Assessment</head><p>Once learned, the optimal head can be identified either using the highest MI, or using a small set of labelled data. Alternatively all heads can be used as different potential alternatives, with posthoc selection of the best head according to some downstream task. The union of all the head probability vectors could be used as a compressed data representation. In this paper the head that maximises the normalised mutual information on the training data is chosen. This is then fully unsupervised, as with the head selection protocol of IIC. We also give results for the best posthoc head to show the potential for downstream analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first show the functionality of DHOG using a toy problem (Section 4.1) to illustrate how DHOG can find distinct and viable groupings of the same data. In Section 2 we give results to show the superiority of DHOG on real images.  are four possible groups. The number of clusters was set as 2 in order to create a circumstance where different groupings were possible. The augmentations were generated by adding additional Gaussian noise to the samples. Without DHOG (a) the network computes the same solution for each head. With DHOG (b) each head produces a unique solution. Even though the single solution found in (a) might very well be the sought after solution, it is not necessarily the solution that maximises the MI between augmentations of data. This is evidence of the suboptimal MI maximisation problem: the network, learned using SGD, latched onto this local minima througout training. DHOG is able to identify different, better, minima because the local minima are already identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real images</head><p>The datasets used for assessment were CIFAR-10 <ref type="bibr" target="#b16">[17]</ref>, CIFAR-100-20 (CIFAR-100 <ref type="bibr" target="#b16">[17]</ref> where classes are grouped into 20 super-classes), CINIC-10 <ref type="bibr" target="#b6">[7]</ref> (an extension of CIFAR-10 using images from Imagenet <ref type="bibr" target="#b7">[8]</ref> of similar classes), street view house numbers <ref type="bibr" target="#b21">[22]</ref> (SVHN), and STL-10 <ref type="bibr" target="#b4">[5]</ref>. For CINIC-10 only the standard training set of 90000 images (without labels) was used for training. <ref type="table" target="#tab_1">Table 1</ref> gives the accuracy, normalised mutual information (NMI), and the adjusted rand index (ARI) between remapped assignments and classification targets. Before assessment a labelling-to-class remapping was computed using the training data and the Hungarian method <ref type="bibr" target="#b17">[18]</ref>. The results listed for DHOG correspond to average over 3 seeded runs. In terms of all measured metrics DHOG outperformed other relevant fully-unsupervised clustering methods, with an accuracy improvement of 4.3% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN. No Sobel edge-detection was used to account for trivial solutions. The DHOG network converged in half the training time (compared to IIC).</p><p>We used a fully unsupervised posthoc head selection according to NMI(z, z ). The selected heads almost always corresponded with the head that maxmimised NMI(z, y), where y are user-defined class labels. This means that the DHOG framework produces data groupings that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy NMI(z, y) ARI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>K-means on pixels 21.18 ± 0.0170 0.0811 ± 0.0001 0.0412 ± 0.0001 Cartesian K-means <ref type="bibr" target="#b24">[25]</ref> 22.89 0.0871 0.0487 JULE <ref type="bibr" target="#b27">[28]</ref> 27   <ref type="bibr" target="#b15">[16]</ref>. NMI(z, y) is between remapped predicted label assignments and class targets, y. Ablation: DHOG with α = 0.0 is the IIC method but with conditions/augmentations matching those of DHOG(α = 0.5).</p><p>Our method is DHOG with α = 0.05, highlighted in blue. We give results for the head chosen for the best NMI(z, z ) and the head chosen for the best NMI(z, y).</p><p>In most cases max MI chooses the optimal head.</p><p>1. Better maximise the widely used MI objective (between mappings of data augmentations); 2. Also corresponds better with the challenging underlying object classification test objective.</p><p>It is only on STL-10 that DHOG never beat the current state-of-the-art. This may be owing to the need for a STL-10 specific hyper-parameter search. Our aim was to show that the simple hierarchical ordering of heads in DHOG improves performance. The difference between STL-10 with and without the MI cross-head minimisation term (controlled by α) shows a marked improvement. Again, note that DHOG uses no preprocessing such as Sobel edge detection to deal with easy solutions to the MI objective.  The advantage of a hierarchical ordering is particularly evident when considering the ablation study: with (α = 0.05) and without (α = 0) cross-head MI minimisation. <ref type="figure">Figure 3 (a)</ref>     <ref type="figure" target="#fig_5">Figure 5</ref> (a) where α = 0 and note the greater prevalence of cross-class confusion. <ref type="table">Table 2</ref> shows images that yielded the highest probability for each class, and an average of the top 10 images, for both an early (i = 2) and a late (i = 8) head. Since the fast auto-augment strategy is broad, the difference between easy-tocompute and complex features is often nuanced. In this case, the grouped images from the earlier head are more consistent in terms of colour or simple patterns. It easier to ascribe notions of 'mostly blue background' (class 3) or 'light blob with three dark spots' (class 6) for the earlier head. This is also exemplified by the average images: the consistency of images in the early head makes the detail of the averages images clearer, whereas those from the later head are difficult to discern owing to diversity amongst the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented deep hierarchical object grouping (DHOG): a method that leverages the challenges faced by current data augmentation-driven unsupervised representation learning methods that maximise mutual information. Learning  <ref type="table">Table 2</ref>: Images that yielded the top probability for each discrete label for an early, i = 2, and late, i = 8, head, taken from a single DHOG run on Cifar-10. The average image for the top 10 is also shown. Note particularly the those images grouped by the early head are less diverse than those grouped by the later head. The label associated largely with frogs (8 for the earlier head and 3 for the later head) exemplifies this well. a good representation of an image using data augmentations is limited by the user, who chooses the set of plausible data augmentations but who is also unable to cost-effectively define an ideal set of augmentations. We argue and show that learning using greedy optimisation typically causes models to get stuck in local optima, since the data augmentations fail to fully describe the sought after invariances to all task-irrelevant information.</p><p>We address this pitfall via a simple-to-complex ordered sequence of representations. DHOG works by minimising mutual information between these representations such that those later in the hierarchy are encouraged to produce unique and independent discrete labellings of the data (w.r.t. earlier representations). Therefore, later heads avoid becoming stuck in the same local optima of the original mutual information objective (between augmentations, applied separately to each head). Our tests showed that DHOG resulted in an improvement of 4.2% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN, without using preprocessing such as Sobel edge detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>shows the architecture and training design. It is based on a ResNet-18 backbone, where each residual block has two layers (with a skip connection over these). Blocks 1 to 3 have 64, 128, and 256 units, respectively. Each parallel final block (4 to 8, here) have 512 units. Each MLP has a single hidden layer of width 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>demonstrates a simple 2D toy problem to show how DHOG can learn a variety of solutions. The data is generated from 4 separate 2D Gaussians: there</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Toy Problem: 4 sets of 2D Gaussian distributed points. The network must learn 2 groups. The probability of being in either is given by the background. Without DHOG the network simply learns a single solution, while DHOG encourages a number of unique solutions, from which it can select the solution with highest mutual information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Accuracy per head. DHOG causes heads to learn distinct solutions. Visualising learned representations via t-SNE plots<ref type="bibr" target="#b20">[21]</ref>. Hierarchically ordered representations produce different representations, the clusters of which become more distinct up the hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Confusion matrices from the same seed (a) without and (b) with DHOG cross-head MI minimisation. These networks struggle with distinguishing natural objects (birds, cats, deer, etc.), although DHOG does improve this.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4</head><label>4</label><figDesc>and the confusion matrix inFigure 5(b) show the classes the final learned network confuses in CIFAR-10. Compare this to the confusion matrix in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, ablation) 41.66 ± 0.8273 0.3276 ± 0.0084 0.2108 ± 0.0034 DHOG, unsup. (α = 0.05) 37.65 ± 2.7373 0.3317 ± 0.0096 0.1993 ± 0.0030 DHOG, best (α = 0.05) 43.06 ± 2.1105 0.3725 ± 0.0075 0.2396 ± 0.0087</figDesc><table><row><cell></cell><cell></cell><cell>.15</cell><cell>0.1923</cell><cell>0.1377</cell></row><row><cell></cell><cell>DEC [27]  †</cell><cell>30.1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DAC [4]</cell><cell>52.18</cell><cell>0.3956</cell><cell>0.3059</cell></row><row><cell></cell><cell>DeepCluster [3]  †</cell><cell>37.40</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ADC [13]</cell><cell>29.3 ± 1.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DCCM [26]</cell><cell>62.3</cell><cell>0.496</cell><cell>0.408</cell></row><row><cell></cell><cell>IIC [16]  †</cell><cell>57.6 ± 5.01</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">DHOG (α = 0, ablation) 57.49 ± 0.8929 0.5022 ± 0.0054</cell><cell>0.4010 ± 0.0091</cell></row><row><cell></cell><cell cols="4">DHOG, unsup. (α = 0.05) 66.61 ± 1.699 0.5854 ± 0.0080 0.4916 ± 0.0160</cell></row><row><cell></cell><cell>DHOG, best (α = 0.05)</cell><cell>66.61 ± 1.699</cell><cell cols="2">0.5854 ± 0.0080 0.4916 ± 0.0160</cell></row><row><cell></cell><cell>K-means on pixels</cell><cell cols="3">13.78 ± 0.2557 0.0862 ± 0.0012 0.0274 ± 0.0005</cell></row><row><cell>CIFAR-100-20</cell><cell cols="4">DAC [4]  † DeepCluster [3]  †  *  ADC [13] IIC [16]  † DHOG (α = 0, ablation) 20.22 ± 0.2584 0.1880 ± 0.0019 0.0846 ± 0.0026 23.8 --18.9 --16.0 --25.5 ± 0.462 --</cell></row><row><cell></cell><cell cols="4">DHOG, unsup. (α = 0.05) 26.05 ± 0.3519 0.2579 ± 0.0086 0.1177 ± 0.0063</cell></row><row><cell></cell><cell>DHOG, best (α = 0.05)</cell><cell>27.57 ± 1.069</cell><cell cols="2">0.2687 ± 0.0061 0.1224 ± 0.0091</cell></row><row><cell cols="5">K-means on pixels K-means on pixels ADC [13] DHOG (α = 0, ablation) 14.27 ± 2.8784 0.0298 ± 0.0321 0.0209 ± 0.0237 20.80 ± 0.8550 0.0378 ± 0.0001 0.0205 ± 0.0007 11.35 ± 0.2347 0.0054 ± 0.0004 0.0007 ± 0.0004 38.6 ± 4.1 --DHOG (α = 0SVHN CINIC-10 DHOG, unsup. (α = 0.05) 45.81 ± 8.5427 0.4859 ± 0.1229 0.3686 ± 0.1296</cell></row><row><cell></cell><cell>DHOG, best (α = 0.05)</cell><cell cols="3">49.05 ± 8.2717 0.4658 ± 0.0556 0.3848 ± 0.0557</cell></row><row><cell></cell><cell>K-means on pixels</cell><cell cols="3">21.58 ± 0.2151 0.0936 ± 0.0005 0.0487 ± 0.0009</cell></row><row><cell></cell><cell>JULE [28]  †</cell><cell>27.7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DEC [27]</cell><cell>35.90</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DAC [4]</cell><cell>46.99</cell><cell>0.3656</cell><cell>0.2565</cell></row><row><cell>STL-10</cell><cell>DeepCluster [3]  † ADC [13] DCCM [26]</cell><cell>33.40 47.8 ± 2.7 48.2</cell><cell>--0.376</cell><cell>--0.262</cell></row><row><cell></cell><cell>IIC [16]  †</cell><cell>59.80 ± 0.844</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="4">DHOG (α = 0, ablation) 38.70 ± 4.4696 0.3878 ± 0.0331 0.2412 ± 0.0265</cell></row><row><cell></cell><cell cols="4">DHOG, unsup. (α = 0.05) 48.27 ± 1.915 0.4127 ± 0.0171 0.2723 ± 0.0119</cell></row><row><cell></cell><cell>DHOG, best (α = 0.05)</cell><cell>48.27 ± 1.915</cell><cell cols="2">0.4127 ± 0.0171 0.2723 ± 0.0119</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test set results on all datasets, taken from papers where possible. Results with † are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and (b) are accuracy versus head curves, showing that without cross-head MI minimisation later heads converge to similar solutions. .010.080.050.000.010.010.000.380.02 0.010.920.010.000.000.000.000.000.020.04 0.310.000.190.200.110.070.100.000.010.00 0.010.000.120.220.280.230.100.010.010.01 0.010.000.220.090.310.020.150.190.010.00 0.000.000.050.170.150.570.020.030.000.00 0.010.000.120.040.070.000.740.000.000.00 0.010.000.040.110.060.090.000.690.000.00 0.070.010.070.010.000.000.000.000.810.03 0.010.040.010.010.000.000.000.000.020.91 .010.050.020.000.020.000.000.300.01 0.010.930.000.000.000.000.000.000.010.04 0.250.000.310.200.130.070.010.000.010.00 0.010.000.180.440.030.290.010.010.010.01 0.010.000.050.140.740.020.010.010.010.00 0.000.000.160.160.040.620.000.020.000.00 0.020.000.060.160.010.010.760.000.000.00 0.000.000.070.070.040.110.000.700.010.01 0.140.010.010.000.000.000.000.000.810.02 0.010.030.010.000.000.000.000.000.020.93</figDesc><table><row><cell>True label</cell><cell>airplane automobile bird cat deer dog frog horse ship truck</cell><cell>airplane automobile bird cat deer dog frog horse ship truck Predicted label 0.450Cifar-10 (test)</cell><cell>0.0 0.2 0.4 0.6 0.8 True label</cell><cell>airplane automobile bird cat deer dog frog horse ship truck</cell><cell>airplane automobile bird Predicted label cat deer dog frog horse ship truck 0.590Cifar-10 (test)</cell><cell>0.0 0.2 0.4 0.6 0.8</cell></row><row><cell></cell><cell cols="2">(a) Without DHOG training</cell><cell></cell><cell cols="2">(b) With DHOG training</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We show this by finding higher mutual information solutions using DHOG, rather than by any analysis of the solutions themselves.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.</p><p>This research was part funded from a Huaweil DDMPLab Innovation Research Grant DDMPLab5800191.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mine: mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-fifth International Conference on Machine Learning</title>
		<meeting>the thirty-fifth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth International Conference on Artificial Intelligence andSstatistics</title>
		<meeting>the fourteenth International Conference on Artificial Intelligence andSstatistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Randaugment: Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03505</idno>
		<title level="m">Cinic-10 is not imagenet or cifar-10</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10069</idno>
		<title level="m">Deep k-means: Jointly clustering with kmeans and learning representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ghasedi Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5736" to="5745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative deep clustering: training a classification network with no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of fourteenth European Conference on Computer Vision</title>
		<meeting>fourteenth European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimized cartesian k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="192" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep comprehensive correlation mining for image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
