<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Location-aware Upsampling for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
							<email>xiangyu.he@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Mo</surname></persName>
							<email>zitao.mo@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<email>qiang.chen@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anda</forename><surname>Cheng</surname></persName>
							<email>anda.cheng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisong</forename><surname>Wang</surname></persName>
							<email>peisong.wang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<email>jcheng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution" key="instit1">NLPR</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Location-aware Upsampling for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many successful learning targets such as minimizing dice loss and cross-entropy loss have enabled unprecedented breakthroughs in segmentation tasks. Beyond these semantic metrics, this paper aims to introduce location supervision into semantic segmentation. Based on this idea, we present a Location-aware Upsampling (LaU) that adaptively refines the interpolating coordinates with trainable offsets. Then, location-aware losses are established by encouraging pixels to move towards wellclassified locations. An LaU is offset prediction coupled with interpolation, which is trained end-to-end to generate confidence score at each position from coarse to fine. Guided by location-aware losses, the new module can replace its plain counterpart (e.g., bilinear upsampling) in a plug-and-play manner to further boost the leading encoder-decoder approaches. Extensive experiments validate the consistent improvement over the state-of-theart methods on benchmark datasets. Our code is available at https://github.com/HolmesShuan/Location-aware-Upsampling-for-Semantic-Segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in deep learning have empowered a wide range of applications <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12]</ref>, including semantic segmentation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6]</ref>. The pioneering Fully Convolutional Network (FCN) <ref type="bibr" target="#b28">[29]</ref> with deconvolutional operations <ref type="bibr" target="#b36">[37]</ref> dramatically surpasses classical methods relying on hand-crafted features. Following FCNs, a series of CNN-based methods further enhance the state-of-the-art by introducing dilated (atrous) convolutions <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b6">7]</ref>, shortcut connections <ref type="bibr" target="#b0">[1]</ref> and CRFs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27]</ref>. Latest semantic segmentation methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b7">8]</ref> exploit fully trainable encoderdecoder architectures with simple bilinear upsamplings to extract high-resolution predictions.</p><p>Although recent works have exhibited state-of-the-art performance, we suggest that the location information hidden in the label map has been overlooked. To introduce location prediction into segmentation models, we revisit the idea of semantic segmentation: the task of assigning each pixel of a photograph to a semantic class label <ref type="bibr" target="#b23">[24]</ref>, then decompose this target into two relatively simple sub-tasks: pixel-level localization and classification. Instead of following previous methods to simply predict class labels, we further propose to predict locations simultaneously, more precisely, to learn offsets with additional supervision.</p><p>This fresh viewpoint leads to the Location-aware Upsampling (LaU) directly, which upsamples classification results while accounting for the effect of offsets. Besides that, LaU tends to meet the key challenges in semantic segmentation: First, CNN-based segmentation methods extract highlevel semantic features at the cost of losing detailed pixellevel spatial information. Second, the feature distribution of objects is imbalanced, where the interior points enjoy the rich knowledge after high-ratio upsampling yet pixels around edges suffer from indiscriminative features due to bilinear interpolation. A natural solution is to borrow the information from interior points to guide the categorization near the boundary, which exactly corresponds to our two sub-tasks: learning offsets and utilizing the information at the new position to make a prediction.</p><p>Our observation is that feature maps used by pixel-level classification, e.g., the second last layer in decoders, can also be used for predicting locations. On top of these features, we construct LaUs by adding an offset prediction branch to produce offsets (∆x, ∆y) at each position in the high-resolution feature map (details in section 3.3). To facilitate the offset prediction of interior points and improve the convergence, we first define the candidate locations ( x k , y k ) then use predicted offsets to regress fine-grained coordinates, i.e., ( x k + ∆x, y k + ∆y). The proposed offset prediction makes it possible to further introduce location supervision implicitly/explicitly into the training process. In this paper, we present two locationaware losses forcing the offsets to predict fine-grained coordinates with the correct class. Besides, our location-aware losses avoid human labeling and produce "label" online. In the experiments, we empirically show that location-aware   <ref type="bibr" target="#b37">[38]</ref> is in the form of periodical shuffling of multiple intermediate feature maps generated by independent filters, which cuts off the strong spatial relationship among adjacent pixels on the low-resolution feature map (connected by gray dashed lines). (b) DUpsampling <ref type="bibr" target="#b39">[40]</ref> replaces convolutions with a matrix product to produce a small patch, yet can be transformed into PixelShuffle. In both cases, networks are forced to learn the new spatial relationship generated by hand-crafted rearrangement. mechanism outperforms strong baseline methods, and it is feasible to apply the rich knowledge in the design of location regression to the location-aware upsampling. We believe dense predictions can widely benefit from the locationaware module, including but not limited to the semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Feature map upsampling has been widely used in deep models, such as encoder-decoder architectures <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37]</ref>, generative models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30]</ref>, super-resolution tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref> and semantic segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6]</ref>. Deconvolution operations <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, also known as transposed convolutions <ref type="bibr" target="#b43">[44]</ref>, first bridge the gap between low-resolution feature maps and desired full-resolution predictions. Despite its successes, recent works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16]</ref> show that deconvolutions can easily suffer from the checkboard problem when the kernel size is not divisible by the stride.</p><p>Advances like PixelShuffle <ref type="bibr" target="#b37">[38]</ref> and DUpsampling <ref type="bibr" target="#b39">[40]</ref> turn to hand-crafted pixel rearrangement to reconstruct high-resolution features. However, the empirical design results in the inconsistent spatial relationship among adjacent pixels on the high-resolution feature map (As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, pixels connected by blue lines are generated by different filters, yet the input of each filter corresponds to the same receptive field. Both filter and receptive filed are different for pixels connected by red lines.). Currently, PixelDCL <ref type="bibr" target="#b15">[16]</ref> generates intermediate feature maps sequentially so that later stages depend on previously generated ones, which is similar to pixel-recurrent neural networks (PixelRNNs) <ref type="bibr" target="#b42">[43]</ref> and PixelCNNs <ref type="bibr" target="#b41">[42]</ref>. Yet pixelwise prediction is time-consuming and there is no explicit <ref type="bibr">(a)</ref> . time sequence relation between adjacent pixels, i.e., previously generated pixels can still rely on later produced ones which leads to a chicken-and-egg problem.</p><formula xml:id="formula_0">(c) (b) (d) . . . . . . . . . . . . . . . ..</formula><p>Conditional Random Field (CRF) is another group of methods, that is primarily used in <ref type="bibr" target="#b4">[5]</ref> as a disjoint postprocessing. Following works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b27">28]</ref> further integrate CRF into networks to model the semantic relationships between objects. Due to the increase of computing complexity caused by integration, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> propose a Gaussian conditional random field to avoid the iterative optimization <ref type="bibr" target="#b52">[53]</ref> while suffering from heavy gradient computation. Similar to CRFs, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> focus on refining the output label map by random walks. However, the dense matrix inversion term and the two branch model with cascaded random walks cannot be easily merged into leading encoder-decoder architectures.</p><p>Spatial Transformer Networks (STN) <ref type="bibr" target="#b20">[21]</ref> is the first work to introduce spatial transformation capacity into neural networks, which enables global parametric transformation like 2D affine transformation. Since the pointwise location sampling in STN is weakly coupled with the input feature map at each position (only through global transformation parameters produced by the preceding feature maps), this greatly limits its capacity in pixel-wise predictions such as semantic segmentation. The module we present in this paper can be seen as a new transformation in a local and dense manner. The upsampled feature map can still enjoy the detailed information of low-resolution inputs by using pixel-wise offsets.</p><p>Deformable convolution networks (DCNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b54">55]</ref> achieve strong performance on object detection and semantic segmentation with trainable offsets. However, it is commonly used in feature extraction and we empirically prove that replacing the last bilinear upsampling with "DCNN+Bilinear" will cause damage to the performance (details in <ref type="table">Table 9</ref> and appendix section 2.2). As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, DCNN aims at augmenting the spatial sampling grid of filters without extra supervision, yet our method directly refines the sampling coordinates with location-guided losses which contribute to higher performance. </p><formula xml:id="formula_1">(b) (a) (d) (c) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Data-independent upsampling such as bilinear interpolation is one of the basic techniques in computer vision and image processing. Despite its high efficiency, this module can be oversimple (illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>) and fail to capture the semantic information. In this section, we alleviate the existing problems in bilinear upsampling by introducing the location-aware mechanism.</p><p>Learning data-dependent offsets is the key step in LaU. This idea is based on the fact that adjacent boundary points on the high-resolution label map can degrade into the same pixel in the low-resolution feature map, since 8× upsampling is commonly used in popular methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b49">50]</ref>. Instead of sampling from the local area, we propose to use the predicted offsets, generating the output map from the location-refined coordinates. We describe the formulation of a location-aware upsampling in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial Upsampling</head><p>To estimate the values in the output feature map V ∈ R C×H×W , a sampler applys the sampling kernel to a particular grid Ω in the input feature map U ∈ R C×h×w . Each coordinate (x u j , y u j ) defines the spatial location in U and ψ(·) determines the kernel function at a particular pixel V cout i in channel c out . Formally,</p><formula xml:id="formula_2">V cout i = hw j ψ(x u j ; x v i ; Φ x ) ψ(y u j ; y v i ; Φ y ) U cin j (1) where ∀i ∈ [1, ..., HW ], ∀c out ∈ [1, ..., C], (x v i , y v i )</formula><p>are the integer coordinates in the output feature map, Φ x and Φ y are the parameters of a generic sampling function ψ(·) which can be preset or data-dependent 1 . Note that the existing upsamplings such as bilinear and PixelShuffle apply the same ψ(·) to every channel of the input (i.e., data-independent), while LaU enables free changes of Φ at each position. That is, the sampling kernel in LaU changes with the input data and coordinates.</p><p>In theory, ψ(·) is a (sub-)differentiable function of the coordinates (x v i , y v i ) and (x u j , y u j ), along with the parameters Φ, which can be in any form. For example,</p><formula xml:id="formula_3">ψ(x u j ; x v i ; Φ x ) = max(0, 1 − |x u j − x v i |)</formula><p>corresponds to the bilinear interpolation. Here, we show that PixelShuffle is another data-independent case, which may lead to suboptimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">PixelShuffle</head><p>PixelShuffle can also be reformulated in the form of Eq.(1).</p><formula xml:id="formula_4">Specifically, ψ(·) in PixelShuffle 2× upsampling is ψ(x u j ; x v i ; Φ x ) 2× = δ(x v i − 2x u j ) ∀x v i ∈ [0, 2, ..., W -2], c in ∈ [1, 3, ..., C-1] δ(x v i − 2x u j − 1) ∀x v i ∈ [1, 3, ..., W -1], c in ∈ [2, 4, ..., C]</formula><p>where δ(·) is the Kronecker delta function. Note that Φ x changes along with channel c in , yet still fixed with respect to input data. From the view of Φ, this sampling kernel equates to the data-independent methods like bilinear interpolation. Furthermore, the output of Kronecker delta is either +1 or 0, which partly pinpoints the root of missing spatial relationship. That is, ψ(·) generated by PixelShuffle can be considered as a bijection. There is no direct relationship between adjacent elements in its codomain (output feature map), unless the source elements in its domain (intput feature map) are strongly correlated. For example, neighboring pixels connected by red lines in <ref type="figure" target="#fig_0">Figure 1</ref> belong to different receptive fields, and are generated by different filters; They are likely to look different (i.e., uncorrelated) unless the filter weights are similar/same. Therefore, the network has to consider the high-resolution spatial relationships when producing low-resolution intermediate feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Location-aware Upsampling</head><p>Mathematically, bilinear upsampling is a non-injective surjective function (not a bijection), denoted as f : U → V . Neighboring points in V sampled from the same grid in U are strongly correlated, since they are generated from the same elements. Compared with PixelShuffle, this scheme avoids checkerboard artifacts. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, location-aware upsampling is a simple yet effective improvement to the bilinear interpolation with the following sampling kernel where k is the upsampling ratio. Inspired by the anchorbased methods in object detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref>, we introduce</p><formula xml:id="formula_5">ψ(x u j ; x v i ; Φ x ) = max(0, 1 − |x u j − ( x v i k + ∆x v i )|) (2) ( , ) ( , ) ( , ) ( , )<label>( , )</label></formula><p>x v i k as a candidate location then regress the detailed location with predicted offsets. To encourage the network to jump out of the local grid, we add no constraint on (∆x v i , ∆y v i ). To learn the offsets in the upsampling module, we utlize the gradients with respect to U and (∆x v i , ∆y v i ). This allows the backpropagation of the loss function through the (sub-)differentiable location-aware upsampling module. For upsampling kernel (2), the partial derivatives are</p><formula xml:id="formula_6">∂V cout i ∂U cin j = hw j max(0, 1 − |x u j − ( x v i k + ∆x v i )|)· max(0, 1 − |y u j − ( y v i k + ∆y v i )|) ∂V cout i ∂∆x v i = hw j U cin j max(0, 1 − |y u j − ( y v i k + ∆y v i )|)·      0 if |x u j − ( x v i k + ∆x v i )| ≥ 1 −1 if ∆x v i &lt; x u j − x v i k 1 if ∆x v i ≥ x u j − x v i k .</formula><p>With the gradients to offsets, we can further define the offset prediction branch to allow the end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Location-aware Loss</head><p>To fully exploit the effect of predicted offsets (∆x v i , ∆y v i ), we introduce (semi-)supervised locationaware losses into the training process. Since offsets could point to any input location, as long as it contains the correct category, we first consider the offset-guided loss with-out hard constraints. Inspired by the bounding box regression, we further propose the offset regression loss to guide the offset prediction directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Offset-guided Loss</head><p>In common training process of semantic segmentation, the network is optimized by per-pixel cross-entropy loss (·). Instead of learning from isolated per-pixel supervision, we consider the loss pair generated by bilinear upsmapling and LaU (shown in <ref type="figure" target="#fig_3">Figure 4a</ref>). Formally, we introduce the auxiliary loss L (with no gradient) and the target loss L,</p><formula xml:id="formula_7">L = (x, w, ψ LaU (Φ x , Φ y )), L = (x, w, ψ bilinear )</formula><p>where x is the input image, w is the model parameters and Φ refers to the predicted offsets. The per-pixel offset-guided loss</p><formula xml:id="formula_8">L off i = L i Λ i = L i · 1 if L i &lt; L i 1 + λ otherwise<label>(3)</label></formula><p>with loss weight Λ i forces the network to "take a step", since the trivial solution ∆x, ∆y = 0 (i.e., L i = L i ) will be punished by the extra cost λ. Besides, L i &lt; L i demostrates that the moved pixel should have a smaller cross-entropy loss (i.e., a higher confidence score) than the original point. Therefore, (3) encourages the pixel to move towards a better position, which contains potentially correct class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Regression Loss</head><p>Since there is no groud-truth label of the offsets, we could intuitively search for the closest well classified point among the neighbouring pixels to generate the offsets. However, the brute search is time-consuming for online label generation. Hence, we emprically constrain the search space to the intergral coordinate points (shown in <ref type="figure" target="#fig_3">Figure 4b</ref>).</p><p>Following the preivous subsection, we further introduce the auxiliary losses L lt , L lb , L rt and L rb , e.g., L rt = (x, w, ψ right top ) with the sampling kernels</p><formula xml:id="formula_9">ψ x right top = 1{x u j = x v i k }, ψ y right top = 1{y u j = y v i k }</formula><p>where 1{·} is the "indicator function", that is 1{true condition} = 1 and 1{false condition} = 0. Then, we define the loss set Ω = {L, L lt , L lb , L rt , L rb } and the coordinate set</p><formula xml:id="formula_10">Θ = ( x k +∆x, y k + ∆y), ( x k , y k ), ( x k , y k ), ( x k , y k ), ( x k , y k )</formula><p>.</p><p>Combining Ω and Θ, we obtain the optimal candidate corrdinates</p><formula xml:id="formula_11">Θ opt = k 1{min(Ω) = Ω k } · Θ k</formula><p>where the summation stops at 1{min(Ω) = Ω k } = 1.</p><p>Given Θ opt (illustrated in <ref type="figure" target="#fig_3">Figure 4c</ref>), we formulate the perpixel offset regression loss as</p><formula xml:id="formula_12">L reg i = γSmoothL 1 Θ opt i , ( x v i k + ∆x v i , y v i k + ∆y v i ) + L i Λ i = γSmoothL 1 + L i · 1 if L i ≤ ∀Ω i 1 + λ otherwise .<label>(4)</label></formula><p>where γ controls the strength of the regression loss. In our experiments, we set γ to 0.1 without fine-tuning. Note that the auxiliary losses will be excluded from the backward propagation (with no gradient) and omitted at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Location-aware Network</head><p>The combination of predicted offsets and interpolation operation forms a location-aware upsampling <ref type="figure" target="#fig_4">(Figure 5</ref>). Note that the offset prediction branch can take any form in practice, such as a fully-connected network like RPN in Faster-RCNN <ref type="bibr" target="#b35">[36]</ref>. In this paper, we use "Conv 1×1 +LReLU+Conv 3×3 +PixelShuffle" to illustrate the basic framework of location-aware mechanism in semantic segmentation.</p><p>The first 1 × 1 convolution layer reduces the channel number of the input feature map U ∈ R C×h×w from C Encoder-decoder architecture with LaU tends to first predict the general location and rough boundary, then recontruct the detailed information using predicted offsets. As shown in <ref type="figure" target="#fig_6">Figure 7c</ref> and 7d, this two-step scheme decomposes the original problem into two sub-tasks: pixel-level localization and classification, which correspond to the two branches in location-aware network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed method on the PASCAL VOC 2012 semantic segmentation benchmark <ref type="bibr" target="#b13">[14]</ref>, PASCAL Context <ref type="bibr" target="#b30">[31]</ref> and ADE20K <ref type="bibr" target="#b53">[54]</ref> datasets. The standard evaluation metrics are pixel accuracy (pixAcc) and mean Intersection of Union (mIoU). In this section, we first introduce datasets used in this paper as well as the implementation details. We then apply LaU to the top of networks and the upsampler in decoders to show the effectiveness of the location-aware mechanism. Moreover, the visualization results demonstrate the superiority of optimizing the proposed location-aware segmentation (shown in <ref type="figure">Figure 6</ref>).  <ref type="figure">Figure 6</ref>: Visualization results of different upsampling modules using EncNet-ResNet50 <ref type="bibr" target="#b49">[50]</ref>. "EncNet" refers to the original bilinear upsampling. "LaU" is 4× locationaware upsampling with offset-guided loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset PASCAL Context dataset <ref type="bibr" target="#b30">[31]</ref> provides pixelwise semantic annotations for whole scenes, which contains 4998 images for training and 5105 images for testing. Following <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>, we use the most frequent 59 object categories plus background (60 classes in total) as the groundtruth labels. PASCAL VOC 2012 consists of 1464 (train), 1449 (val) and 1456 (test) pixel-level annotated images, which involves 20 foreground object classes and one background class. Following previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, we augment the dataset by the extra annotations, resulting in 10582 (trainaug) training images. The mIoU is averaged across 21 classes. ADE20K <ref type="bibr" target="#b53">[54]</ref> scene parsing benchmark consists of 150 categories. There are 20K images for training, 2K for validation and 3K for testing. The results on test set are provided by the ADE20K online evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For the experiments on PAS-CAL Context, we adopt the same settings as described in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46]</ref>. The learning rate starts at 0.001, which gradually decrease to 0 using the "poly" strategy (power=0.9). The networks are trained for 80 epochs with SGD. Concretely, the momentum is set to 0.9 and weight decay is 0.0001. To encourage the LaU to "look wider", we set the weight decay of the convolutional layer in LaU to 0. For data augmentation, we randomly scale (from 0.5 to 2.0) and horizontally flip the input images, then apply a 480 × 480 random crop to the images. All the experiments are conducted on 4 GPUs with batch size 16. Since ImageNet pre-trained backbones can significantly contribute to higher performances, we follow the practice of <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40]</ref> to use the stan-   <ref type="bibr" target="#b18">[19]</ref>.</p><p>For training on PASCAL VOC, we follow the setting in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b7">8]</ref>. The initial learning rate is set to 0.007 with "poly" policy and weight decay is 0.0001. The total iteration is 30k for ResNet-50 experiments with batch size 48, yet another 30k iterations are required for training ResNet-101 with initial learning rate being 0.001 <ref type="bibr" target="#b7">[8]</ref>. The scaling with flipping data augmentation and LaU settings remain the same as PASCAL Context.</p><p>For the experiments on ADE20K, as described in EncNet <ref type="bibr" target="#b49">[50]</ref>, we train our model on the training set for 180 epochs with learning rate 0.004 and evaluate it on the validation set using multi-scale testing. We then fine-tune our models on the train-val set for another 30 epochs with learning rate 4e −4 and submit results to the test server. We use a 520 × 520 base size with 480 × 480 crop for training. The backbone networks are the same as previous experiments.</p><p>Unless specified, we employ the same loss function as described in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref> plus a location-aware loss and report the single-scale results, without any bells and whistles.</p><p>Ablation Study for LaU To evaluate LaU, the key parameter is the upsampling factor. As listed in <ref type="table" target="#tab_1">Table 1</ref>, 4× upsampling works better than other settings with negligible additional time and space complexity. With proposed LaU, the results consistently outperform standard bilinear upsampling and the best setting exceeds baseline by +1.2 mIoU with only 0.16M extra parameters. Since loss weight of the original auxiliary loss in EncNet is 0.2, in this section, we intuitively set λ to 0.3 to enlarge the gradient of location-aware loss. (a) Applying to encoder-decoder methods with ResNet-50. "OS" refers to the output stride. We first apply an upsampling with LaU/PixelShuffle, then cover the remaining upsampling factor with bilinear upsampling. "2 × +2×" means the cascade of two 2×, i.e., 4×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head</head><p>Upsampling Ablation Study for Location-aware Loss We experiment with loss weight λ between 0 and 0.4 and show the results in <ref type="table" target="#tab_2">Table 2</ref>. Note that LaU without additional supervision (λ = 0) still produces performance gain and λ = 0.3 yields the best performance. Both location-aware upsampling and location-aware loss contribute to higher performance than bilinear baseline (similar results are reported in <ref type="table">Table 9</ref>). Due to the limited computing resources, we directly apply the best settings to regression loss experiments and emprically set γ to 0.1 without grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Applying to Label Prediction</head><p>In this section, we apply LaU to the top of the decoders (as shown in <ref type="figure" target="#fig_4">Figure 5</ref>), which corresponds to the prediction of label maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative Analysis</head><p>PASCAL Context To estimate the generalization ability of LaU, we conduct experiments on two popular methods Encoding <ref type="bibr" target="#b49">[50]</ref> and ASPP <ref type="bibr" target="#b6">[7]</ref> with different output strides. <ref type="table" target="#tab_3">Table 3a</ref> shows that our methods consistently outperform the original bilinear methods. Following the setting in super-resolution tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38]</ref>, one may decompose a single 8× upsampling into three cascaded 2× upsamplings. To simplify the experimental settings, we leave this scheme as future work.</p><p>Compared with PixelShuffle <ref type="bibr" target="#b37">[38]</ref>  <ref type="table" target="#tab_3">(Table 3a)</ref> and Deformable <ref type="bibr" target="#b8">[9]</ref> bilinear upsampling <ref type="table">(Table 9</ref>), LaU notably outperforms those methods with/without location-aware loss. Note that PixelShuffle and Deformable cause damages to the performance; We show the visualization results and give a brief discussion in the appendix section 2.  <ref type="table" target="#tab_3">Table 3b</ref> further shows that single-scale LaUs surpass recent upsampling method DUpsampling <ref type="bibr" target="#b39">[40]</ref> with strong Xception-71 backbone, e.g., ImageNet Top-1 accuracy of Xception-65 is +2.21% higher than ResNet-101. Our multi-scale result achieves +2.2 mIoU over EncNet, while LaU is still compatible with attention mechanisms and stronger encoder-decoder architectures to further enhance the leading performances.</p><p>ADE20K As shown in <ref type="table">Table 5</ref>, our methods achieve similar results on ADE20K dataset. With ResNet-50 backbone, LaUs outperform EncNet baseline by +1.8 and +1.3 mIoU. Regression loss seems to perform worse than locationguided loss. We attribute this to the hand-crafted design of optimal candidate coordinates and the selection of γ. The <ref type="table">Table 5</ref>: ADE20K val set results with top-performing models (using mutli-scale testing) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone pixAcc% mIoU% EncNet <ref type="bibr" target="#b49">[50]</ref> ResNet   <ref type="table" target="#tab_7">Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative Analysis</head><p>We show the PASCAL-Context visual results in <ref type="figure">Figure 6</ref>. LaU labels out the missing parts in EncNet and corrects the misclassifications. More examples in ADE20K and the visual comparison between offset-guided loss and regression loss are shown in the appendix <ref type="figure" target="#fig_0">(Figure 11</ref>). To further illustrate the effect of location-aware mechanism, we compare the segmentation results before and after LaU in <ref type="figure" target="#fig_6">Figure 7</ref>. LaU first produces coarse segmentation of objects with rough boundary (in <ref type="figure" target="#fig_6">Figure 7c)</ref>, then refines the boundary using offsets (in <ref type="figure" target="#fig_6">Figure 7d</ref>). The whole segmentation process of LaU is from coarse to fine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Applying to Decoder</head><p>Recent state-of-the-arts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> utilize upsampling modules in decoders to combine the high-resolution features with low-resolution ones. To show the general applicability  of proposed method, we replace the bilinear upsampling in DeepLabv3+ <ref type="bibr" target="#b7">[8]</ref> with LaU. The performance of ResNet-50+LaU (+2.03% mIoU in <ref type="table" target="#tab_8">Table 7</ref>) has approached the original ResNet-101 backbone results. Even with strong ResNet-101 baseline, LaU can still boost the performance by one point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we provide a new perspective for optimizing the semantic segmentation problem. We decompose the original pixel-level classification problem into offsets prediction and classification, which introduces the idea of location prediction into semantic segmentation. Based on this</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Our experiments base on the PyTorch framework <ref type="bibr" target="#b33">[34]</ref> and pre-trained models provided by EncNet <ref type="bibr" target="#b49">[50]</ref>. We use the official open source code <ref type="bibr" target="#b54">[55]</ref> to conduct the deformable convolution experiments. In this section, we employs ResNet-50 as the backbone without multi-scale testing. The results are reported on PASCAL Context val set. To make a fair comparison, we follow the same setting in our main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. PixelShuffle</head><p>We assume that the class number is 59. Following the popular setting in super-resolutions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref>, the archi- where 236C 3 refers to the 3 × 3 convolution layer with 236 filters. The intuitive idea is that, in the worst case, pixelshuffle should degrade into nearest neighbor upsampling (same feature map and weight for each shuffle group). However, the experiment results in <ref type="table" target="#tab_3">Table 3a</ref> demostrates that PixelShuffle damages the performance. In this subsection, we show the segmentation results to reveal checkboard problems in Pix-elShuffle. As illustrated in <ref type="figure">Figure 8</ref>, both EncNet <ref type="bibr" target="#b49">[50]</ref> head and ASPP <ref type="bibr" target="#b6">[7]</ref> head suffer from the checkboard artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Deformable Convolution</head><p>We replace the traditional bilinear upsamling in encoderdecoder networks with "Deformable 3 × 3 Convolution [9] + Blinear 8× " module. Since "Blinear + Deformable Convolution" will hughly increase the extra computing cost, we only conduct experiments under the first setting.</p><p>Although deformable convolution has been proved to improve the segmentation task when properly used in feature extraction, we show that the "deformable upsampling" is probably not appropriate for the final prediction. As illustrated in <ref type="figure" target="#fig_8">Figure 9</ref>, deformable bilinear upsampling tends to focus on the main body of the objects yet miss the details, which consists with the design of deformable convolution networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization Results</head><p>In this section, we show the visual results from ADE20K dataset. For the qualitative analysis, we compare the outputs generated by LaU and bilinear upsampling, shown in <ref type="figure" target="#fig_0">Figure 10</ref>. Since offset-guided loss and regression loss achieve similar mIoU and pixel accuracy, we visualize the segmentation results on val set in <ref type="figure" target="#fig_0">Figure 11</ref> and make a qualitative evaluation. Regression loss performs slightly better than offset-guided loss, which is consistent with the mIoU results.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) PixelShuffle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The illustrations of the original convolution (a), deformable convolution [9] (b), bilinear upsampling (c) and LaU (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Different cases in 2D bilinear interpolation. (a) An ideal case that fully exploits four points to obtain the desired estimate. (b) If one of the variables (i.e., x or y) happens to be an integer, bilinear interpolation degrades into a linear interpolation. (c) The special case : both x and y are integers. Bilinear interpolation is an injective non-surjective function. (d)/(e) A natural solution to the degradation in (b,c) is to slightly move the interpolation point, which leads to another problem: how to determine the offsets?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustrations of the location-aware loss. ↓, , refer to the interpolation and upsampling. • is the coordinate point for interpolating. · , · rounds input upward and downward, respectively. (a) Offset-guided loss will punish the moved pixel with loss weight λ if it yields even larger loss than the original point. (b) The illustrations of different sampling kernels ψ left top , ψ left bottom , ψ right top and ψ right bottom . (c) The optimal candidate coordinates correspond to pixels with the smallest loss among LaU, left top, left bottom, right top and right bottom upsamplings. Ground-truth is the same as subfigure (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The framework overview of LaU. Our approach employs the same encoder-decoder architecture as the original method, which adds a 1 × 1 convolution layer over the decoder output to generate per-pixel classification scores (i.e., U ∈ R C×h×w ). The offset prediction branch (green blocks) shares the input feature map with the 1 × 1 convolution then produces (∆x v i , ∆y v i ). A location-aware upsampling merges two branches and generates high-resolution outputs.to C . Then, the last convolution layer produces an offset tensor O ∈ R 2M ×H×W where H = kh and W = kw. M can be the number of class to allow the per-element shift. However, the extra computing cost will be unacceptable if M is too large. When we set M to 1, the output feature map shares offset across different channels. Compared with M = 150, this setting saves over 19G FLOPs on ADE20K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Illustrations of the effect of predicted offsets generated by LaU off4× . We visualize the segmentation results of EncNet+LaU before and after LaU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 : 3 3 × : 531C 3 + 3 4 × : 236C 3 +</head><label>83333</label><figDesc>PixelShuffle 2× segmentation results. Figure (b) and (c) show the checkerboard artifacts. Although the last convolution layer (i.e., 59C 3 ) may smooth the outputs, the results still consist of strange checkerboard pattern of artifacts.tecture of PixelShuffle-based upsamplings are as follow :2 × : 236C 3 + PixelShuffle 2× + 59C PixelShuffle 3× + 59C PixelShuffle 2× + 236C 3 + PixelShuffle 2× + 59C 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Visualization results using deformable bilinear upsampling with EncNet-ResNet50<ref type="bibr" target="#b49">[50]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Visual improvements on ADE20K with ResNet101 backbone. "EncNet" is the standard bilinear upsampling. "LaU" refers to LaU off4× .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Visual comparison on ADE20K val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance on Pascal Context val set with EncNet-ResNet50 (60 classes) using different upsampling ratios. "LaU off " refers to LaU with offset-guided loss. We cover the reamining upsampling factor of LaU with bilinear upsampling.</figDesc><table><row><cell>Upsampling</cell><cell cols="3">Ratio mIoU% Params</cell><cell>FLOPs</cell></row><row><cell>Bilinear ([50])</cell><cell>8×</cell><cell>49.08</cell><cell cols="2">41.82M 150.69G</cell></row><row><cell></cell><cell>2×</cell><cell>49.61</cell><cell cols="2">+0.14M +0.51G</cell></row><row><cell>LaU off</cell><cell>3× 4×</cell><cell>49.67 50.27</cell><cell cols="2">+0.15M +0.55G +0.16M +0.60G</cell></row><row><cell></cell><cell>5×</cell><cell>49.65</cell><cell cols="2">+0.18M +0.68G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on loss weight λ using EncNet-ResNet50+LaU off4× (Pascal Context val 60 classes).</figDesc><table><row><cell>λ</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell></row><row><cell cols="6">mIoU% 49.77 49.84 49.94 50.27 49.98</cell></row><row><cell cols="6">pixAcc% 78.69 78.77 78.75 79.01 78.89</cell></row><row><cell cols="3">dard ResNet-50 and ResNet-101</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation results on Pascal Context val set. mIoU on 60 classes w/ background.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Results with state-of-the-art methods. "off4×" and "reg4×" are offset-guided loss and offset regression loss, respectively."MS" means multi-scale testing.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Method</cell><cell>Backbone</cell><cell>mIoU% MS</cell></row><row><cell></cell><cell></cell><cell cols="2">OS mIoU%</cell><cell>CCL [11]</cell><cell>ResNet-101</cell><cell>50.7</cell></row><row><cell></cell><cell>PixelShuffle 2×</cell><cell>8</cell><cell>43.4</cell><cell cols="2">DUpsampling [40] Xception-65</cell><cell>51.4</cell></row><row><cell></cell><cell>PixelShuffle 3×</cell><cell>8</cell><cell>38.3</cell><cell cols="2">DUpsampling [40] Xception-71</cell><cell>52.5</cell></row><row><cell>EncNet [50]</cell><cell>PixelShuffle 2×+2×</cell><cell>8</cell><cell>27.9</cell><cell>DANet [15]</cell><cell>ResNet-101</cell><cell>52.6</cell></row><row><cell></cell><cell>Bilinear</cell><cell>8</cell><cell>49.1</cell><cell>HRNet [39]</cell><cell>HRNetV2-W48</cell><cell>53.1</cell></row><row><cell></cell><cell>LaU off4×</cell><cell>8</cell><cell>50.3</cell><cell>JPU [46]</cell><cell>ResNet-101</cell><cell>53.1</cell></row><row><cell></cell><cell>PixelShuffle 2×</cell><cell>32</cell><cell>37.8</cell><cell>BFP [10]</cell><cell>ResNet-101</cell><cell>52.7</cell></row><row><cell>EncNet [50]</cell><cell>Bilinear</cell><cell>32</cell><cell>45.7</cell><cell>EncNet [50]</cell><cell>ResNet-101</cell><cell>51.7</cell></row><row><cell></cell><cell>LaU off4×</cell><cell>32</cell><cell>46.7</cell><cell cols="2">EncNet + LaU off4× ResNet-101</cell><cell>53.9</cell></row><row><cell></cell><cell>PixelShuffle 2×</cell><cell>8</cell><cell>43.3</cell><cell cols="2">EncNet + LaU off4× ResNet-101</cell><cell>52.7</cell></row><row><cell>ASPP [7]</cell><cell>Bilinear</cell><cell>8</cell><cell>48.4</cell><cell cols="2">EncNet + LaU reg 4× ResNet-101</cell><cell>53.9</cell></row><row><cell></cell><cell>LaU off4×</cell><cell>8</cell><cell>49.8</cell><cell cols="2">EncNet + LaU reg 4× ResNet-101</cell><cell>52.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with Deformable Convolution Netwrok<ref type="bibr" target="#b8">[9]</ref> on Pascal Context val set. We add an "LaU/Deformable+Bilinear" module to the top of Enc-Net/ASPP. "LaU 4× " refers to LaU without location-aware loss. "OS" is the output stride.</figDesc><table><row><cell>Method</cell><cell>Upsampling</cell><cell cols="2">OS mIoU%</cell></row><row><cell></cell><cell cols="2">Deformable+Bilinear 32</cell><cell>44.7</cell></row><row><cell></cell><cell>LaU 4×</cell><cell>32</cell><cell>46.2</cell></row><row><cell>EncNet [50]</cell><cell>LaU off4× Deformable+Bilinear</cell><cell>32 8</cell><cell>46.7 48.0</cell></row><row><cell></cell><cell>LaU 4×</cell><cell>8</cell><cell>49.8</cell></row><row><cell></cell><cell>LaU off4×</cell><cell>8</cell><cell>50.3</cell></row><row><cell></cell><cell>Deformable+Bilinear</cell><cell>8</cell><cell>47.9</cell></row><row><cell>ASPP [7]</cell><cell>LaU 4×</cell><cell>8</cell><cell>49.4</cell></row><row><cell></cell><cell>LaU off4×</cell><cell>8</cell><cell>49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results on ADE20K test set. Best entries in COCO-Place challenge 2017 are listed. EncNet on val set, LaUs notably surpass Enc-Net and all entries in COCO-Place challenge 2017 on the final test score (see results in</figDesc><table><row><cell>Team</cell><cell>Model</cell><cell cols="2">Final Score ↑ mIoU%</cell></row><row><cell>PSPNet [52]</cell><cell>ResNet-269</cell><cell>0.5538</cell><cell>-</cell></row><row><cell cols="2">WinterISComing 2nd -</cell><cell>0.5544</cell><cell>-</cell></row><row><cell>CASIA IVA JD 1st</cell><cell>-</cell><cell>0.5547</cell><cell>-</cell></row><row><cell>EncNet [50]</cell><cell>ResNet-101</cell><cell>0.5567</cell><cell>-</cell></row><row><cell>JPU [46]</cell><cell>ResNet-101</cell><cell>0.5584</cell><cell>38.39</cell></row><row><cell>EncNet+LaU off4×</cell><cell>ResNet-101</cell><cell>0.5641</cell><cell>39.04</cell></row><row><cell>EncNet+LaU reg 4×</cell><cell>ResNet-101</cell><cell>0.5632</cell><cell>39.14</cell></row><row><cell cols="4">current search space contains only four points, which can</cell></row><row><cell cols="4">be suboptimal. We will include a larger search space in our</cell></row><row><cell>future work.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Although our methods achieve slightly worse perfor-</cell></row><row><cell>mance than</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="4">mIoU over Pascal VOC 2012 val set using</cell></row><row><cell>DeepLabv3+ [8].</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Upsampling</cell><cell>Backbone</cell><cell>mIoU%</cell></row><row><cell></cell><cell>Bilinear</cell><cell>ResNet-50</cell><cell>76.52</cell></row><row><cell>DeepLabv3+</cell><cell>LaU 4× Bilinear</cell><cell>ResNet-50 ResNet-101</cell><cell>78.55 78.85</cell></row><row><cell></cell><cell>LaU 4×</cell><cell>ResNet-101</cell><cell>80.16</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Φ can be 0, 1 in max(0, 1 − |x u j − x v i |), and ∆x v i , ∆y v i in LaU.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>fresh viewpoint, we propose the location-aware upsampling and location-aware losses. Our models achieve promising performance compared with various baseline methods. The effectiveness of learning offsets is also verified through qualitative and quantitative results.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Run-time Speed</head><p>In this section, we employ the frame per second (FPS) as the evaluation metirc to show the time efficiency about the proposed method. Compared with the traditional bilinear upsampling, LaU consistently improves the performance at the cost of negligible extra time complexity. We report the FPS averaged over 100 iterations. As shown in <ref type="table">Table 8</ref>, LaU is competitive with the bilinear upsamling network at actual run-time speed. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional random walk networks for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6137" to="6145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="402" to="418" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense and low-rank gaussian crfs using deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="5113" to="5122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Magnenat Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pixel transposed convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11-06" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ode-inspired network design for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Difnet: Semantic segmentation by diffusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanglin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="1637" to="1646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pylon model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Granada, Spain.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="1485" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Winner-take-all autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2791" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014-06-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference Munich</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">High-resolution representations for labeling pixels and regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1904.04514</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, MM &apos;15</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference, MM &apos;15<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gaussian conditional random field network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3224" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fastfcn: Rethinking dilated convolution in the backbone for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kongming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11-06" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deformable convnets V2: more deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
