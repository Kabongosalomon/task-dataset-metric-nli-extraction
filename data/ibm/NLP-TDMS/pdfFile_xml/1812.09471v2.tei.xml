<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Slot Filling and Intent Detection via Capsule Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<email>yaliang.li@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<postCode>98004</postCode>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">§</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent Medical AI Lab</orgName>
								<address>
									<postCode>94301</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent Medical AI Lab</orgName>
								<address>
									<postCode>94301</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@uic.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Institute for Data Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Slot Filling and Intent Detection via Capsule Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterancelevel intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsulebased neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A rerouting schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the ever-increasing accuracy in speech recognition and complexity in user-generated utterances, it becomes a critical issue for mobile phones or smart speaker devices to understand the natural language in order to give informative responses. Slot filling and intent detection play important roles in Natural Language Understanding (NLU) systems. For example, given an utterance from the user, the slot filling annotates the utterance on a word-level, indicating the slot type mentioned by a certain word such as the slot artist mentioned by the word Sungmin, while the intent detection works on the utterance-level to give categorical intent label(s) to the whole utterance.  <ref type="figure">Figure 1</ref>: An example of an utterance with BOI format annotation for slot filling, which indicates the slot of artist, play list owner, and play list name from an utterance with an intent AddToPlaylist.</p><p>To deal with diversely expressed utterances without additional feature engineering, deep neural network based user intent detection models <ref type="bibr" target="#b8">(Hu et al., 2009;</ref><ref type="bibr" target="#b19">Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b21">Zhang et al., 2016;</ref><ref type="bibr" target="#b10">Liu and Lane, 2016;</ref><ref type="bibr" target="#b18">Xia et al., 2018)</ref> are proposed to classify user intents given their utterances in the natural language.</p><p>Currently, the slot filling is usually treated as a sequential labeling task. A neural network such as a recurrent neural network (RNN) or a convolution neural network (CNN) is used to learn contextaware word representations, along with sequence tagging methods such as conditional random field (CRF) <ref type="bibr" target="#b9">(Lafferty et al., 2001</ref>) that infer the slot type for each word in the utterance.</p><p>Word-level slot filling and utterance-level intent detection can be conducted simultaneously to achieve a synergistic effect. The recognized slots, which possess word-level signals, may give clues to the utterance-level intent of an utterance. For example, with a word Sungmin being recognized as a slot artist, the utterance is more likely to have an intent of AddToPlayList than other intents such as GetWeather or BookRestaurant.</p><p>Some existing works learn to fill slots while detecting the intent of the utterance <ref type="bibr" target="#b19">(Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b10">Liu and Lane, 2016;</ref><ref type="bibr" target="#b4">Goo et al., 2018)</ref>: a convolution layer filling by learning to assign each word in the WordCaps to the most appropriate slot in SlotCaps via dynamic routing. The weights learned via dynamic routing indicate how strong each word in WordCaps belongs to a certain slot type in SlotCaps. The dynamic routing also learns slot representations using WordCaps and the learned weight. The learned slot representations in SlotCaps are further aggregated to predict the utterance-level intent of the utterance. Once the intent label of the utterance is determined, a novel re-routing process is proposed to help improve word-level slot filling by the inferred utterance-level intent label. The solid lines indicate the dynamic-routing process and dash lines indicate the re-routing process. or a recurrent layer is adopted to sequentially label word with their slot types: the last hidden state of the recurrent neural network, or an attentionweighted sum of all convolution outputs are used to train an utterance-level classification module for intent detection. Such approaches achieve decent performances but do not explicitly consider the hierarchical relationship between words, slots, and intents: intents are sequentially summarized from the word sequence. As the sequence becomes longer, it is risky to simply rely on the gate function of RNN to compress all context information in a single vector <ref type="bibr" target="#b1">(Cheng et al., 2016)</ref>.</p><p>In this work, we make the very first attempt to bridge the gap between word-level slot modeling and the utterance-level intent modeling via a hierarchical capsule neural network structure <ref type="bibr" target="#b6">(Hinton et al., 2011;</ref><ref type="bibr" target="#b12">Sabour et al., 2017)</ref>. A capsule houses a vector representation of a group of neurons. The capsule model learns a hierarchy of feature detectors via a routing-by-agreement mechanism: capsules for detecting low-level features send their outputs to high-level capsules only when there is a strong agreement of their predictions to high-level capsules.</p><p>The aforementioned properties of capsule models are appealing for natural language understanding from a hierarchical perspective: words such as Sungmin are routed to concept-level slots such as artist, by learning how each word matches the slot representation. Conceptlevel slot features such as artist, playlist owner, and playlist collectively contribute to an utterance-level intent AddToPlaylist. The dynamic routing-by-agreement assigns a larger weight from a lower-level capsule to a higher-level when the low-level feature is more predictive to one high-level feature, than other high-level features. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates this idea.</p><p>The inferred utterance-level intent is also helpful in refining the slot filling result. For example, once an AddToPlaylist intent representation is learned in IntentCaps, the slot filling may capitalize on the inferred intent representation and recognize slots that are otherwise neglected previously. To achieve this, we propose a re-routing schema for capsule neural networks, which allows high-level features to be actively engaged in the dynamic routing between WordCaps and Slot-Caps, which improves the slot filling performance.</p><p>To summarize, the contributions of this work are as follows:</p><p>• Encapsulating the hierarchical relationship among word, slot, and intent in an utterance by a hierarchical capsule neural network structure.</p><p>• Proposing a dynamic routing schema with rerouting that achieves synergistic effects for joint slot filling and intent detection.</p><p>• Showing the effectiveness of our model on two real-world datasets, and comparing with existing models as well as commercial NLU services.</p><p>We propose to model the hierarchical relationship among each word, the slot it belongs to, and the intent label of the whole utterance by a hierarchical capsule neural network structure called CAPSULE-NLU. The proposed architecture consists of three types of capsules: 1) WordCaps that learn context-aware word representations, 2) SlotCaps that categorize words by their slot types via dynamic routing, and construct a representation for each type of slot by aggregating words that belong to the slot, 3) IntentCaps determine the intent label of the utterance based on the slot representation as well as the utterance contexts.</p><p>Once the intent label has been determined by In-tentCaps, the inferred utterance-level intent helps re-recognizing slots from the utterance by a rerouting schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WordCaps</head><p>Given an input utterance x = (w 1 , w 2 , ..., w T ) of T words, where each word is initially represented by a vector of dimension D W . Here we simply trained word represenations from scratch. Various neural network structures can be used to learn context-aware word representations. For example, a recurrent neural network such as a bidirectional LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> can be applied to learn representations of each word in the utterance:</p><formula xml:id="formula_0">h t = LSTM f w (w t , h t−1 ), ← h t = LSTM bw (w t , ← h t+1 ).</formula><p>(1)</p><p>For each word w t , we concatenate each forward hidden state h t obtained from the forward LSTM f w with a backward hidden state ← h t from LSTM bw to obtain a hidden state h t . The whole hidden state matrix can be defined as</p><formula xml:id="formula_1">H = (h 1 , h 2 , ..., h T ) ∈ R T ×2D H ,</formula><p>where D H is the number of hidden units in each LSTM. In this work, the parameters of WordCaps are trained with the whole model, while sophisticated pretrained models such as ELMo <ref type="bibr" target="#b11">(Peters et al., 2018)</ref> or BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> may also be integrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SlotCaps</head><p>Traditionally, the learned hidden state h t for each word w t is used as the logit to predict its slot tag. When H for all words in the utterance is learned, sequential tagging methods like the linear-chain CRF models the tag dependencies by assigning a transition score for each transition pattern between adjacent tags to ensure the best tag sequence of the utterance from all possible tag sequences.</p><p>Instead of doing slot filling via sequential labeling which does not directly consider the dependencies among words, the SlotCaps learn to recognize slots via dynamic routing. The routingby-agreement explicitly models the hierarchical relationship between capsules. For example, the routing-by-agreement mechanism send a lowlevel feature, e.g. a word representation in Word-Caps, to high-level capsules, e.g. SlotCaps, only when the word representation has a strong agreement with a slot representation.</p><p>The agreement value on a word may vary when being recognized as different slots. For example, the word three may be recognized as a party size number slot or a time slot. The SlotCaps first convert the word representation obtained in WordCaps with respect to each slot type. We denote p k|t as the resulting prediction vector of the t-th word when being recognized as the kth slot:</p><formula xml:id="formula_2">p k|t = σ(W k h T t + b k ),<label>(2)</label></formula><p>where k ∈ {1, 2, ..., K} denotes the slot type and t ∈ {1, 2, ..., T }. σ is the activation function such as tanh. W k ∈ R D P ×2D H and b k ∈ R D P ×1 are the weight and bias matrix for the k-th capsule in SlotCaps, and D P is the dimension of the prediction vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slot Filling by Dynamic Routing-by-agreement</head><p>We propose to determine the slot type for each word by dynamically route prediction vectors of each word from WordCaps to SlotCaps. The dynamic routing-by-agreement learns an agreement value c kt that determines how likely the t-th word agrees to be routed to the k-th slot capsule. c kt is calculated by the dynamic routing-by-agreement algorithm <ref type="bibr" target="#b12">(Sabour et al., 2017)</ref>, which is briefly recalled in Algorithm 1. The above algorithm determines the agreement value c kt between WordCaps and SlotCaps while learning the slot representations v k in an unsupervised, iterative fashion. c t is a vector that consists of all c kt where k ∈ K. b kt is the logit (initialized as zero) representing the log prior probability that the t-th word in WordCaps agrees to be routed to the k-th slot capsule in SlotCaps (Line 2). During each iteration (Line 3), each slot representation v k Return v k 10: end procedure is calculated by aggregating all the prediction vectors for that slot type {p k|t |t∈T }, weighted by the agreement values c kt obtained from b kt (Line 5-6):</p><formula xml:id="formula_3">s k = T t c kt p k|t ,<label>(3)</label></formula><formula xml:id="formula_4">v k = squash(s k ) = s k 2 1 + s k 2 s k s k ,<label>(4)</label></formula><p>where a squashing function squash(·) is applied on the weighted sum s k to get v k for each slot type. Once we updated the slot representation v k in the current iteration, the logit b kt becomes larger when the dot product p k|t · v k is large. That is, when a prediction vector p k|t is more similar to a slot representation v k , the dot product is larger, indicating that it is more likely to route this word to the k-th slot type (Line 7). An updated, larger b kt will lead to a larger agreement value c kt between the t-th word and the k-th slot in the next iteration. On the other hand, it assigns low c kt when there is inconsistency between p k|t and v k . The agreement values learned via the unsupervised, iterative algorithm ensures the outputs of the Word-Caps get sent to appropriate subsequent SlotCaps after iter slot iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Entropy Loss for Slot Filling</head><p>For the t-th word in an utterance, its slot type is determined as follows:</p><formula xml:id="formula_5">y t = arg max k∈K (c kt ).<label>(5)</label></formula><p>The slot filling loss is defined over the utterance as the following cross-entropy function:</p><formula xml:id="formula_6">L slot = − t k y k t log(ŷ k t ),<label>(6)</label></formula><p>where y k t indicates the ground truth slot type for the t-th word. y k t = 1 when the t-th word belongs to the k-th slot type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">IntentCaps</head><p>The IntentCaps take the output v k for each slot k ∈ {1, 2, ..., K} in SlotCaps as the input, and determine the utterance-level intent of the whole utterance. The IntentCaps also convert each slot representation in SlotCaps with respect to the intent type:</p><formula xml:id="formula_7">q l|k = σ(W l v T k + b l ),<label>(7)</label></formula><p>where l ∈ {1, 2, ..., L} and L is the number of intents. W l ∈ R D L ×D P and b l ∈ R D L ×1 are the weight and bias matrix for the l-th capsule in IntentCaps.</p><p>IntentCaps adopt the same dynamic routing-byagreement algorithm, where:</p><formula xml:id="formula_8">u l = DYNAMIC ROUTING(q l|k , iter intent ). (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-margin Loss for Intent Detection</head><p>Based on the capsule theory, the orientation of the activation vector u l represents intent properties while its length indicates the activation probability. The loss function considers a max-margin loss on each labeled utterance:</p><formula xml:id="formula_9">L intent = L l=1 {[[z = z l ]] · max(0, m + − u l ) 2 + λ [[z = z l ]] · max(0, u l − m − ) 2 },<label>(9)</label></formula><p>where u l is the norm of u l and [[]] is an indicator function, z is the ground truth intent label for the utterance x. λ is the weighting coefficient, and m + and m − are margins. The intent of the utterance can be easily determined by choosing the activation vector with the largest normẑ = arg max l∈{1,2,...,L} u l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Re-Routing</head><p>The IntentCaps not only determine the intent of the utterance by the length of the activation vector, but also learn discriminative intent representations of the utterance by the orientations of the activation vectors. Previously, the dynamic routingby-agreement shows how low-level features such as slots help construct high-level ideas such as intents. While the high-level features also work as a guide that helps learn low-level features. For example, the AddToPlaylist intent activation vector in IntentCaps also helps strength the existing slots such as artist name during slot filling on the words Sungmin in SlotCaps.</p><p>Thus we propose a re-routing schema for Slot-Caps where the dynamic routing-by-agreement is realized by the following equation that replaces the Line 7 in Algorithm 1:</p><formula xml:id="formula_10">b kt ← b kt + p k|t · v k + α · p T k|t W RRû T z ,<label>(10)</label></formula><p>whereûẑ is the intent activation vector with the largest norm. W RR ∈ R D P ×D L is a bi-linear weight matrix, and α as the coefficient. The routing information for each word is updated toward the direction where the prediction vector not only coincides with representative slots, but also towards the most-likely intent of the utterance. As a result, the re-routing makes SlotCaps obtain updated routing information as well as updated slot representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment Setup</head><p>To demonstrate the effectiveness of our proposed models, we compare the proposed model CAPSULE-NLU with existing alternatives, as well as commercial natural language understanding services.</p><p>Datasets For each task, we evaluate our proposed models by applying it on two real-word datasets: SNIPS Natural Language Understanding benchmark 1 (SNIPS-NLU) and the Airline Travel Information Systems (ATIS) dataset <ref type="bibr" target="#b16">(Tur et al., 2010)</ref>. The statistical information on two datasets are shown in <ref type="table">Table 1</ref>. SNIPS-NLU contains natural language corpus collected in a crowdsourced fashion to benchmark the performance of voice assistants. ATIS is a widely used dataset in spoken language understanding, where audio recordings of people making flight reservations are collected. Baselines We compare the proposed capsulebased model CAPSULE-NLU with other alternatives: 1) CNN TriCRF <ref type="bibr" target="#b19">(Xu and Sarikaya, 2013)</ref> introduces a Convolution Neural Network (CNN) based sequential labeling model for slot filling. The hidden states for each word are summed up to predict the utterance intent. We adopt the performance with lexical features. 2) Joint Seq. ) adopts a Recurrent Neural Network (RNN) for slot filling and the last hidden state of the RNN is used to predict the utterance intent. 3) Attention BiRNN <ref type="bibr" target="#b10">(Liu and Lane, 2016)</ref> further introduces a RNN based encoderdecoder model for joint slot filling and intent detection. An attention weighted sum of all encoded hidden states is used to predict the utterance intent. 4) Slot-gated Full Atten. (Goo et al., 2018) utilizes a slot-gated mechanism as a special gate function in Long Short-term Memory Network (LSTM) to improve slot filling by the learned intent context vector. The intent context vector is used for intent detection. 5) DR-AGG <ref type="bibr" target="#b3">(Gong et al., 2018)</ref> aggregates word-level information for text classification via dynamic routing. The high-level capsules after routing are concatenated, followed by a multilayer perceptron layer that predicts the utterance label. We used this capsule-based text classification model for intent detection only. 6) IntentCap-sNet <ref type="bibr" target="#b18">(Xia et al., 2018</ref>) adopts a multi-head selfattention to extract intermediate semantic features from the utterances, and uses dynamic routing to aggregate semantic features into intent representations for intent detection. We use this capsulebased model for intent detection only.</p><p>We also compare our proposed model CAPSULE-NLU with existing commercial natural language understanding services, including api.ai (Now called DialogFlow) 2 , Waston Assistant 3 , Luis 4 , wit.ai 5 , snips.ai 6 , recast.ai 7 , and Amazon Lex 8 . Implementation Details The hyperparameters used for experiments are shown in <ref type="table" target="#tab_3">Table 2</ref>.    We use the validation data to choose hyperparameters. For both datasets, we randomly initialize word embeddings using Xavier initializer and let them train with the model. In the loss function, the down-weighting coefficient λ is 0.5, margins m + and m − are set to 0.8 and 0.2 for all the existing intents. α is set as 0.1. RMSProp optimizer <ref type="bibr" target="#b15">(Tieleman and Hinton, 2012</ref>) is used to minimize the loss. To alleviate over-fitting, we add the dropout to the LSTM layer with a dropout rate of 0.2.</p><formula xml:id="formula_11">Dataset D W D H D P D L iter slot iter intent SNIPS-NLU</formula><p>Since the original data split is not available, we report the results with stratified 5-fold cross validation. From <ref type="figure" target="#fig_2">Figure 3</ref> we can see that the proposed model CAPSULE-NLU is highly competitive with off-the-shelf systems that are available to use. Note that, our model archieves the performance without using pre-trained word representations: the word embeddings are simply trained from scratch. Ablation Study To investigate the effectiveness of CAPSULE-NLU in joint slot filling and intent detection, we also report ablation test results in <ref type="table" target="#tab_4">Table 3</ref>. "w/o Intent Detection" is the model without intent detection: only a dynamic routing is performed between WordCaps and SlotCaps for the slot filling task, where we minimize L slot during training; "w/o Joint Training" adopts a two-stage training where the model is first trained for slot filling by minimizing L slot , and then use the fixed slot representations to train for the intent detection task which minimizes L intent . From the lower part of <ref type="table" target="#tab_4">Table 3</ref> we can see that by using a capsulebased hierarchical modeling between words and slots, the model CAPSULE-NLU w/o Intent Detection is already able to outperform current alternatives on slot filling that adopt a sequential labeling schema. The joint training of slot filling and intent detection is able to give each subtask further improvements when the model parameters are updated jointly.</p><p>Visualizing Agreement Values between Capsule Layers Thanks to the dynamic routing-byagreement schema, the dynamically learned agreement values between different capsule layers naturally reflect how low-level features are collectively aggregated into high-level ones for each input utterance. In this section, we harness the intepretability of the proposed capsule-based model via hierarchical modeling and provide case studies and visualizations.</p><p>Between WordCaps and SlotCaps First we study the agreement value c kt between the t-th word in the WordCaps and the k-th slot capsule in SlotCaps. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we observe that the dynamic routing-by-agreement is able to converge to an agreement quickly after the first iteration (shown in blue bars). It is able to assign a confident probability assignment close to 0 or 1. After the second iteration (shown in orange bars), the model is more certain about the routing decisions: probabilities are more leaning towards 0 or 1 as the model is confident about routing a word in WordCaps to its most appropriate slot in SlotCaps. However, we do find that when unseen slot values like new object names emerge in utterances like show me the movie operetta for the theatre organ with an intent of SearchCreativeWork, the iterative dynamic routing process would be even more appealing. <ref type="figure">Figure 5</ref> shows the agreement values learned by dynamic routing-by-agreement. Since the dynamic routing-by-agreement is an iterative process controlled by the variable iter slot , we show the agreement values after the first iteration in the left part of <ref type="figure">Figure 5</ref>, and the values after the second iteration in the right part.</p><p>From the left part of <ref type="figure">Figure 5</ref>, we can see that after the first iteration, the model considers the word operetta itself alone is likely to be an ob-  ject name, probably because the following word for is usually a context word being annotated as O. Thus it tends to route word for to both the slot O and the slot I-object name. However, from the right part of <ref type="figure">Figure 5</ref> we can see that after the second iteration, the dynamic routing found an agreement and is more certain to have operetta for the theatre organ as a whole for the slot B-object name and I-object name. Between SlotCaps and IntentCaps Similarly, we visualize the agreement values between each slot capsule in SlotCaps and each intent capsule in IntentCaps. The left part of <ref type="figure" target="#fig_4">Figure 6</ref> shows that after the first iteration, since the model is not able to correctly recognize operetta for the theatre organ as a whole, only the context slot O (correspond to the word show me the) and B-object name (correspond to the word operetta) contribute significantly to the final intent capsule. From the right part of <ref type="figure" target="#fig_4">Figure  6</ref>, we found that with the word operetta for the theatre organ being recognized in the lower capsule, the slots I-object name and B-object type contribute more to the correct intent capsule SearchCreativeWork, when comparing with other routing alternatives to other intent capsules.</p><p>Intent Detection With recent developments in deep neural networks, user intent detection models <ref type="bibr" target="#b8">(Hu et al., 2009;</ref><ref type="bibr" target="#b19">Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b21">Zhang et al., 2016;</ref><ref type="bibr" target="#b10">Liu and Lane, 2016;</ref><ref type="bibr" target="#b18">Xia et al., 2018)</ref> are proposed to classify user intents given their diversely expressed utterances in the natural language. As a text classification task, the decent performance on utterance-level intent detection usually relies on hidden representations that are learned in the intermediate layers via multiple non-linear transformations.</p><p>Recently, various capsule based text classification models are proposed that aggregate wordlevel features for utterance-level classification via dynamic routing-by-agreement <ref type="bibr" target="#b3">(Gong et al., 2018;</ref><ref type="bibr" target="#b22">Zhao et al., 2018;</ref><ref type="bibr" target="#b18">Xia et al., 2018)</ref>. Among them, <ref type="bibr" target="#b18">Xia et al. (2018)</ref> adopts self-attention to extract intermediate semantic features and uses a capsulebased neural network for intent detection. However, existing works do not study word-level supervisions for the slot filling task. In this work, we explicitly model the hierarchical relationship between words and slots on the word-level, as well as intents on the utterance-level via dynamic routingby-agreement. Slot Filling Slot filling annotates the utterance with finer granularity: it associates certain parts of the utterance, usually named entities, with predefined slot tags. Currently, the slot filling is usually treated as a sequential labeling task. A recurrent neural network such as Gated Recurrent Unit (GRU) or Long Short-term Memory Network (LSTM) is used to learn context-aware word representations, and Conditional Random Fields (CRF) are used to annotate each word based on its slot type. Recently, <ref type="bibr" target="#b13">Shen et al. (2017)</ref>; <ref type="bibr" target="#b14">Tan et al. (2017)</ref> introduce the self-attention mechanism for CRFfree sequential labeling. Joint Modeling via Sequence Labeling To overcome the error propagation in the word-level slot filling task and the utterance-level intent detection task in a pipeline, joint models are proposed to solve two tasks simultaneously in a unified framework. <ref type="bibr" target="#b19">Xu and Sarikaya (2013)</ref> propose a Convolution Neural Network (CNN) based sequential labeling model for slot filling. The hidden states corresponding to each word are summed up in a classification module to predict the utterance intent. A Conditional Random Field module ensures the best slot tag sequence of the utterance from all possible tag sequences.  adopt a Recurrent Neural Network (RNN) for slot filling and the last hidden state of the RNN is used to predict the utterance intent. <ref type="bibr" target="#b10">Liu and Lane (2016)</ref> further introduce an RNN based encoderdecoder model for joint slot filling and intent detection. An attention weighted sum of all encoded hidden states is used to predict the utterance intent. Some specific mechanisms are designed for RNNs to explicitly encode the slot from the utterance. For example, Goo et al. (2018) utilize a slot-gated mechanism as a special gate function in Long Short-term Memory Network (LSTM) to improve slot filling by the learned intent context vector. However, as the sequence becomes longer, it is risky to simply rely on the gate function to sequentially summarize and compress all slots and context information in a single vector <ref type="bibr" target="#b1">(Cheng et al., 2016)</ref>.</p><p>In this paper, we harness the capsule neural network to learn a hierarchy of feature detectors and explicitly model the hierarchical relationships among word-level slots and utterance-level intent. Also, instead of doing sequence labeling for slot filling, we use a dynamic routing-by-agreement schema between capsule layers to route each word in the utterance to its most appropriate slot type. And we further route slot representations, which are learned dynamically from words, to the most appropriate intent capsule for intent detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, a capsule-based model, namely CAPSULE-NLU, is introduced to harness the hierarchical relationships among words, slots, and intents in the utterance for joint slot filling and intent detection. Unlike treating slot filling as a sequential prediction problem, the proposed model assigns each word to its most appropriate slots in SlotCaps by a dynamic routing-by-agreement schema. The learned word-level slot representations are futher aggregated to get the utterancelevel intent representations via dynamic routingby-agreement. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of the proposed models when compared with other alternatives as well as existing NLU services.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1illustrates this idea.Code and data available at https://github.com/ czhang99/Capsule-NLU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed CAPSULE-NLU model for joint slot filling and intent detection. The model does slot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Stratified 5-fold cross validation for benchmarking with existing NLU services on SNIPS-NLU dataset. Black bars indicate the standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The distribution of all agreement values between WordCaps and SlotCaps on the test split of SNIPS-NLU dataset. Blue: the distribution of values after the first iteration. Yellow: the distribution after the second iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The learned agreement values between SlotCaps (y-axis) and IntentCaps (x-axis). Left: after the first iteration. Right: after the second iteration. The same sample utterance used inFigure 5is used here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Dynamic routing-by-agreement 1: procedure DYNAMIC ROUTING(p k|t , iter)2: for each WordCaps t and SlotCaps k: b kt ← 0. WordCaps t: c t ← softmax(b t ) 5: for all SlotCaps k: s k ← Σ r c kt p k|t 6: for all SlotCaps k: v k = squash(s k )</figDesc><table><row><cell>3:</cell><cell>for iter iterations do</cell></row><row><cell cols="2">4: for all 7: for all WordCaps t and SlotCaps k: b kt ←</cell></row><row><cell></cell><cell>b kt + p k|t · v k</cell></row><row><cell>8:</cell><cell>end for</cell></row><row><cell>9:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameter settings.</figDesc><table><row><cell>Model</cell><cell cols="6">SNIPS-NLU Slot (F1) Intent (Acc) Overall (Acc) Slot (F1) Intent (Acc) Overall (Acc) ATIS</cell></row><row><cell>CNN TriCRF (Xu and Sarikaya, 2013)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.944</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint Seq. (Hakkani-Tür et al., 2016)</cell><cell>0.873</cell><cell>0.969</cell><cell>0.732</cell><cell>0.942</cell><cell>0.926</cell><cell>0.807</cell></row><row><cell>Attention BiRNN (Liu and Lane, 2016)</cell><cell>0.878</cell><cell>0.967</cell><cell>0.741</cell><cell>0.942</cell><cell>0.911</cell><cell>0.789</cell></row><row><cell>Slot-Gated Full Atten. (Goo et al., 2018)</cell><cell>0.888</cell><cell>0.970</cell><cell>0.755</cell><cell>0.948</cell><cell>0.936</cell><cell>0.822</cell></row><row><cell>DR-AGG (Gong et al., 2018)</cell><cell>-</cell><cell>0.966</cell><cell>-</cell><cell>-</cell><cell>0.914</cell><cell>-</cell></row><row><cell>IntentCapsNet (Xia et al., 2018)</cell><cell>-</cell><cell>0.974</cell><cell>-</cell><cell>-</cell><cell>0.948</cell><cell>-</cell></row><row><cell>CAPSULE-NLU</cell><cell>0.918</cell><cell>0.973</cell><cell>0.809</cell><cell>0.952</cell><cell>0.950</cell><cell>0.834</cell></row><row><cell>CAPSULE-NLU w/o Intent Detection</cell><cell>0.902</cell><cell>-</cell><cell>-</cell><cell>0.948</cell><cell>-</cell><cell>-</cell></row><row><cell>CAPSULE-NLU w/o Joint Training</cell><cell>0.902</cell><cell>0.977</cell><cell>0.804</cell><cell>0.948</cell><cell>0.847</cell><cell>0.743</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Slot filling and intention detection results using CAPSULE-NLU on two datasets.</figDesc><table><row><cell></cell><cell>1.00 0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>api.ai ibm.watson</cell></row><row><cell></cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>microsoft.luis</cell></row><row><cell>F1</cell><cell>0.96 0.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>wit.ai snips.ai</cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>recast.ai</cell></row><row><cell></cell><cell>0.93 0.94</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>amazon.lex Capsule-NLU</cell></row><row><cell></cell><cell>AddToPlaylist</cell><cell>BookRestaurant</cell><cell>GetWheather</cell><cell>PlayMusic</cell><cell>RateBook</cell><cell>SearchCreativeWork SearchScreeningEvent</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/snipsco/nlubenchmark/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://dialogflow.com/ 3 https://www.ibm.com/cloud/watsonassistant/ 4 https://www.luis.ai/ 5 https://wit.ai/ 6 https://snips.ai/ 7 https://recast.ai/ 8 https://aws.amazon.com/lex/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">ResultsQuantitative Evaluation The intent detection results on two datasets are reported inTable 3, where the proposed capsule-based model performs consistently better than current learning schemes for joint slot filling and intent detection, as well as capsule-based neural network models that only focuses on intent detection. These results demonstrate the novelty of the proposed capsule-based model CAPSULE-NLU in jointly modeling the hierarchical relationships among words, slots and intents via the dynamic routing between capsules.Also, we benchmark the intent detection performance of the proposed model with existing natural language understanding services 9 in Figure 3. 9 https://www.slideshare.net/KonstantinSavenkov/nluintent-detection-benchmark-by-intento-august-2017</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank the reviewers for their valuable comments. This work is supported in part by NSF through grants IIS-1526499, IIS-1763325, and  CNS-1626432.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end memory networks with knowledge carryover for multiturn spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gökhan</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3245" to="3249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information aggregation via dynamic routing for sequence encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2742" to="2752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Slot-gated modeling for joint slot filling and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chih-Wen Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Kai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Li</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Chieh</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="753" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional rnn-lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gökhan</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="715" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding user&apos;s query intent with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Lochovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="471" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="685" to="689" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnnfree language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01586</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What is left to be understood in atis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="19" to="24" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-shot user intent detection via capsule neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3090" to="3099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular crf for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bringing semantic structures to user intent detection in online medical queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Big Data</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1019" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining user intentions from medical queries: A neural network based heterogeneous jointly modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1373" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00538</idno>
		<title level="m">Investigating capsule networks with dynamic routing for text classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
