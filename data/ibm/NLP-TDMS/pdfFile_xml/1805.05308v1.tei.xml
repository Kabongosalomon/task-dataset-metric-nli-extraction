<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Engin</surname></persName>
							<email>deniz.engin@itu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Hazım Kemal Ekenel SiMiT Lab</orgName>
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anıl</forename><surname>Genç</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Hazım Kemal Ekenel SiMiT Lab</orgName>
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an end-to-end network, called Cycle-Dehaze, for single image dehazing problem, which does not require pairs of hazy and corresponding ground truth images for training. That is, we train the network by feeding clean and hazy images in an unpaired manner. Moreover, the proposed approach does not rely on estimation of the atmospheric scattering model parameters. Our method enhances CycleGAN formulation by combining cycle-consistency and perceptual losses in order to improve the quality of textural information recovery and generate visually better haze-free images. Typically, deep learning models for dehazing take low resolution images as input and produce low resolution outputs. However, in the NTIRE 2018 challenge on single image dehazing, high resolution images were provided. Therefore, we apply bicubic downscaling. After obtaining low-resolution outputs from the network, we utilize the Laplacian pyramid to upscale the output images to the original resolution. We conduct experiments on NYU-Depth, I-HAZE, and O-HAZE datasets. Extensive experiments demonstrate that the proposed approach improves CycleGAN method both quantitatively and qualitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Bad weather events such as fog, mist, and haze dramatically reduce the visibility of any scenery and constitute significant obstacles for computer vision applications, e.g. object detection, tracking, and segmentation. While images captured from hazy fields usually preserve most of their major context, they require some visibility enhancement as a pre-processing before feeding them into computer vision algorithms, which are mainly trained on the images captured at clear weather conditions. This pre-processing is generally called as image dehazing/defogging. Image dehazing techniques aim to generate haze-free images purified from the bad weather events. Sample hazy and haze-free * indicates equal contribution images from the NTIRE 2018 challenge on single image dehazing <ref type="bibr" target="#b3">[4]</ref> are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In the recent literature, researchers concentrate on single image dehazing methods, which can dehaze an input image without requiring any extra information, e.g. depth information or known 3D model of the scene. Single image dehazing approaches are divided into prior information-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref> and learning based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Prior information-based methods are mainly based on the parameter estimation of atmospheric scattering model by utilizing the priors, such as dark channel priors <ref type="bibr" target="#b15">[16]</ref>, color attenuation prior <ref type="bibr" target="#b37">[38]</ref>, haze-line prior <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. On the other hand, these parameters are obtained from training data by learning based methods, which rely mostly on deep learning approaches. The proliferation of deep neural networks increases the use of large-scale datasets, therefore, researchers tend to create synthetic dehazing datasets like FRIDA <ref type="bibr" target="#b32">[33]</ref> and D-HAZY <ref type="bibr" target="#b1">[2]</ref>, which have a more practical creation process than real dehazing datasets. Even though most of the deep learning approaches use the estimation of intermediate parameters, e.g. transmis-sion map and atmospheric light <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>, there are also other approaches based on generative adversarial networks (GANs), which build a model without benefiting from these intermediate parameters <ref type="bibr" target="#b29">[30]</ref>.</p><p>GANs, introduced by Goodfellow et al. <ref type="bibr" target="#b13">[14]</ref>, are found to be very successful at image generation tasks, e.g. data augmentation, image inpainting, and style transfer. Their major goal is the generation of fake images indistinguishable from the original images on the targeted domain. By utilizing GANs, there exist state-of-the-art methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref> for single image dehazing, which require hazy input image and its ground truth in a paired manner. Recently, the need of paired data is removed after the cycle-consistency loss has been proposed by CycleGAN <ref type="bibr" target="#b36">[37]</ref> for image-to-image translation. Inspired by the cycle-consistency loss, Disentangled Dehazing Network (DDN) has been introduced by Yang et al. for single image dehazing. Unlike Cy-cleGAN <ref type="bibr" target="#b36">[37]</ref> architecture, DDN reconstructs cyclic-image via the atmospheric scattering model instead of using another generator. Therefore, it requires the scene radiance, medium transmission map, and global atmospheric light <ref type="bibr" target="#b33">[34]</ref> at the training phase.</p><p>In this work, we introduce Cycle-Dehaze network by utilizing CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture via aggregating cycle-consistency and perceptual losses. Our main purpose is building an end-to-end network regardless of atmospheric scattering model for single image dehazing. In order to feed the input images into our network, they are resized to 256 × 256 pixel resolution via bicubic downscaling. After dehazing the input images, bicubic upscaling to their original size is not sufficient to estimate the missing information. To be able to obtain high-resolution images, we employed a simple upsampling method based on Laplacian pyramid. We perform our experiments on NYU-Depth <ref type="bibr" target="#b27">[28]</ref> part of D-HAZY <ref type="bibr" target="#b1">[2]</ref> and the NTIRE 2018 challenge on single image dehazing <ref type="bibr" target="#b3">[4]</ref> datasets: I-HAZE <ref type="bibr" target="#b5">[6]</ref> &amp; O-HAZE <ref type="bibr" target="#b6">[7]</ref>. According to our results, Cycle-Dehaze achieves higher image quality metrics than CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture. Moreover, we analyze the performance of Cycle-Dehaze on cross-dataset scenarios, that is, we use different datasets at training and testing phases.</p><p>Our main contributions are summarized as follows:</p><p>• We enhance CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture for single image dehazing via adding cyclic perceptualconsistency loss besides cycle-consistency loss.</p><p>• Our method requires neither paired samples of hazy and ground truth images nor any parameters of atmospheric scattering model during the training and testing phases.</p><p>• We present a simple and efficient technique to upscale dehazed images by benefiting from Laplacian pyramid.</p><p>• Due to its cyclic structure, our method provides a generalizable model demonstrated with the experiments on cross-dataset scenarios.</p><p>The rest of this paper is organized as follows: In Section 2, a brief overview of related work is provided. The proposed method is described in Section 3. Experimental results are presented and discussed in Section 4. Finally, the conclusions are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image dehazing methods aim at recovering the clear scene reflection, atmospheric light color, and transmission map from an input hazy image. The requirement to know several parameters of the scene makes this problem challenging. Image dehazing methods can be categorized in terms of their inputs: (i) multiple images dehazing, (ii) polarizing filter-based dehazing, (iii) single image dehazing via utilizing additional information, e.g. depth or geometrical information methods, and (iv) single image dehazing <ref type="bibr" target="#b18">[19]</ref>.</p><p>Multiple images based methods overcome dehazing problem by obtaining changed atmospheric conditions from multiple images <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b35">36]</ref>. In other words, it is required to wait until the weather condition or haze level are changed; thus, it is not practical for real-world applications. The polarization filter based approach has been proposed to dismiss the requirement of changed weather conditions <ref type="bibr" target="#b26">[27]</ref>. In this approach, various filters are applied on different images to simulate changed weather conditions. Nevertheless, the static scenes are only considered when polarization filter based approaches are used. Therefore, this method still is not applicable for real-time scenarios. To address the necessities of these methods, single image dehazing via using additional information such as depth information <ref type="bibr" target="#b14">[15]</ref> and the approximation of the 3D model of the scene <ref type="bibr" target="#b16">[17]</ref> have been suggested. Since there is usually a single captured image of hazy scenes in the real-world conditions, obtaining additional information about the scene is extremely hard. Due to problems of previous approaches, researchers focus on single image dehazing methods.</p><p>Single Image Dehazing. Single image dehazing methods are mainly based on estimating parameters of the physical model, which is also known as the atmospheric scattering model. This model depends on the atmospheric condition of the scene, and can be expressed as follow:</p><formula xml:id="formula_0">I(x) = J(x)t(x) + A(1 − t(x))<label>(1)</label></formula><p>where I(x) is the hazy image, J(x) is the haze-free image or the scene radiance, t(x) is the medium transmission map, and A is the global atmospheric light on each x pixel coordinates. t(x) can be formulated as:</p><formula xml:id="formula_1">t(x) = e −βd(x)<label>(2)</label></formula><p>where d(x) is the depth of the scene point and β is defined as the scattering coefficient of the atmosphere.</p><p>Single image based methods can be categorized into two main approaches: prior information-based methods and learning-based methods. Prior information-based methods have been introduced as the pioneer of single image dehazing methods in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref>. Following these studies, the dark channel prior (DCP) based on the statistics about the haze-free images has been presented by He et al. <ref type="bibr" target="#b15">[16]</ref>. In this method, haze transmission map is estimated by utilizing dark pixels, which have a low-intensity value of at least one color channel. Dark channel prior has been enhanced by optimizing the inherent boundary constraint with weighted L1-norm contextual regularization to estimate transmission map <ref type="bibr" target="#b19">[20]</ref>. In addition, Zhu et al. proposed a color attenuation prior (CAP) in order to recover depth information by creating a linear model on local priors <ref type="bibr" target="#b37">[38]</ref>. Contrary to using local priors, Berman et al. introduced non-local color prior (NCP), which is based on an approximation of the entire haze-free images including few hundred distinct colors <ref type="bibr" target="#b7">[8]</ref>. Each distinct color on a haze-free image is clustered and represented a line in RGB color space. Distance map and dehazed image are obtained by using these lines. Haze-line prior based approach has been improved by intersection with air-light to estimate global air-light in <ref type="bibr" target="#b8">[9]</ref>. Moreover, due to non-uniform lighting conditions on the entire image, local airlight is estimated for each patch for night-time dehazing by multi-scale fusion in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Recently, learning based methods have been employed by utilizing CNNs and GANs for single image dehazing. CNN based methods mainly focus on estimating transmission map and/or atmospheric light <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> to recover clean images via atmospheric scattering model. On the other hand, GANs based methods produce haze-free images and estimate parameters of the physical model <ref type="bibr" target="#b33">[34]</ref>. Also, combination of them has been proposed in <ref type="bibr" target="#b34">[35]</ref>.</p><p>Ren et al. <ref type="bibr" target="#b23">[24]</ref> proposed a Multi-Scale CNN (MSCNN), which consists of coarse-scale and fine-scale networks in order to estimate the transmission map. The coarse-scale network estimates the transmission map, which is also improved locally by the fine-scale network. Another transmission map estimation network, called as DehazeNet, is designed differently from classical CNNs by adding feature extraction and non-linear regression layers, and it has been suggested by Cai et al. <ref type="bibr" target="#b9">[10]</ref>. In addition to previous approaches, All-In-One Dehazing Network (AOD-net) has been presented in <ref type="bibr" target="#b17">[18]</ref> to be able to produce clean images directly without estimating intermediate parameters independently. Atmospheric scattering model is re-formulated to implement it in an end-to-end network. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> suggested a multi-task method that includes three modules, which are transmission map estimation via GANs, hazy feature extraction, and image dehazing. All modules have been trained jointly and image level loss functions, e.g. perceptual loss and pixel-wise Euclidean loss, have been utilized. Similarly, Yang et al. <ref type="bibr" target="#b33">[34]</ref> introduced Disentangled Dehazing Network (DDN) to estimate the scene radiance, transmission map, and global atmosphere light by utilizing three generators jointly. Different from our method, these methods require estimation parameters of the atmospheric scattering model during training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Cycle-Dehaze is an enhanced version of CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture for single image dehazing. In order to increase visual quality metrics, PSNR, SSIM, it utilizes the perceptual loss inspired by EnhanceNet <ref type="bibr" target="#b24">[25]</ref>. The main idea of this loss is comparing images in a feature space rather than in a pixel space. Therefore, Cycle-Dehaze compares the original image with the reconstructed cyclic-image at both spaces, where cycle-consistency loss ensures a high PSNR value and perceptual loss preserves the sharpness of the image. Moreover, Cycle-Dehaze uses traditional Laplacian pyramid to provide better upsampling results after the main dehazing process. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall representation of Cycle-Dehaze architecture.</p><p>As can be demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>, Cycle-Dehaze architecture is composed of two generators G, F and two discriminators D x , D y . In favor of cleaning/adding the haze, the architecture profits from the combination of cycle-consistency and cyclic perceptual-consistency losses besides the regular discriminator and generator losses. As a result of this, the architecture is forced to preserve textural information of the input images and generate unique haze-free outputs. On the other hand, pursuing the balance between cycle-consistency and perceptual-consistency losses is not a trivial task. Giving over-weight to perceptual loss causes the loss of color information after dehazing process. Therefore, cycle-consistency loss needs to have higher weights than the perceptual loss.</p><p>Cyclic perceptual-consistency loss. CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture introduces cycle-consistency loss, which calculates L1 − norm between the original and cyclic image for unpaired image-to-image translation task. However, this calculated loss between the original and cyclic image is not enough to recover all textural information, since hazy images are mostly heavily-corrupted. Cyclic perceptualconsistency loss aims to preserve original image structure by looking the combination of high and low-level features extracted from 2nd and 5th pooling layers of VGG16 <ref type="bibr" target="#b28">[29]</ref> architecture. Under the constraints of x ∈ X, y ∈ Y and generator G : X → Y , generator F : Y → X, the formulation of cyclic perceptual-consistency loss is given below, where (x, y) refers to hazy and ground truth unpaired image set and φ is a VGG16 <ref type="bibr" target="#b28">[29]</ref> feature extractor from 2nd and 5th pooling layers:</p><formula xml:id="formula_2">L P erceptual = φ(x) − φ(F (G(x))) 2 2 + φ(y) − φ(G(F (y))) 2 2 .<label>(3)</label></formula><p>Full objective of Cycle-Dehaze.</p><p>Cycle-Dehaze has one extra loss compared to CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture. Therefore, the objective of Cycle-Dehaze can be formulated as follows, where L CycleGAN (G, F, D x , D y ) is the full objective of CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture, D stands for the discriminator and γ controls the effect of cyclic perceptualconsistency loss:</p><formula xml:id="formula_3">L(G, F, D x , D y ) = L CycleGAN (G, F, D x , D y ) + γ * L P erceptual (G, F ),<label>(4)</label></formula><p>G * , F * = arg min max G,F,Dx,Dy</p><formula xml:id="formula_4">L(G, F, D x , D y ).<label>(5)</label></formula><p>Conclusively, Cycle-Dehaze optimizes CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture with the additional cyclic perceptual-consistency loss given in Equation 3 according to Equations 4 and 5. In order to obtain haze-free images, the generator G * is used at the testing time.</p><p>Laplacian upscaling. Cycle-Dehaze architecture takes 256 × 256 pixel resolution input image and produces 256 × 256 pixel resolution output image because of GPU limitation. In order to reduce deterioration of the image quality during the downscaling and upscaling process, we have taken advantage of Laplacian pyramid, which is created by using high-resolution hazy images. In order to get the high-resolution dehazed image, we have changed the top layer of Laplacian pyramid with our dehazed low-resolution image and performed Laplacian upscaling process as usual. This basic usage of Laplacian pyramid especially preserves most of the edges of the hazy image during the cleaning process and boosts SSIM value at the upscaling stage. Laplacian upscaling is an optional post-processing step while working on the high-resolution images.</p><p>Implementation details (indoor/outdoor). We used TensorFlow <ref type="bibr" target="#b0">[1]</ref> framework for the training and testing phases, and MATLAB for resizing images. We trained our model with NVIDIA TITAN X graphics card. We performed around 40 epochs on each dataset in order to ensure convergence. Our testing time is about 8 seconds per image on Intel Core i7-5820K CPU. During the training phase of our model, we used Adam optimizer with the learning rate 1e−4. Moreover, we took γ as 0.0001 which is 1e + 5 times lower than the weight of the cycle-consistency loss.  <ref type="table">Table 1</ref>: Average PSNR and SSIM results on NYU-Depth <ref type="bibr" target="#b27">[28]</ref> dataset. Most of the accuracies taken from the paper <ref type="bibr" target="#b33">[34]</ref>. Numbers in red and blue indicate first and second best results, respectively. The second column of the table shows the values which are average PSNR and SSIM results calculated directly between the each hazy and its ground truth image.</p><p>Our network is similar to the original CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture except for the cyclic perceptual-consistency loss and Laplacian pyramid as a post-processing step. At the highest level of Laplacian pyramid, we scale the images to 256 × 256 pixel resolution because of our network's requirements. To calculate cyclic perceptual-consistency loss, we used VGG16 <ref type="bibr" target="#b28">[29]</ref> architecture, which is initialized by ImageNet <ref type="bibr" target="#b10">[11]</ref> pre-trained model. The source code of the proposed method will be publicly available through project's GitHub page 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we have presented the experimental results and discussed them with the results of the challenge. The first experiment is to compare our result with the state-of-the-art approaches on NYU-Depth <ref type="bibr" target="#b27">[28]</ref> dataset. Then, we have investigated our performance on the NTIRE 2018 challenge on single image dehazing <ref type="bibr" target="#b3">[4]</ref> datasets: I-HAZE <ref type="bibr" target="#b5">[6]</ref> &amp; O-HAZE <ref type="bibr" target="#b6">[7]</ref>. In addition, we have emphasized differences between CycleGAN <ref type="bibr" target="#b36">[37]</ref> and our proposed method, Cycle-Dehaze, via qualitative and quantitative results. Furthermore, we have provided comparative qualitative results on natural images. <ref type="bibr" target="#b27">[28]</ref> dataset consists of 1449 pairs of clean and synthesized hazy images of the same scene. The dataset is the part of D-HAZY <ref type="bibr" target="#b1">[2]</ref> dataset, which includes two individual environments presented as Middelbury <ref type="bibr" target="#b25">[26]</ref> and NYU-Depth <ref type="bibr" target="#b27">[28]</ref>. We have chosen NYU-Depth <ref type="bibr" target="#b27">[28]</ref>, which is considerably larger scale than Middlebury <ref type="bibr" target="#b25">[26]</ref> part. NYU-Depth <ref type="bibr" target="#b27">[28]</ref> contains also depth map of each scene, which is not used for this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU-Depth</head><p>The NTIRE 2018 challenge on single image dehazing <ref type="bibr" target="#b3">[4]</ref> datasets were collected via professional fog generators and camera setup for image dehazing problem. Each image includes a Macbeth ColorChecker mostly used for color calibration with the real-world. The challenge <ref type="bibr" target="#b3">[4]</ref> has two main datasets: I-HAZE <ref type="bibr" target="#b5">[6]</ref> &amp; O-HAZE <ref type="bibr" target="#b6">[7]</ref>, which have 25 indoor and 35 outdoor hazy images and their ground truth images, respectively. The captured images are in very high resolution. During the challenge, the organizers provide 1 github.com/engindeniz/Cycle-Dehaze additional 10 images for each dataset as a validation and test set. We did not include them in the training data.</p><p>Data Augmentation. We have employed data augmentation by taking random crops as the pre-processing step before the training phase. This procedure makes our model robust for different scales and textures. Our data augmentation procedure is as in Algorithm 1. if i = factor then 7: WRITE(crops); break <ref type="bibr">8:</ref> x, y ← select a random pixel coordinate on image <ref type="bibr">9:</ref> w, h ← select a random width, height <ref type="bibr">10:</ref> crops(i) ← CROP(x,y,w,h) <ref type="bibr">11:</ref> crops(i) ← RESIZE(crops(i), [256, 256]) <ref type="bibr">12:</ref> i ← i + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Data Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>goto loop.</p><p>According to Algorithm 1, we take random crops from the image by selecting random pixel coordinates and crop sizes. Then, we resize our crops to 256×256 before feeding them into our network. We run our augmenter function for each image in our datasets.</p><p>During data augmentation, we have obtained 200 images per original image in the training set of the I-HAZE <ref type="bibr" target="#b5">[6]</ref> and O-HAZE <ref type="bibr" target="#b6">[7]</ref> datasets, since this dataset contains too few images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on NYU-Depth Dataset</head><p>We have conducted our experiments on the benchmark NYU-Depth <ref type="bibr" target="#b27">[28]</ref> dataset to illustrate the performance of our approach compared to the other state-of-the-art approaches. We have employed Cycle-Dehaze by taking hazy images as input, and compared haze-free outputs with theirs ground truths. We have tested our method on all images of NYU-Depth <ref type="bibr" target="#b27">[28]</ref> dataset and reported average PSNR and SSIM values in <ref type="table">Table 1</ref>.    <ref type="bibr" target="#b6">[7]</ref>. According to preliminary results, the first row demonstrates the best accuracies of the NTIRE 2018 challenge on single image dehazing <ref type="bibr" target="#b3">[4]</ref>. The first row of the results shows the values which are average PSNR and SSIM results calculated directly between the each hazy and its ground truth image. <ref type="table">Table 1</ref> compares our quantitative results with the other approaches including CycleGAN <ref type="bibr" target="#b36">[37]</ref>. By outperforming the state-of-the-art methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref> according to PSNR metric, our model achieves the second best result. Moreover, Cycle-Dehaze reaches higher PSNR and SSIM values than CycleGAN <ref type="bibr" target="#b36">[37]</ref>. This demonstrates that adding perceptual-consistency loss on CycleGAN <ref type="bibr" target="#b36">[37]</ref> improves the architecture further for PSNR and SSIM metrics on NYU-Depth <ref type="bibr" target="#b27">[28]</ref> dataset. The results also indicate that Cycle-Dehaze could get nearly similar PSNR results with the approaches profited from parameters of the atmospheric scattering model <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on I-HAZE and O-HAZE Datasets</head><p>We have focused on the NTIRE 2018 challenge on single image dehazing <ref type="bibr" target="#b3">[4]</ref> datasets: I-HAZE <ref type="bibr" target="#b5">[6]</ref> &amp; O-HAZE <ref type="bibr" target="#b6">[7]</ref> during the preparation of this work. We have analyzed effects of Laplacian pyramid and cyclic perceptual-loss, especially on I-HAZE <ref type="bibr" target="#b5">[6]</ref> dataset. The challenge datasets are considerably higher resolution than other image dehazing datasets e.g. NYU-Depth <ref type="bibr" target="#b27">[28]</ref>. Therefore, the scaling process on images has a larger effect on I-HAZE <ref type="bibr" target="#b5">[6]</ref> and O-HAZE <ref type="bibr" target="#b6">[7]</ref> datasets according to PSNR and SSIM metrics. Our Laplacian pyramid reduces this deforming effect of the scaling process. We have tested our method on all validation and test images of the challenge datasets provided by organizers of the NTIRE 2018 challenge on single image dehazing <ref type="bibr" target="#b3">[4]</ref>. We have trained our final model only on the training set, which is also provided by the organizers. <ref type="table" target="#tab_3">Table 2</ref> presents average PSNR and SSIM values and <ref type="figure" target="#fig_2">Figure 3</ref> shows sample qualitative results.</p><p>Quantitative Results. According to <ref type="table" target="#tab_3">Table 2</ref>, our proposed method gave better PSNR and SSIM values than CycleGAN <ref type="bibr" target="#b36">[37]</ref> for each track of the challenge. This shows that additional cyclic perceptual-consistency loss and Laplacian pyramid increase the performance of original CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture. Moreover, PSNR and SSIM differences between the I-HAZE <ref type="bibr" target="#b5">[6]</ref> and O-HAZE <ref type="bibr" target="#b6">[7]</ref> datasets presents that outdoor scenes suffer from SSIM values because of the long shot of the captured images. On the other hand, they have higher PSNR values since the produced fog spreads the atmosphere and the captured images seem less hazed than the indoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>He et al. <ref type="bibr" target="#b15">[16]</ref> Zhu et al. <ref type="bibr" target="#b37">[38]</ref> Ren et al. <ref type="bibr" target="#b23">[24]</ref> Berman et al. <ref type="bibr" target="#b7">[8]</ref> Cai et al. <ref type="bibr" target="#b9">[10]</ref> Cycle-Dehaze <ref type="figure">Figure 4</ref>: Qualitative results on natural hazy images by comparing with state-of-the-art-results.</p><p>Qualitative Results. <ref type="figure" target="#fig_2">Figure 3</ref> shows the qualitative difference between CycleGAN <ref type="bibr" target="#b36">[37]</ref> and Cycle-Dehaze. From the qualitative results, it can be clearly seen that dehazed images by Cycle-Dehaze has less noise and sharper edges for both indoor and outdoor scenes, where cyclic perceptual-consistency loss reduces the noise of the dehazed images and Laplacian pyramid leads sharper edges. Since outdoor scenes include more textural repetitions than indoor scenes, recovering textures of O-HAZE <ref type="bibr" target="#b6">[7]</ref> is harder than I-HAZE <ref type="bibr" target="#b5">[6]</ref>. Therefore, our sharpness on edges reduces in outdoor conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Cross-Dataset Image Dehazing</head><p>CNNs mostly tend to overfit on a specific dataset rather than learning the targeted task. To the best of our knowledge, fine-tuning the trained model on a targeted dataset is the most popular solution of overfitting. On the other hand, we have analyzed our method with two distinct experiments in cross-dataset setups, in which entirely different datasets have been used for the training and testing phases. Firstly, we have tested Cycle-Dehaze on some popular natural images used by image dehazing community by scaling them to 256 × 256. <ref type="figure">Figure 4</ref> provides the qualitative results obtained on them. Secondly, we have tested the final model trained on NYU-Depth <ref type="bibr" target="#b27">[28]</ref> dataset on I-HAZE <ref type="bibr" target="#b5">[6]</ref> dataset and vice versa, since both of the datasets have been created under indoor conditions. <ref type="table" target="#tab_5">Table 3</ref> presents the accuracies of cross-dataset testing and <ref type="figure" target="#fig_3">Figure 5</ref> shows the visual difference of cross-dataset testing between the datasets captured at indoor scenes: NYU-Depth <ref type="bibr">[</ref>   Quantitative Results. According to <ref type="table" target="#tab_5">Table 3</ref>, Cycle-Dehaze obtains considerably high PSNR and SSIM values on cross-dataset testing. As a matter of fact, the results both on NYU-Depth <ref type="bibr" target="#b27">[28]</ref> and I-HAZE <ref type="bibr" target="#b5">[6]</ref> are as fine as original CycleGAN <ref type="bibr" target="#b36">[37]</ref> architecture on regular single dataset testing. This shows that Cycle-Dehaze mostly learns the dehazing task rather than overfitting on a dataset. Due to the cyclic mechanism of Cycle-Dehaze, our method focus on adding a haze on images beside cleaning a haze. Therefore, Cycle-Dehaze learns what is haze regardless of the image dehazing problem. On the other side, the methods only addressed the dehazing process tend to focus on color enhancement on the specific dataset. From the result of cross-dataset experiments, Cycle-Dehaze can be considered as a practical solution on real-world conditions for single image dehazing.</p><p>Qualitative Results. <ref type="figure">Figure 4</ref> shows the comparative qualitative results of Cycle-Dehaze on natural hazy images with respect to state-of-the-art image dehazing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38]</ref>. According to qualitative results, the performance of Cycle-Dehaze is perceptually satisfying on natural images, especially when the color tones of neighboring pixels are very close to each other. Specifically, Cycle-Dehaze preserves the natural color toning of hazy image after dehazing process. Consequently, Cycle-Dehaze keeps the shadows and depth on the image more perceptible. <ref type="figure" target="#fig_3">Figure 5</ref> includes the images, which are dehazed by regular Cycle-Dehaze and by the cross-dataset version of it. According to qualitative results, both methods can clear the haze from the input images. On the other hand, the color recovery on the single dataset is better than on cross-dataset which leads lower PSNR results on cross-dataset scenario. Since the haze thickens at some parts of the images, our model can not estimate the actual ground truth color if it is trained on another dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a single image dehazing network, named as Cycle-Dehaze, which directly generates haze-free images from hazy input images without estimating parameters of the atmospheric scattering model. Besides, our network provides a training process of hazy and ground truth images in an unpaired manner. In order to retain the high visual quality of haze-free images, we improved cycle-consistency loss of CycleGAN architecture by combining it with the perceptual loss. Cycle-Dehaze takes low-resolution images as input, so it requires downscaling of its inputs as a pre-processing step. For reducing distortion on images while resizing, we utilized Laplacian pyramid to upscale low-resolution images instead of using directly bicubic upscaling. The experimental results show that our method produces visually better images and achieves higher PSNR and SSIM values than CycleGAN architecture. Moreover, we performed additional experiments on the cross-dataset scenario to demonstrate generalizability of our model for different domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Hazy and clean examples from the NTIRE 2018 challenge on single image dehazing datasets: I-HAZE [6] &amp; O-HAZE [7] datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of Cycle-Dehaze Network where G &amp; F refers to the generators, and D x &amp; D y to the discriminators. For the sake of clarity, the representation is split into two parts: hazy to clean image, and clean to hazy image. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on I-HAZE [6] &amp; O-HAZE [7] datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparative qualitative results between single and cross dataset experiments via Cycle-Dehaze.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Metrics None He et al. [16] Zhu et al. [38] Berman et al. [8] Ren et al. [24] Cai et al. [10] CycleGAN Yang et al. [34] Ours</figDesc><table><row><cell>PSNR</cell><cell>9.46</cell><cell>10.98</cell><cell>12.78</cell><cell>12.26</cell><cell>13.04</cell><cell>12.84</cell><cell>13.38</cell><cell>15.54</cell><cell>15.41</cell></row><row><cell>SSIM</cell><cell>0.58</cell><cell>0.64</cell><cell>0.70</cell><cell>0.70</cell><cell>0.66</cell><cell>0.71</cell><cell>0.52</cell><cell>0.77</cell><cell>0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Average PSNR and SSIM results on the NTIRE 2018 challenge on single image dehazing [4] datasets: I-HAZE [6] &amp; O-HAZE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Cross-dataset quantitative results of Cycle-Dehaze architecture on the datasets captured at indoor scenes.</figDesc><table><row><cell>Single-dataset</cell><cell>Cross-dataset</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank our colleagues from SiMiT Lab at ITU and LTS5 at EPFL, especially Christophe Renè Joseph Ecabert and Saleh Bagher Salimi, for their valuable comments. Travel grant for this research is provided by Yapı Kredi Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">D-HAZY: a dataset to evaluate quantitatively dehazing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Night-time dehazing by fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2256" to="2260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NTIRE 2018 challenge on image dehazing: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast semi-inverse approach to detect and remove the haze from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="501" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">I-HAZE: a dehazing benchmark with real hazy and haze-free indoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">O-HAZE: a dehazing benchmark with real hazy and haze-free outdoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Air-light estimation using haze-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hierarchical rank-based veiling light estimation for underwater dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Emberton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chittka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards fog-free in-vehicle vision systems through contrast restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep photo: Model-based photograph enhancement and viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4770" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Haze visibility enhancement: A survey and quantitative benchmarking. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chromatic framework for vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nešić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instant dehazing of images using polarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Candy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02892</idno>
		<title level="m">Conditional adversarial networks based fully end-to-end system for single image haze removal</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2201" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vision enhancement in homogeneous and heterogeneous fog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caraffa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Halmaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gruyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards perceptual image dehazing by physics-based disentanglement and adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Joint transmission map estimation and dehazing using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00581</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Atmospheric scatteringbased multiple images fog removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Congress on Image and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
