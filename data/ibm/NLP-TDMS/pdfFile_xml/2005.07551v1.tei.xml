<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">L</forename><surname>Westhausen</surname></persName>
							<email>nils.westhausen@uol.de</email>
							<affiliation key="aff0">
								<orgName type="department">Communication Acoustics &amp; Cluster of Excellence Hearing4all</orgName>
								<orgName type="institution">Carl von Ossietzky University</orgName>
								<address>
									<settlement>Oldenburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><forename type="middle">T</forename><surname>Meyer</surname></persName>
							<email>bernd.meyer@uol.de</email>
							<affiliation key="aff0">
								<orgName type="department">Communication Acoustics &amp; Cluster of Excellence Hearing4all</orgName>
								<orgName type="institution">Carl von Ossietzky University</orgName>
								<address>
									<settlement>Oldenburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: noise suppression</term>
					<term>deep-learning</term>
					<term>real-time</term>
					<term>speech enhancement</term>
					<term>deep learning</term>
					<term>audio</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a dual-signal transformation LSTM network (DTLN) for real-time speech enhancement as part of the Deep Noise Suppression Challenge (DNS-Challenge). This approach combines a short-time Fourier transform (STFT) and a learned analysis and synthesis basis in a stacked-network approach with less than one million parameters. The model was trained on 500 h of noisy speech provided by the challenge organizers. The network is capable of real-time processing (one frame in, one frame out) and reaches competitive results. Combining these two types of signal transformations enables the DTLN to robustly extract information from magnitude spectra and incorporate phase information from the learned feature basis. The method shows state-of-the-art performance and outperforms the DNS-Challenge baseline by 0.24 points absolute in terms of the mean opinion score (MOS).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of noise suppression is an important discipline in field of speech enhancement; it is for instance of special importance in work-from-home scenarios where a robust and effective noise reduction can improve the communication quality and thereby reduce the cognitive effort of video conferencing. With the uprising of deep neural networks, several novel approaches for audio processing methods based on deep models were proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. However, these have often been developed for offline processing which does not require real-time capabilities or the consideration of causality in the processing chain. Such models process complete sequences and exploit past and future information of the signals to suppress undesired signal parts. Classic signal processing algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> often work on sample or frame level to provide a low input-output delay. When designing frame-based algorithms with neural networks, recurrent neural networks (RNN) are a common choice. RNNs have produced convincing results in the field of speech enhancement <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and speech separation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Long short term memory networks (LSTM) <ref type="bibr" target="#b11">[12]</ref> represent the state-of-the-art in separation <ref type="bibr" target="#b12">[13]</ref>. The best-performing networks are often build in a non-causal way by using bidirectional LSTMs where the time sequence is processed causally as well in the reversed direction. Bidirectional RNNs always require a full sequence as input and are therefore principally not suited for real-time frame processing.</p><p>The baseline system of the the deep-noise-suppression challenge (DNS-Challenge) <ref type="bibr" target="#b13">[14]</ref> is called NSNet <ref type="bibr" target="#b14">[15]</ref> and is also based on RNN layers and provides real-time capability by calculating one output frame per input frame. Based on the log power spectrum of the short-time Fourier transform (STFT) of the noisy time signal, this model predicts a gain or mask which is applied to a noisy STFT. The predicted speech signal is reconstructed by using the estimated magnitude and the phase of the noisy mixture. This approach results in a competitive baseline system, but it does not incorporate any phase information, which could be useful for enhanced speech quality. Different approaches are tackling phase estimation such as estimating the masks for the real and imaginary part of the STFT instead of the magnitude <ref type="bibr" target="#b15">[16]</ref> or calculating an iterative phase reconstruction <ref type="bibr" target="#b16">[17]</ref>. Research studies such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> have shown promising results for speaker separation tasks with a learned analysis and synthesis basis that is not decoupling magnitude and phase information. The representation is calculated by multiplying time-domain frames with learned basis functions. This approach was also applied in <ref type="bibr" target="#b19">[20]</ref> for separating speech and noise.</p><p>The motivation of the current study is to merge both analysis and synthesis approaches in one model by using a stacked dual signal transformation LSTM network (DTLN). Stacked or cascaded networks were already used in the Deep Clustering speaker separation approach <ref type="bibr" target="#b8">[9]</ref> where an additional enhancement network was added after the separation network. In related research, cascaded models were used for denoising and dereverberation <ref type="bibr" target="#b18">[19]</ref>. The proposed model presented here cascades two separation cores, the first features an STFT signal transformation while the second used a learned signal representation similar to <ref type="bibr" target="#b17">[18]</ref>. This combination is explored for the first time in the context of noise reduction and could provide beneficial effects due to the complementarity of classic and learned features transformations while maintaining a relatively small computational footprint. The stacked network in this paper is considerably smaller as most previously proposed LSTM networks and ensures real-time capability in terms of computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Signal transformations</head><p>In speaker separation, a time-frequency masking approach is often chosen to separate the speakers' signals. Noise suppression is a related source separation problem, but is different in that it only returns the speech signal and discards the noise. In the time frequency domain, the separation problem can be formulated as follows: The microphone signal y is described by</p><formula xml:id="formula_0">y[n] = xs + xn<label>(1)</label></formula><p>where xs and xn are the speech and noise components of time signals, respectively. In a noise suppression task, the desired signal is the speech signal. When the signal y is transformed with an STFT in a complex time-frequency representation (TF), the TF representation of the estimated speech signalXs can be predicted as arXiv:2005.07551v1 [eess.AS] 15 May 2020</p><formula xml:id="formula_1">follows:X s(t, f ) = M (t, f ) · |Y (t, f )| · e jφy ,<label>(2)</label></formula><p>where |Y | is the magnitude of the STFT of y. M is a mask (with masking values ranging from 0 to 1) that is applied to Y , and e jφy is the phase of the noisy signal.Xs can now be transformed back with an inverse STFT toxs. In this formulation, the phase of the noisy signal is used to predict the clean speech signal.</p><p>The second signal transformation of the DTLN was first proposed by Luo and colleagues <ref type="bibr" target="#b10">[11]</ref>. The formulation of the approach is described in the following: The mixture is split into overlapping frames y k of length L with frame index k. The frames are multiplied by U , which has N × L learned basis functions w k = y k U (3) to create the feature representation w k with dimension N × 1 of frame y k . To recover the speech representation d k from w k , a mask m k can be estimated given bŷ</p><formula xml:id="formula_2">d k = m k · w k ,<label>(4)</label></formula><p>whered k is the feature representation at index k of the estimated speech signal.d k can be transformed back to the time domain byx</p><formula xml:id="formula_3">k =d k V,<label>(5)</label></formula><p>where V contains N learned basis functions of length L.x k is the estimated frame at index k. The estimated time signalxs is reconstructed by using an overlap-add procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network architecture</head><p>The stacked dual-signal transformation LSTM network architecture introduced in this paper has two separation cores containing two LSTM layers followed by a fully-connected (FC) layer and a sigmoid activation to create a mask output. The first separation core uses an STFT analysis and synthesis base. The mask predicted by the FC layer and the sigmoid activation is multiplied by the magnitude of the mixture and transformed back to the time domain using the phase of the input mixture, but without reconstructing the waveform. The frames coming from the first network are processed by an 1D-Conv layer to create the feature representation. The feature representation is processed by a normalization layer before it is fed to the second separation core. The predicted mask of the second core is multiplied with the unnormalized version of the feature representation. The result is used as input to a 1D-Conv layer for transforming the estimated representation back to the time domain. In a last step, the signal is reconstructed with an overlap and add procedure. The architecture is visualized in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To account for the real-time character of the model, instant layer normalization (iLN) is used. Instant layer normalization is similar to standard layer normalization <ref type="bibr" target="#b20">[21]</ref> and was introduced as channel-wise layer normalization in <ref type="bibr" target="#b21">[22]</ref>. All frames are normalized individually without accumulating statistics over time and are scaled with the same learnable parameters. In the current work, this normalization scheme is referred to as instant layer normalization to differentiate from cumulative layer normalisation <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Datasets</head><p>The training dataset was created from the provided audio data of the DNS-Challenge. The speech data is part of the Librispeech corpus <ref type="bibr" target="#b22">[23]</ref>, and the noise signals originated from the Audioset corpus <ref type="bibr" target="#b23">[24]</ref>, Freesound and DEMAND corpus <ref type="bibr" target="#b24">[25]</ref>. 500 h of data were created by using the provided scripts. The default SNR range (0 to 40 dB) was changed to -5 to 25 dB to include negative SNRs and limit the total range. To cover a more fine-grained SNR distribution, the number of SNR levels was increased from 5 to 30. All further parameters remained unchanged. The 500 h dataset was divided into training (400 h) and cross validation data (100 h), which corresponds to the common 80:20 % split. All training data was sampled at 16 kHz.</p><p>The challenge organizers also provided a test set which contains four different categories each containing 300 samples. The categories are synthetic clips without reverb, synthetic clips with reverb, real recordings collected internally at Microsoft and real recordings from Audioset. The synthetic data was taken from the Graz Universitys clean speech dataset <ref type="bibr" target="#b25">[26]</ref>. The SNRs of the synthetic data were randomly distributed from 0 to 25 dB SNR. The impulse responses of the reverberant data were measured in multiple rooms at Microsoft with reverberation times (RT60) ranging from 300 to 1300 ms. Further, a blind test set was created by the organizers which is evaluated in an ITU P-808 <ref type="bibr" target="#b26">[27]</ref> setup. The full details of training and test sets are provided in <ref type="bibr" target="#b13">[14]</ref>.</p><p>To correctly estimate the performance with all objective measures in a noisy reverberant environment, the reverberant single speaker and noise test set of the WHAMR corpus <ref type="bibr" target="#b18">[19]</ref> at 16 kHz sampling frequency was used. We turned to this dataset because some objective measures need a properly delayed but clean reference signal for correct calculation. Since these signals are not provided in the DNS-Challenge test set, we used the WHAMR dataset, which has clean non-reverberant speech files accounting for the delay of the impulse response. The used WHAMR test set consists of 3000 mixtures. The speech files are taken from the WSJ0-mix corpus <ref type="bibr" target="#b27">[28]</ref> that are often used in speaker separation. The speech files are convolved with room impulse responses with the RT60 ranging from 100 to 1000 ms that were simulated with Pyroomacoustics <ref type="bibr" target="#b28">[29]</ref>. The noise consists of real-life recordings of situations such as coffee shops, restaurants, bars, office buildings and parks. The SNRs range from -3 to 6 dB relative to the speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Model configuration and training setup</head><p>The DTLN in this paper 1 has 128 units in each of its four LSTM layers. The frame size is 32 ms and the shift 8 ms. The FFT size is 512 and equal to the frame length. The 1D-Conv Layer to create the learned feature representation has 256 filters. During training, 25% of dropout is applied between the LSTM layers. The Adam optimizer is used with a learning rate of 10e-3 and and a gradient norm clipping of 3. The learning rate is halved if the loss on the validation set does not improve for three consecutive epochs. Early stopping is applied if loss on the validation set does not decrease for ten epochs. The model is trained on a batch size of 32, and each sample has the length of 15 s. The average time for one training epoch on a Nvidia RTX 2080 TI is around 21 minutes.</p><p>As training objective the scale-sensitive negative SNR <ref type="bibr" target="#b19">[20]</ref> was used. Compared to the Scale Invariant Signal to Noise Ratio (SI-SNR) <ref type="bibr" target="#b10">[11]</ref> it should avoid possible level offsets between input mixture and predicted cleaned speech, which is desirable in real-time-processing systems. Also, since it operates in time domain, the phase information can implicitly be considered. In contrast the Mean Squared Error between the estimated and clean magnitude STFT of the speech signal as training objective is not able to use any phase information in the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Baselines</head><p>The first baseline is the noise suppression network (NSNet) provided by the challenge organizers. NSNet was optimized with an MSE-based speech distortion weighted loss in frequency domain and was trained on a rather small corpus of 84 h of speech and noise mixtures. It consists of three recurrent layer with 256 gated recurrent units (GRU) <ref type="bibr" target="#b29">[30]</ref> and a fully-connected layer with sigmoid activation for mask prediction. The frame size is 20 ms and the frame shift 10 ms. GRUs are similar to LSTMs but without a cell state passed over time.</p><p>Additionally, our DTLN method is compared to four models with the same training setup as the proposed model: These models are explored to quantify the effect of using one feature representation only in two different topologies (stacked versus densely-connected LSTMs): The first and the second model consist of four LSTM layers followed by a fully-connected layer with a sigmoid activation to predict a mask. The first one (B1) uses an STFT analysis and synthesis basis, while B2 used a learned basis of size 256. The third (B3) and the fourth model (B4) are stacked models similar to the proposed method. Both separation kernels of B3 are using an STFT base. B4 has a learned feature base of size 256 for both separation kernels. The size of the LSTM layers is chosen with the aim of obtaining a similar size as the DTLN method in terms of the number of parameters. The configurations are again shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Objective and subjective evaluation</head><p>For comparison of the DTLN approach and the baselines, we use three objective measures, i.e., the Perceptual Evaluation of Speech Quality (PESQ) <ref type="bibr" target="#b30">[31]</ref>, the Scale-Invariant Signal to Distortion Ratio (SI-SDR) <ref type="bibr" target="#b31">[32]</ref> and the Short Time Objective Intelligibility measure (STOI) <ref type="bibr" target="#b32">[33]</ref>. The subjective evaluation was performed with the a ITU-T P.808 setup on the Amazon mechanical Turk (AMT) implemented and organized by Microsoft. In total, there were two evaluation runs, one on the known test data set of the DNS-Challenge and one on a blind test set provided later on. Each file was rated by five or ten judges in the first and second run, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>The results of the objective evaluation are shown in <ref type="table" target="#tab_1">Table 2</ref> and the subjective evaluation in <ref type="table" target="#tab_2">Table 3</ref>. The results are described in the following: Objective results for the non-reverberant DNS-Challenge test set: In the non-reverberant condition, all models produce improvements over the noisy condition. NSNet is outperformed by the DTLN and all additional baselines. All models trained on 500 h of data are producing similar results. The best results in terms of PESQ, SI-SDR and STOI were reached by the DTLN network. The high values obtained with B3 and the DTLN show the strength of stacked models. Even though B4 is also a stacked model, it performed considerably worse, which is discussed in Section 4.</p><p>Objective results for the reverberant DNS-Challenge test set: In this condition, results are not as clear as in the nonreverberant condition. In terms of PESQ, only B4 shows a slight improvement over the noisy condition. For the SI-SDR all models show an improvement, while STOI predicts the highest quality for the original noisy condition. One issue with the intrusive or double-ended measures is that they require a reference signal which is in this case the reverberant clean speech. With this reference signal a potential dereverberation effect by any speech enhancement model would result in a decrease of the objective measures, which presumably is an important factor for these results.</p><p>Objective results on the WHAMR test set: All methods are showing an improvement over the noisy condition, with the best scores obtained by the DTLN approach. Similar performance levels are again reached by B3. The baseline shows just a slight improvement for all objective measures. It should be mentioned that the mixtures used in this corpus have a smaller SNR range around 0 and is therefore a more challenging condition for the models. Subjective results on the DNS-Challenge test sets: The subjective results for the known non-reverberant test set are in line with the objective results. For the reverberant test set, the subjective evaluation shows a clear benefit for the DTLN, which was not reflected by the objective measures with the exception of SI-SDR. The decrease in quality of the NSNet predicted by  Results on execution time: In the context of the DNS-Challenge, the execution time of one 32 ms frame on a quadcore I5 6600K CPU was measured. The measurement was performed by either processing a complete sequence or by using frame-wise processing. Execution times of 0.23 ms and 2.08 ms were measured for the sequence and frame-wise processing, respectively. The large difference between sequence and frame processing can be explained by the overhead caused by calling models for prediction in Keras. Converting the model to Tensorflow's SavedModel format reduces the execution time for frame by frame processing to 0.65 ms, which is a large improvement. However, the sequence processing time is nearly three times lower and demonstrates the potential performance on a CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>In the following, we first discuss differences between baseline systems, which also has implications for the components of the DTLN system. The results on the non-reverberant, the reverberant and the WHAMR test are showing better results for the systems B1 and B3 (using STFT features) than for B2 and B4 (that used learned feature representations). One potential reason for the better performance with STFTs is the fixed number of parameters across networks, and -since the STFT is fixed and rule-based -it is possible that B1 and B3 exploit the higher number of parameters available for LSTM layers in comparison to learned-feature approaches. Secondly, we assume that STFT features provide a higher robustness for noisy input since phase information -which is not useful in high-noise conditions -is discarded. Vice versa, net-works using learned features have to determine masks implicitly for both magnitude and phase information. Another possible reason for the difference could be the compression which is performed by the learned feature representation in this work. The learned feature representation maps 512 audio samples to a feature representation of size 256. Feature representations with greater size would have cost even more parameters and it was empirically found that the reduction of the feature representation doesn't have a great impact on the speech quality of the proposed model.</p><p>The results also show that stacking networks using STFT and learned feature transformation slightly improves overall baseline systems by using fewer LSTM units than the pure STFT systems. LSTM units are computational more complex as fully-connected or 1D-Conv layers, i.e., a reduction of units is especially desirable for this network type. However, the relatively small difference in terms of objective measures between DTLN and the related systems (B1-B4) also suggests that a part of the performance is generated by the large amount of training data and the training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper introduced an approach for noise suppression based on a stacked dual signal transformation LSTM network for realtime enhancement, which was trained on a large-scale data set. We were able to show an advantage of using two types of analysis and synthesis bases in a stacked network approach. The DTLN works robustly in noisy reverberant environments. Although we combined a basic training setup with a straightforward architecture, we observed absolute improvements of 0.22 in terms of MOS over all subjective evaluations relative to the noisy conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed network architecture. The processing chain on the left shows the first separation core using the STFT signal transformation while the building blocks on the right represent the second core with learned feature transformations based on 1D-Conv layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of parameters and RNN units in each layerfor the proposed DTLN approach as well as for the baseline systems.</figDesc><table><row><cell>Method</cell><cell cols="2"># Prams # Units</cell></row><row><cell>NSNet (3 Layer, STFT)</cell><cell>1.27 M</cell><cell>256</cell></row><row><cell>B1 (4 Layer, STFT)</cell><cell>988 K</cell><cell>166</cell></row><row><cell>B2 (4 Layer, learned)</cell><cell>984 K</cell><cell>139</cell></row><row><cell>B3 (2x2 Layer, STFT)</cell><cell>988 K</cell><cell>156</cell></row><row><cell>B4 (2x2 Layer, learned)</cell><cell>987 K</cell><cell>95</cell></row><row><cell>DTLN (2x2 layer, STFT+learned)</cell><cell>987 K</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results in terms of PESQ [MOS], SI-SDR [dB] and STOI [%] of the non reverberant test set, the reverberant test set of the DNS challenge and the reverberant single mix test set of the WHAMR corpus.</figDesc><table><row><cell></cell><cell cols="2">DNS test set no reverb</cell><cell cols="2">DNS test set with reverb</cell><cell cols="2">WHAMR test set</cell></row><row><cell cols="3">Method PESQ SI-SDR STOI</cell><cell cols="2">PESQ SI-SDR STOI</cell><cell cols="2">PESQ SI-SDR STOI</cell></row><row><cell>Noisy</cell><cell>2.45</cell><cell>9.07 91.52</cell><cell>2.75</cell><cell>9.03 86.62</cell><cell>1.83</cell><cell>-2.73 73.00</cell></row><row><cell>NSNet</cell><cell>2.70</cell><cell>12.47 90.56</cell><cell>2.47</cell><cell>9.18 82.15</cell><cell>1.91</cell><cell>0.34 73.02</cell></row><row><cell>B1</cell><cell>3.00</cell><cell>16.05 94.53</cell><cell>2.75</cell><cell>11.33 85.41</cell><cell>2.20</cell><cell>1.95 79.93</cell></row><row><cell>B2</cell><cell>3.00</cell><cell>15.87 94.22</cell><cell>2.74</cell><cell>10.92 85.05</cell><cell>2.18</cell><cell>1.88 79.34</cell></row><row><cell>B3</cell><cell>3.03</cell><cell>16.27 94.74</cell><cell>2.70</cell><cell>10.84 84.80</cell><cell>2.23</cell><cell>1.94 80.23</cell></row><row><cell>B4</cell><cell>2.96</cell><cell>15.51 93.86</cell><cell>2.76</cell><cell>10.77 84.90</cell><cell>2.20</cell><cell>1.80 78.90</cell></row><row><cell>DTLN</cell><cell>3.04</cell><cell>16.34 94.76</cell><cell>2.70</cell><cell>10.53 84.68</cell><cell>2.23</cell><cell>2.12 80.40</cell></row><row><cell cols="4">PESQ and STOI in the reverberant condition is also observed in</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the subjective data. Consistent results are obtained with the real</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">recordings, both for the conditions known and blind.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Subjective ratings in terms of the MOS for the known and blind test set of the DNS-Challenge. The overall 95% confidence intervals for the known and blind test sets are 0.04 and 0.02, respectively.</figDesc><table><row><cell>Method</cell><cell>No</cell><cell></cell><cell>With</cell><cell></cell><cell>Real</cell></row><row><cell cols="2">reverb</cell><cell cols="2">reverb</cell><cell cols="2">recordings</cell></row><row><cell cols="6">known blind known blind known blind</cell></row><row><cell>Noisy 3.02</cell><cell>3.32</cell><cell>2.44</cell><cell>2.78</cell><cell>3.01</cell><cell>2.97</cell></row><row><cell>NSNet 3.14</cell><cell>3.49</cell><cell>2.16</cell><cell>2.64</cell><cell>2.99</cell><cell>3.00</cell></row><row><cell>DTLN 3.41</cell><cell>3.58</cell><cell>2.56</cell><cell>2.95</cell><cell>3.14</cell><cell>3.21</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A Keras implementation of the DTLN can be found at https://github.com/breizhn/DTLN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported by the DFG (Cluster of Excellence 1077/1 Hearing4all; URL: http://hearing4all.eu. The architecture was partially developed on a GPU donated by Nvidia GPU Grant program. Thanks goes to challenge organizers from Microsoft for conducting the DNS-Challenge and providing the data and the scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4628" to="4632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09452</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A fully convolutional neural network for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07132</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ephraim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An alternative approach to linearly constrained adaptive beamforming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on antennas and propagation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Investigating rnn-based speech enhancement methods for noiserobust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A hybrid dsp/deep learning approach to real-time full-band speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02173</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weighted speech distortion losses for neural-networkbased real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="871" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Whamr!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="175" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07454</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database: A database of multichannel environmental noise recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3591" to="3591" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A pitch tracking corpus with evaluation on multipitch tracking scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pirker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wohlmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Itu-t p. 808: Subjective evaluation of speech quality with a crowdsourcing approach</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<ptr target="https://www.merl.com/publications/TR2016-003" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyroomacoustics: A python package for audio room simulation and array processing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scheibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bezzam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dokmanić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="351" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (pesq): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">862</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sdrhalf-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A shorttime objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
