<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Attentive Alignment for Video Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<email>zkira@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
							<email>alregib@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Attentive Alignment for Video Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose a larger-scale dataset with larger domain discrepancy: UCF-HMDB f ull . Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA 3 N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on three video DA datasets. The code and data are released at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised Domain adaptation (DA) <ref type="bibr" target="#b0">[1]</ref> has been studied extensively in recent years to address the domain shift problem <ref type="bibr" target="#b8">[9]</ref>, which means the models trained on source labeled datasets do not generalize well to target datasets and tasks, without access to any target labels. While many DA approaches are able to diminish the distribution gap between source and target domains while learning discriminative deep features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>, most methods have been developed only for images and not videos.</p><p>Furthermore, unlike image-based DA work, there do not exist well-organized datasets to evaluate and benchmark the performance of DA algorithms for videos. The most common datasets are UCF-Olympic and UCF-HMDB small <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref>, which have only a few overlapping categories between source and target domains. This introduces limited domain discrepancy so that a deep CNN architecture can achieve nearly perfect performance even without any DA method (details in Section 3.2 and <ref type="table" target="#tab_3">Table 2</ref>). Therefore, we propose a larger-scale video DA dataset, UCF-HMDB f ull , by collecting all relevant and overlapping categories be-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Shift</head><p>Temporal Temporal Temporal Alignment Spatial Spatial Spatial Alignment In addition to spatial discrepancy between frame images, videos also suffer from temporal discrepancy between sets of time-ordered frames that contain multiple local temporal dynamics with different contributions to the overall domain shift, as indicated by the thickness of green dashed arrows. Therefore, we propose to focus on aligning the temporal dynamics which have higher domain discrepancy using a learned attention mechanism to effectively align the temporal-embedded feature spaces for videos. Here we use the action basketball as the example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Target</head><p>tween UCF101 <ref type="bibr" target="#b10">[11]</ref> and HMDB51 <ref type="bibr" target="#b5">[6]</ref>, leading to three times larger than previous two datasets, and contains larger domain discrepancy (details in Section 3.2 and <ref type="table" target="#tab_5">Table 3</ref>). Videos can suffer from domain discrepancy along both the spatial and temporal directions, bringing the need of alignment for embedded feature spaces along both directions, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. However, most DA approaches have not explicitly addressed the domain shift problem in the temporal direction. Therefore, we first investigate different DA integration methods for video classification and propose Temporal Adversarial Adaptation Network (TA 2 N), which simultaneously aligns and learns temporal dynamics, to effectively align domains spatio-temporally, outperforming other approaches which naively apply more sophisticated image-based DA methods for videos. To further achieve more effective temporal alignment, we propose to focus more on aligning those which have higher contribution to the overall domain shift, such as the local temporal features connected by thicker green arrows shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Therefore, we propose Temporal Attentive Adversarial Adaptation Network (TA 3 N) to explicitly attend to the temporal dynamics by taking into account the domain distribution discrepancy, achieving state-of-the-art performance on all three investigated video DA datasets. (GitHub)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Technical Approach</head><p>Here we introduce our baseline model (Section 2.1) and proposed methods for video DA (Sections 2.2 and 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline Model</head><p>Given the recent success of large-scale video classification using CNNs <ref type="bibr" target="#b4">[5]</ref>, we build our baseline on such architectures, as shown in the lower part of <ref type="figure" target="#fig_2">Figure 2</ref>.  We first feed the frame-level feature representations extracted from ResNet <ref type="bibr" target="#b2">[3]</ref> pre-trained on ImageNet into our model, which can be divided into two parts: 1) Spatial module G sf , which consists of multilayer perceptrons (MLP) that aims to convert the general-purpose feature vectors into task-driven feature vectors, where the task is video classification in this paper; 2) Temporal module G tf , which aggregates the frame-level feature representations to form a single video-level feature representation for each video. Our baseline architecture conducts mean-pooling along the temporal direction in G tf , so we note it as TemPooling.</p><p>To address domain shift, we integrate TemPooling with Adversarial DiscriminatorĜ d , which is the combination of a gradient reversal layer (GRL) and a domain classifier G d , inspired by <ref type="bibr" target="#b1">[2]</ref>. Through adversarial training, G d is optimized to discriminate data across domains, while the feature generator is optimized to gradually align the feature distributions across domains.Ĝ d is integrated in two ways: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Integration of Temporal Dynamics with DA</head><p>One main drawback of directly integrating image-based DA approaches into TemPooling is that the relation between frames is missing. Therefore, we would like to address this question: Does the video DA problem benefit from encoding temporal dynamics into features?</p><p>Given the fact that humans can recognize actions by reasoning the observations across time, we propose the TemRelation architecture by replacing the temporal pooling mechanism in G tf with the Temporal Relation module, which is modified from <ref type="bibr" target="#b14">[15]</ref>, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Specifically, We fuse the time-ordered feature representations with MLPs into n-frame relation R n , and then sum up all R n into the final video representation to capture temporal relations at multiple time scales. To align and encode temporal dynamics simultaneously instead of solely modifying the Temporal module, we propose Temporal Adversarial Adaptation Network (TA 2 N), which integrates relation discrim-inatorsĜ n rd inside the Temporal module with corresponding n-frame relations to properly align different temporal relations, outperforming those which are extended from sophisticated image-based DA approaches although TA 2 N is adopted from a simpler DA method (details in <ref type="table" target="#tab_5">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Temporal Attentive Alignment for Videos</head><p>Although aligning temporal features across domains benefits video DA, not all the features equally contribute to the overall domain shift. Therefore, we propose to focus more on aligning those which have larger domain discrepancy and assign them with larger attention weighting values, as shown in <ref type="figure">Figure 3</ref>. The main question becomes: How to incorporate domain discrepancy for attention?</p><p>To address this, we propose Temporal Attentive Adversarial Adaptation Network (TA 3 N), as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, by introducing the domain attention mechanism, which assigns higher domain attention values to n-frame relation features with lower domain entropy, which correspond to higher domains discrepancy. Morevoer, we minimize the class entropy for the videos with low domain discrepancy to refine the classifier adaptation.</p><p>The overall loss of TA 3 N can be expressed as follows:</p><formula xml:id="formula_0">L = 1 NS N S i=1 L i y + 1 NS∪T N S∪T i=1 γL i ae − 1 NS∪T N S∪T i=1 (λ s L i sd + λ r L i rd + λ t L i td )<label>(1)</label></formula><p>where N S is the total number of source data, N S∪T equals the number of all data. i represents the i-th video. λ s , λ r and λ t is the trade-off weighting for each domain loss L sd , L rd and L td . γ is the weighting for the attentive entropy loss L ae . All the weightings are chosen via grid search. Our proposed TA 3 N and TADA <ref type="bibr" target="#b12">[13]</ref> both utilize entropy functions for attention but with different perspectives. TADA aims to focus on the foreground objects for image DA, while TA 3 N aims to find important and discriminative parts of temporal dynamics to align for video DA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate on three datasets: UCF-Olympic, UCF-HMDB small and UCF-HMDB f ull , as shown in <ref type="table" target="#tab_2">Table 1</ref>. The adaptation setting is noted as "Source → Target".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Setup</head><p>UCF-Olympic and UCF-HMDB small : First, we evaluate our approaches on two publicly used datasets, UCF-Olympic and UCF-HMDB small , and compare with all other works that also evaluate on these two datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref>.   UCF-HMDB f ull : To ensure large domain discrepancy to investigate DA approaches, we build UCF-HMDB f ull , which is around 3 times larger than UCF-HMDB small and UCF-Olympic, as shown in <ref type="table" target="#tab_2">Table 1</ref>. To compare with image-based DA approaches, we extend several state-ofthe-art methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> for video DA with our TemPooling and TemRelation architectures, as shown in <ref type="table" target="#tab_5">Table 3</ref>. The difference between the "Target only" and "Source only" settings is the domain used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Results and Discussion</head><p>UCF-Olympic and UCF-HMDB small : In these two datasets, our approach outperforms all the previous methods, as shown in <ref type="table" target="#tab_3">Table 2</ref> (e.g. at least 9% absolute difference on "U → H"). These results also show that the performance on these datasets is saturated. With a strong CNN as the backbone architecture, even our baseline architecture (TemPooling) can achieve high accuracy without any DA method. This suggests that these two datasets are not enough to evaluate more sophisticated DA approaches, so larger-scale datasets for video DA are needed.</p><p>UCF-HMDB f ull : It is worth noting that the "Source only" accuracy on UCF-HMDB f ull is much lower than UCF-HMDB small , which implies much larger domain discrepancy in UCF-HMDB f ull . We now answer the question in Section 2.2: Does the video DA problem benefit from encoding temporal dynamics into features? (details in <ref type="table" target="#tab_5">Table 3</ref>) For the same DA method, TemRelation outperforms TemPooling in most cases, especially for the gain value (e.g. on "U → H", "TemRelation+RevGrad" reaches 2.77% absolute accuracy gain while "TemPooling+RevGrad" only reaches 0.83% gain). By simultaneously aligning and encoding temporal dynamics, TA 2 N outperforms other approaches which are extended from more sophisticated    image-based DA methods. Finally, with the domain attention mechanism, our proposed TA 3 N reaches 78.33% (6.66% gain) on "U → H" and 81.79% (7.88% gain) on "H → U", achieving state-of-the-art performance in terms of accuracy and gain, as shown in <ref type="table" target="#tab_5">Table 3</ref>. Integration ofĜ d . Here we investigate how temporal modules affect eachĜ d without considering the attention mechanism. For the TemRelation architecture,Ĝ td integration outperformsĜ sd integration (averagely 0.58% absolute gain improvement across two tasks), while the TemPooling does not show improvement, as shown in <ref type="table" target="#tab_6">Table 4</ref>. This implies that TemRelation better encodes temporal dynamics than TemPooling. Finally, by combining allĜ d , the performance improves even more (4.20% improvement). Visualization of distribution. <ref type="figure" target="#fig_4">Figure 5</ref> shows that TA 3 N can group source data (blue dots) into denser clusters and generalize the distribution into the target domains (orange dots) as well, outperforming the baseline model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of proposed TA 3 N for video DA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Baseline architecture (TemPooling) with the adversarial discriminatorsĜ sd andĜ td . L y is the class prediction loss, and L sd and L td are the domain losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The overall architecture of the proposed TA 3 N. In the temporal relation module, time-ordered frames are used to generate K-1 n-frame relation feature representations R = {R 2 , ..., R K }. The attentive entropy loss L ae , which is calculated by domain entropy H(d) and class entropy H(ŷ), aims to enhance the certainty of those videos that are more similar across domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The comparison of t-SNE visualization. The blue and orange dots represent source and target data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The comparison of the video DA datasets.</figDesc><table><row><cell>Source → Target</cell><cell cols="2">U → O O → U</cell><cell cols="2">U → H H → U</cell></row><row><cell>W. Sultani et al. [12]</cell><cell>33.33</cell><cell>47.91</cell><cell>68.70</cell><cell>68.67</cell></row><row><cell>T. Xu et al. [14]</cell><cell>87.00</cell><cell>75.00</cell><cell>82.00</cell><cell>82.00</cell></row><row><cell>AMLS (GFK) [4]</cell><cell>84.65</cell><cell>86.44</cell><cell>89.53</cell><cell>95.36</cell></row><row><cell>AMLS (SA) [4]</cell><cell>83.92</cell><cell>86.07</cell><cell>90.25</cell><cell>94.40</cell></row><row><cell>DAAA [4]</cell><cell>91.60</cell><cell>89.96</cell><cell>-</cell><cell>-</cell></row><row><cell>TemPooling</cell><cell>96.30</cell><cell>87.08</cell><cell>98.67</cell><cell>97.35</cell></row><row><cell>TemPooling + RevGrad [2]</cell><cell>98.15</cell><cell>90.00</cell><cell>99.33</cell><cell>98.41</cell></row><row><cell>Ours (TA 2 N)</cell><cell>98.15</cell><cell>91.67</cell><cell>99.33</cell><cell>99.47</cell></row><row><cell>Ours (TA 3 N)</cell><cell>98.15</cell><cell>92.92</cell><cell>99.33</cell><cell>99.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The accuracy (%) comparison on UCF-Olympic and UCF-HMDB small (U: UCF, O: Olympic, H: HMDB).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The comparison of accuracy (%) for other approaches on UCF-HMDB f ull . The gain values are in ().</figDesc><table><row><cell>S → T</cell><cell cols="2">UCF → HMDB</cell><cell cols="2">HMDB → UCF</cell></row><row><cell>Temporal Modulê</cell><cell>TemPooling</cell><cell>TemRelation</cell><cell>TemPooling</cell><cell>TemRelation</cell></row><row><cell>G sd</cell><cell>71.11 (0.83)</cell><cell>74.44 (2.77)</cell><cell>75.13 (0.17)</cell><cell>74.44 (1.05)</cell></row><row><cell>G td</cell><cell>71.11 (0.83)</cell><cell>74.72 (3.05)</cell><cell>75.13 (0.17)</cell><cell>75.83 (1.93)</cell></row><row><cell>AllĜ d</cell><cell>71.11 (0.83)</cell><cell>77.22 (5.55)</cell><cell>75.13 (0.17)</cell><cell>80.56 (6.66)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The full evaluation of accuracy (%) for integratinĝ G d in different positions without the attention mechanism.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation in Computer Vision Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep domain adaptation in action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshad</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Deodhare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dataset Shift in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human action recognition across datasets by foreground-weighted histogram decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weirui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dual many-to-one-encoder-based transfer learning for crossdataset human action recognition. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
