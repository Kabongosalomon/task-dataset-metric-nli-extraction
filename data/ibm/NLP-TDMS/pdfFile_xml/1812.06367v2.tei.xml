<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Quality Assessment Across Multiple Actions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
							<email>parmap1@unlv.nevada.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Las Vegas</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
							<email>brendan.morris@unlv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Nevada</orgName>
								<address>
									<settlement>Las Vegas</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Quality Assessment Across Multiple Actions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Can learning to measure the quality of an action help in measuring the quality of other actions? If so, can consolidated samples from multiple actions help improve the performance of current approaches? In this paper, we carry out experiments to see if knowledge transfer is possible in the action quality assessment (AQA) setting. Experiments are carried out on our newly released AQA dataset (http: //rtis.oit.unlv.edu/datasets.html) consisting of 1106 action samples from seven actions with quality as measured by expert human judges. Our experimental results show that there is utility in learning a single model across multiple actions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action quality assessment (AQA) is the process of quantifying how well an action was performed or computing a score representing the execution quality of an action. Automatic AQA, in particular, can be an important component of many practical applications. For e.g., AQA can be employed in low-cost at-home physiotherapy to manage diseases like cerebral palsy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">27]</ref>. High quality AQA can be incorporated into medical training to assess the surgical skills of a student <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b4">5]</ref>. AQA systems can be used to mimic Olympic judges during sports training <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b28">28]</ref>; or provide a second opinion in light of recent judging scandals <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">24]</ref>. Despite having numerous potential applications, automatic AQA and skill assessment have received little attention in literature. Consequently, there is a dearth of available datasets.</p><p>Current AQA systems/frameworks <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b4">5]</ref> are concerned with measuring the quality of a single action. As such, the models are trained on examples of that particular action. However, existing AQA datasets are typically very small, consisting of only a few samples since the collection of training data requires the use of a domain expert (significant effort by comparison with action recognition). Due to this data limit, current AQA approaches may not be reaching their full performance potential.</p><p>Pre-training helps with almost every computer vision task. For e.g., object detection <ref type="bibr" target="#b6">[7]</ref> is aided by pre-training on ImageNet <ref type="bibr" target="#b3">[4]</ref>, performance of the C3D network <ref type="bibr" target="#b22">[22]</ref> was boosted by pre-training on I380K <ref type="bibr" target="#b22">[22]</ref> and then finetuning Sports-1M <ref type="bibr" target="#b10">[11]</ref>, UCF-101 <ref type="bibr" target="#b19">[19]</ref> action classification performance was enhanced when the network is pre-trained on Sports-1M <ref type="bibr" target="#b10">[11]</ref>, etc. Transfer learning is particularly helpful when target datasets are small. Inspired by several instances where pre-training has helped, we pose the following questions: are there common action quality elements (see Sec. 3.2) among different actions? If so, would it be helpful to train/pre-train a model across various actions (instead of following current approach of training on one particular action)? Can a model trained on various actions measure the quality of an unseen action?</p><p>The remaining of the paper is as follows. First, related work in AQA is reviewed in Sec. 2. Then, a new AQA dataset (the largest to date) is introduced, and notion of common action quality elements is discussed in Sec. 3. Following which, we answer previously stated questions and introduce a different approach of using all-action model, a model that learns to measure the quality of multiple actions. In Sec. 5, we discuss the experiments and results, which provide us the evidence that there is a utility in learning an all-action model over single-action model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The first major work in the area of AQA was by Pirsiavash et al. <ref type="bibr" target="#b17">[17]</ref>, which used pose (of divers and skaters) as features and learned a SVR to map those features to a quality score. By focusing on Olympic sports, they were able to use professionals to provide an objective measure of quality through the judged score. Venkataraman et al. <ref type="bibr" target="#b23">[23]</ref> extended this work by modifying the feature representation using concatenated approximate entropy of poses rather than DCT of pose. This better encoding resulted in a 4% improvement in performance. Inspired by the success of deep-learning on the task of action recognition, Parmar and Morris <ref type="bibr" target="#b15">[16]</ref> hypothesized and verified that spatiotemporal features from C3D <ref type="bibr" target="#b22">[22]</ref> capture the quality aspect of the actions well. Furthermore, they compared different video feature aggregation schemes, such as averaging clip-level features, and LSTM, to get sample-level features; and regression schemes, SVR, and fully connected layer.</p><p>A unique concept from AQA is feedback for performance improvement. Pirsiavash et al. <ref type="bibr" target="#b17">[17]</ref> were able to suggest minor pose tweaks would could have been adopted in order to improve scores. The LSTM-based approaches by Parmar and Morris <ref type="bibr" target="#b15">[16]</ref> did not offer such detailed feedback, but could identify action segments (video clips) where an error might have been made. These types of error detection mechanisms are important for coaching and faster analysis of athlete performance.</p><p>Steering away from sports, Zia et al. <ref type="bibr" target="#b28">[28]</ref> proposed an approach to measure the surgical skills in an automated fashion. In this work, a classifier is trained on frequency-domain transformed spatiotemporal interest points. Doughty et al. <ref type="bibr" target="#b4">[5]</ref> also address the problem of surgical skill assessment as well as the use of chopsticks, doughrolling, and drawing. Outside of surgery task, quality was assessed not with a score but based on pairwise comparison of skill in a "which is better" framework. While other quality or skills assessment works do exist, they tend to measure specific characteristics of the task which are not easily generalizable. An example is the basketball skills assessment work by Bertasius et al. <ref type="bibr" target="#b1">[2]</ref>, wherein they present an approach to measure a person's basketball skills from analyzing the video captured by the camera worn by the person. Firstly, atomic basketball events are detected, which are then passed through a Gaussian mixtures to obtain a features which would be representative of player's skills. A skill assessment model is finally learnt on these features to assess basketball skills. Also, it should be noted that skills assessed in <ref type="bibr" target="#b1">[2]</ref> are subjective to evaluator's preference, since annotation was based on a single basketball coach's assessment. In contrast, our approach would not be biased towards any one judge, since our annotated scores are averages of scores from individual judges (usually 5-7 judges).</p><p>All of these works use one model each for every action class. Single-acion models or action-specific models don't exploit the fact that there are some common, shared action quality concepts/elements among actions (refer to Sec. 3.2). Transfer learning is not new and has been used effectively in the literature <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. In a recent work by Abramovich and Pensky <ref type="bibr" target="#b0">[1]</ref>, it was seen that having many classes may be a blessing and not a curse for a classification purposes suggesting that similar result could be true for AQA. In this work, we evaluate to see if there are shared action quality concepts, and if we can exploit them by learning a single model on datapoints from all actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AQA-7 Dataset</head><p>To the best of our knowledge, there are only two publicly available AQA datasets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b15">16]</ref> and they have limited number of samples for each individual activity. This is partly due to the extra effort required to collect samples as compared to an action recognition dataset. In case of action recognition dataset compilation, an annotator might go to video hosting website such as YouTube and run a search query on names of actions. While in compiling an AQA dataset, annotator has to mark starting and ending frames of a sample, and note down execution score, difficulty level, final score, etc. Additionally, field experts are required to evaluate and score AQA data samples which can be quite difficult in many domains. These factors limit dataset size.</p><p>Furthermore, for action recognition, many more meaningful short samples can be produced from one original longer video. By meaningful sample, we mean that, even if you clip an action recognition video, you would still have the action class concept captured -it has been shown that action classification task maybe performed using as little as a single frame <ref type="bibr" target="#b10">[11]</ref>. Unlike action recognition, in case of AQA, a full action sequence must be examined because an error in execution can be made at any time during the action sequence. Leaving out frames where the error occurred would result in predicted scores that are not indicative of the true quality. This limits data augmentation tricks for more effective sample utilization.</p><p>To fill this data void, we introduce Action Quality Assessment 7 (AQA-7) dataset ( <ref type="figure" target="#fig_0">Fig. 1</ref>), comprising samples from seven actions: {singles diving-10m platform, gymnastic vault, big air skiing, big air snowboarding, synchronous diving-3m springboard, synchronous diving-10m platform, and trampoline} captured during Summer and Winter Olympics. Scoring rules change from time to time; however, we made sure that rules were same for the samples that we collected. A summary of the AQA-7 dataset characteristics are given in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description of Actions</head><p>Diving: Individual diving has been a Summer Olympic sport since the early 1900s and synchronized diving was introduced in 2000. Divers perform acrobatic stunts such as somersaults and twists either from a platform or a springboard. The score is based on the difficulty of and the quality of execution of the dive. AQA-7 contains three different events -the 10m platform (Diving in first row of <ref type="figure" target="#fig_0">Fig. 1</ref>), synchronized 10m platform (Sync Dive 10m in sixth row of <ref type="figure" target="#fig_0">Fig. 1</ref>), and 3m springboard (Sync Dive 3m in fifth row of <ref type="figure" target="#fig_0">Fig. 1</ref>). The three diving events have negligible view variation as they are all shot from a consistent angle for all samples. The synchronized dives have two athletes participating and their synchronization, which is given more importance than execution, is also a measure of quality of the action. The singles diving-10m platform class is an extension of the 159 samples originally from Pirsiavash et al. <ref type="bibr" target="#b17">[17]</ref> to 370 dives. Gymnastic vault: Gymvault is a gymnastic event from the Summer Olympics where the athlete runs down a runway and uses a springboard to launch themselves over the vault and perform aerial tricks. The gymvault dataset was collected from various international events in addition to Olympics. Like diving, the quality of gymvault is assessed based on the difficulty and the quality of execution. Examples tend to have large view variation both over the course of a single vault attempt (running, planting on vault, spinning in air, and landing) as well as due to differences broadcast camera placement at different events (see second row of <ref type="figure" target="#fig_0">Fig. 1</ref>). Big Air: The Ski (BigSki) and Snowboard (BigSnow) Big Air events were new events in the 2018 Winter Olympics. Samples were obtained from previous X-Games since it is the premiere venue for the event. Big Air has significant view variation between events and within an example due to camera location (rows 3 and 4 of <ref type="figure" target="#fig_0">Fig. 1</ref>). Scoring is based on four components -difficulty, execution, amplitude, and progression and landing -a more complicated formula than the other actions especially since there is a qualitative component of pushing Big Air forward by doing tricks that nobody else is doing. Note that for Snowboard Big Air the pixel coverage size of the person is similar to that of the snowboard. Trampoline: The Trampoline event was added to the Olympics in 2000. It is judged based on difficulty, execution, and time of (in air) flight. Like for diving, the camera is in a consistent side-view position but also moves up and down to follow the athlete for minimal view variation. Unlike all the other sports which consist of a single "action" performed in less than five seconds, a trampoline sample will contain multiple (10) action phases, where the athlete twists and turns, over its 20 seconds.  <ref type="table">Table 1</ref>: Characteristics of AQA-7 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Common Action Quality Elements</head><p>The AQA-7 sports have similar action elements which include flipping and twisting ( <ref type="figure" target="#fig_0">Fig. 1</ref>). As such, the quality of actions are assessed in similar fashion. For e.g. in Diving and Gymvault, judges expect athletes to have legs perfectly straight in pike position (execution quality aspect) and degree of difficulty is directly proportional to the number of twists and somersaults. Similarly, in ski and snowboard Big Air events, the difficulty is related to the number of vertical and horizontal spins. In all sports there is a expectation of a perfect landing with high impact on final score (an entry with minimal splash or 'ripping' is the diving equivalent).</p><p>The reason behind these similarities is that having to complete more number of somersaults, twists, or spins (difficulty aspect) while keeping legs straight, and having the body in tight tuck or pike position (execution quality aspect) in the limited time from take-off to landing makes it harder to achieve and, therefore, worthy of more points from judges (equivalently, higher quality). The final score is a function of execution quality aspect and the degree of difficulty. In some cases, like Diving, it will be the product function, in another case, like Gymault, it will be the summation, while other actions (BigAir events) may use a more holistic combination approach. Given the similarities in actions, it is believed that knowledge of what aspects to value in one action can help in measuring the quality in another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Approach</head><p>Although, in general, action quality may be highly dependent on the action, we hypothesize that actions do have commonalities (Sec. 3.2) that can be exploited despite individual differences such as judging criteria. However, it may be that each action is unique and do not have common quality elements and our hypothesis is wrong, and hence it would not be possible to share knowledge or learn a consistent model that can measure quality across multiple actions.</p><p>In order to test if our hypothesis is correct, and see if we can transfer action quality knowledge across actions, we propose to use the C3D-LSTM framework <ref type="bibr" target="#b15">[16]</ref>, illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. This network consists of a smaller version of C3D   <ref type="bibr" target="#b22">[22]</ref> followed by a single 256-dimensional LSTM <ref type="bibr" target="#b7">[8]</ref> layer and a fully-connected (fc) layer, which outputs the final AQA score. A video is processed in clips of 16 frames to generate C3D features at the fc6 layer which is connected to the LSTM layer for temporal feature aggregation. The C3D network is kept frozen during training so only the LSTM and final fc layer parameters are tuned. Euclidean distance between the predicted scores and true scores is used as the loss function to be minimized. The main difference in this work is rather than building an individual model for each action (we refer to these as actionspecific or single-action models), a single model is learned by training with samples from all/multiple actions (we refer to our model as all-action or multi-action models). Avg. Corr.</p><p>Pose+DCT <ref type="bibr" target="#b17">[17]</ref> 0.5300 ------Single-action C3D-SVR <ref type="bibr" target="#b15">[16]</ref> 0   <ref type="table">Table 3</ref>: Zero-shot AQA. Performance comparison of randomly-initialized model, single-action models (for e.g., first row shows the results of training on diving action measuring the quality of the remaining (unseen) action classes), and multiaction model (all-action model trained on five action classes) on unseen action classes. In multi-action class, the model is trained on five action classes and tested on the remaining action class (column-wise). In single-action model rows, diagonal entries show results of training and testing on the same action. Avg. Corr. shows the result of average (using Fisher's z-score) correlation across all columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Three different experiments are performed to test whether it is advisable to learn a joint action quality model rather than individual model. To be consistent with existing literature, Spearman's rank correlation (higher is better) is used as the performance metric. When presenting aggregated results, the average Spearman's rank correlation is computed from individual per action correlations using Fisher's z-value as described in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Data Preparation: Since different actions have different ranges for scores, as shown in <ref type="table">Table 1</ref>, we divide the raw scores of all the actions by the training standard deviation of the corresponding action. At test time, we multiply the predicted scores by the standard deviation of the appropriate action to get the final judged value. Experiments were only conducted on six of the action classes since they are of similar short length. All videos are normalized to a fixed size of 103 frames by zero-padding the first frames where needed. Trampoline is excluded from the experiments since the average length of 650 frames is much longer and composed of multiple "tricks." The model is trained using 803 videos, and tested on the remaining 303 videos. During training, temporal data augmentation is used to have six different copies of the same video sample with one frame start difference (effectively 4818 training samples).</p><p>Implementation details: Caffe <ref type="bibr" target="#b9">[10]</ref> was used to implement our model on a Titan-X GPU. The C3D network was pretrained on UCF-101 <ref type="bibr" target="#b19">[19]</ref> for 100k iterations with an initial learning rate of 0.001 and annealed by a factor of 10 after every 40k iterations, momentum of 0.9, and a weight   For the LSTM layer, the ADAM solver <ref type="bibr" target="#b11">[12]</ref> is used with an initial learning rate of 0.001 and annealed by a factor of 2 after every 3K iterations. Optimization was carried out for 20K iterations after the LSTM layer was initialized with Gauassian noise with standard deviation of 0.1. Using a stride of 16 frames, the C3D network "sees" a whole action sequence in 6 LSTM steps. We use a batch size of 90 clips (15 full video samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">All-Action vs. Single-Action Models</head><p>In the first experiment, rather than learning a model using only samples from a single action, our All-action model is learned using diverse and larger dataset with samples from across six actions. A comparison of the all-action model with other state-of-the-art approaches is given in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Pirsiavash et al. <ref type="bibr" target="#b17">[17]</ref> optimized their pose estimator for each action (diving, figure-skating) and therefore cannot be compared with actions other than diving fairly, so we pro-vide their results only on diving. The two other comparisons, C3D-SVR and C3D-LSTM, are the state-of-the-art single-action (action-specific) results from Parmar and Morris <ref type="bibr" target="#b15">[16]</ref>. Pose+DCT <ref type="bibr" target="#b17">[17]</ref> and C3D-SVR <ref type="bibr" target="#b15">[16]</ref> use SVR as regression module. The baseline for the All-action model is the single-action C3D-LSTM since both use same feature aggregation method (LSTM) and regression module (fc layer).</p><p>It is clear that the use of spatio-temporal features (C3D net, fc6 layer activations) improves over Pose+DCT. The proposed all-action model outperforms the single-action model for five of the six actions -all but for Snowboarding which was less by 0.14. On average across the actions, the all-action model improves Spearman's rank correlation performance by 0.03 without changing the network and instead leveraging data samples from all actions.</p><p>The 0.6478 correlation value from all-action model is competitive even with action-specific C3D-SVR's 0.6937 (six different C3D-SVR's are used, one for each action). This is noteworthy since C3D-SVR was the best performing AQA system in <ref type="bibr" target="#b15">[16]</ref> and, as they mentioned, LSTM aggregation is preferred for its ability to temporally localize action quality drops (error identification).</p><p>Note that our strategy to train a single model using datapoints from all the actions is complementary to the existing approaches and employing it may help improve their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Zero-Shot AQA</head><p>One open question for AQA is whether the learned concept of quality can generalize. If action quality concepts are shared among actions, then the knowledge of how to measure the quality of a group of actions, is likely to help in measuring the quality of other, unseen action/s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Random-initialization vs.</head><p>Multi-action pretraining While the previous experiment showed that all-action C3D-LSTM model performed better than the single-action counterpart, it did not necessarily indicate that learning one action helped to learn another action. To address this issue, a zero-shot AQA experiment is designed. In the zero-shot setting, a model is trained on five actions and tested on the remaining unseen sixth action. The baseline for comparison is the same network with random LSTM weights (C3D weights remain same for all models). Comparing multi-action model with randomly-initialized weights may not seem very fair, but interestingly, a hierarchy of random, untrained convolution filters have been shown to perform almost as well as learned filter weights in a work by Jarrett et al. <ref type="bibr" target="#b8">[9]</ref>. It is assumed that if innate quality concepts were not learned, then the resulting multi-action model (an all-action model trained using five action classes) would have similar performance to random initialization. If instead the multi-action model is much better than random initialization on the zero-shot AQA task then it indicates that there are common quality elements across actions and that there is utility in pre-training a model across multiple actions.</p><p>A summary of AQA performance on an unseen action is detailed in <ref type="table">Table 3</ref>. With random initialization the AQA system is not able to perform with any reliability, as indicated by Spearman's rank correlation close to zero value. In contrast, the all-action version shows some positive correlation. We believe that the reason for better performance of the all-action model is that the use of multiple actions provides a good initialization since there are common/shared action quality elements. In addition, with all-action model there is an advantage of having access to more training videos from which to learn. Although, our all-action works better than random-initialization across four actions, there are two actions -Gymvault, and Skiing -that seem to be providing very weak indication. So, we explore further (refer Sec. 5.3) in order to be sure that our proposed all-action actually attains a good initialization, in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Single-Action vs. Multi-Action Transfer</head><p>To further examine knowledge transfer, a sub-experiment is conducted to assess the performance of a single-action model on the unseen action classes. The results are compiled in middle six rows of <ref type="table">Table 3</ref>. Note: single-action pre-training is actually a special case of our proposed multiaction pre-training.</p><p>When examining the single-action results, note they resemble a confusion matrix with high diagonal elements (matched training and testing data) but with low offdiagonals (mismatched test classes to training class). The single-action models tend to perform well on very similar actions e.g. Diving on Sync. Dive 3m/10m, or vice-versa, or Snowboarding on Skiing, and vice-versa. This result is intuitive since similar actions have more shared concepts. However, non-intuitive relationships are apparent as well. An example is the Skiing model tested on Diving, which have quite different judging criteria yet still manages significant rank correlation. While no one single-action model is strong across all classes it does support the idea of shared quality concepts across actions.</p><p>On average, the All-action model greatly outperforms any single-action model. With more action classes included in training, more quality concepts/elements are shared and there are more datapoints from which to learn each of the elements of quality resulting in better learning of the concepts. Also, the ratio of the overall number of quality concepts to the total number of datapoints is reduced due to more sharing of elements. In other words, as we increase our action bank, by including more actions, chances of sharing quality elements with an unseen action increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Fine-tuning to a Novel Action Class</head><p>In the final experiment, fine-tuning is used to adjust a pre-trained model for a new unseen action. The experiment considers the situation where there are limited number of samples in the dataset of a newly considered action to learn with (i.e. it is difficult to obtain many labeled samples as might be the case for an obscure action).</p><p>In particular, the all-action model is pre-trained on five actions and fine-tuned on the remaining unseen action with minimal datapoints. For the more data rich actions <ref type="table">(</ref>  <ref type="figure" target="#fig_4">Figure 3</ref> shows Spearman's rank correlation performance at every hundred training iterations on the test set for each of the different training set sizes for all the actions. Finetuning from multi-action pretrained model is shown by blue-colored curves while red curves represent the result of finetuning from scratch (i.e. random initialization). Overall, the blue curve is above the red in most situations. Further, the all-action blue curve quickly reaches a high value after only a few tens of iterations. The random-initialization red curve takes longer to gradually update weights and climb from a low starting value. <ref type="table" target="#tab_4">Table 4</ref> provides the the best performance during training where we see that all-action is better in 16 of the 18 cases.</p><p>Note that even in the case of Gymvault and Skiing which seemed to have poor all-action initialization (from the zeroshot AQA experiment in <ref type="table">Table 3</ref>) the performance quickly climbs during fine-tuning. Deep networks don't typically have smooth error surfaces. So, it could be the case that there are valleys near good solution in cases of Gymvault and Skiing; and during finetuning, a good solution was reached because relatively higher initial learning rate of 0.001 helped in escaping local minima. Individually setting hyperparameters for each individual action may further enhance performance but is outside the scope of this work. Instead the emphasis here is on the ability to learn generalizable quality concepts by pre-training on multiple actions and providing a better initialization for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion</head><p>The key outcome of this experimental evaluation was that learning AQA models by learning from multiple actions is advisable. The experiments showed that there are quality concepts shared among actions that can be learned. In addition, another advantage of using the multi-action training procedure is enhanced scalability. We also followed a simple rule to set the hyperparameters. Moreover, pre-training with datapoints from all actions provides a better initialization point for faster convergence with the need for fewer training samples when fine-tuning for a new action. One experiment that was not performed is using the all-action model for pre-training and fine-tuning for a single action. However, this is not advisable since the data has already been incorporated into the all-action model and may lead to over-fitting.</p><p>Actions that we have considered in this work, although from different sports, have common action elements which is a condition necessary for the transfer to work. We have not yet explored transferring or sharing knowledge among actions from different domains. For e.g., our approach is not intended to share knowledge between a surgery task and a completely different task like gymnastic vault. To be able to apply our approach, tasks should be from same domains. In the surgery skills domain, one should consider sharing knowledge between tasks like Knot-tying, Needle-passing and Suturing. We are looking forward to exploring crossdomain knowledge sharing in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work demonstrates that like many other computer vision tasks, such as object classification or action recognition, action quality assessment (AQA) can benefit from knowledge transfer/sharing by training a shared model across samples from multiple actions. We experimentally demonstrated this on our newly introduced dataset, AQA-7. The experiments showed: 1) that by considering multiple actions, limited per-action data is better leveraged for improved per action performance (leveraging data more efficiently), 2) multi-action pre-training provides better initialization for novel actions, hinting at an underlying consistency in the notion of quality in actions (demonstrating that all-action model is generalizable than single-action models). The results hint at the potential to extend AQA beyond sports scenarios into more general actions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Preview of our dataset. To see the videos play, please download the manuscript and view in an Adobe Reader.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>C3D-LSTM network network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>To answer our fundamental questions regarding quality elements shared across actions, three different experiments are designed: 1) to check if it is possible to learn an allaction, and if so, compare the performance of our proposed all-action model against action-specific model (Sec. 5.1), 2) to evaluate how well can an all-action model quantify the quality of unseen action classes (Sec. 5.2), and 3) to evaluate the generalization of all-action model to novel action classes (Sec. 5.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>S.D 3m -15 (n) S.D 3m -25 (o) S.D 3m -35 (p) S.D 10m -15 (q) S.D 10m -25 (r) S.D 10m -35 Finetuning from scratch vs. finetuning from pre-trained multi-action model. Plot of Spearman's rank correlation against every hundred iterations for different number of training samples. Blue and red curves represent multi-action and randomly initialized models, respectively. The gap in the initial iterations suggest that good initialization of LSTM weights was achieved by training on multiple actions. In most of the cases, multi-action model has better performance than randomly initialized model on test samples throughout all the iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>All-Action vs. Single-Action models. Performance evaluation of single-action and all-action models in terms of action-wise and average Spearman's rank correlation (higher is better). First two frameworks simply average features to aggregate them and use SVR as the regression module. The bottom two frameworks use LSTM to aggregate features and use a fully-connected layer as the regression module. Our approach can be directly compared with single-action C3D-LSTM<ref type="bibr" target="#b15">[16]</ref>, since both have the same architecture.</figDesc><table><row><cell cols="2">Unseen action</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell>class Diving</cell><cell>Gym-vault</cell><cell>Skiing</cell><cell>Snow-board</cell><cell>Sync-Dive 3m</cell><cell>Sync-Dive 10m</cell><cell>Avg. Corr.</cell></row><row><cell>action class</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random Wts./Ini.</cell><cell>0.0590</cell><cell cols="3">0.0280 -0.0602 -0.0703</cell><cell>-0.0146</cell><cell cols="2">-0.0729 -0.0218</cell></row><row><cell>Diving</cell><cell cols="2">0.6997 -0.0162</cell><cell>0.0425</cell><cell>0.0172</cell><cell>0.2337</cell><cell>0.0221</cell><cell>0.0599</cell></row><row><cell>Gymvault</cell><cell>0.0906</cell><cell>0.8472</cell><cell>0.0517</cell><cell>0.0418</cell><cell>-0.1642</cell><cell cols="2">-0.3200 -0.0600</cell></row><row><cell>Skiing</cell><cell cols="2">0.2653 -0.1856</cell><cell>0.6711</cell><cell>0.1807</cell><cell>0.1195</cell><cell>0.2858</cell><cell>0.1331</cell></row><row><cell>Snowboard</cell><cell cols="2">0.2115 -0.2154</cell><cell>0.3314</cell><cell>0.6294</cell><cell>0.0945</cell><cell>0.1818</cell><cell>0.1208</cell></row><row><cell>Sync. Dive 3m</cell><cell cols="4">0.1500 -0.0066 -0.0494 -0.1102</cell><cell>0.8084</cell><cell>0.0428</cell><cell>0.0053</cell></row><row><cell>Sync. Dive 10m</cell><cell cols="2">0.0767 -0.1842</cell><cell>0.0679</cell><cell>0.0360</cell><cell>0.4374</cell><cell>0.7397</cell><cell>0.0868</cell></row><row><cell>Multi-action</cell><cell>0.2258</cell><cell>0.0538</cell><cell>0.0139</cell><cell>0.2259</cell><cell>0.3517</cell><cell>0.3512</cell><cell>0.2037</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Finetuning from scratch vs. finetuning from pre-trained multi-action model. Experimental results (Spearman's rank correlation) of finetuning a randomly-initialized (RI) model and an all-action (AA) model pre-trained on five action classes. The numbers represent the best results from all the iterations.</figDesc><table><row><cell>decay of 0.005. After pre-training, C3D is frozen and used</cell></row><row><cell>as a feature extractor.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Diving, Gymvault, Skiing, and Snowboarding) {25, 75, 125} training samples are used, while for data poor (Sync. Dive 3m/10m) only {15, 25, 35} training samples are used for fine-tuning. Testing is performed on 50 of the remaining samples for all actions. The settings are the same for this experiment except the hyperparameters are directly proportional to the training set size -a very simple rule. The learning rate step size is adjusted as {100, 300, 500} and training iterations of {500, 1200, 2000} for the training set size of {25, 75, 125} respectively. Similar scaling was performed for the {15, 35} training sizes as well.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We would like to thank Nicolas Gomez, Ryan Callier, and Cameron Carmendy for helping us with dataset collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Classification with many classes: challenges and pluses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Abramovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Am i a baller? basketball performance assessment from first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2196" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Olympic figure skating controversy: Judging system is most to blame for uproar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-J</forename><surname>Borzilleri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Who&apos;s better, who&apos;s best: Skill determination in video using deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09913</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A tutorial on correlation coefficients</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regionbased convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition? In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Label efficient learning of transferable representations acrosss domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="164" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crossstitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Measuring the quality of exercises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 38th Annual International Conference of the</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2241" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Assessing the quality of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="556" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminability-based transfer between neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="204" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sharing visual features for multiclass and multiview object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="854" to="869" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dynamical regularity for action analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">List of olympic games scandals and controversies -Wikipedia, the free encyclopedia</title>
	</analytic>
	<monogr>
		<title level="m">Wikipedia contributors</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multivariate analysis and machine learning in cerebral palsy research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neurology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated video-based assessment of surgical skills for training and evaluation in medical schools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ploetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1623" to="1636" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
