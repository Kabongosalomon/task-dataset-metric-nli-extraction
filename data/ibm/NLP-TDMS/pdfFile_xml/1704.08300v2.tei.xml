<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversity driven Attention Model for Query-based Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preksha</forename><surname>Nema</surname></persName>
							<email>preksha@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
							<email>miteshk@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Laha</surname></persName>
							<email>anirlaha@in.ibm.comravi@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversity driven Attention Model for Query-based Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="sl">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>arXiv:1704.08300v2 [cs.CL]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years neural models based on the encode-attend-decode <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>) paradigm have shown great success in various natural language generation (NLG) tasks such as machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>, abstractive summarization <ref type="bibr">((Rush et al., 2015)</ref>, <ref type="bibr" target="#b13">(Nallapati et al., 2016)</ref>) dialog <ref type="bibr" target="#b10">(Li et al., 2016)</ref>, etc. One such NLG problem which has not received enough attention in the past is query based abstractive text summarization where the aim is to generate the summary of a document in the context of a query. In general, abstractive summarization, aims to cover all the salient points of a document in a compact and coherent fashion. On the other hand, query focused summarization highlights those points that are relevant in the context of the query. Thus given a document on "the super bowl", the query "How was the half-time show?", would result in a summary that would not cover the actual game itself.</p><p>Note that there has been some work on query based extractive summarization in the past where the aim is to simply extract the most salient sentence(s) from a document and treat these as a summary. There is no natural language generation involved. Since, we were interested in abstractive (as opposed to extractive) summarization we created a new dataset based on debatepedia. This dataset contains triplets of the form (query, document, summary). Further, each summary is abstractive and not extractive in the sense that the summary does not necessarily comprise of a sentence which is simply copied from the original document.</p><p>Using this dataset as a testbed, we focus on a recurring problem in models based on the encode-attend-decode paradigm. Specifically, it is observed that the summaries produced by such models contain repeated phrases. <ref type="table">Table 1</ref> shows a few such examples of summaries gener-Document Snippet: The "natural death" alternative to euthanasia is not keeping someone alive via life support until they die on life support. That would, indeed, be unnatural. The natural alternative is, instead, to allow them to die off of life support. Query: Is euthanasia better than withdrawing life support (non-treatment)? Ground Truth Summary: The alternative to euthanasia is a natural death without life support. Predicted Summary: the large to euthanasia is a natural death life life use Document Snippet: Legalizing same-sex marriage would also be a recognition of basic American principles, and would represent the culmination of our nation's commitment to equal rights. It is, some have said, the last major civil-rights milestone yet to be surpassed in our two-century struggle to attain the goals we set for this nation at its formation. Query: Is gay marriage a civil right? Ground Truth Summary: Gay marriage is a fundamental equal right. Predicted Summary: gay marriage is a appropriate right right <ref type="table">Table 1</ref>: Examples showing repeated words in the output of encoder-decoder models ated by such a model when trained on this new dataset. This problem has also been reported by <ref type="bibr" target="#b1">(Chen et al., 2016)</ref> in the context of summarization and by <ref type="bibr" target="#b19">(Sankaran et al., 2016)</ref> in the context of machine translation.</p><p>We first provide an intuitive explanation for this problem and then propose a solution for alleviating it. A typical encode-attend-decode model first computes a vectorial representation for the document and the query and then produces a contextual summary one word at a time. Each word is produced by feeding a new context vector to the decoder at each time step by attending to different parts of the document and query. If the decoder produces the same word or phrase repeatedly then it could mean that the context vectors fed to the decoder at these time steps are very similar.</p><p>We propose a model which explicitly prevents this by ensuring that successive context vectors are orthogonal to each other. Specifically, we subtract out any component that the current context vector has in the direction of the previous context vector. Notice that, we do not require the current context vector to be orthogonal to all previous context vectors but just its immediate predecessor. This enables the model to attend to words repeatedly if required later in the process. To account for the complete history (or all previous context vectors) we also propose an extension of this idea where we pass the sequence of context vectors through a LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> and ensure that the current state produced by the LSTM is orthogonal to the history. At each time step, the state of the LSTM is then fed to the decoder to produce one word in the summary.</p><p>Our contributions can be summarized as follows: (i) We propose a new dataset for query based abstractive summarization and evaluate encode-attend-decode models on this dataset (ii) We study the problem of repeating phrases in NLG in the context of this dataset and propose two solutions for countering this problem. We show that our method outperforms a vanilla encoder-decoder model with a gain of 28% (absolute) in ROUGE-L score (iii) We also demonstrate that our method clearly outperforms a recent state of the art method proposed for handling the problem of repeating phrases with a gain of 7% (absolute) in ROUGE-L scores (iv) We do a qualitative analysis of the results and show that our model indeed produces outputs with fewer repetitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Summarization has been studied in the context of text ( <ref type="bibr" target="#b12">(Mani, 2001)</ref>, <ref type="bibr" target="#b4">(Das and Martins, 2007)</ref>, <ref type="bibr" target="#b14">(Nenkova and McKeown, 2012)</ref>) as well as speech <ref type="bibr" target="#b21">((Zhu and Penn, 2006)</ref>, <ref type="bibr" target="#b22">(Zhu et al., 2009)</ref>). A vast majority of this work has focused on extractive summarization where the idea is to construct a summary by selecting the most relevant sentences from the document ( <ref type="bibr" target="#b15">(Neto et al., 2002)</ref>, <ref type="bibr" target="#b5">(Erkan and Radev, 2004)</ref>, <ref type="bibr" target="#b6">(Filippova and Altun, 2013)</ref>, <ref type="bibr" target="#b3">(Colmenares et al., 2015)</ref>, <ref type="bibr" target="#b17">(Riedhammer et al., 2010)</ref>, <ref type="bibr" target="#b16">(Ribeiro et al., 2013)</ref>). There has been some work on abstractive summarization in the context of DUC-2003 and DUC-2004 contests <ref type="bibr">(Zajic et al.)</ref>. We refer the reader to <ref type="bibr" target="#b4">(Das and Martins, 2007)</ref> and <ref type="bibr" target="#b14">(Nenkova and McKeown, 2012)</ref> for an excellent survey of the field.</p><p>Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>. For example, <ref type="bibr">(Rush et al., 2015)</ref>, report state of the art results on the Gi-gaWord and DUC corpus using such a model. Similarly, the work of <ref type="bibr" target="#b11">Lopyrev (2015)</ref> uses neural networks to generate news headline from short news stories. <ref type="bibr" target="#b2">Chopra et al. (2016)</ref> extend the work of <ref type="bibr">Rush et al. (2015)</ref> and report further improvements on the two datasets. <ref type="bibr" target="#b8">Hu et al. (2015)</ref> introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it.</p><p>One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency). <ref type="bibr" target="#b19">Sankaran et al. (2016)</ref> study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other. Similarly, and more relevant to this work, <ref type="bibr" target="#b1">Chen et al. (2016)</ref> propose a distraction based attention model which maintains a history of attention vectors and context vectors. It then subtracts this history from the current attention and context vector. When evaluated on our dataset their method performs poorly. This could be because their method is very aggressive in dealing with the history (as explained later in the Experiments section). On the other hand, our method has a better way of handling history (by passing context vectors through an LSTM recurrent network) which gives us the flexibility to forget/retain some portions of the history and at the same time produce diverse context vectors at successive time steps.</p><p>We evaluate our method in the context of query based abstractive summarization -a problem which has received almost no attention in the past due to unavailability of datasets. We create a new dataset for this task and show that our method indeed produces better output by reducing the number of repeated phrases produced by encoder decoder models.  As mentioned earlier, there are no existing datasets for query based abstractive summarization. We create such a dataset from Debatepedia an encyclopedia of pro and con arguments and quotes on critical debate topics. There are 663 debates in the corpus (we have considered only those debates which have at least one query with one document). These 663 debates belong to 53 overlapping categories such as Politics, Law, Crime, Environment, Health, Morality, Religion, etc. A given topic can belong to more than one category. For example, the topic "Eye for an Eye philosophy" belongs to both "Law" as well as "Morality". The average number of queries per debate is 5 and the average number of documents per query is 4. Please refer to the dataset url 1 for more details about number of debates per category.</p><p>For example, <ref type="figure">Figure 1</ref> shows the queries associated with the topic "Algae Biofuel". It also lists the set of documents and an abstractive summary associated with each query. As is obvious from the example, the summary is an abstractive summary and not extracted directly from the document. We crawled 12695 such {query, document, summary} triples from debatepedia (these were all the triples that were available). <ref type="table" target="#tab_1">Table 2</ref> reports the average length of the query, summary and documents in this dataset.</p><p>We used 10 fold cross validation for all our experiments. Each fold uses 80% of the documents for training, 10% for validation and 10% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed model</head><p>Given a query q = q 1 , q 2 , ..., q k containing k words, a document d = d 1 , d 2 , ..., d n containing n words, the task is to generate a contextual summary y = y 1 , y 2 , ..., y m containing m words. This can be modeled as the problem of finding a y * that maximizes the probability p(y|q, d) which can be further decomposed as:</p><formula xml:id="formula_0">y * = arg max y m t=1 p(y t |y 1 , ..., y t−1 , q, d) (1)</formula><p>We now describe a way of modeling p(y t |y 1 , ..., y t−1 , q, d) using the neural encoderattention-decoder paradigm.</p><p>The proposed model contains the following components: (i) an encoder RNN for the query (ii) an encoder RNN for the document (iii) attention mechanism for the query (iv) attention mechanism for the document and (v) a decoder RNN. All the RNNs use a GRU cell. Encoder for the query: We use a recurrent neural network with Gated Recurrent Units (GRU) for encoding the query. It reads the query q = q 1 , q 2 , ..., q k from left to right and computes a hidden representation for each time-step as:</p><formula xml:id="formula_1">h q i = GRU q (h q i−1 , e(q i ))<label>(2)</label></formula><p>where e(q i ) ∈ R d is the d-dimensional embedding of the query word q i . Encoder for the document: This is similar to the query encoder and reads the document d = d 1 , d 2 , ..., d n from left to right and computes a hidden representation for each time-step as:</p><formula xml:id="formula_2">h d i = GRU d (h d i−1 , e(d i ))<label>(3)</label></formula><p>where e(d i ) ∈ R d is the d-dimensional embedding of the document word d i .</p><p>Attention mechanism for the query : At each time step, the decoder produces an output word by focusing on different portions of the query (document) with the help of a query (document) attention model. We first describe the query attention model which assigns weights α q t,i to each word in the query at each decoder timestep using the following equations.</p><formula xml:id="formula_3">a q t,i = v T q tanh(W q s t + U q h q i ) (4) α q t,i = exp(a q t,i ) k j=1 exp(a q t,j )<label>(5)</label></formula><p>where s t is the current state of the decoder at time step t (we will see an exact formula for this soon). W q ∈ R l 2 ×l 1 , U q ∈ R l 2 ×l 2 , v q ∈ R l 2 , l 1 is the size of the decoder's hidden state, l 2 is both the size of h q i and also the size of the final query representation at time step t, which is computed as:</p><formula xml:id="formula_4">q t = k i=1 α q t,i h q i<label>(6)</label></formula><p>Attention mechanism for the document : We now describe the document attention model which assigns weights to each word in the document using the following equations.</p><formula xml:id="formula_5">a d t,i = v T d tanh(W d s t + U d h d i + Zq t ) (7) α d t,i = exp(a d t,i ) n j=1 exp(a d t,j )</formula><p>where s t is the current state of the decoder at time step t (we will see an exact formula for this soon). W d ∈ R l 4 ×l 1 , U d ∈ R l 4 ×l 4 , Z ∈ R l 4 ×l 2 , v d ∈ R l 2 , l 4 is the size of h d i and also the size of the final document representation d t which is passed to the decoder at time step t as:</p><formula xml:id="formula_6">d t = n i=1 α d t,i h d i<label>(8)</label></formula><p>Note that d t now encodes the relevant information from the document as well as the query (see Equation <ref type="formula">(7)</ref>) at time step t. We refer to this as the context vector for the decoder.</p><p>Decoder: The hidden state of the decoder s t at each time t is again computed using a GRU as follows:</p><formula xml:id="formula_7">s t = GRU dec (s t−1 , [e(y t−1 ), d t−1 ]) (9)</formula><p>where, y t−1 gives a distribution over the vocabulary words at timestep t − 1 and is computed as:</p><formula xml:id="formula_8">y t = softmax(W o f (W dec s t + V dec d t )) (10) where W o ∈ R N ×l 1 , W dec ∈ R l 1 ×l 1 , V dec ∈ R l 1 ×l 4 ,</formula><p>N is the vocabulary size, y t is the final output of the model which defines a probability distribution over the output vocabulary. This is exactly the quantity defined in Equation <ref type="formula">(1)</ref> that we wanted to model (p(y t |y 1 , ..., y t−1 , q, d)).</p><p>Further, note that, e(y t−1 ) is the d-dimensional embedding of the word which has the highest probability under the distribution y t−1 . Also [e(y t−1 ), d t−1 ] means a concatenation of the vectors e(y t−1 ), d t−1 . We chose f to be the identity function. The model as described above is an instantiation of the encoder-attention-decoder idea applied to query based abstractive summarization. As mentioned earlier (and demonstrated later through experiments), this model suffers from the problem of repeating the same phrase/word in the output. We now propose a new attention model which we refer to as diversity based attention model to address this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Diversity based attention model</head><p>As hypothesized earlier, if the decoder produces the same phrase/word multiple times then it is possible that the context vectors being fed to the decoder at consecutive time steps are very similar. We propose four models (D 1 , D 2 , SD 1 , SD 2 ) to directly address this problem. D 1 : In this model, after computing d t as described in Equation <ref type="formula" target="#formula_6">(8)</ref>, we make it orthogonal to the context vector at time t − 1:</p><formula xml:id="formula_9">d t = d t − d T t d t−1 d T t−1 d t−1 d t−1<label>(11)</label></formula><p>SD 1 : The above model imposes a hard orthogonality constraint on the context vector(d t ).</p><p>We also propose a relaxed version of the above model which uses a gating parameter. This gating parameter decides what fraction of the previous context vector should be subtracted from the current context vector using the following equations:</p><formula xml:id="formula_10">γ t = W g d t−1 + b g d t = d t − γ t d T t d t−1 d T t−1 d t−1 d t−1</formula><p>where W g ∈ R l 4 ×l 4 , b g ∈ R l 4 , l 4 is the dimension of d t as defined in equation (8). D 2 : The above model only ensures that the current context vector is diverse w.r.t the previous context vector. It ignores all history before time step t − 1. To account for the history, we treat successive context vectors as a sequence and use a modified LSTM cell to compute the new state at each time step. Specifically, we use the following set of equations to compute a diverse context at time t: <ref type="formula" target="#formula_6">(8)</ref>; l 5 is number of hidden units in the LSTM cell. This final d t from Equation <ref type="formula" target="#formula_2">(13)</ref> is then used in Equation <ref type="formula">(9)</ref>. Note that Equation <ref type="formula" target="#formula_1">(12)</ref> ensures that state of the LSTM at time step t is orthogonal to the previous history. <ref type="figure" target="#fig_1">Figure 3</ref> shows a pictorial representation of the model with a diversity LSTM cell. SD 2 : This model again uses a relaxed version of the orthogonality constraint used in D 2 . Specifically, we define a gating parameter g t and replace (12) above by (14) as define below:</p><formula xml:id="formula_11">i t = σ(W i d t + U i h t−1 + b i ) f t = σ(W f d t + U f h t−1 + b f ) o t = σ(W o d t + U o h t−1 + b o ) c t = tanh(W c d t + U c h t−1 + b c ) c t = i t ĉ t + f t c t−1 c diverse t = c t − c t T c t−1 c T t−1 c t−1 c t−1 (12) h t = o t tanh(c diverse t ) d t = h t (13) where W i , W f , W o , W c ∈ R l 5 ×l 4 , U i , U f , U o , U c ∈ R l 5 ×l 4 , d t is the l 4 - dimensional output of Equation</formula><formula xml:id="formula_12">g t = σ(W g d t + U g h t−1 + b o ) c diverse t = c t − g t c t T c t−1 c T t−1 c t−1 c t−1<label>(14)</label></formula><p>where W g ∈ R l 5 ×l 4 , U g ∈ R l 5 ×l 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baseline Methods</head><p>We compare with two recently proposed baseline diversity methods <ref type="bibr" target="#b1">(Chen et al., 2016)</ref> as described below. Note that these methods were proposed in the context of abstractive summarization (not query based abstractive summarization) and we adapt them for the task of query based abstractive summarization. Below we just highlight the key differences from our model in computing the context vector d t passed to the decoder. M1: This model accumulates all the previous context vectors as t−1 j=1 d j and incorporates this history while computing a diverse context vector:</p><formula xml:id="formula_13">d t = tanh(W c d t − U c t−1 j=1 d j )<label>(15)</label></formula><p>where W c , U c ∈ R l 4 ×l 4 are diagonal matrices. We then use this diversity driven context d t in Equation <ref type="formula">(9)</ref> and <ref type="formula">(10)</ref>.</p><p>M2: In this model, in addition to computing a diverse context as described in Equation <ref type="formula" target="#formula_3">(15)</ref>, the attention weights at each time step are also forced to be diverse from the attention weights at the previous time step.</p><formula xml:id="formula_14">α t,i = v T a tanh(W a s t + U a d t − b a t−1 j=1 α j,i ) where W a ∈ R l 1 ×l 1 , U a ∈ R l 1 ×l 4 , b a , v a ∈ R l 1 ,</formula><p>l 1 is the number of hidden units in the decoder GRU. Once again, they maintain a history of attention weights and compute a diverse attention vector by subtracting the history from the current attention vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>We evaluate our models on the dataset described in section 3. Note that there are no prior baselines on query based abstractive summarization so we could only compare with different variations of the encoder decoder models as described above. Further, we compare our diversity based attention models with existing models for diversity by suitably adapting them to this problem as described earlier. Specifically, we compare the performance of the following models:</p><p>• Vanilla e-a-d: This is the vanilla encoderattention-decoder model adapted to the problem of abstractive summarization. It contains the following components (i) document encoder (ii) document attention model (iii) decoder. It does not contain an encoder or attention model for the query. This helps us understand the importance of the query. • Query enc : This model contains the query encoder in addition to the three components used in the vanilla model above. It does not contain any attention model for the query.</p><p>• Query att : This model contains the query attention model in addition to all the components in Query enc .</p><p>• D 1 : The diversity attention model as described in Section 4.1.</p><p>• D 2 : The LSTM based diversity attention model as described in Section 4.1.</p><p>• SD 1 : The soft diversity attention model as described in Section 4.1</p><p>• SD 2 : The soft LSTM based diversity attention model as described in Section 4.1</p><p>• B 1 : Diversity cell in Figure3 is replaced by the basic LSTM cell (i.e. c diverse t = c t instead of using Equation <ref type="formula" target="#formula_1">(12)</ref>. This helps us understand whether simply using an LSTM to track the history of context vectors (without imposing a diversity constraint) is sufficient.</p><p>• M 1 : The baseline model which operates on the context vector as described in Section 5.</p><p>• M 2 : The baseline model which operates on the attention weights in addition to the context vector as described in Section 5.</p><p>We used 80% of the data for training, 10% for validation and 10% for testing. We create 10 such folds and report the average Rouge-1, Rouge-2, Rouge-L scores across the 10 folds. The hyperparameters (batch size and GRU cell sizes) of all the models are tuned on the validation set. We tried the following batch sizes : 32, 64 and the following GRU cell sizes 200, 300, 400. We used Adam <ref type="bibr" target="#b9">(Kingma and Ba, 2014)</ref> as the optimization algorithm with the initial learning rate set to 0.0004, β 1 = 0.9, β 2 = 0.999. We used pre-trained publicly available Glove word embeddings 2 and fine-tuned them during training. The same word embeddings are used for the query words and the document words. <ref type="table" target="#tab_3">Table 3</ref> summarizes the results of our experiments.  In this section, we discuss the results of the experiments reported in <ref type="table" target="#tab_3">Table 3</ref>. 1. Effect of Query: Comparing rows 1 and 2 we observe that adding an encoder for the query and allowing it to influence the outputs of the decoder indeed improves the performance. This is expected as the query contains some keywords which could help in sharpening the focus of the summary. 2. Effect of Query attention model: Comparing rows 2 and 3 we observe that using an attention model to dynamically compute the query representation at each time step improves the results. This suggests that the attention model indeed learns to focus on relevant portions of the query at different time steps. 3. Effect of Diversity models: All the diversity models introduced in the paper <ref type="bibr">(rows 7, 8, 9, 10)</ref> give significant improvement over the nondiversity models. In particular, the modified LSTM based diversity model gives the best results. This is indeed very encouraging and <ref type="table">Table  4</ref> shows some sample summaries comparing the performance of different models. 4. Comparison with baseline diversity models: The baseline diversity model M1 performs at par with our models D1 and SD1 but not as good as D2 and SD2. However, the model M2 performs very poorly. We believe that simultaneously adding a constraint on the context vectors as well as attention weights (as is indeed the case with M2) is a bit too aggressive and leads to poor performance (although this needs further investigation). 5. Quantitative Analysis: In addition to the qualitative analysis reported in <ref type="table">Table 4</ref> we also did a quantitative analysis by counting the num-Source:Although cannabis does indeed have some harmful effects, it is no more harmful than legal substances like alcohol and tobacco. As a matter of fact, research by the British Medical Association shows that nicotine is far more addictive than cannabis. Furthermore, the consumption of alcohol and the smoking of cigarettes cause more deaths per year than does the use of cannabis (e.g. through lung cancer, stomach ulcers, accidents caused by drunk driving etc.). The legalization of cannabis will remove an anomaly in the law whereby substances that are more dangerous than cannabis are legal whilst the possession and use of cannabis remains unlawful. Query: is marijuana harmless enough to be considered a medicine G: marijuana is no more harmful than tobacco and alcohol Query attn : marijuana is no the drug drug for tobacco and tobacco D1: marijuana is no more harmful than tobacco and tobacco SD1: marijuana is more for evidence than tobacco and health D2: marijuana is no more harmful than tobacco and use SD2: marijuana is no more harmful than tobacco and alcohol Source:Fuel cell critics point out that hydrogen is flammable, but so is gasoline. Unlike gasoline, which can pool up and burn for a long time, hydrogen dissipates rapidly. Gas tanks tend to be easily punctured, thin-walled containers, while the latest hydrogen tanks are made from Kevlar. Also, gaseous hydrogen isn't the only method of storage under consideration-BMW is looking at liquid storage while other researchers are looking at chemical compound storage, such as boron pellets. Query: safety are hydrogen fuel cell vehicles safe G: hydrogen in cars is less dangerous than gasoline Query attn : hydrogen is hydrogen hydrogen hydrogen fuel energy D1:hydrogen in cars is less natural than gasoline SD1: hydrogen in cars is reduce risk than fuel D2: hydrogen in waste is less effective than gasoline SD2:hydrogen in cars is less dangerous than gasoline Source:The basis of all animal rights should be the Golden Rule: we should treat them as we would wish them to treat us, were any other species in our dominant position. Query: do animals have rights that makes eating them inappropriate G: animals should be treated as we would want to be treated Query att : animals should be treated as we would protect to be treated D1: animals should be treated as we most individual to be treated SD1: animals should be treated as we would physically to be treated D2: animals should be treated as we would illegal to be treated SD2: animals should be treated as those would want to be treated <ref type="table">Table 4</ref>: Summaries generated by different models. In general, we observed that the baseline models which do not use a diversity based attention model tend to produce more repetitions. Notice that the last example shows that our model is not very aggressive in dealing with the history and is able to produce valid repetitions (treated ... treated) when needed ber of sentences containing repeated words generated by different models. Specifically for the 1268 test instances we counted the number of sentences containing repeated words as generated by different modes. <ref type="table">Table 5</ref> summarizes this analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work we proposed a query-based summarization method. The unique feature of  <ref type="table">Table 5</ref>: Average number of sentences with repeating words across 10 folds the model is a novel diversification mechanism based on successive orthogonalization. This gives us the flexibility to: (i) provide diverse context vectors at successive time steps and (ii) pay attention to words repeatedly if need be later in the summary (as opposed to existing models which aggressively delete the history). We also introduced a new data set and empirically verified we perform significantly better (gain of 28% (absolute) in ROUGE-L score) than applying a plain encode-attend-decode mechanism to this problem. We observe that adding an attention mechanism on the query string gives significant improvements. We also compare with a state of the art diversity model and outperform it by a good margin (gain of 7% (absolute) in ROUGE-L score). The diversification model proposed is general enough to apply to other NLG tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Queries associated with the topic "algae biofuel" Documents and summaries for a given query</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Proposed model for Query based Abstractive Summarization with (i) query encoder (ii) document encoder (iii) query attention model (iv) diversity based document attention model and (v) decoder. The green and red arrows show the connections for timestep 3 of the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average length of documents/queries/summaries in the dataset</figDesc><table><row><cell>3 Dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Performance on various models using full-</cell></row><row><cell>length ROUGE metrics</cell></row><row><cell>7 Discussions</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/PrekshaNema25/ DiverstiyBasedAttentionMechanism</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for modeling documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2754" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harvard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT16 pages</title>
		<meeting>NAACL-HLT16 pages</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Heads: Headline generation as sequence prediction using an abstract feature-rich space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Carlos A Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Mantrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silvestri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on automatic text summarization. Literature Survey for the Language and</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics II course at CMU</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="192" to="195" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangze</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05865</idno>
		<title level="m">Lcsts: A large scale chinese short text summarization dataset</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06155</idno>
		<title level="m">A persona-based neural conversation model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Generating news headlines with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01712</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Benjamins Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of text summarization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic text summarization using a machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Larocca</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Celso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaestner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Symposium on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="205" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self reinforcement for important passage retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martins De Matos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 36th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long story short-global unsupervised models for keyphrase based meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbinian</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="801" to="815" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Temporal attention model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Baskaran Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02927</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<title level="m">Bbn/umd at duc-2004: Topiary</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparing the roles of textual, acoustic and spoken-language features on spontaneous-conversation summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers. Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="197" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Summarizing multiple spoken documents: finding evidence from untranscribed audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="549" to="557" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
