<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Faiyaz Khan</surname></persName>
							<email>mfaiyazkhan@student.sust.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shahjalal University of Science and Technology</orgName>
								<address>
									<settlement>Sylhet</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Bengali Image Captioning via deep convolutional neural network based encoder-decoder model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image captioning · ResNet-50 · CNN · sequence-to-sequence · encoder-decoder</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0000−0002−2155−5991] , S.M. Sadiq-Ur-Rahman 2[0000−0003−2428−6595] , and Md. Saiful Islam 3[0000−0001−9236−380X]</p><p>Abstract. Image Captioning is an arduous task of producing syntactically and semantically correct textual descriptions of an image in natural language with context related to the image. Existing notable pieces of research in Bengali Image Captioning (BIC) are based on encoder-decoder architecture. This paper presents an end-to-end image captioning system utilizing a multimodal architecture by combining a one-dimensional convolutional neural network (CNN) to encode sequence information with a pre-trained ResNet-50 model image encoder for extracting regionbased visual features. We investigate our approach's performance on the BanglaLekhaImageCaptions dataset using the existing evaluation metrics and perform a human evaluation for qualitative analysis. Experiments show that our approach's language encoder captures the finegrained information in the caption, and combined with the image features, it generates accurate and diversified caption. Our work outperforms all the existing BIC works and achieves a new state-of-the-art (SOTA) performance by scoring 0.651 on BLUE-1, 0.572 on CIDEr, 0.297 on METEOR, 0.434 on ROUGE, and 0.357 on SPICE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic image captioning is the process of generating a human-like description of an image in natural language. It is a significantly challenging task as it requires identifying salient objects in the image, understanding their relationships, and generating relevant descriptions of these image features in natural language. The process of generating captions of the images can be applied to automate self-driving cars, implement facial recognition systems, aid visually impaired people, describe CCTV footage, improve image search quality, etc. Despite numerous research attentions in encoder-decoder-based image captioning in the English language, a few works are done in BIC. Researches in these languages can have far-reaching consequences in solving many region-based socio-economic problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caption Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>একটি নৗকায় কেয়কজন মানু ষ আেছ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Features</head><p>Predicted Caption একটি নৗকায় চারজন পু রুষ আেছ। একটি নৗকায় ৪ জন পু রুষ মাথায় গাল টু িপ পের বেস আেছন। The fields of computer vision and natural language processing have seen significant progress due to the recent development in deep learning. The task of image captioning lies at the intersection of these two. Most notable works are based on the encoder-decoder framework, which is very similar to the sequenceto-sequence model for machine translation <ref type="bibr" target="#b26">[26]</ref>. The framework contains a CNNbased image feature extractor and a recurrent neural network (typically LSTM <ref type="bibr" target="#b16">[16]</ref>) based caption decoder to generate the relevant words iteratively. The decoder's job is to take in the caption generated so far and predict the next word with the highest probability among all the vocabulary words until an ending token is generated. All the existing works in the Bengali language <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b6">7]</ref> follow the same architecture, as mentioned above. In <ref type="bibr" target="#b27">[27]</ref>, there is a comparison between the two architectures of image captioning. The first one is the inject architecture, where the RNN is used as a caption generator conditioned by the image features. The second one is the merge or mixture architecture, where the RNN is primarily used for encoding linguistic representation only. The encoded linguistic features and the image features are then merged and passed as input to a multi-modal layer that performs the word by word prediction of the caption. The comparative study shows that models with merge architecture serve better than models with inject architecture. In natural language processing tasks like chunking, part-of-speech tagging, and named entity recognition, CNN based models have provided faster and accurate results <ref type="bibr" target="#b4">[5]</ref>. Also, In <ref type="bibr" target="#b30">[30]</ref>, it has been shown that CNN-CNN models are competitive in performance with CNN-RNN models in terms of image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Captions</head><p>Inspired by the aforementioned successes of the fusion model and CNN in NLP, we propose an encoder-decoder-based model following merge architecture for Bengali image captioning. We used ResNet-50 <ref type="bibr" target="#b15">[15]</ref> for encoding image features and a one dimensional CNN for encoding the linguistic features. Unlike <ref type="bibr" target="#b30">[30]</ref>, the CNN used in our work is followed by a pooling layer for capturing meaningful and significant features. Later, both the image and language features are mingled and passed to the decoder to generate the image's caption. We evaluate our work on the BanglaLekhaImageCaptions dataset <ref type="bibr" target="#b22">[22]</ref>. The experimental results show that our model performs better than all the existing models in the Bengali language. We also conducted a qualitative and quantitative comparison between our CNN-CNN based mixture model and the CNN-LSTM based mixture model proposed in <ref type="bibr" target="#b27">[27]</ref>. The experimental results confirm that the CNN-based language encoder is responsible for the proposed model's overall better performance. In summary, the main technical contributions of this paper are the following:</p><p>-We present CNN instead of regular LSTM to learn linguistic information and use it for word prediction during the caption decoding phase. Meanwhile, we use ResNet-50 <ref type="bibr" target="#b15">[15]</ref> architecture as an image feature extractor. -We qualitatively and quantitatively evaluate our approach on the BanglaLekhaIm-ageCaptions dataset. -Our model achieves SOTA performance on the BanglaLekhaImageCaptions dataset and outperforms the existing encoder-decoder models while describing complex scenes. We also present the human evaluated score for qualitative evaluation of the generated captions. The code is available on Github 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>In the early years of research in image captioning, many complex systems consisting of primitive visual object identifiers and language models were used <ref type="bibr" target="#b14">[14]</ref>. These systems were rule-based and predominantly hand-designed. Moreover, these systems worked only on a limited domain of images. In <ref type="bibr" target="#b12">[12]</ref>, image captioning was treated as a machine translation task. But this system failed to capture the fine-grained relationship among the objects in the image. Along with the advancements of deep learning methods, image captioning systems produced considerably improved performances following the same deep learningbased architecture as machine translation <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b2">3]</ref>. These works adopted the same encoder-decoder framework <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b18">18]</ref> and framed the idea of image captioning as translating the image into text. These systems used CNN for encoding images and RNN for decoding the images into sentences. Later, attention mechanisms were introduced to mimic the human behavior of capturing only the important features in an image and translating them into a natural language description <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b34">34]</ref>. These systems generated the captions conditioned by the attention at a specific place of the image at each time step. Most of the systems built for English language are evaluated on MSCOCO <ref type="bibr" target="#b21">[21]</ref>, Flickr30k <ref type="bibr" target="#b35">[35]</ref> and Flickr8k <ref type="bibr" target="#b17">[17]</ref> datasets.</p><p>Researches on other languages like Japanese <ref type="bibr" target="#b33">[33]</ref>, Chinese <ref type="bibr" target="#b19">[19]</ref>, German <ref type="bibr" target="#b13">[13]</ref>, Arabic <ref type="bibr" target="#b0">[1]</ref> etc have also been performed. Most of these research works are experimented and evaluated on the translated versions of the MSCOCO <ref type="bibr" target="#b21">[21]</ref> and Flickr8k <ref type="bibr" target="#b17">[17]</ref> datasets in their respective languages. In <ref type="bibr" target="#b24">[24]</ref>, a Bengali image captioning dataset <ref type="bibr" target="#b22">[22]</ref> was introduced along with a model which is very similar to <ref type="bibr" target="#b29">[29]</ref>. While the results generated are not accurate enough, but it surely instigated further research works in Bengali. In <ref type="bibr" target="#b6">[7]</ref>, a comparative analysis of the existing encoder-decoder LSTM decoder based models was presented. The models were evaluated on a trimmed down, machine-translated version of the Flickr8k <ref type="bibr" target="#b17">[17]</ref> dataset in Bengali. The Bengali captions generated, however, do not maintain the typical Bengali sentence structure and lacks usability. In this work, we present an encoder-decoder model with a CNN language encoder. We also provide experimental results on the BanglaLekhaImageCaptions dataset comparing our work with the existing LSTM based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>We trained and evaluated our model on the BanglaLekhaImageCaptions dataset <ref type="bibr" target="#b22">[22]</ref>. It is a modified version from the one introduced in <ref type="bibr" target="#b24">[24]</ref>. It contains 9,154 images with two captions for each image. The captions are generated by two native Bengali speakers. While this data set is not big in volume compared to the existing datasets in the English language, it maintains relevance with the Bengali culture to some extent. But the dataset also has a considerable amount of human bias. This bias hinders any model's ability to describe non-human subjects. Also, the captions are not detailed in some cases, which causes the training and evaluation of any model to be not as accurate as expected.</p><p>To train our model, we divided the data set into three parts, which are train, test, and validation. For training, we used 7154 images. 1000 images were used during validation, and the rest 1000 images were used during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>The model is based on encoder-decoder architecture. A two-dimensional convolutional neural network is used to encode the image features, and a one-dimensional convolutional neural network is used to encode the word sequences of the caption data. Later, both the encoded image and text features are merged and passed to a decoder to predict the caption in a word by word manner (figure 2) <ref type="figure">Fig. 2</ref>: The overview of the framework of our proposed CNN-ResNet-50 merged model, consisting of a ResNet-50 as image feature extractor and 1D-CNN with word embedding for generating linguistic information. Later, these two features are given as inputs to a multimodal layer that predicts what to generate next using this information at each time step</p><p>The model is divided into three parts:</p><p>-Image Feature Encoder: We used pre-trained ResNet-50 <ref type="bibr" target="#b15">[15]</ref> as image feature extractor. It is trained on ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>. Traditionally, neural networks with many layers tend to perform well in recognizing patterns. However, they also suffer from overfitting issues and are not easy to optimize. Residual CNNs are designed to have shortcut connections between layers. These connections perform identity mapping. ResNets are easy to optimize, and their performance increase with increasing network depth. We discard the final output layer of the ResNet-50 as it contains the output of image classification and used only the encoded image features produced by the hidden layers. -Word Sequence Encoder: Two-dimensional convolutional neural networks have been extensively used in pattern recognition, image classification, and time series forecasting. The same property of these networks can be used in sequence processing. In our model, we used one-dimensional CNN for extracting one-dimensional patches from a sequence of words. The CNN has 512 filters with a kernel size of 3. The activation used is Rectified Linear Units(ReLU). The CNN is followed by a Global Max Pooling Layer, which captures critical features from the convolutional layer's output. -Caption Generator: The caption generator is a simple decoder containing a Dense 512 layer with ReLU activation. The output of the image feature encoder and word sequence encoder are combined by concatenation and used as input to the dense layer. The dense layer generates a softmax prediction for each word in the vocabulary to be the next word in the sequence, and the word with the highest probability is selected. This process continues until an ending token is generated.</p><p>The caption generator's output is transformed into a probability score for each word in the vocabulary. The greedy method chooses the word with the highest probability for each time step. This method may not always provide the best possible caption as any word's prediction depends on all the previously predicted words. So, it is more efficient to select the sequence with the highest overall score from a candidate of sequences. So we use the beam search technique with a beam size of 5. It considers the top 5 candidate words at the first decode step. For each of the first words, it generates five-second words and chooses the top five combinations of first and second words based on the additive score. After the termination of five sequences, the sequence with the best overall score is selected. This method allows the process to be flexible and generate consistent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result and Analysis</head><p>This section provides a quantitative and qualitative analysis of the performance of our model. While the evaluation metrics give a numeric idea about the captions' correctness, they can sometimes misinterpret any result. Qualitative analysis can evaluate the subtle difference in the generated captions compared with the natural human language description. Predicted captions from the models were evaluated using the existing evaluation metrics BLEU <ref type="bibr" target="#b23">[23]</ref> (Bilingual Evaluation Understudy), METEOR <ref type="bibr" target="#b9">[9]</ref> (Metric for Evaluation of Translation with Explicit Ordering), ROUGE <ref type="bibr" target="#b20">[20]</ref> (Recall-Oriented Understudy for Gisting Evaluation), CIDEr <ref type="bibr" target="#b28">[28]</ref> (Consensusbased Image Description Evaluation) and SPICE <ref type="bibr" target="#b1">[2]</ref> (Semantic Propositional Image Caption Evaluation).</p><p>A comparison among our model, inject architecture-based model with CNN language encoder, mixture architecture-based model with LSTM language encoder, and the model proposed in <ref type="bibr" target="#b24">[24]</ref> (Bi-directional LSTM language encoder with inject architecture) can be found in table 1. We replicated the model of <ref type="bibr" target="#b24">[24]</ref> using the same ResNet-50 as image feature extractor instead of the VGG-16 <ref type="bibr" target="#b25">[25]</ref> used in the original work to make sure that the better performance of our model is not only due to the better image model. We also present the scores of both the greedy and beam search method. From 1, it can be seen, our model based on CNN word sequence processor has achieved better results in all the metrics than the traditional LSTM based models with both mixture and inject architectures and the CNN based models with inject architecture. Our model's superior performance can be attributed to the one dimensional CNN model we used for sequence processing with the merge architecture. We used a window of size 3 for CNN. This window size with merge architecture enabled our model to learn words or word fragments of size 3. As a result, the fine-grained information present in the captions is learned during training. Following the CNN layer, the pooling layer filters only the significant features, which means the correlation between the words is stored better. Besides, this combination of one-dimensional CNN as a sequence processor with merge architecture can remember more diversified words while generating captions. These are evident in the comparison of the quality of the captions generated by the models in the <ref type="figure" target="#fig_1">figure-3</ref>. The scores with the highest accuracy have been shown in table 1 with boldface. Among the evaluation metrics the most important metrics for evaluating image captions are CIDEr <ref type="bibr" target="#b28">[28]</ref> and SPICE [2] since these are specially prepared for evaluating image captions. Better scores in these two metrics indicate the quality of performance of our model. The scores of the evaluation metrics were calculated using pycocoevalcap 5 library for python 3 available in github 6 which is a support for MS COCO caption evaluation tools <ref type="bibr" target="#b3">[4]</ref>. The performance comparison among our model and other models can be seen in <ref type="figure" target="#fig_1">figure-3</ref>. We present the predicted Bengali caption and corresponding English Translated caption for non-native Bengali speakers. Our model performed better not only in the scores but also in the quality of captions. In <ref type="figure" target="#fig_1">figure-3a</ref>, it is observable that our model predicted the most relevant caption compared to other models describing the gender of the human subject and the work he is doing in the image. In <ref type="figure" target="#fig_1">figure-3b</ref>, our model detected the gender correctly and captured the person's age range. It is also noticeable that all the metrics' score, including the SPICE, is better, which indicates the better quality caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference :</head><p>একজন বু েড়া পু রুষ চাকা ঘু িরেয় মাটির পািতল বানােচ্ছেন। An old man is turning a wheel and making a clay pot. Human evaluated score: 0.70</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>একজন পু রুষ বেস কাজ করেছ।</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>Reference :</p><p>মাথায় হাত ঘু িরেয় রেখ দািড়েয় আেছন একজন বাচ্চা ছেল যার</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>হােত একটি তািবজ বাধা িপছেন সিরষা ক্ষেত।</head><p>A little boy with an amulet in his hand is standing, rotating his hand back of his head and, there is a mustard field behind him.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>একটি িশশু আেছ।</head><p>There is a child.</p><p>Predicted :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>একজন বাচ্চা ছেল হািস িদেয় দাঁ িড়েয় তািকেয় আেছন।</head><p>A little boy is standing and staring with a smiling face.</p><p>Human evaluated score: 0.80</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>Reference :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>িকছু সবিজ িনেয় নৗকা চািলেয় যােচ্ছেন একজন বঠা হােত পু রুষ।</head><p>A man is carrying some vegetables in a boat with an oar in his hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>একজন পু রুষ নৗকা চালােচ্ছে।</head><p>A man is boating.</p><p>Predicted :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>একজন পু রুষ নৗকা চালােচ্ছে।</head><p>A man is boating.</p><p>Human evaluated score: 0.71</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c)</head><p>Reference :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>পাশাপািশ কেয়কজন পু রুষ দািড়েয় আেছ।</head><p>There are a few men standing side by side.  <ref type="figure" target="#fig_2">figure-4</ref> to represent the quality of captions generated. For qualitative evaluation, two native Bengali speakers were asked to give each caption a score between 0 and 1 from a set of sample images randomly selected from the test set. The scores are then averaged to generate the overall human evaluated score. In <ref type="figure" target="#fig_2">figure-4a</ref>, the model generated a decent caption correctly predicting the number of humans considering the human subjects' small size. In figure-4b, our model described the expression of face and the person's age range, i.e., child correctly. The caption is also well detailed. Also, in <ref type="figure" target="#fig_2">figure-4c and figure-4d</ref>  The faulty predictions made by our model can be attributed mainly to the dataset introduced by <ref type="bibr" target="#b24">[24]</ref>. In this dataset, most of the images contain human subjects which have almost similar types of captions. As a result, the model is trained with a massive amount of similar type human subjects and hence fails to detect and describe non-human subjects during testing. Besides, number of captions for each image is only two compared to five in the widely used English datasets. The dataset contains significant amount of spelling mistakes in Bengali caption which is also evident in the examples above. All of these are indicative of lacking details and variety in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a CNN-CNN merged encoder-decoder-based image captioning system instead of a traditional sequence-to-sequence model. A substantial test conducted on the BanglaLekhaImageCaptions dataset with superlative performance validates the efficacy of our proposed model. Additionally, experimental results show that the CNN language model, combined with the merge architecture, captures the fined-grained sentence structure information with better linguistic diversity and produces more accurate and humanoid captions than the traditional LSTM. Nevertheless, the proposed model suffers from recognizing non-human subjects as the dataset is biased towards human subjects. This leaves us desired for a well-varied and detailed captioned dataset for Bengali Image Captioning. Therefore, we are motivated to develop a gold standard image caption dataset for Bengali for future work. Using other search methods such as constrained beam search in the decoding phase can be an area of future work. Besides, multilingual transformers available in NLP (Natural Language Processing) tasks like BERT <ref type="bibr" target="#b10">[10]</ref>, XLM <ref type="bibr" target="#b5">[6]</ref>, XLNet <ref type="bibr" target="#b32">[32]</ref> etc can generate promising results in the Bengali language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of sequence-to-sequence basic architecture where image and linguistic features are merged to generate meaningful captions in the Bengali language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>(inject): একজন আিদবাসী পু রুষ বেস আেছ। (An aboriginal man is sitting.) BLEU-1 : 0.60 BLEU-2 : 0.39 ROUGE_L: 0.6 SPICE : 0.44 CNN+LSTM (merge): একটি পািখ আেছ। (There is a bird.) বেস আেছ। (A man is sitting.) BLEU-1 : 0.58 BLEU-2 : 0.55 ROUGE_L: 0.65 SPICE : 0.47 (a) Reference : একজন বু েড়া পু রুষ তািকেয় আেছন যার লম্বা সাদা চু ল দািড়েগাঁ ফ। An aged man is staring, who has long white hair and a beard. একজন বালক তািকেয় আেছ। (A boy is staring.) BLEU-1 : 0.96 BLEU-2 : 0.0 ROUGE_L: 0.5 SPICE : 0.44 CNN+CNN (inject): একজন পু রুষ বেস আেছ। (A man is sitting.) BLEU-1 : 0.75 BLEU-2 : 0.0 ROUGE_L: 0.75 SPICE : 0.44 (b) Comparison among the captions generated by different models Reference : একটি নৗকার উপর দু ইজন মানু ষ আেছ। There are two people on a boat. একটি নৗকায় ২ জন মানু ষ এবং পােশ জিম। There are two people on a boat beside lands. Predicted : একটি নৗকায় দু ইজন মানু ষ আেছ। There are two people on a boat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>৮Fig. 4 :</head><label>4</label><figDesc>জন ছেল একসােথ একটি দয়ােল ধাক্কা িদেয় দাঁ িড়েয় তািকেয় আেছ সামেন। 8 boys are standing together, pushing against a wall and looking at the front. Predicted : কেয়কজন পু রুষ দািড়েয় আেছ। A few men are standing. Human evaluated score: 0.84 (d) A glimpse of captions predicted by our model with qualitative evaluation 5.1 Qualitative Analysis Some samples along with the English translation are depicted in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Few incongruous captions generated by our model In figure-5, some wrong predictions made by our model are shown. In figure-5a, it could not correctly describe the content of the image at all, and in the figure-5b, it misinterpreted the birds as persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative analysis of performances among different models</figDesc><table><row><cell>Search Type</cell><cell>Models</cell><cell cols="7">BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr SPICE</cell></row><row><cell></cell><cell>Our Model</cell><cell cols="4">0.651 0.426 0.278 0.175</cell><cell>0.297</cell><cell>0.417</cell><cell>0.501 0.357</cell></row><row><cell>Greedy</cell><cell cols="2">CNN + LSTM [mixture][27] 0.632</cell><cell>0.414</cell><cell>0.269</cell><cell>0.168</cell><cell>0.291</cell><cell>0.395</cell><cell>0.454 0.350</cell></row><row><cell></cell><cell cols="2">CNN + Bi-LSTM [inject][24] 0.619</cell><cell>0.403</cell><cell>0.261</cell><cell>0.163</cell><cell>0.296</cell><cell>0.380</cell><cell>0.433 0.344</cell></row><row><cell></cell><cell>CNN + CNN [inject]</cell><cell>0.538</cell><cell>0.347</cell><cell>0.228</cell><cell>0.145</cell><cell>0.250</cell><cell>0.378</cell><cell>0.318 0.334</cell></row><row><cell></cell><cell>Our Model</cell><cell cols="4">0.589 0.395 0.267 0.175</cell><cell>0.294</cell><cell>0.434</cell><cell>0.572 0.353</cell></row><row><cell>Beam</cell><cell cols="2">CNN + LSTM [mixture][27] 0.562</cell><cell>0.381</cell><cell>0.257</cell><cell>0.166</cell><cell>0.286</cell><cell>0.423</cell><cell>0.558 0.345</cell></row><row><cell></cell><cell cols="2">CNN + Bi-LSTM [inject][24] 0.575</cell><cell>0.374</cell><cell>0.241</cell><cell>0.149</cell><cell>0.286</cell><cell>0.412</cell><cell>0.532 0.349</cell></row><row><cell></cell><cell>CNN + CNN [inject]</cell><cell>0.433</cell><cell>0.287</cell><cell>0.185</cell><cell>0.113</cell><cell>0.255</cell><cell>0.386</cell><cell>0.328 0.324</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Some birds are sitting on the dead rotten water lily leaves.There are three birds.</head><label></label><figDesc>, the model described the content of the image in a very similar way to human description and hence, has achieved good evaluation scores.</figDesc><table><row><cell>Beam:</cell><cell></cell></row><row><cell>কেয়কজন মানু ষ আেছ।</cell><cell></cell></row><row><cell>There are a few people.</cell><cell></cell></row><row><cell>BLEU-1 : 0.33 BLEU-2 : 0.0</cell><cell>ROUGE_L: 0.33 SPICE : 0.25</cell></row><row><cell>(a)</cell><cell></cell></row><row><cell>Reference :</cell><cell></cell></row><row><cell>িতনটি পািখ আেছ।</cell><cell></cell></row><row><cell>৩ টি চরই পািখ বেস আেছ।</cell><cell></cell></row><row><cell cols="2">Three sparrows are sitting.</cell></row><row><cell>Predicted :</cell><cell></cell></row><row><cell>Greedy:</cell><cell></cell></row><row><cell>একজন পু রুষ বেস আেছ।</cell><cell></cell></row><row><cell>A man is sitting.</cell><cell></cell></row><row><cell cols="2">BLEU-1 : 0.50 BLEU-2 : 0.41 ROUGE_L: 0.39 SPICE : 0.42</cell></row><row><cell>Beam:</cell><cell></cell></row><row><cell>একজন মানু ষ বেস আেছ।</cell><cell></cell></row><row><cell>A man is sitting.</cell><cell></cell></row><row><cell cols="2">BLEU-1 : 0.50 BLEU-2 : 0.41 ROUGE_L: 0.39 SPICE : 0.43</cell></row><row><cell>(b)</cell><cell></cell></row></table><note>Reference :পাঁ চটি হাঁ স আেছ। There are five ducks.মরা পচা শাপলা পাতার উপের িকছু পািখ বেস আেছ।Predicted :Greedy: পাহােড়র উপর িকছু মানু ষ এবং পােড় িকছু গাছ উড়েছ। Some people are flying on the hill and some trees on the bank .BLEU-1 : 0.11 BLEU-2 : 0.0 ROUGE_L: 0.11 SPICE : 0.19</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/FaiyazKhan11/Improved-Bengali-Image-Captioning-via-deepconvolutional-neural-network-based-encoder-decoder-model</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/salaniz/pycocoevalcap 6 https://github.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We want to thank the Natural Language Processing Group, Dept. of CSE, SUST, for their valuable guidelines in our research work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic arabic image captioning using rnn-lst m-based language model and cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Al-Muzaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Al-Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Benhidour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7059" to="7069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Oboyob: A sequential-semantic bengali image captioning engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Firoze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Tahmeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="7427" to="7439" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi30k: Multilingual englishgerman image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language</title>
		<meeting>the 5th Workshop on Vision and Language</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge representation for the generation of quantified natural language descriptions of vehicle traffic in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd IEEE International Conference on Image Processing</title>
		<meeting>3rd IEEE International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="805" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adding chinese captions to images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2016 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Momen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matiur</surname></persName>
		</author>
		<idno type="DOI">10.17632/rxxch9vw59.2</idno>
		<ptr target="http://dx.doi.org/10.17632/rxxch9vw59.2" />
		<title level="m">Banglalekhaimagecaptions, mendeley data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chittron: An automatic bangla image captioning system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Momen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What is the role of recurrent neural networks (RNNs) in an image caption generator?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Camilleri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cnn+ cnn: Convolutional decoders for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2018)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stair captions: Constructing a large-scale japanese image caption dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="417" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
