<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifting Transformer for 3D Human Pose Estimation in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Ding</surname></persName>
							<email>dingrunwei@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Intelligent Systems Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
							<email>pichao.wang@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lifting Transformer for 3D Human Pose Estimation in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite great progress in video-based 3D human pose estimation, it is still challenging to learn a discriminative single-pose representation from redundant sequences. To this end, we propose a novel Transformer-based architecture, called Lifting Transformer, for 3D human pose estimation to lift a sequence of 2D joint locations to a 3D pose. Specifically, a vanilla Transformer encoder (VTE) is adopted to model long-range dependencies of 2D pose sequences. To reduce redundancy of the sequence and aggregate information from local context, fully-connected layers in the feed-forward network of VTE are replaced with strided convolutions to progressively reduce the sequence length. The modified VTE is termed as strided Transformer encoder (STE) and it is built upon the outputs of VTE. STE not only significantly reduces the computation cost but also effectively aggregates information to a single-vector representation in a global and local fashion. Moreover, a full-tosingle supervision scheme is employed at both the full sequence scale and single target frame scale, applying to the outputs of VTE and STE, respectively. This scheme imposes extra temporal smoothness constraints in conjunction with the single target frame supervision. The proposed architecture is evaluated on two challenging benchmark datasets, namely, Human3.6M and HumanEva-I, and achieves stateof-the-art results with much fewer parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation aims to estimate 3D joint locations of a human body from images or videos, which is a regression problem. This task has drawn tremendous attention in the past decades <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref>, with wide applications in computer animation <ref type="bibr" target="#b32">[33]</ref>, action understanding <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref>, and human-robot interactions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. It is a challenging problem due to the depth ambiguity and selfocclusions.</p><p>Many state-of-the-art approaches adopt a two-stage … … … STE VTE <ref type="figure">Figure 1</ref>. Our strided Transformer encoder (STE) takes the outputs of vanilla Transformer encoder (VTE) as input (yellow) and generates a 3D pose for the target frame as output (top). The selfattention mechanism (blue) concentrates on global context and strided convolution (green) aggregates information from local context.</p><p>pipeline <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref>, which first estimates 2D keypoints and then lifts them to the 3D space. Although the 2D-3D lifting methods benefit from the reliable performance of 2D pose detectors, it is still an ill-posed problem due to the depth ambiguity, where multiple 3D interpretations can map to the same 2D keypoints from monocular images. To resolve this ambiguity, some methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref> exploit temporal information by leveraging past and future data in the sequence to predict the 3D pose of the target frame. For instance, Cai et al. <ref type="bibr" target="#b0">[1]</ref> presented a local-to-global graph convolutional network to exploit spatial-temporal relations to estimate 3D poses from a sequence of skeletons. However, these approaches cannot explicitly and effectively model long-term dependencies result in small temporal receptive fields. Vanilla Transformer <ref type="bibr" target="#b39">[40]</ref> is developed for exploiting long-range dependencies and achieves tremendous success in natural language processing <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b50">51]</ref> and computer vision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. It consists of self-attention module and point-wise feed-forward network (FFN). The selfattention module computes pairwise dot-product between all input elements to capture global-context information and the FFN acts as pattern detectors over the input across all layers <ref type="bibr" target="#b9">[10]</ref>. Such design is a good choice for video-based 3D human pose estimation to capture long-range dependencies. However, the full-length representation in the vanilla Transformer encoder (VTE) <ref type="bibr" target="#b39">[40]</ref> actually contains significant redundancy for video-based pose estimation, as nearby poses are quite similar. Based on the aforementioned observations, we propose to gradually merge nearby poses to reduce the sequence length till to one target pose representation. An alternative is to perform the pooling operation after the FFN <ref type="bibr" target="#b50">[51]</ref>. However, lots of valuable information may be lost if using pooling operation and the local information can not be well exploited. Inspired by previous methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref> that take temporal convolutions to handle sequences with different input lengths, we propose to replace fully-connected layers in FFN with strided convolutions to progressively reduce the sequence length. The modified Transformer is dubbed strided Transformer encoder (STE), and it trades off the computation in FFN for constructing a deeper model and aggregates information in a global and local fashion to boost the model capacity, as shown in <ref type="figure">Figure 1</ref>. In addition, based on the outputs of VTE and STE, a novel full-to-single supervision scheme is proposed at both the full and single scales. More precisely, the full sequence scale can enforce temporal smoothness and single target frame scale helps learn a specific representation for the target frame. The proposed architecture is called Lifting Transformer, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Extensive experiments are conducted on two standard 3D human pose estimation datasets, i.e., Human3.6M <ref type="bibr" target="#b15">[16]</ref> and HumanEva-I <ref type="bibr" target="#b35">[36]</ref>. Experimental results show that our model achieves state-of-the-art perfor-mance.</p><p>We summarize our contributions as follows:</p><p>• We propose the Lifting Transformer, a Transformerbased architecture for 3D human pose estimation, which is simple and efficient to lift 2D joint locations to 3D poses.</p><p>• To reduce the sequence redundancy and computation cost, STE is introduced to progressively reduce the temporal dimensionality and aggregate information to a single-vector representation of pose sequences in a global and local fashion.</p><p>• A full-to-single supervision scheme is designed to impose extra temporal smoothness constraints during training at the full sequence scale and further refine the estimation at the single target frame scale.</p><p>• State-of-the-art results are achieved with fewer parameters on two commonly used benchmark datasets, making it a strong baseline for Transformer-based 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>One-stage pose estimation. At the early stage of applying deep neural networks on 3D pose estimation task, many methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref> learned the direct mapping from RGB images to 3D poses, termed as one-stage pose estimation. However, these methods require sophisticating architectures, which are impractical in realistic applications.</p><p>Two-stage pose estimation. Two-stage methods formulate the problem of 3D human pose estimation as 2D keypoint detection followed by 2D-3D lifting estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>. For example, Martinez et al. <ref type="bibr" target="#b27">[28]</ref> lifted 2D joint locations to 3D space via a fully-connected residual network. Fang et al. <ref type="bibr" target="#b7">[8]</ref> proposed a pose grammar model to encode the human body configuration of human poses from 2D space to 3D space. We follow this two-stage pipeline because pre-trained 2D pose estimators <ref type="bibr" target="#b4">[5]</ref> are mature enough to be deployed elsewhere.</p><p>Video pose estimation. Since past and future frames are beneficial for 3D human pose estimation when the pose of a person is ambiguous or the body is partially occluded in one frame, many approaches tried to exploit temporal information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>. To predict temporally consistent 3D poses, Hossain et al. <ref type="bibr" target="#b34">[35]</ref> designed a sequenceto-sequence network with LSTM. Pavllo et al. <ref type="bibr" target="#b31">[32]</ref> introduced a fully convolutional model based on dilated temporal convolution. Cai et al. <ref type="bibr" target="#b0">[1]</ref> directly chose the 3D pose of the target frame from outputs of the proposed graph-based method and then fed it to a refinement model. To produce smoother 3D sequences, Wang et al. <ref type="bibr" target="#b41">[42]</ref> designed an Ushaped graph convolutional network and involved motion modeling into learning. However, this architecture is limit to embed fixed-length sequences. Different from most existing works that employed LSTM-based, graph-based, or fully convolutional architectures to exploit temporal information, we propose to leverage a Transformer-based architecture to capture long-range dependencies from input 2D pose sequences. Furthermore, compared to previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref> that either utilizing a refinement model or using a motion loss, we design a full-to-single supervision scheme to produce predictions at both the full sequence scale and single target frame scale rather than using a single component with a single output.</p><p>Transformer network. Transformer architecture was first proposed by <ref type="bibr" target="#b39">[40]</ref> and commonly used in various language tasks. Recently, Transformer has shown promising performance in computer vision task, such as object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50]</ref> and image classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b45">46]</ref>. Unlike DETR <ref type="bibr" target="#b1">[2]</ref> and ViT <ref type="bibr" target="#b6">[7]</ref> that directly applied Transformer to images, we use Transformer to map 2D keypoints to 3D poses. Additionally, local context is incorporated into the standard Transformer to deal with the redundancy of sequences for the video-based 3D pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Lifting Transformer</head><p>Our proposed Lifting Transformer is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Given a sequence of the estimated 2D poses P = {p 1 , . . . , p T } from videos, we aim to reconstruct 3D joint locations X ∈ R J×3 for a target frame, where p t ∈ R J×2 denotes the 2D joint locations at frame t, T is the number of video frames, and J is the number of joints. The network contains a vanilla Transformer encoder (VTE) followed by a strided Transformer encoder (STE) and is trained in a fullto-single prediction scheme at both the full sequence scale and single target frame scale. Specifically, VTE is first used to model long-range information and is supervised by the full sequence scale to enforce temporal smoothness. Then, the information is aggregated in a global and local fashion to one target pose representation from the proposed STE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer for 3D Pose Estimation</head><p>Motivated by the substantial performance gains achieved by the Transformer architecture in NLP, we propose to employ the Transformer encoder to solve the 3D human pose estimation. Each layer of the encoder has two sub-modules in <ref type="bibr" target="#b39">[40]</ref>: a multi-head self-attention and a position-wise feed-forward network (FFN).</p><p>Multi-head attention. The core mechanism of Transformer is the multi-head self-attention. Specifically, suppose there are a set of queries (Q) and keys (K) of dimension d k , and values (V ) of dimension d v , then a multi-head attention <ref type="bibr" target="#b39">[40]</ref> can be computed as:</p><formula xml:id="formula_0">MultiHead(Q, K, V ) = Concat (head 1 , . . . , head h ) W O , head i = Self-Attn QW Q i , KW K i , V W V i ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">Self-Attn(Q, K, V ) = softmax QK T √ d k V and W Q i ∈ R dm×d k , W K i ∈ R dm×d k , W V i ∈ R dm×dv , and W O ∈ R hdv×dm are parameter matrices. The hyperparam- eters h is the number of multi-attention heads, d m is the dimension of model, and d k = d v = d m /h in our imple- mentation.</formula><p>Feed-forward network. In the existing FFN (formulation (2)) from VTE, it always maintains a full-length sequence of hidden representations across all layers. It contains significant redundancy for video-based pose estimation, as nearby poses are quite similar. However, to reconstruct more accurate 3D body joints of the target frame, crucial information should be extracted from the entire pose sequences. Therefore, it requires selectively aggregating the useful information.</p><p>To tackle this issue, inspired from the previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref> that employ temporal convolutions to handle varying length sequences, we make modifications to the generic FFN. Given the input feature vector x ∈ R T ×Cin with T sequences and C in channels to generate an output of (T , C out ) features, the operation performed by FFN can be computed as:</p><formula xml:id="formula_2">FFN t,cout (x) = Cin i w cout,i * x t,i<label>(2)</label></formula><p>If 1D convolution is considered with kernel size K and strided factor S, a convolution feed-forward network can be computed as:  <ref type="figure">Figure 3</ref>. The network architecture of our proposed Lifting Transformer, where the left is VTE and the right is STE with strided convolutions that reconstructs the target 3D body joints by progressively reducing the sequence length. Here, N1 and N2 denotes the layers of the two modules, respectively. The hyperparameters k, s, dm and d f are the kernel size, the strided factor, the dimensions, and the number of hidden units. We slice the residuals to match the temporal dimensions of subsequent tensors.</p><formula xml:id="formula_3">CFFN S(t),cout (x) = Cin i K k w cout,i,k * x S(t− K−1 2 +k),i<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strided Transformer Encoder.</head><p>In this way, fullyconnected layers in FFN of VTE are replaced with strided convolutions. The modified VTE is termed as strided Transformer encoder (STE), which consists of a multi-head selfattention and a convolution feed-forward network, as presented in <ref type="figure">Figure 3</ref> (right). STE is a global and local architecture, where self-attention mechanism models global context and strided convolution helps capture local context. It gradually reduces the temporal dimensionality from layer to layers and merges the nearby poses to a short sequence length representation. More importantly, the redundancy of all frames is reduced and hence boosts the model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer-based Full-to-single Prediction</head><p>The iterative refinement scheme aimed at producing predictions in multiple processing stages is effective for 3D pose estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>. Inspired by the success of such iterative processing, we also consider a refinement scheme. Furthermore, we note that directly supervised the model at the single target frame scale always ignores temporal smoothness between video frames, while only supervised at a full sequence scale cannot explicitly learn a specific representation for the target frame.</p><p>To incorporate both scale constraints into the framework, a full-to-single scheme is proposed, which further refines the intermediate predictions to produce more accurate estimations rather than using a single component with a single output. The first step is to supervise with full sequence scale by imposing extra temporal smoothness constraints during training from the outputs of VTE. A sequence loss L f is adopted to improve upon single frame predictions for temporal consistency over a sequence. This loss ensures that the estimated 3D poses from VTE coincide with the groundtruth 3D joint sequences:</p><formula xml:id="formula_4">L f = T t=1 J i=1 Y t i −X t i 2 ,<label>(4)</label></formula><p>whereX t i and Y t i represents the estimated 3D poses and ground truth 3D joint locations of joint i at frame t, respectively.</p><p>In the second step, the supervision is upon the output of STE, which is a progressive reduction architecture to reduce the temporal dimensionality from layer to layer. The output is a prediction of the 3D poses for all frames in the input sequences using both past and future data. A single-frame loss L s is used to minimize the distance between the estimated 3D poses X from STE and the target ground-truth 3D joint annotations Y :</p><formula xml:id="formula_5">L s = J i=1 Y i − X i 2 .<label>(5)</label></formula><p>In our implementation, the model is supervised at both the full sequence scale and single target frame scale. We train the entire network in an end-to-end manner with the combined loss:</p><formula xml:id="formula_6">L = λ f L f + λ s L s ,<label>(6)</label></formula><p>where λ f = 1 and λ s = 1 are weighting factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation</head><p>The proposed method is evaluated on two challenging benchmark datasets, i.e., Human3.6M <ref type="bibr" target="#b15">[16]</ref> and HumanEva-I <ref type="bibr" target="#b35">[36]</ref>.</p><p>The Human3.6M dataset is the largest publicly available dataset for 3D human pose estimation, which consists of 3.6 million images captured from 4 synchronized 50Hz cameras. There are 7 professional subjects performing 15 everyday activities such as "Waiting", "Smoking", and "Posing". Following the standard protocol in prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref>, 5 subjects (S1, S5, S6, S7, S8) are used for training and 2 subjects (S9 and S11) are used for evaluation. The frames from all views are trained by a single model for all actions. HumanEva-I is a much smaller dataset with fewer subjects and actions compared to Human3.6M. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref>, a single model is trained for all subjects (S1, S2, S3) and all actions (Walk, Jog, Box). Two common evaluation protocols are used in the experiments. The mean per joint position error (MPJPE) is the average Euclidean distance between the ground-truth and predicted positions of the joints, which is referred to as protocol #1 in many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. Procrustes analysis MPJPE (P-MPJPE) is adopted, where the estimated 3D pose is aligned to the ground truth in translation, rotation, and scale, which is referred to as protocol #2 <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In our experiments, the input layer takes the concatenated (x, y) coordinates of the J joints for each frame and extracts sequence features using a linear layer. VTE <ref type="bibr" target="#b39">[40]</ref> is adopted as the basic architecture. We choose encoder layers N 1 = N 2 = 3, multi-attention heads h = 8, dimensions d m = 256, and hidden units d f = 512 for both VTE and STE. The kernel size and strided factor s f are set to 1 in all STE layers. The strided factor s m is set to {3, 3, 3} for the receptive field of 27 frames, {3, 3, 9} for 81, and {3, 9, 9} for 243. The learnable position embeddings are used for the first layer of VTE and every layer of STE due to the different sequence lengths. Finally, we reproject the outputs via a linear layer. Note that we only use the outputs of STE as the final predictions for the full-to-single prediction scheme.</p><p>The 2D poses can be obtained by performing any classic 2D pose detections or directly using the 2D ground truth. Following <ref type="bibr" target="#b31">[32]</ref>, the cascaded pyramid network (CPN) <ref type="bibr" target="#b4">[5]</ref> is used for Human3.6M and Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> is adopted for HumanEva-I to obtain 2D poses for a fair comparison.</p><p>In this work, all experiments are conducted on the Py-Torch framework with one GeForce GTX 3090 GPU. The network is trained using Adam optimizer with a mini-batch  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-art</head><p>We compare our method with the previous state-of-theart approaches on Human3.6M dataset. As shown in <ref type="table">Table  1</ref>, the performance of our 243-frame model with CPN input is presented. Our method outperforms the state-of-the-art methods on Human3.6 under all metrics (44.3 mm on protocol #1 and 35.8 mm on protocol #2). <ref type="table">Table 2</ref> compares the MPJPE metric and number of parameters with several state-of-the-art methods in different receptive fields (T = 27, 81, 243) on Human3.6M. It can be seen that our method has the fewest parameters but achieves the best performance from overall receptive fields. This shows that our proposed Transformer-based network is more efficient than fully convolutional architectures at the same level of accuracy for video 3D pose estimation. Besides, our model is lightweight and parameters hardly increase with the increased receptive fields, which is practical for real-time applications. <ref type="figure">Figure 5</ref> shows the visualized qualitative results from the 243-frame models of TCN <ref type="bibr" target="#b31">[32]</ref>, ATTN-TCN <ref type="bibr" target="#b26">[27]</ref>, and our model.</p><p>Additionally, we report the results when using ground truth 2D poses to explore the upper bound of our method. As illustrated in <ref type="table" target="#tab_3">Table 3</ref>, it can be seen that our method achieves the best result (29.1 mm in MPJPE) outperforming all other methods. Moreover, it has a larger performance gain than CPN input compared with state-of-the-art meth-ods, which indicates that our method has a stronger capacity in capturing temporal dependencies from more accurate 2D representations.</p><p>To evaluate the generalizability of our model to smaller datasets, experiments are conducted on HumanEva-I based on Mask R-CNN 2D detections and 2D ground truth. The results are given in <ref type="table">Table 4</ref>, which demonstrates that our method achieves promising results in each action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Input sequence length.</p><p>The MPJPE results of our model with different sequence lengths on Human3.6M are shown in <ref type="figure" target="#fig_1">Figure 4</ref> (left). It can be seen that with more input frames used for predictions, our proposed method obtains larger gains when using ground truth 2D poses. This is expected since direct lifting 3D poses from disjointed 2D poses leads to temporally incoherent outputs <ref type="bibr" target="#b5">[6]</ref>. Particularly the best results are obtained with T = 243 (44.3 mm) by using CPN. However, the performance decreases when T = 297 (45.1 mm) and T = 351 (45.5 mm), which is the same phenomenon as <ref type="bibr" target="#b26">[27]</ref>. The reason is that the 2D detector CPN is limited to provide useful information for our model to distinguish the discriminative representation from redundant long sequences. Next, we choose T = 27 on Human3.6M in the following ablation experiments as a compromise between the accuracy and the computational complexity.</p><p>2D detections. For the 2D-3D lifting task, the accuracy of the 2D detections directly influences the results of 3D pose estimation <ref type="bibr" target="#b27">[28]</ref>. To show the effectiveness of our method on different 2D pose detectors, we carry out experiments with the detections from Stack Hourglass (SH) <ref type="bibr" target="#b28">[29]</ref>, Detectron <ref type="bibr" target="#b31">[32]</ref>, and CPN <ref type="bibr" target="#b4">[5]</ref>. Moreover, to test the tolerance of our method to different levels of noise, we also train our network by 2D ground truth (GT) with different levels of additive Gaussian noises. The results are shown in <ref type="figure" target="#fig_1">Figure 4</ref> (right). It can be observed that the MPJPE of 3D poses increases linearly with the two-norm errors of 2D poses. Note that 2D pose detector methods achieve worse estimation performance than 2D ground truth with noises, which is due to the self-occlusions problem for 2D pose estimation.</p><p>Model hyperparameters. As shown in <ref type="table">Table 5</ref>, we  <ref type="table">Table 6</ref>, it shows that using a strided factor s m = {3, 3, 3} has the best performance. It demonstrates the benefit of leveraging a progressive reduction architecture to gradually reduce the temporal dimensionality from layer to layer with a small strided factor. Model components. An ablation study is performed to assess the effectiveness of different components of our method. As shown in <ref type="table">Table 7</ref>, we select the center frames of the intermediate predictions from VTE as the final results, the MPJPE decreases by 1.9 mm (from 46.9 mm to 48.8 mm). Additionally, the sequence loss L f leads to a significant performance gain (2.0 mm). The empirical results indicate that the intermediate supervision allows for a stronger capacity in producing smooth 3D sequences. Following <ref type="bibr" target="#b50">[51]</ref>, we perform pooling operation after FFN of VTE and then replace STE of our proposed model with it, <ref type="figure">Figure 6</ref>. Multi-head attention maps (h = 8) from VTE (left) and STE (right) of our 243-frame model. It illustrates the self-attention mechanism systematically assigns a weight distribution to frames, all of which might contribute to the inference. The brighter color indicates stronger attention across frames. the new architecture is termed as Pooing Transformer. The error increases by 0.4 mm, which highlights that our STE can preserve more valuable information. Removing VTE (only trained with single-frame loss) leads to 1.4 mm increase in MPJPE error. Meanwhile, removing STE (only trained with sequence loss), the performance decreases. These results validate the effectiveness of our proposed fullto-single mechanism by using further processing to produce more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention visualization.</head><p>Our method is easily interpretable through visualizing the attention score across frames to explain what temporal dependencies the target frame relies on. Visualization results of the multi-head attention maps of the first attention layers from VTE and STE (243-frame model) are shown in <ref type="figure">Figure 6</ref>. The left map shows it selectively identifies important sequences close to the input frames <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref> and the right map mainly pays strong attention to the target frame across all the sequences. This is expected since the proposed full-to-single strategy enables the VTE and STE modules to learn different representations, where VTE models long-range dependencies and enforces temporal consistency across frames, and STE learns a specific representation to reach an optimal inference for the target frame. Note that few attention head maps are sparse due to the different temporal patterns or semantics.</p><p>3D reconstruction visualization. We further evaluate our method on wild videos from YouTube, as shown in <ref type="figure" target="#fig_2">Figure 7</ref>. Despite the challenging samples with huge movements and hard actions, our model can produce realistic and structurally plausible 3D predictions from complicated pose articulation. This demonstrates that our method is robust to partial occlusions and tolerant to depth ambiguity. More results can be seen in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present Lifting Transformer network with strided Transformer encoder (STE) and full-to-single supervision scheme for lifting a sequence of 2D joint locations to a 3D pose. Compared with standard Transformer encoder, the proposed STE can aggregate long-range information to a single-vector pose in a global and local fashion. Meanwhile, the computation cost can be reduced by a large margin. The proposed full-to-single supervision scheme enforces temporal smoothness and further refines the estimation. Comprehensive experiments on two benchmark datasets demonstrate that our method achieves superior performance compared to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose</head><p>Ground Truth 3D Pose Ground Truth <ref type="figure">Figure 8</ref>. Visual results of our proposed method on Human3.6M dataset (first 3 rows) and HumanEva-I dataset (last 2 rows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ours ATTN-TCN TCN GCN <ref type="figure">Figure 9</ref>. Qualitative comparisons on challenging in-the-wild videos with previous state-of-the-art methods, ATTN-TCN <ref type="bibr" target="#b26">[27]</ref>, TCN <ref type="bibr" target="#b31">[32]</ref>, and GCN <ref type="bibr" target="#b0">[1]</ref>. The last row shows the failure case, where the 2D detector has failed badly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our proposed Lifting Transformer for predicting the 3D joint locations of the target frame from the estimated 2D pose sequences. The network first models long-range information via a vanilla Transformer encoder (VTE), and then aggregates the information to one target pose representation from the proposed strided Transformer encoder (STE). The model is trained end-to-end at both the full sequence scale and single target frame scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Left: ablation studies on different sequence lengths of our method on Human3.6M with the MPJPE metric. Right: the impact of 2D detections on Human3.6M. Here, N (0, σ 2 ) represents the Gaussian noise with mean zero and σ is the standard deviation. (CPN) -Cascaded Pyramid Network; (SH) Stack Hourglass; (GT) -2D ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on challenging wild videos. The number is the frame index of input videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 Table 2</head><label>12</label><figDesc>Protocol #1Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.</figDesc><table><row><cell>Martinez et al. (ICCV'17)[28]</cell><cell cols="3">51.8 56.2 58.1 59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell cols="2">74.0 94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. (AAAI'18) [8]</cell><cell cols="3">50.1 54.3 57.0 57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell><cell cols="2">72.8 88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Zhao et al. (CVPR'19) [48]</cell><cell cols="3">47.3 60.7 51.4 60.5</cell><cell>61.1</cell><cell>49.9</cell><cell>47.3</cell><cell>68.1</cell><cell cols="2">86.2 55.0</cell><cell>67.8</cell><cell>61.0</cell><cell>42.1</cell><cell>60.6</cell><cell>45.3</cell><cell>57.6</cell></row><row><cell>Lee et al. (ECCV'18) [19]</cell><cell cols="3">40.2 49.2 47.8 52.6</cell><cell>50.1</cell><cell>75.0</cell><cell>50.2</cell><cell>43.0</cell><cell cols="2">55.8 73.9</cell><cell>54.1</cell><cell>55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3</cell><cell>52.8</cell></row><row><cell>Cai et al. (ICCV'19) [1]</cell><cell cols="3">44.6 47.4 45.6 48.8</cell><cell>50.8</cell><cell>59.0</cell><cell>47.2</cell><cell>43.9</cell><cell cols="2">57.9 61.9</cell><cell>49.7</cell><cell>46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4</cell><cell>48.8</cell></row><row><cell>Pavllo et al. (CVPR'19) [32]</cell><cell cols="3">45.2 46.7 43.3 45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell cols="2">57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Xu et al. (CVPR'20) [45]</cell><cell cols="3">37.4 43.5 42.7 42.7</cell><cell>46.6</cell><cell>59.7</cell><cell>41.3</cell><cell>45.1</cell><cell cols="2">52.7 60.2</cell><cell>45.8</cell><cell>43.1</cell><cell>47.7</cell><cell>33.7</cell><cell>37.1</cell><cell>45.6</cell></row><row><cell>Liu et al. (CVPR'20) [27]</cell><cell cols="3">41.8 44.8 41.1 44.9</cell><cell>47.4</cell><cell>54.1</cell><cell>43.4</cell><cell>42.2</cell><cell cols="2">56.2 63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell>Zeng et al. (ECCV'20) [47]</cell><cell cols="3">46.6 47.1 43.9 41.6</cell><cell>45.8</cell><cell>49.6</cell><cell>46.5</cell><cell>40.0</cell><cell cols="2">53.4 61.1</cell><cell>46.1</cell><cell>42.6</cell><cell>43.1</cell><cell>31.5</cell><cell>32.6</cell><cell>44.8</cell></row><row><cell>Wang et al. (ECCV'20) [42]</cell><cell cols="3">40.2 42.5 42.6 41.1</cell><cell>46.7</cell><cell>56.7</cell><cell>41.4</cell><cell>42.3</cell><cell cols="2">56.2 60.4</cell><cell>46.3</cell><cell>42.2</cell><cell>46.2</cell><cell>31.7</cell><cell>31.0</cell><cell>44.5</cell></row><row><cell>Ours (T=243 CPN)</cell><cell cols="3">41.2 44.0 40.2 42.9</cell><cell>45.6</cell><cell>52.0</cell><cell>43.1</cell><cell>40.8</cell><cell cols="2">56.2 62.4</cell><cell>45.6</cell><cell>43.5</cell><cell>44.5</cell><cell>30.5</cell><cell>31.6</cell><cell>44.3</cell></row><row><cell>Protocol #2</cell><cell cols="7">Dir. Disc Eat Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="5">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell cols="4">Martinez et al. (ICCV'17) [28] 39.5 43.2 46.4 47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell><cell cols="2">56.5 69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Fang et al. (AAAI'18) [8]</cell><cell cols="3">38.2 41.7 43.7 44.9</cell><cell>48.5</cell><cell>55.3</cell><cell>40.2</cell><cell>38.2</cell><cell cols="2">54.5 64.4</cell><cell>47.2</cell><cell>44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell cols="4">Pavlakos et al. (CVPR'18) [30] 34.7 39.8 41.8 38.6</cell><cell>42.5</cell><cell>47.5</cell><cell>38.0</cell><cell>36.6</cell><cell cols="2">50.7 56.8</cell><cell>42.6</cell><cell>39.6</cell><cell>43.9</cell><cell>32.1</cell><cell>36.5</cell><cell>41.8</cell></row><row><cell>Liu et al. (ECCV'20) [24]</cell><cell cols="3">35.9 40.0 38.0 41.5</cell><cell>42.5</cell><cell>51.4</cell><cell>37.8</cell><cell>36.0</cell><cell cols="2">48.6 56.6</cell><cell>41.8</cell><cell>38.3</cell><cell>42.7</cell><cell>31.7</cell><cell>36.2</cell><cell>41.2</cell></row><row><cell>Cai et al. (ICCV'19) [1]</cell><cell cols="3">35.7 37.8 36.9 40.7</cell><cell>39.6</cell><cell>45.2</cell><cell>37.4</cell><cell>34.5</cell><cell cols="2">46.9 50.1</cell><cell>40.5</cell><cell>36.1</cell><cell>41.0</cell><cell>29.6</cell><cell>33.2</cell><cell>39.0</cell></row><row><cell>Pavllo et al. (CVPR'19) [32]</cell><cell cols="3">34.1 36.1 34.4 37.2</cell><cell>36.4</cell><cell>42.2</cell><cell>34.4</cell><cell>33.6</cell><cell cols="2">45.0 52.5</cell><cell>37.4</cell><cell>33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell>Xu et al. (CVPR'20) [45]</cell><cell cols="3">31.0 34.8 34.7 34.4</cell><cell>36.2</cell><cell>43.9</cell><cell>31.6</cell><cell>33.5</cell><cell cols="2">42.3 49.0</cell><cell>37.1</cell><cell>33.0</cell><cell>39.1</cell><cell>26.9</cell><cell>31.9</cell><cell>36.2</cell></row><row><cell>Ours (T=243 CPN)</cell><cell cols="3">33.8 35.7 33.6 35.8</cell><cell>35.7</cell><cell>41.4</cell><cell>34.5</cell><cell>32.1</cell><cell cols="2">44.9 51.4</cell><cell>37.3</cell><cell>34.1</cell><cell>35.7</cell><cell>24.3</cell><cell>26.4</cell><cell>35.8</cell></row><row><cell>Model</cell><cell></cell><cell>Parameters</cell><cell cols="3">MPJPE (mm)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pavllo et al. [32] (T = 27)</cell><cell>8.56M</cell><cell></cell><cell>48.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pavllo et al. [32] (T = 81)</cell><cell>12.75M</cell><cell></cell><cell>47.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pavllo et al. [32] (T = 243)</cell><cell>16.95M</cell><cell></cell><cell>46.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Liu et al. [27] (T = 27)</cell><cell>5.69M</cell><cell></cell><cell>48.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Liu et al. [27] (T = 81)</cell><cell>8.46M</cell><cell></cell><cell>46.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Liu et al. [27] (T = 243)</cell><cell>11.25M</cell><cell></cell><cell>45.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (T = 27)</cell><cell></cell><cell>3.22M</cell><cell></cell><cell>46.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (T = 81)</cell><cell></cell><cell>3.28M</cell><cell></cell><cell>45.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (T = 243)</cell><cell></cell><cell>3.44M</cell><cell></cell><cell>44.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>. Quantitative comparisons with the state-of-the-art methods on Human3.6M under protocol #1 and protocol #2, where T denotes the number of input frames used. (CPN) -Cascaded Pyramid Network. Best in bold.. Quantitative comparisons with state-of-the-art methods in different receptive fields on Human3.6M. The MPJPE metric and number of parameters are reported. Best in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg. Martinez et al. (ICCV'17)[28] 37.7 44.4 40.3 42.1</figDesc><table><row><cell>Protocol #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48.2</cell><cell>54.9</cell><cell>44.4</cell><cell>42.1</cell><cell cols="2">54.6 58.0</cell><cell>45.1</cell><cell>46.4</cell><cell>47.6</cell><cell>36.4</cell><cell>40.4</cell><cell>45.5</cell></row><row><cell>Lee et al. (ECCV'18) [19]</cell><cell cols="4">32.1 36.6 34.3 37.8</cell><cell>44.5</cell><cell>49.9</cell><cell>40.9</cell><cell>36.2</cell><cell cols="2">44.1 45.6</cell><cell>35.3</cell><cell>35.9</cell><cell>30.3</cell><cell>37.6</cell><cell>35.5</cell><cell>38.4</cell></row><row><cell>Cai et al. (ICCV'19) [1]</cell><cell cols="4">32.9 38.7 32.9 37.0</cell><cell>37.3</cell><cell>44.8</cell><cell>38.7</cell><cell>36.1</cell><cell cols="2">41.0 45.6</cell><cell>36.8</cell><cell>37.7</cell><cell>37.7</cell><cell>29.5</cell><cell>31.6</cell><cell>37.2</cell></row><row><cell>Liu et al. (CVPR'20) [27]</cell><cell cols="4">34.5 37.1 33.6 34.2</cell><cell>32.9</cell><cell>37.1</cell><cell>39.6</cell><cell>35.8</cell><cell cols="2">40.7 41.4</cell><cell>33.0</cell><cell>33.8</cell><cell>33.0</cell><cell>26.6</cell><cell>26.9</cell><cell>34.7</cell></row><row><cell>Chen et al. (TCSVT'21) [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.3</cell></row><row><cell>Zeng et al. (ECCV'20) [47]</cell><cell cols="4">34.8 32.1 28.5 30.7</cell><cell>31.4</cell><cell>36.9</cell><cell>35.6</cell><cell>30.5</cell><cell cols="2">38.9 40.5</cell><cell>32.5</cell><cell>31.0</cell><cell>29.9</cell><cell>22.5</cell><cell>24.5</cell><cell>32.0</cell></row><row><cell>Ours (T=243 GT)</cell><cell cols="4">26.9 30.6 27.2 27.4</cell><cell>29.7</cell><cell>33.6</cell><cell>31.9</cell><cell>26.3</cell><cell cols="2">37.8 38.6</cell><cell>29.0</cell><cell>29.2</cell><cell>27.5</cell><cell>19.8</cell><cell>20.3</cell><cell>29.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparisons of MPJPE in millimeter on Human3.6M under protocol #1, using ground truth 2D joint locations as input. Best in bold.</figDesc><table><row><cell></cell><cell></cell><cell>Walk</cell><cell></cell><cell></cell><cell>Jog</cell><cell></cell><cell></cell><cell>Box</cell><cell></cell></row><row><cell></cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Avg.</cell></row><row><cell>Martinez et al. [28]</cell><cell cols="6">19.7 17.4 46.8 26.9 18.2 18.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pavlakos et al. [31]</cell><cell cols="6">22.3 19.5 29.7 28.9 21.9 23.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Lee et al. [19]</cell><cell cols="10">18.6 19.9 30.5 25.7 16.8 17.7 42.8 48.1 53.4 30.3</cell></row><row><cell>Pavllo et al. [32]</cell><cell cols="10">13.9 10.2 46.6 20.9 13.1 13.8 23.8 33.7 32.0 23.1</cell></row><row><cell cols="11">Ours (T=27 MRCNN) 14.0 10.0 32.8 19.5 13.6 14.2 22.4 21.6 22.5 18.9</cell></row><row><cell>Ours (T=27 GT)</cell><cell>9.7</cell><cell cols="9">7.6 15.8 12.3 9.4 11.2 14.8 12.9 16.5 12.2</cell></row><row><cell cols="11">Table 4. Quantitative results on HumanEva-I dataset under proto-</cell></row><row><cell cols="11">col #2. Best in bold, second-best underlined. (MRCNN) -Mask-</cell></row><row><cell cols="4">RCNN; (GT) -2D ground truth.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">size of 256 for Human3.6M, 64 for HumanEva-I. An ini-</cell></row><row><cell cols="11">tial learning rate of 0.001 is used with a shrink factor of</cell></row><row><cell cols="11">0.95 applied after each epoch and 0.5 after every 5 epochs.</cell></row><row><cell cols="11">Note that we only adopt horizontal flip augmentation dur-</cell></row><row><cell cols="11">ing training and test stages and only compute MPJPE loss</cell></row><row><cell>for training.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Ablation study on the hyperparameters of our model on Human3.6M under protocol #1. N1 and N2 is the number of VTE and STE layers, respectively. dm and d f are the dimensions and the number of hidden units.first analyze the effect of the number of VTE layers. Empirically, it can be found that the performance cannot be improved when naively stacking multiple standard Transformer encoder layers. However, our model that introduces STE is more accurate at the same level of the number of Transformer encoder layers and model parameters. For example, our method (N 1 = 3 and N 2 = 3) has better performance and fewer FLOPs than N 1 = 6 at the same d m = 256 and d f = 512 hidden units (46.9 mm vs. 47.9 mm, 60.15M vs. 85.54M). Meanwhile, our STE (N 2 = 3, 17.68M) also have fewer FLOPs than standard Transformer encoder (N 1 = 3, 43.07M) with similar parameters, which achieves 2.4× less computation. It verifies the effectiveness of our proposed STE that merges the nearby poses to reduce the redundancy of the sequence in a simple form. Then, we investigate the hyperparameters on the performance and parameters of both modules. It can be observed that using 3 encoder layers, 256 dimensions, and 512 hidden units achieves the best performance.Strided factor. We also explore the design choice of Ablation study on the strided factor of STE with the receptive field T = 3 × 3 × 3 = 27. The evaluation is performed on Human3.6M under protocol #1.</figDesc><table><row><cell>Ground Truth</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Anatomy-aware 3d human pose estimation with bone-based pose decomposition. IEEE Transactions on Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10895" to="10904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human 3d pose estimation with a tilting camera for social mobile robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>Garcia-Salguero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco-Angel</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">4943</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transformer feed-forward layers are key-value memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14913</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching robots to predict human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting better feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaozheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1469" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transreid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04378</idno>
		<title level="m">Transformer-based object reidentification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convbert: Improving bert with span-based dynamic convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Trear: Transformerbased rgb-d egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03904.2</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformer guided geometry model for flow-based unsupervised visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature boosting network for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="501" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Sen-ching Cheung, and Vijayan Asari</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Motion capture assisted animation: Texturing and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Pullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 29th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under selfocclusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1888" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lourdes Agapito, and Chris Russell. Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13985</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rgb-d-based human motion recognition with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="118" to="139" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lite transformer with long-short range attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Funnel-transformer: Filtering out sequential redundancy for efficient language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Zihang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><surname>Guokun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
