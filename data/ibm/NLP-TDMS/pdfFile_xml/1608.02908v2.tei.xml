<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Residual Networks of Residual Networks: Multilevel Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20161">AUGUST 2016 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>On L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Residual Networks of Residual Networks: Multilevel Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20161">AUGUST 2016 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image classification</term>
					<term>residual networks</term>
					<term>residual networks of residual networks</term>
					<term>shortcut</term>
					<term>stochastic depth</term>
					<term>Ima- geNet data set</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A residual-networks family with hundreds or even thousands of layers dominates major image recognition tasks, but building a network by simply stacking residual blocks inevitably limits its optimization ability. This paper proposes a novel residual-network architecture, Residual networks of Residual networks (RoR), to dig the optimization ability of residual networks. RoR substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping. In particular, RoR adds level-wise shortcut connections upon original residual networks to promote the learning capability of residual networks. More importantly, RoR can be applied to various kinds of residual networks (ResNets, Pre-ResNets and WRN) and significantly boost their performance. Our experiments demonstrate the effectiveness and versatility of RoR, where it achieves the best performance in all residual-networklike structures. Our RoR-3-WRN58-4+SD models achieve new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN, with test errors 3.77%, 19.73% and 1.59%, respectively. RoR-3 models also achieve state-of-the-art results compared to ResNets on ImageNet data set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C ONVOLUTIONAL Neural Networks (CNNs) have given the computer vision community a significant shock <ref type="bibr" target="#b0">[1]</ref>, and have been improving state-of-the-art results in many computer vision applications. Since AlexNets' <ref type="bibr" target="#b1">[2]</ref> groundbreaking victory at the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC 2012) <ref type="bibr" target="#b2">[3]</ref>, deeper and deeper  <ref type="figure">Fig. 1</ref>. The image on the left of the dashed line is an original residual network, which contains a series of residual blocks, and each residual block has one shortcut connection. The number <ref type="bibr">(16, 32,</ref> or 64) on each residual block is the number of output feature map. F (x) is the residual mapping and x is the identity mapping. The original mapping is represented as F (x) + x. The image on the right of the dashed line is our new residual networks of residual networks architecture with three shortcut levels. RoR is constructed by adding identity shortcuts level by level based on original residual networks.</p><p>CNNs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> have been proposed and achieved better performance on ImageNet or other benchmark data sets. The results of these models revealed the importance of network depth, as deeper networks lead to superior results.</p><p>With a dramatic increase in depth, Residual Networks (ResNets) <ref type="bibr" target="#b11">[12]</ref> achieved the state-of-the-art performance award at the ILSVRC 2015 for classification, localization, detection, and COCO detection as well as segmentation tasks. However, very deep models will suffer vanishing gradients and overfitting problems; Thus, the performance of thousandlayer ResNets is worse than hundred-layer ResNets. Then the Identity Mapping ResNets (Pre-ResNets) <ref type="bibr" target="#b12">[13]</ref> simplified the residual networks training by BN-ReLU-conv order. Pre-ResNets can alleviate the vanishing gradients problem, so that the performance of thousand-layer Pre-ResNets can be further improved. The latest Wide Residual Networks (WRN) <ref type="bibr" target="#b13">[14]</ref> arXiv:1608.02908v2 [cs.CV] 5 Mar 2017 treated the vanishing gradients problem by decreasing depth and increasing the width of residual networks. Nevertheless, the exponentially increasing number of parameters brought by broader networks worsens the overfitting problem. As a result, dropout and drop-path methods are usually used to alleviate overfitting, and the leading method on ResNets is Stochastic Depth residual networks (SD) <ref type="bibr" target="#b14">[15]</ref>, which can improve test accuracy and reduce training time. All kinds of residual networks are based on one basic hypothesis: By using shortcut connections, residual networks perform residual mapping fitted by stacked nonlinear layers, which is easier to be optimized than the original mapping <ref type="bibr" target="#b11">[12]</ref>. Furthermore, we hypothesize that the residual mapping of residual mapping is easier to optimize than original residual mapping. So on top of this hypothesis, we can construct a better residual-network architecture to enhance performance.</p><p>In this paper, we presented a novel and simple architecture called Residual networks of Residual networks (RoR). First, compared to the original one-level-shortcut-connection ResNets, we added extra shortcut connections to the original ResNets level by level. A multilevel network was then constructed, as seen in <ref type="figure">Fig. 1</ref>. For this network, we analyzed the effects of different shortcut level numbers, shortcut types and maximum epoch numbers. Second, to alleviate overfitting, we trained RoR with the drop-path method, and obtained an apparent performance boost. Third, we built RoR on various residual networks (Pre-ResNets and WRN), and found that RoR is not only suitable for original ResNets, but also fits in nicely with other residual networks. Through massive experiments on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b15">[16]</ref>, SVHN <ref type="bibr" target="#b16">[17]</ref> and ImageNet <ref type="bibr" target="#b2">[3]</ref>, our RoR obtained as much optimization ability of the residual networks as possible, only by adding a few shortcuts. Although this approach seems quite simple, it is surprisingly effective in practice and achieves the new stateof-the-art results on the above data sets.</p><p>Our main contribution is threefold:</p><p>1. Our introduction of RoR improved the optimization ability of ResNets by adding a few identity shortcuts. And RoR achieved better performance than ResNets by using the same number of layers on different data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>RoR is suitable for other residual networks as well and will boost their performance, which makes it an effective complement of the residual-networks family.</p><p>3. Through experiments, we analyzed the effects of different depths, widths, shortcuts level numbers, shortcut types and maximum epoch numbers to RoR, and developed reasonable strategies for RoR applications in different data sets.</p><p>The remainder of the paper is organized as follows. Section II briefly reviews related work for deep convolutional neural networks and the residual-networks family. The proposed RoR method is illustrated in Section III. The optimization of RoR is described in Section IV. Experimental results and analysis are presented in Section V, leading to conclusions in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deeper and Deeper Convolutional Neural Networks</head><p>In the past several years, deeper and deeper CNNs have been constructed, as the more convolutional layers are in CNNs, the better optimization ability CNNs can obtain. From 5-conv+3-fc AlexNet (ILSVRC2012 winner) <ref type="bibr" target="#b1">[2]</ref> to the 16-conv+3-fc VGG networks <ref type="bibr" target="#b6">[7]</ref> and 21-conv+1-fc GoogleNet (ILSVRC2014 winner) <ref type="bibr" target="#b10">[11]</ref>, both the accuracy and the depth of CNNs have continued to increase. However, very deep CNNs face the crucial problem of vanishing gradients <ref type="bibr" target="#b17">[18]</ref>. Earlier works adopted initialization methods and layer-wise training to reduce this problem <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Moreover, the ReLU activation function <ref type="bibr" target="#b20">[21]</ref> and its variants ELU <ref type="bibr" target="#b21">[22]</ref>, PReLU <ref type="bibr" target="#b22">[23]</ref>, and PELU <ref type="bibr" target="#b23">[24]</ref> also prevent vanishing gradients. Fortunately, this problem could be largely addressed by batch normalization (BN) <ref type="bibr" target="#b24">[25]</ref> and carefully normalized weights initialization <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref> according to recent research. BN <ref type="bibr" target="#b24">[25]</ref> standardized the mean and variance of hidden layers for each mini-batch, while MSR <ref type="bibr" target="#b22">[23]</ref> initialized the weights by a more reasonable variance. On the other hand, a degradation problem has been exposed <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> that is, not all systems are easy to optimize. In order to resolve this problem, several methods were proposed. Highway Networks <ref type="bibr" target="#b27">[28]</ref> consisted of a mechanism allowing 2D-CNNs to interact with a simple memory mechanism. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. ResNets <ref type="bibr" target="#b11">[12]</ref> simplified Highway Networks using a simple skip connection mechanism to propagate information to deeper layers of networks. ResNets are simpler and more effective than highway Networks. Recently, FractalNet <ref type="bibr" target="#b28">[29]</ref> generated an extremely deep network whose structural layout was precisely a truncated fractal by repeating application of a single expansion rule, and this method showed that residual learning was not required for ultra-deep networks. However, in order to get the competitive performance of ResNets, FractalNet must have many more parameters than ResNets. Hence, more and more residual network variants and architectures have been proposed, and they form a residualnetworks family together <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Residual-Networks Family</head><p>The basic idea of ResNets <ref type="bibr" target="#b11">[12]</ref> is that residual mapping is easy to optimize, so ResNets skip blocks of convolutional layers by using shortcut connections to form shortcut blocks (residual blocks). These stacked residual blocks greatly improve training efficiency and largely resolve the degradation problem by employing BN <ref type="bibr" target="#b24">[25]</ref> and MSR <ref type="bibr" target="#b22">[23]</ref>. The ResNets architecture and residual blocks are shown in <ref type="figure">Fig. 1</ref>, where each residual block can be expressed in a general form:</p><formula xml:id="formula_0">y l = h(x l ) + F (x l , W l ), x l+1 = f (y l )<label>(1)</label></formula><p>where x l and x l + 1 are input and output of the l-th block, respectively. F is a residual mapping function, h(x l ) = x l is an identity mapping function, and f is a ReLU function. However, the vanishing gradients problem still exists, as the test result of 1202-layer ResNets is worse than 110-layer ResNets on CIFAR-10 <ref type="bibr" target="#b11">[12]</ref>.</p><p>In the Pre-ResNets, He et al. <ref type="bibr" target="#b12">[13]</ref> created a "direct" path for propagating information through the entire network by letting both h(x l ) and f serve as identity mappings. The residual block of Pre-ResNets performs the follow computation:</p><formula xml:id="formula_1">x l+1 = h(x l ) + F (x l , W l )<label>(2)</label></formula><p>The new residual block with a BN-ReLU-conv order can reduce training difficulties, so that Pre-ResNets can get better performance than original ResNets. More importantly, Pre-ResNets can reduce the vanishing gradients problem even for 1001-layer Pre-ResNets. Inspired by Pre-ResNets, Shen et al. <ref type="bibr" target="#b32">[33]</ref> proposed weighted residuals for very deep networks (WResNet), which removed the ReLU from highway and used weighted residual functions to create a "direct" path. This method is also capable of 1000+ layers residual networks training and achieves good accuracy. In order to further reduce vanishing gradients, Shah et al. <ref type="bibr" target="#b30">[31]</ref>, Trottier et al. <ref type="bibr" target="#b23">[24]</ref> proposed the use of ELU and PELU respectively instead of ReLU in residual networks. In addition to the vanishing gradients problem, overfitting is another challenging issue for CNNs. Huang and Sun et al. <ref type="bibr" target="#b14">[15]</ref> proposed a drop-path method, Stochastic Depth residual networks (SD), which randomly dropped a subset of layers and bypassed them with identity mapping for every mini-batch. SD can alleviate overfitting and reduce vanishing problem, so it is a good complement of residual networks. By combining dropout and SD, Singh et al. <ref type="bibr" target="#b31">[32]</ref> proposed a new stochastic training method, SwapOut, which can be viewed as an ensemble of ResNets, dropout ResNets, and SD ResNets.</p><p>Recently, more variants of residual networks have been proposed, and they all promote learning capability by expending width of the model. Resnet in Resnet (RiR) <ref type="bibr" target="#b29">[30]</ref> is a generalized residual architecture which combines ResNets and standard CNNs in parallel residual and non-residual streams. WRN <ref type="bibr" target="#b13">[14]</ref> decreases depth and increases width of residual networks by adding more feature planes, and achieves the latest state-of-the-art results on CIFAR-10 and SVHN. The newest Convolutional Residual Memory Networks (CRMN) <ref type="bibr" target="#b33">[34]</ref> was inspired by WRN and Highway Networks. CRMN augments convolutional residual networks with a long short-term memory mechanism based on WRN, and achieves the latest stateof-the-art performance on CIFAR-100. These wider residual networks indicates that wide and shallow residual networks result in good performance and easy training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESIDUAL NETWORKS OF RESIDUAL NETWORKS</head><p>RoR is based on a hypothesis: To dig the optimization ability of residual networks, we can optimize the residual mapping of residual mapping. So we add shortcuts level by level to construct RoR based on residual networks. <ref type="figure">Fig. 2</ref> shows the original residual network with L residual blocks. These L original residual blocks are denoted as the final-level shortcuts. First, we add a shortcut above all residual blocks, and this shortcut can be called a root shortcut or first-level shortcut. Generally, we use 16, 32 and 64 filters <ref type="bibr" target="#b15">16</ref>   sequentially in the convolutional layers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and each kind of filter has L/3 residual blocks which form three residual block groups. Second, we add a shortcut above each residual block group, and these three shortcuts are called secondlevel shortcuts. Then we can continue adding shortcuts as inner-level shortcuts by separating each residual block group equally. Finally, the shortcuts in the original residual blocks are regarded as the final-level shortcuts. Let m denote a shortcut level number, m=1, 2, 3.... When m=1, RoR is an original residual networks with no other shortcut level. When m=2, the RoR has a root level and a final level. In this paper, m is 3, so the RoR has a root level, middle level and final level, as shown in <ref type="figure">Fig. 2</ref>. Compared to the top-right residual block,the bottom-right block is without ReLU in the end, because there are extra additions following it.</p><p>When m=3, three residual blocks located at the end of each residual block group can be expressed by the following formulations, and the other original residual blocks remain the same.</p><formula xml:id="formula_2">y L/3 = g(x 1 ) + h(x L/3 ) + F (x L/3 , W L/3 ), x L/3+1 = f (y L/3 )<label>(3)</label></formula><formula xml:id="formula_3">y 2L/3 = g(x L/3+1 ) + h(x 2L/3 ) + F (x 2L/3 , W 2L/3 ), x 2L/3+1 = f (y 2L/3 )<label>(4)</label></formula><formula xml:id="formula_4">y L = g(x 1 ) + g(x 2L/3+1 ) + h(x L ) + F (x L , W L ), x L+1 = f (y L )<label>(5)</label></formula><p>where x l and x l +1 are input and output of the l-th block, and F is a residual mapping function; h(x l ) = x l , and g(x l ) = x l are both identity mapping functions. g(x l ) expresses the identity <ref type="bibr" target="#b15">16</ref>   <ref type="figure">Fig. 3</ref>. Pre-RoR-3 and RoR-3-WRN architectures. m=3. The addition is followed by the ReLU. Projection shortcut is done by 1×1 convolutions. BN-ReLU-conv order in residual blocks is adopted. If k=1, this is a Pre-RoR-3 architecture; Otherwise, this is an RoR-3-WRN architecture. There are several "direct" paths for propagating information created by identity mappings. mapping of the first-level and second-level shortcuts, and h(x l ) denotes the identity mapping of the final-level shortcuts. g(x l ) function is a type B projection shortcut <ref type="bibr" target="#b11">[12]</ref>.</p><p>In this paper, we will construct RoR based on ResNets, Pre-ResNets and WRN, in this order. When we use original ResNets as basic residual networks, f is an ReLU function, and RoR adopts a conv-BN-ReLU order in the residual blocks. The architecture of RoR-3 in detail is shown in <ref type="figure">Fig. 2</ref>. RoR constructed on Pre-ResNets, and WRN are named after Pre-RoR and RoR-WRN. Their order of residual blocks is BN-ReLU-conv, and all g(x l ), h(x l ), and f functions are identity mapping. The architectures of Pre-RoR and RoR-WRN in detail are shown in <ref type="figure">Fig. 3</ref>. The formulations of the three different residual blocks are changed by the following formulas:</p><formula xml:id="formula_5">x L/3+1 = g(x 1 ) + h(x L/3 ) + F (x L/3 , W L/3 ) (6) x 2L/3+1 = g(x L/3+1 ) + h(x 2L/3 ) + F (x 2L/3 , W 2L/3 ) (7) x L+1 = g(x 1 ) + g(x 2L/3+1 ) + h(x L ) + F (x L , W L ) (8)</formula><p>At least two reasons for promoting the optimization ability of RoR by adding extra shortcut connections are presented. First, ResNets transform the learning of y l into the learning of F (x l , W l ) by residual block structure, since residual mapping function F is easier to learn than y l , as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. RoR adds extra shortcuts above the original residual blocks, and y l also becomes residual mapping. So RoR transfers the learning problem to learning the residual mapping of residual mapping which is simpler and easier to learn than the original residual mapping. Second, ResNets use shortcuts to propagate information only between neighboring layers in residual blocks. RoR creates several direct paths for propagating information between different original residual blocks by adding extra shortcuts, so layers in upper blocks can propagate information to layers in lower blocks. By information propagation, RoR can alleviate the vanishing gradients problem. The good results of the following experiments show that RoR benefits from the standpoint of optimization through RoR residual mapping and the extra shortcuts to expedite information propagation between layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OPTIMIZATION OF ROR</head><p>In order to optimize RoR, we must determine some important parameters and principles, such as shortcut level number, identity mapping type, maximum epoch number and whether to use the drop-path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shortcut level number of RoR</head><p>It is important to choose a suitable number of RoR levels for a satisfying performance. The more shortcut levels chosen, the more branches and parameters are added. The overfitting problem will be exacerbated, and the performance may decrease. However, RoR improvements will be less obvious if the number of levels is too small. So we must find a suitable number to keep the balance. We conducted some experiments on CIFAR-10 with different depths and shortcut levels, and the results are described in <ref type="figure">Fig. 5</ref>. RoR (m=3) had the best performance of all, and the performance of RoR (m=4 or 5) was worse than the original ResNets (m=1). So we chose m=3, which is shown in <ref type="figure">Fig. 2</ref> and denoted as RoR-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Identity Mapping Types of RoR</head><p>He et al. <ref type="bibr" target="#b11">[12]</ref> investigated three types of projection shortcuts: (A) Zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter-free; (B) projection shortcuts (done by 1×1 convolutions) are used for increasing dimensions, while other shortcuts are identity connections, and (C) all shortcuts are projections. Type B is slightly better than   Type A, and Type C is marginally better than B. But Type C has too many extra parameters, so we used Type A or B as the type of final-level shortcuts.</p><p>Because CIFAR-10 has only 10 classes, the problem of overfitting is not critical, and extra parameters will not obviously escalate overfitting; Thus, we used Type B in the original residual blocks on CIFAR-10. <ref type="figure" target="#fig_4">Fig. 6</ref> shows that we can achieve better performance using Type B than Type A on CIFAR-10. However, for CIFAR-100, which has 100 classes with less training examples, overfitting is critical, so we used Type A in the final level. <ref type="figure" target="#fig_5">Fig. 7</ref> shows that we can achieve better performance using Type A than Type B on CIFAR-100. The original ResNets were used in all these experiments.</p><p>The shortcuts in Level 1 and Level 2 of RoR are all projection shortcuts. We used Type B in these levels, because the input and output planes of these shortcuts are very different (especially for Level 1), and the zero-padding (Type A) will bring more deviation. <ref type="table" target="#tab_4">Table I</ref> shows the final level where using Type A (on CIFAR-100) or Type B (on CIFAR-10), while other levels using Type B can achieve better performance than pure type A, independent of whether m=2 or m=3. In <ref type="table" target="#tab_4">Table I</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Maximum Epoch Number of RoR</head><p>He et al. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> adopted 164 epochs to train CIFAR-10 and CIFAR-100, but we found this epoch number inadequate to optimize ResNets and RoR. <ref type="figure">Fig. 8</ref> and <ref type="figure" target="#fig_6">Fig. 9</ref> show that training 500 epochs can get significant promotion. So in this paper, we choose 500 as the maximum epoch number.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Drop Path by Stochastic Depth</head><p>Overfitting can be a critical problem for the CIFAR-100 data set. Adding extra shortcuts to the original ResNets can cause the overfitting problems to be even more severe. So our RoR must employ a method to alleviate the overfitting problem. The most frequently used methods are dropout <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> and drop-path <ref type="bibr" target="#b36">[37]</ref>, which modify interactions between sequential network layers in order to discourage co-adaptaion. Dropout is less effective when used in convolutional layers, and the results of WRN <ref type="bibr" target="#b13">[14]</ref> also proved that the effect of dropout in residual networks was unapparent. So we did not employ dropout in RoR. Drop-paths prevent co-adaptation of parallel paths by randomly dropping the path. He et al. <ref type="bibr" target="#b12">[13]</ref> proved that the network cannot converge to a good solution by dropping an identity mapping path randomly, because dropping an identity mapping path greatly influences training. However, Huang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a stochastic depth drop-path method which only dropped the residual mapping path randomly. Their experiments showed that the method reduced the test errors significantly. So in this paper we use the stochastic depth droppath method in our RoR except for the ImageNet data set, and it can significantly alleviate overfitting, especially on the CIFAR-100 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND ANALYSIS</head><p>We empirically demonstrated the effectiveness of RoR on a series of benchmark data sets: CIFAR-10, CIFAR-100, SVHN and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>For these data sets we compared between the results of RoR and the original ResNets baseline, and other state-of-theart methods. In the case of CIFAR, we used the same 110layer and 164-layer ResNets used by <ref type="bibr" target="#b11">[12]</ref> to construct RoR architecture. The original ResNets contained three groups of 16 filters, 32 filters and 64 filters of residual blocks, and the feature map sizes are 32, 16 and 8, respectively. The 110-layer RoR contained 18 final residual blocks, three middle-level residual blocks (every middle-level residual block contained six final residual blocks), and one root-level residual block (the root-level residual block contained three middle-level residual blocks). The 164-layer RoR contained 27 final residual blocks, three middle-level residual blocks (every middle-level residual block contained nine final residual blocks), and one root-level residual block. Our implementations are based on Torch 7 with one Nvidia Geforce Titan X. We adopted batch normalization (BN) <ref type="bibr" target="#b24">[25]</ref> after each convolution in residual mapping paths and before activation (ReLU) <ref type="bibr" target="#b20">[21]</ref>, as shown in <ref type="figure">Fig. 2</ref>. In Pre-RoR and RoR-WRN experiments, we adopted BN-ReLU-conv order, as shown in <ref type="figure">Fig. 3</ref>. We initialized the weights as in <ref type="bibr" target="#b22">[23]</ref>. For CIFAR data sets, we used SGD with a mini-batch size of 128 for 500 epochs. The learning rate starts from 0.1, and is divided by a factor of 10 after epoch 250 and 375 as in <ref type="bibr" target="#b14">[15]</ref>. For the SVHN data set, we used SGD with a mini-batch size of 128 for 50 epochs. The learning rate starts from 0.1, and it is divided by a factor of 10 after epoch 30 and 35 as in <ref type="bibr" target="#b14">[15]</ref>. We used a weight decay of 1e-4, a momentum of 0.9, and a Nesterov momentum with 0 dampening on all data sets <ref type="bibr" target="#b37">[38]</ref>. For the stochastic depth drop-path method, we set p l with the linear decay rule of p 0 = 1 and p L =0.5 <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CIFAR-10 Classification by RoR</head><p>CIFAR-10 is a data set of 60,000 32×32 color images, with 10 classes of natural scene objects. The training set and test set contain 50,000 and 10,000 images. Two standard data augmentation techniques <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> were adopted in our experiments: random sampling and horizontal flipping. We preprocessed the data by subtracting the mean and dividing the standard deviation.</p><p>In <ref type="table" target="#tab_4">Table II</ref> and <ref type="figure">Fig. 10</ref>, the 110-layer ResNets without SD resulted in a competitive 5.43% error on the test set. The 110layer RoR-3 without SD had a 5.08% error on the test set and outperformed the 110-layer ResNets without SD by 6.4% (Because all state-of-the-art methods have achieved similarly small error rates, we used relative percentage to measure the improvements in this paper) on CIFAR-10 with a similar number of parameters. The 164-layer RoR-3 without SD resulted in a 4.86% error on the test set, and it outperformed the 164layer ResNets without SD by 4.1%. As can be observed, the 164-layer RoR-3 without SD can also outperform the 4.92% error of the 1001-layer Pre-ResNets with the same mini-batch size <ref type="bibr" target="#b12">[13]</ref>. We then added SD on ResNets and RoR-3, but those performances were similar to the models without SD. We concluded that overfitting on CIFAR-10 is not critical, and SD is not effective. However, adding SD can reduce training time <ref type="bibr" target="#b14">[15]</ref> and does not affect the performance, so we added SD in the following experiments on CIFAR-10.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CIFAR-100 Classification by RoR</head><p>Similar to CIFAR-10, CIFAR-100 is a data set of 60,000 32×32 color images, but with 100 classes of natural scene objects. The training set and test set contain 50,000 and 10,000 images, respectively. The augmentation and preprocessing techniques adopted in our experiments are the same as on CIFAR-10.</p><p>In <ref type="table" target="#tab_4">Table III</ref> and <ref type="figure">Fig. 11</ref>, the 110-layer and 164-layer ResNets without SD resulted in a competitive 26.80% and 25.85% error on the test set, but the results of the 110-layer RoR-3 and 164-layer RoR-3 without SD were not ideal. We argue that this is because adding extra branches and convolutional layers may escalate overfitting. It is gratifying that the 110-layer RoR-3+SD and 164-layer RoR-3+SD resulted in a 23.48% and 22.47% error on the test set, and they outperformed the 110-layer ResNets, 110-layer ResNets+SD, 164-layer ResNets and 164-layer ResNes+SD by 12.4%, 1.5%, 13.1% and 3.5%, respectively on CIFAR-100. This indicates that the SD drop-path can alleviate overfitting, so we will add SD in the next experiments on CIFAR-100. In addition, we observe that the optimization ability of RoR-2 is better than original ResNets, but worse than RoR-3, and that is why we chose m=3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Residual Block Size Analysis</head><p>In the above experiments, we used the residual block with two 3×3 convolution layers B(3,3) <ref type="bibr" target="#b11">[12]</ref>. In order to analyze  the effects of different residual block sizes, we increased the convolution layer number of every residual block to three with the same total number of layers, and the new residual block was denoted by B <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref>. The results are shown in <ref type="table" target="#tab_4">Table IV</ref>. When the epoch number was 164, we achieved better performance by B <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref>. But if the epoch number is 500 (RoR fully trained), we found the results by B <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref> to be better than those obtained by B <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3)</ref>. WRN <ref type="bibr" target="#b13">[14]</ref> tried more kinds of residual blocks, and B(3,3) remained the best residual block type and size. So we chose B(3,3) as the basic residual block size in RoR. Again, the importance of 500 epochs and m=3 was validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Versatility of RoR for other residual networks</head><p>Recently several variants of residual networks have become available, which can improve the performance of original ResNets <ref type="bibr" target="#b11">[12]</ref>. For example, Pre-ResNets <ref type="bibr" target="#b12">[13]</ref> can reduce vanishing gradients by BN-ReLU-conv order, and WRN <ref type="bibr" target="#b13">[14]</ref> can achieve a dramatic performance increase by adding more feature planes based on Pre-ResNets. In this paper, we constructed the RoR architecture based on these two residual networks.</p><p>First, we changed the residual blocks of the original RoR with a BN-ReLU-conv order, which can only be done by adding two-level shortcuts on the Pre-ResNets. <ref type="figure">Fig. 3</ref> shows the architecture of Pre-RoR (k=1) in detail. We did the same Second, we used (16×k, 32×k, 64×k) filters instead of (16, 32, 64) filters of the original Pre-RoR because WRN is constructed based on Pre-ResNets. <ref type="figure">Fig. 3</ref> shows the architecture of RoR-WRN (k=2, 4, in detail. We did the same experiment by RoR-3-WRN on CIFAR-10 and CIFAR-100 and showed the results in <ref type="table" target="#tab_4">Table VI</ref>. <ref type="figure">Fig. 12</ref> and <ref type="figure">Fig. 13</ref> showed the test errors on CIFAR-10 and CIFAR-100 at different training epochs. As can be observed, the performance of RoR-3-WRN is worse than WRN. In our opinion, WRN has more feature planes, so it is easier to get overfitting when we add extra branches and parameters. SD can alleviate overfitting, so the performance of RoR-3-WRN+SD is always better than others. RoR-3-WRN40-2+SD achieved 4.59% error on CIFAR-10 and 22.48% error on CIFAR-100.</p><p>Through analysis and experiments, we can prove that our RoR architecture can also promote optimization abilities of other residual networks, such as Pre-ResNets and WRN. Because RoR has good versatility for other residual networks, we have reasons to believe that our RoR is a good application prospect for the residual-networks family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Depth and Width Analysis</head><p>According to preceding experiments in this section, we determined that performance can be improved by increasing depth or width. In this section we analyze how to choose depth and width of RoR.</p><p>The basic RoR is based on the original ResNets, but very deep ResNets encounter serious vanishing gradients problems. So even though the performance of RoR is better than ResNets, RoR still cannot resolve the vanishing gradients problem. We repeated the RoR experiments by increasing the number  <ref type="table" target="#tab_4">Table VI</ref>. RoR-3-WRN40-2+SD (the red curve) yields lower test errors than the other curves. of convolutional layers, as shown in <ref type="table" target="#tab_4">Table VII</ref>. As can be observed, when the number of layers increased from 164 to 182, and then to 218, the performance gradually decreased. These experiments demonstrated that the vanishing problem exists in very deep RoR.</p><p>Pre-ResNets reduced vanishing problem, so Pre-RoR should inherit this property too. We repeated the Pre-RoR experiments by increasing the number of convolutional layers, as shown in <ref type="table" target="#tab_4">Table VIII</ref>. We observed that the accuracy increased as the number of layers increased. The 1202-layer Pre-RoR-3+SD with a mini-batch size of 32 achieved the 4.49% error on CIFAR-10 and 20.64% error on CIFAR-100. These results mean that the vanishing gradients can be reduced, even on very deep models. So we can use Pre-RoR to push the depth limit. WRN is not very deep, so the vanishing problem is not obvious. However, overfitting can become severe because of adding more feature planes and parameters. We followed the same protocol with RoR-WRN with different depths and widths on CIFAR-10 and CIFAR-100, as shown in <ref type="table" target="#tab_4">Table IX</ref>. We found both deepening and widening the network can improve the performance. But when we widened the RoR-WRN, weight parameters increased exponentially. So we had to complement RoR-WRN by SD to reduce overfitting. As can be observed, RoR-3-WRN-58-4+SD achieved an extraordinary 3.77% error on CIFAR-10 and a 19.73% error on CIFAR-100. We found that the RoR-WRN with similar order of magnitude parameters was more effective than Pre-RoR, because the vanishing problem already existed in very deep Pre-RoR.</p><p>Through experiments and analysis, we determined that the depth and width of RoR are equally important to model learning capability. We must carefully choose suitable depth and width on each given task to achieve satisfying results. In this paper we proposed a two-step strategy to choose depths and widths. The first step is to increase the depth of RoR gradually until the performance was saturated. Then increased the width of RoR gradually until the best results were achieved. We compared the training time of the ResNets-110, RoR-3-110 and RoR-3-110+SD on CIFAR-10/100, as shown in <ref type="table" target="#tab_13">Table X</ref>. The training times of ResNets-110 and RoR-3-110 were similar, so RoR did not add more extra training time than original residual networks. In addition, from Table X we got the same conclusion that stochastic depth consistently gave about 25% speedup in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. SVHN Classification by RoR</head><p>The Street View House Number (SVHN) data set used in this research contains 32×32 colored images of cropped out house numbers from Google Street View. The task is to classify the digit at center (and ignore any additional digit that might appear on the side) of the images. There are 73,257 digits in the training set, 26,032 in the test set and 531,131 easier samples for additional training. Following the common practice, we used all the training samples but did not perform data augmentation. We preprocessed the data by subtracting the mean and dividing the standard deviation. Batch size was set to 128, and test error was calculated every 200 iterations. We used our best architecture RoR-3-WRN58-4+SD to train SVHN and achieved the excellent result of 1.59% test error, as shown in <ref type="table" target="#tab_4">Table XI</ref>. This result outperformed WRN58-4 and WRN58-4+SD by 5.9% and 4.2% on SVHN, respectively. <ref type="figure" target="#fig_2">Fig. 14 shows</ref> the test error at different training epochs. We could see that the results of WRN58-4 and RoR-3-WRN58-4 were also good, but they started to overfit after 700×200 iterations.</p><p>I. Comparisons with state-of-the-art results on CIFAR-10/100 and SVHN Although some variants of residual networks (WRN and CRMN) or other CNNs (FractalNet) can achieve competitive results, the number of parameters in these models is too large (as shown in <ref type="table" target="#tab_4">Table XII</ref>). Through experiments and analysis, we argue that our RoR method can outperform other methods with a similar order of magnitude parameters. Our RoR models with only 3M parameters (Pre-RoR-3-164+SD, RoR-3-WRN40-2+SD and Pre-RoR-3-218+SD) can outperform FractalNet (30M parameters) and CRMN-28 (more than 40M parameters) on CIFAR-10. Our RoR-3-WRN40-4+SD model with 8.9M parameters can outperform all of the exiting methods. Our best RoR-3-WRN58-4+SD model with 13.3M parameters achieved the new state-of-the-art performance. We also contend that a better performance can be achieved by using additional depths and widths.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. ImageNet Classification</head><p>The preceding data sets were all small scale and lowresolution image data sets. We also conducted experiments on large scale and high-resolution image data set. We evaluated our RoR method on the ImageNet 2012 classification data set <ref type="bibr" target="#b2">[3]</ref>, which contains 1.28 million high-resolution training images and 50,000 validation images with 1000 object categories. During training of RoR, we noticed that RoR is slower than ResNets. So instead of training RoR from scratch, we used the ResNets models from <ref type="bibr" target="#b37">[38]</ref> for pretraining. ResNets for ImageNet required 64, 128, 256, 512 filters (Basic Residual Block) or 256, 512, 1024, 2048 filters (Bottleneck Residual Block) sequentially in the convolutional layers, and each kind of filter had a different number of residual blocks, which formed four residual block groups. We constructed RoR-3 models based on pretrained ResNets models by adding first-level and second-level shortcuts as discussed in Section III. The weights from pretrained ResNets models remained unchanged, but the new added weights were initialized as in <ref type="bibr" target="#b22">[23]</ref>. We used SGD with a mini-batch size of 128 (18 layers and 34 layers) or 64 (101 layers) or 48 (152 layers) for 10 epochs to fine-tune RoR. The learning rate started from 0.001 and was divided by a factor of 10 after epoch 5. For data augmentation, we used scale and aspect ratio augmentation <ref type="bibr" target="#b37">[38]</ref>. Both Top-1 and Top-5 error rates with 10crop testing were evaluated. In addition, SD was not used here because SD made RoR difficult to converge on ImageNet. From <ref type="table" target="#tab_4">Table XIII</ref>, our implementation of residual networks achieved a state-of-the-art performance compared to ResNets methods for single model evaluation on validation data set. These experiments verified the effectiveness of RoR on large scale and high-resolution image data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Further Analysis</head><p>In preceding section, the RoR method achieved state-ofthe-art results when working with all the best known data sets used for image classification. This final section analyzes the characteristics of the RoR method. Different strategies were adopted for different data sets. On CIFAR-10, RoR alone achieved the best results, as shown in <ref type="figure">Fig 10.</ref> On CIFAR-100 and SVHN, the residual networks encountered severe overfitting. Due to the increasing parameters and extra shortcut levels, using RoR on CIFAR-100 and SVHN proved to increase overfitting which offset the advantages of RoR. However, RoR still improved ResNets marginally. Therefore, we used SD method to reduce overfitting, and both ResNets and RoR benefited from SD. Our RoR+SD outperformed ResNets+SD, as shown in <ref type="figure">Fig. 11, Fig. 13</ref> and <ref type="figure" target="#fig_2">Fig. 14,</ref> as RoR can be sufficiently utilized due to SD. For ImageNet, because of the difficulties to train from scratch, we fine-tuned RoR based on original ResNets models. Because SD slowed the converging process of fine-tuning, so we did not employ SD for ImageNet. By doing so, we achieved better performance than ResNets on ImageNet as well. The final analysis showed good results, which were attributed not only to RoR, but also to the different strategies used in the different data sets. On the other hand, different kinds of residual networks also benefited from RoR, such as Pre-ResNets and WRN. In conclusion, we argue that RoR not only represents a network structure such as ResNets, Pre-ResNets and WRN, but is also an effective complement to the residual-networks family. VI. CONCLUSION This paper proposes a new Residual networks of Residual networks architecture (RoR), which was proved capable of obtaining a new state-of-the-art performance on CIFAR-10, CIFAR-100, SVHN and ImageNet for image classification. Through empirical studies, this work not only significantly advanced the image classification performance, but can also provided an effective complement to the residual-networks family in the future. In other words, any residual network can be improved by RoR. Hence, RoR has a good prospect of successful application on various image recognition tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 2 LFig. 2 .</head><label>122</label><figDesc>RoR-3 architecture. The shortcut on the left is a root-level shortcut, and the remaining shortcuts are made up of three orange shortcuts, which are middle-level shortcuts. The blue shortcuts are final-level shortcuts. ReLU is followed by an addition. The projection shortcut is done by 1×1 convolutions. Finally, RoR adopts a conv-BN-ReLU order in residual blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Different mapping in residual block of ResNets. Residual networks transfer the learning problem from reaching y l (the blue line) to reaching F (x l , W l ) (the red line), and we can find the red line is simpler and easier to learn than the blue line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of ResNets with different identity mapping types on CIFAR-10. Using Type B can achieve a better performance than using Type A on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of ResNets with different identity mapping types on CIFAR-100. Using Type A can achieve a better performance than using Type B on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison of 110-layer ResNets and RoR-3 with different epoch numbers on CIFAR-100. 500 epochs can achieve better performance than 164 epochs on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Smoothed test error on CIFAR-10 by WRN40-2, WRN40-2+SD and RoR-3-WRN40-2+SD during training, corresponding to results inTable VI. RoR-3-WRN40-2+SD (the red curve) yields lower test errors than the other curves. Smoothed test error on CIFAR-100 by WRN40-2, WRN40-2+SD and RoR-3-WRN40-2+SD during training, corresponding to results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Residual Block with ReLU in the end Residual Block without ReLU in the end</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>image</cell><cell></cell><cell></cell></row><row><cell>1 level</cell><cell cols="2">3×3,conv,16 BN ReLU</cell><cell>16-d</cell><cell>x</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3×3,conv,16</cell></row><row><cell></cell><cell cols="2">RB,16</cell><cell></cell><cell>BN BN</cell></row><row><cell>1×1,conv,16</cell><cell>…… ……</cell><cell>final level</cell><cell></cell><cell>3×3,conv,16 ReLU ReLU</cell></row><row><cell></cell><cell cols="2">16 RB,16</cell><cell></cell><cell>BN</cell></row><row><cell>2 level</cell><cell cols="2">ReLU</cell><cell></cell><cell>ReLU</cell></row><row><cell>1×1,conv,64,/4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">RB,32</cell><cell></cell></row><row><cell>1×1,conv,32,/2</cell><cell>……</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">RB,32</cell><cell></cell></row><row><cell></cell><cell cols="2">ReLU</cell><cell></cell></row><row><cell></cell><cell cols="2">RB,64</cell><cell>64-d</cell><cell>x</cell></row><row><cell>1×1,conv,64,/2</cell><cell>……</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3×3,conv,64</cell></row><row><cell></cell><cell cols="2">RB,64</cell><cell></cell><cell>BN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ReLU</cell></row><row><cell></cell><cell cols="2">Avg pool ReLU</cell><cell></cell><cell>3×3,conv,64 BN</cell></row><row><cell></cell><cell>fc</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">RoR-3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 5. Comparison of RoR with different shortcut levels m. When m=1, it is the original ResNets. When m=3, RoR reaches the best performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>110−layer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>146−layer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.87%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.54%</cell></row><row><cell cols="2">6.04% 5.92%</cell><cell>5.81%5.86%</cell><cell>5.71% 5.62%</cell><cell>5.92% 6.04%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CIFAR−10 164 Epoch</cell></row><row><cell></cell><cell>6.6</cell><cell></cell><cell></cell><cell>110−layer</cell></row><row><cell></cell><cell></cell><cell>6.43%</cell><cell></cell><cell>164−layer</cell></row><row><cell></cell><cell>6.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>6.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>6.04%</cell></row><row><cell>test error(%)</cell><cell>5.8 6</cell><cell>5.93%</cell><cell></cell><cell>5.75%</cell></row><row><cell></cell><cell>5.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell>5.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>5.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell>ResNet Type A</cell><cell cols="2">ResNet Type B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I TEST</head><label>I</label><figDesc></figDesc><table><row><cell cols="6">ERROR (%) ON CIFAR-10/100 WITH DIFFERENT SHORTCUT TYPE</cell></row><row><cell></cell><cell></cell><cell cols="2">AND LEVEL</cell><cell></cell><cell></cell></row><row><cell>500 Epoch</cell><cell cols="2">ResNets RoR-2</cell><cell>RoR-3</cell><cell>RoR-2</cell><cell>RoR-3</cell></row><row><cell></cell><cell></cell><cell>TypeA</cell><cell>TypeA</cell><cell>TypeB</cell><cell>TypeB</cell></row><row><cell>CIFAR-10</cell><cell>5.43</cell><cell>6.32</cell><cell>7.45</cell><cell>5.22</cell><cell>5.08</cell></row><row><cell>110-layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR-100</cell><cell>26.80</cell><cell>28.36</cell><cell>30.12</cell><cell>27.19</cell><cell>26.64</cell></row><row><cell>110-layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>found the results were comparable whether we used Type B or Type A on CIFAR-10. So in order to keep consistent with shortcut types on CIFAR-100, we all used Type A in the final shortcut level, and Type B in the other shortcut levels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II TEST</head><label>II</label><figDesc>ERROR (%) ON CIFAR-10 RESNETS AND ROR</figDesc><table><row><cell>CIFAR-10 500 Epoch</cell><cell>ResNets</cell><cell>ResNets+SD</cell><cell>RoR-3</cell><cell>RoR-3+SD</cell></row><row><cell>110-layer</cell><cell>5.43</cell><cell>5.63</cell><cell>5.08</cell><cell>5.04</cell></row><row><cell>164-layer</cell><cell>5.07</cell><cell>5.06</cell><cell>4.86</cell><cell>4.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE III TEST</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">ERROR (%) ON CIFAR-100 RESNETS AND ROR</cell><cell></cell></row><row><cell>CIFAR-100</cell><cell cols="2">ResNets ResNets</cell><cell>RoR-</cell><cell>RoR-2</cell><cell>RoR-</cell><cell>RoR-3</cell></row><row><cell>500 Epoch</cell><cell></cell><cell>+SD</cell><cell>2</cell><cell>+SD</cell><cell>3</cell><cell>+SD</cell></row><row><cell>110-layer</cell><cell>26.80</cell><cell>23.83</cell><cell>27.19</cell><cell>23.60</cell><cell>26.64</cell><cell>23.48</cell></row><row><cell>164-layer</cell><cell>25.85</cell><cell>23.29</cell><cell>-</cell><cell>-</cell><cell>27.45</cell><cell>22.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="5">TEST ERROR (%) ON CIFAR-10 WITH DIFFERENT BLOCK SIZE</cell></row><row><cell>CIFAR-10</cell><cell>RoR-3</cell><cell>RoR-3</cell><cell>RoR-4</cell><cell>RoR-4</cell></row><row><cell></cell><cell>B(3,3)</cell><cell>B(3,3,3)</cell><cell>B(3,3)</cell><cell>B(3,3,3)</cell></row><row><cell>164-layer</cell><cell>6.34</cell><cell>5.77</cell><cell>5.94</cell><cell>5.21</cell></row><row><cell>164 Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>164-layer</cell><cell>4.86</cell><cell>5.12</cell><cell>5.09</cell><cell>5.20</cell></row><row><cell>500 Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE V TEST</head><label>V</label><figDesc>ERROR (%) ON CIFAR-10/100 BY PRE-RESNETS AND PRE-ROR</figDesc><table><row><cell>500 Epoch</cell><cell>Pre-</cell><cell>Pre-RoR-3</cell><cell>Pre-</cell><cell>Pre-RoR-</cell></row><row><cell></cell><cell>ResNets</cell><cell></cell><cell>ResNest+SD</cell><cell>3+SD</cell></row><row><cell>164-layer</cell><cell>5.04</cell><cell>5.02</cell><cell>4.67</cell><cell>4.51</cell></row><row><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>164-layer</cell><cell>25.54</cell><cell>25.33</cell><cell>22.49</cell><cell>21.94</cell></row><row><cell>CIFAR-100</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="5">TEST ERROR (%) ON CIFAR-10/100 BY WRN AND ROR-WRN</cell></row><row><cell>500 Epoch</cell><cell>WRN40-2</cell><cell>RoR-3-</cell><cell>WRN40-</cell><cell>RoR-3-</cell></row><row><cell></cell><cell></cell><cell>WRN40-2</cell><cell>2+SD</cell><cell>WRN40-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2+SD</cell></row><row><cell>CIFAR-10</cell><cell>4.81</cell><cell>5.01</cell><cell>4.80</cell><cell>4.59</cell></row><row><cell>CIFAR-100</cell><cell>24.70</cell><cell>25.19</cell><cell>22.87</cell><cell>22.48</cell></row><row><cell cols="5">experiment by Pre-RoR-3 on CIFAR-10 and CIFAR-100, and</cell></row><row><cell cols="5">the results are shown in Table V where Pre-RoR is compared</cell></row><row><cell cols="5">with Pre-ResNets. As can be observed, the 164-layer Pre-</cell></row><row><cell cols="5">RoR-3+SD had a surprising 4.51% error on CIFAR-10 and a</cell></row><row><cell cols="5">21.94% error on CIFAR-100. Particularly, the 164-layer Pre-</cell></row><row><cell cols="5">RoR-3+3D outperformed the 164-layer Pre-ResNets and 164-</cell></row><row><cell cols="5">layer Pre-ResNets+SD by 14.1% and 2.4% on CIFAR-100.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VII TEST</head><label>VII</label><figDesc>ERROR (%) ON CIFAR-10/100 BY ROR WITH DIFFERENT DEPTHS</figDesc><table><row><cell>Depth</cell><cell cols="3">CIFAR-10 RoR-3 without SD</cell><cell cols="2">CIFAR-100 RoR-3+SD</cell></row><row><cell>110-layer</cell><cell></cell><cell>5.08</cell><cell></cell><cell>23.48</cell></row><row><cell>164-layer</cell><cell></cell><cell>4.86</cell><cell></cell><cell>22.47</cell></row><row><cell>182-layer</cell><cell></cell><cell>4.98</cell><cell></cell><cell>22.76</cell></row><row><cell>218-layer</cell><cell></cell><cell>5.12</cell><cell></cell><cell>22.99</cell></row><row><cell></cell><cell></cell><cell>TABLE VIII</cell><cell></cell><cell></cell></row><row><cell cols="6">TEST ERROR (%) ON CIFAR-10/100 BY PRE-ROR WITH DIFFERENT</cell></row><row><cell></cell><cell></cell><cell>DEPTHS</cell><cell></cell><cell></cell></row><row><cell>Depth</cell><cell></cell><cell>CIFAR-10</cell><cell>Pre-</cell><cell>CIFAR-100</cell><cell>Pre-</cell></row><row><cell></cell><cell></cell><cell>RoR-3+SD</cell><cell></cell><cell>RoR-3+SD</cell></row><row><cell>110-layer</cell><cell></cell><cell>4.63</cell><cell></cell><cell>23.05</cell></row><row><cell>164-layer</cell><cell></cell><cell>4.51</cell><cell></cell><cell>21.94</cell></row><row><cell>218-layer</cell><cell></cell><cell>4.51</cell><cell></cell><cell>21.43</cell></row><row><cell cols="2">1202-layer with 32</cell><cell>4.49</cell><cell></cell><cell>20.64</cell></row><row><cell cols="2">mini-batch size</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE IX</cell><cell></cell><cell></cell></row><row><cell cols="6">TEST ERROR (%) ON CIFAR-10/100 BY ROR-WRN WITH DIFFERENT</cell></row><row><cell></cell><cell></cell><cell cols="2">DEPTHS AND WIDTHS</cell><cell></cell></row><row><cell cols="2">Depth and Width</cell><cell cols="2">CIFAR-10 RoR-3-</cell><cell cols="2">CIFAR-100 RoR-3-</cell></row><row><cell></cell><cell></cell><cell>WRN+SD</cell><cell></cell><cell>WRN+SD</cell></row><row><cell cols="2">RoR-3-WRN40-2</cell><cell>4.59</cell><cell></cell><cell>22.48</cell></row><row><cell cols="2">RoR-3-WRN40-4</cell><cell>4.09</cell><cell></cell><cell>22.11</cell></row><row><cell cols="2">RoR-3-WRN58-2</cell><cell>4.23</cell><cell></cell><cell>21.50</cell></row><row><cell cols="2">RoR-3-WRN58-4</cell><cell>3.77</cell><cell></cell><cell>19.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE X TRAINING</head><label>X</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">TIME COMPARISON ON CIFAR-10/100</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell cols="2">CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell></cell><cell>ResNet-110</cell><cell></cell><cell>9h40m</cell><cell></cell><cell>9h43m</cell></row><row><cell></cell><cell>RoR-3-110</cell><cell></cell><cell>9h47m</cell><cell></cell><cell>9h51m</cell></row><row><cell></cell><cell cols="2">RoR-3-110+SD</cell><cell>7h43m</cell><cell></cell><cell>7h45m</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE XI</cell><cell></cell></row><row><cell cols="6">TEST ERROR (%) ON SVHN BY WRN58-4, WRN58-4+SD,</cell></row><row><cell></cell><cell cols="5">WRN58-4+SD AND ROR-3-WRN58-4+SD</cell></row><row><cell>50 Epoch</cell><cell>WRN58-4</cell><cell cols="2">RoR-3-</cell><cell cols="2">WRN58-</cell><cell>RoR-3-</cell></row><row><cell></cell><cell></cell><cell cols="2">WRN58-4</cell><cell cols="2">4+SD</cell><cell>WRN58-4+SD</cell></row><row><cell>SVHN</cell><cell>1.69</cell><cell>1.66</cell><cell></cell><cell cols="2">1.66</cell><cell>1.59</cell></row><row><cell cols="6">G. Training time comparison on CIFAR-10/100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Table XIIcompares the state-of-the-art methods on CIFAR-10/100 with SVHN, where we achieved overwhelming results. We obtain these results via a simple concept in which the residual mapping of residual mapping was expected to be easier to optimize. No complicated architectures or any other tricks were used. We only added two shortcut levels with thousands of parameters, which made better optimized the original residual networks. For data augmentation, RoR only used naive translation and horizontal flipping, even though 77% on CIFAR-10, 19.73% on CIFAR-100 and 1.59% on SVHN, which are now state-of-the-art performance standards, to the best of our knowledge. These results demonstrate the effectiveness and versatility of RoR. No matter what kind of basic residual networks are available, RoR can always achieve better results than its basic residual networks with the same number of layers.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">RoR−3−WRN58−4 on SVHN</cell></row><row><cell></cell><cell>0.028</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>WRN58−4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RoR−3−WRN58−4</cell></row><row><cell></cell><cell>0.026</cell><cell></cell><cell></cell><cell>WRN58−4+SD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RoR−3−WRN58−4+SD</cell></row><row><cell></cell><cell>0.024</cell><cell></cell><cell></cell></row><row><cell>test error</cell><cell>0.022</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.018</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.016</cell><cell></cell><cell></cell></row><row><cell></cell><cell>700</cell><cell>750 800</cell><cell>850 900</cell><cell>950 1000 1050 1100 1150</cell></row><row><cell></cell><cell></cell><cell></cell><cell>itr*200</cell></row><row><cell cols="5">Fig. 14. Smoothed test error on SVHN by WRN58-4RoR-3-WRN58-4,</cell></row><row><cell cols="5">WRN58-4+SD and RoR-3-WRN58-4+SD during training. RoR-3-WRN58-</cell></row><row><cell cols="5">4+SD (the black curve) yields lower test errors than the curves.</cell></row><row><cell cols="5">other methods often adopted more complicated data augmen-</cell></row><row><cell cols="5">tation techniques. Our 164-layer RoR-3 had an error of 4.86%</cell></row><row><cell cols="5">on CIFAR-10, which was better than the 4.92% of 1001-layer</cell></row><row><cell cols="5">Pre-ResNets with the same batch size. Our 164-layer RoR-</cell></row><row><cell cols="5">3+SD had an error of 22.47% on CIFAR-100, which was</cell></row><row><cell cols="5">better than the 22.71% of the 1001-layer Pre-ResNets. Most</cell></row><row><cell cols="5">importantly, it is not only compatible with the original ResNets</cell></row><row><cell cols="5">but also with other kinds of residual networks (Pre-ResNets</cell></row><row><cell cols="5">and WRN). The performance of our Pre-RoR-3 and RoR-3-</cell></row><row><cell cols="5">WRN outperformed original Pre-ResNets and WRN. Particu-</cell></row><row><cell cols="5">larly, our RoR-3-WRN58-4+SD obtained a single-model error</cell></row><row><cell cols="2">of 3.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XII TEST</head><label>XII</label><figDesc>ERROR (%) ON CIFAR-10, CIFAR-100 AND SVHN BY DIFFERENT METHODS</figDesc><table><row><cell cols="2">Method(#Parameters)</cell><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-</cell><cell>SVHN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>100</cell><cell></cell></row><row><cell>NIN [5]</cell><cell></cell><cell></cell><cell>8.81</cell><cell>35.68</cell><cell>2.35</cell></row><row><cell>FitNet [8]</cell><cell></cell><cell></cell><cell>8.39</cell><cell>35.04</cell><cell>2.42</cell></row><row><cell>DSN [9]</cell><cell></cell><cell></cell><cell>7.97</cell><cell>34.57</cell><cell>1.92</cell></row><row><cell>All-CNN [10]</cell><cell></cell><cell></cell><cell>7.25</cell><cell>33.71</cell><cell>-</cell></row><row><cell>Highway [28]</cell><cell></cell><cell></cell><cell>7.72</cell><cell>32.39</cell><cell>-</cell></row><row><cell>ELU [22]</cell><cell></cell><cell></cell><cell>6.55</cell><cell>24.28</cell><cell>-</cell></row><row><cell cols="2">FractalNet (30M) [29]</cell><cell></cell><cell>4.59</cell><cell>22.85</cell><cell>1.87</cell></row><row><cell cols="3">ResNets-164 (2.5M) [12] (re-</cell><cell>5.93</cell><cell>25.16</cell><cell>-</cell></row><row><cell cols="2">ported by [13])</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FitResNet, LSUV [26]</cell><cell></cell><cell>5.84</cell><cell>27.66</cell><cell>-</cell></row><row><cell cols="3">Pre-ResNets-164 (2.5M) [13]</cell><cell>5.46</cell><cell>24.33</cell><cell>-</cell></row><row><cell cols="2">Pre-ResNets-1001</cell><cell></cell><cell>4.62</cell><cell>22.71</cell><cell>-</cell></row><row><cell>(10.2M) [13]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ELU-ResNets-110 (1.7M) [31]</cell><cell>5.62</cell><cell>26.55</cell><cell>-</cell></row><row><cell cols="2">PELU-ResNets-110</cell><cell></cell><cell>5.37</cell><cell>25.04</cell><cell>-</cell></row><row><cell>(1.7M) [24]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ResNets-110+SD (1.7M) [15]</cell><cell>5.23</cell><cell>24.58</cell><cell>1.75 (152-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>layer)</cell></row><row><cell>ResNet</cell><cell>in</cell><cell>ResNet</cell><cell>5.01</cell><cell>22.90</cell><cell>-</cell></row><row><cell>(10.3M) [30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SwapOut (7.4M) [32]</cell><cell></cell><cell>4.76</cell><cell>22.72</cell><cell>-</cell></row><row><cell cols="3">WResNet-d (19.3M) [33]</cell><cell>4.70</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">WRN28-10 (36.5M) [14]</cell><cell>4.17</cell><cell>20.50</cell><cell>1.64</cell></row><row><cell>CRMN-28</cell><cell>(more</cell><cell>than</cell><cell>4.65</cell><cell>20.35</cell><cell>1.68</cell></row><row><cell>40M) [34]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">RoR-3-164 (2.5M)</cell><cell></cell><cell>4.86</cell><cell cols="2">22.47(+SD) -</cell></row><row><cell cols="3">Pre-RoR-3-164+SD (2.5M)</cell><cell>4.51</cell><cell>21.94</cell><cell>-</cell></row><row><cell cols="3">RoR-3-WRN40-2+SD (2.2M)</cell><cell>4.59</cell><cell>22.48</cell><cell>-</cell></row><row><cell cols="3">Pre-RoR-3-1202+SD (19.4M)</cell><cell>4.49</cell><cell>20.64</cell><cell>-</cell></row><row><cell cols="3">RoR-3-WRN40-4+SD (8.9M)</cell><cell>4.09</cell><cell>20.11</cell><cell>-</cell></row><row><cell cols="2">RoR-3-WRN58-4+SD</cell><cell></cell><cell>3.77</cell><cell>19.73</cell><cell>1.59</cell></row><row><cell>(13.3M)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XIII</head><label>XIII</label><figDesc></figDesc><table><row><cell cols="3">VALIDATION ERROR (%, 10-CROP TESTING) ON IMAGENET BY RESNETS</cell></row><row><cell cols="3">AND ROR-3 WITH DIFFERENT DEPTHS</cell></row><row><cell>Method</cell><cell>Top-1 Error</cell><cell>Top-5 Error</cell></row><row><cell>ResNets-18 [38]</cell><cell>28.22</cell><cell>9.42</cell></row><row><cell>RoR-3-18</cell><cell>27.84</cell><cell>9.22</cell></row><row><cell>ResNets-34 [12]</cell><cell>24.52</cell><cell>7.46</cell></row><row><cell>ResNets-34 [38]</cell><cell>24.76</cell><cell>7.35</cell></row><row><cell>RoR-3-34</cell><cell>24.47</cell><cell>7.13</cell></row><row><cell>ResNets-101 [12]</cell><cell>21.75</cell><cell>6.05</cell></row><row><cell>ResNets-101 [38]</cell><cell>21.08</cell><cell>5.35</cell></row><row><cell>RoR-3-101</cell><cell>20.89</cell><cell>5.24</cell></row><row><cell>ResNets-152 [12]</cell><cell>21.43</cell><cell>5.71</cell></row><row><cell>ResNets-152 [38]</cell><cell>20.69</cell><cell>5.21</cell></row><row><cell>RoR-3-152</cell><cell>20.55</cell><cell>5.14</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the editor and the anonymous reviewers for their careful reading and valuable remarks.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhenvshky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generic object detection with dense neural patterns and regional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4316</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. -Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Identity mapping in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09382</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhenvshky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Toronto, ON, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Comput. Sci., Univ. of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.Sc. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS Workshop Deep Learning and Unsupervised feature learning</title>
		<meeting>NIPS Workshop Deep Learning and Unsupervised feature learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Art. Intell. Stat</title>
		<meeting>Conf. Art. Intell. Stat</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. -A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Parametric exponential linear unit for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Trottier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Giguere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09322</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5353" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">FractalNet: ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Targ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lyman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08029</idno>
		<title level="m">Resnet in resnet: generalizing residual architectures</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep residual networks with exponential linear unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shinde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kadam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04112</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Swapout: learning an ensemble of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06465</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Weighted residuals for very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08831</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05262</idno>
		<title level="m">Convolutional residual memory networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<imprint>
			<pubPlace>Facebook AI Research, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avilable</forename></persName>
		</author>
		<ptr target="http://torch.ch/blog/2016/02/04/resnets.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
