<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><forename type="middle">Zeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Hay</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
						</author>
						<title level="a" type="main">Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that Vote3Deep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40% while remaining highly competitive in terms of processing time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>3D point cloud data is ubiquitous in mobile robotics applications such as autonomous driving, where efficient and robust object detection is pivotal for planning and decision making. Recently, computer vision has been undergoing a transformation through the use of convolutional neural networks (CNNs) (e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>). Methods which process 3D point clouds, however, have not yet experienced a comparable breakthrough. We attribute this lack of progress to the computational burden introduced by the third spatial dimension. The resulting increase in the size of the input and intermediate representations renders a naive transfer of CNNs from 2D vision applications to native 3D perception in point clouds infeasible for large-scale applications. As a result, previous approaches tend to convert the data into a 2D representation first, where nearby features are not necessarily adjacent in the physical 3D space -requiring models to recover these geometric relationships.</p><p>In contrast to image data, however, typical point clouds encountered in mobile robotics are spatially sparse, as most regions are unoccupied. This fact was exploited in <ref type="bibr" target="#b5">[5]</ref>, where the authors propose Vote3D, a feature-centric voting algorithm leveraging the sparsity inherent in these point clouds. The computational cost is proportional only to the number of occupied cells rather than the total number of cells in a 3D grid. <ref type="bibr" target="#b5">[5]</ref> proves the equivalence of the voting scheme to a dense convolution operation and demonstrates its effectiveness by discretising point clouds into 3D grids and performing exhaustive 3D sliding window detection with  a linear Support Vector Machine (SVM). Consequently, <ref type="bibr" target="#b5">[5]</ref> achieves the previous state of the art in both performance and processing speed for detecting cars, pedestrians and cyclists in point clouds on the object detection task from the popular KITTI Vision Benchmark Suite <ref type="bibr" target="#b6">[6]</ref>.</p><p>Inspired by <ref type="bibr" target="#b5">[5]</ref>, we propose to exploit feature-centric voting to build efficient CNNs to detect objects in point clouds natively in 3D -that is to say without projecting the input into a lower-dimensional space first or constraining the search space of the detector <ref type="figure" target="#fig_1">(Fig. 1</ref>). This enables our ap-proach, named Vote3Deep, to learn high-capacity, non-linear models while providing constant-time evaluation at test-time, in contrast to non-parametric methods. Furthermore, in order to enhance the computational benefits associated with sparse inputs throughout the entire CNN stack, we demonstrate the benefits of encouraging sparsity in the inputs to intermediate layers by imposing an L 1 model regulariser during training.</p><p>To the best of our knowledge, this is the first work to propose sparse convolutional layers based on voting and L 1 regularisation for efficient processing of full 3D point clouds with CNNs at scale. In particular, the contributions of this paper can be summarised as follows: 1) the construction of efficient convolutional layers as basic building blocks for CNN-based point cloud processing by leveraging a voting mechanism to exploit the inherent sparsity in the input data; 2) the use of rectified linear units and an L 1 sparsity penalty to specifically encourage data sparsity in the intermediate representations in order to exploit sparse convolutional layers throughout the entire CNN stack. We demonstrate that Vote3Deep models with as few as three layers achieve state-of-the-art performance amongst purely laser-based approaches across all classes considered on the popular KITTI object detection benchmark. Vote3Deep models exceed the previous state of the art in 3D point cloud based object detection in average precision by a margin of up to 40% while only running slightly slower in terms of detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A number of works have attempted to apply CNNs in the context of 3D point cloud data. A CNN-based approach in <ref type="bibr" target="#b7">[7]</ref> obtains comparable performance to <ref type="bibr" target="#b5">[5]</ref> on KITTI for car detection by projecting the point cloud into a 2D depth map, with an additional channel for the height of a point from the ground. Their model predicts detection scores and regresses to bounding boxes. However, the projection to a specific viewpoint discards valuable information, which is particularly detrimental, for example, in crowded scenes. It also requires the network filters to learn local dependencies with regards to depth, information that is readily available in a 3D representation and which can be efficiently extracted with sparse convolutions.</p><p>Dense 3D occupancy grids obtained from point clouds are processed with CNNs in <ref type="bibr" target="#b8">[8]</ref> and <ref type="bibr" target="#b9">[9]</ref>. With a minimum cell size of 0.1m, <ref type="bibr" target="#b8">[8]</ref> reports a speed of 6ms on a GPU to classify a single crop with a grid-size of 32×32×32 cells. Similarly, a processing time of 5ms per m 3 for landing zone detection is reported in <ref type="bibr" target="#b9">[9]</ref>. With 3D point clouds often being larger than 60m × 60m × 5m, this would result in a processing time of 60 × 60 × 5 × 5 × 10 −3 = 90s per frame, which does not comply with speed requirements typically encountered in robotics applications.</p><p>An alternative approach that takes advantage of sparse representations can be found in <ref type="bibr" target="#b10">[10]</ref> and <ref type="bibr" target="#b11">[11]</ref>, in which sparse convolutions are applied to comparatively small 2D and 3D crops respectively. While the convolutional kernels are only applied at sparse feature locations, the presented algorithm still has to consider neighbouring values which take a value of either zero or a constant bias, leading to unnecessary operations and memory consumption. Another method for performing sparse convolutions is introduced in <ref type="bibr" target="#b12">[12]</ref> who make use of "permutohedral lattices", but only consider comparatively small inputs, as opposed to our work.</p><p>CNNs have also been applied to dense 3D data in biomedical image analysis (e.g. <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>). A 3D equivalent of residual networks <ref type="bibr" target="#b4">[4]</ref> is utilised in <ref type="bibr" target="#b13">[13]</ref> for brain image segmentation. A cascaded model with two stages is proposed in <ref type="bibr" target="#b14">[14]</ref> for detecting cerebral microbleeds. A combination of three CNNs is suggested in <ref type="bibr" target="#b15">[15]</ref>. Each CNN processes a different 2D plane and the three streams are joined in the last layer. These systems run on relatively small inputs and in some cases take more than a minute for processing a single frame with GPU acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>This section describes the application of convolutional neural networks for the prediction of detection scores from sparse 3D input grids of variable sizes. As the input to the network, a point cloud is discretised into a sparse 3D grid as in <ref type="bibr" target="#b5">[5]</ref>. For each cell that contains a non-zero number of points, a feature vector is extracted based on the statistics of the points in the cell. The feature vector holds a binary occupancy value, the mean and variance of the reflectance values and three shape factors. Cells in empty space are not stored which leads to a sparse representation.</p><p>We employ the voting scheme from <ref type="bibr" target="#b5">[5]</ref> to perform a sparse convolution across this native 3D representation, followed by a ReLU non-linearity, which returns a new sparse 3D representation. This process can be repeated and stacked as in a traditional CNN, with the output layer predicting the detection scores.</p><p>Similar to <ref type="bibr" target="#b5">[5]</ref>, a CNN is applied to a point cloud at N different angular orientations in N parallel threads to handle objects at different orientations at a minimal increase in computation time. Duplicate detections are pruned with nonmaximum suppression (NMS) in 3D space. NMS in 3D is better able to handle objects that are behind each other as the 3D bounding boxes overlap less than their 2D projections.</p><p>Based on the premise that bounding boxes in 3D space are similar in size for object instances of the same class, we assume a fixed-size bounding box for each class, which eliminates the need to regress the size of a bounding box. We select 3D bounding box dimensions for each class of interest based on the 95 th percentile ground truth bounding box size over the training set.</p><p>The receptive field of a network should be at least as large as the bounding box of an object, but not excessively large which would waste computation time. We therefore employ several class-specific networks which can be run in parallel at test time, each with a different total receptive field size depending on the object class. In principle, it is possible to compute detection scores for multiple classes with a single network; a task left for future work. An illustration of the voting procedure on a sparse 2D example input without a bias. The voting weights are obtained by flipping the convolutional weights along each dimension. Whereas a standard convolution applies the filter at every location in the input, the equivalent voting procedure only needs to be applied at each non-zero location to compute the same result. Instead of a 2D grid with a single feature, Vote3Deep applies the voting procedure to 3D inputs with several feature maps. For a full mathematical justification, the reader is referred to <ref type="bibr" target="#b5">[5]</ref>. Best viewed in colour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse Convolutions via Voting</head><p>When running a dense 3D convolution across a discretised point cloud, most of the computation time is wasted as the majority of operations are multiplications by zero. The additional third spatial dimension makes this process even more computationally expensive compared to 2D convolutions, which form the basis of image-based CNNs.</p><p>Using the insight that meaningful computation only takes place where the 3D features are non-zero, <ref type="bibr" target="#b5">[5]</ref> introduce a feature-centric voting scheme. The basis of this algorithm is the idea of letting each non-zero input feature vector cast a set of votes, weighted by the filter weights, to its surrounding cells in the output layer, as defined by the receptive field of the filter. The voting weights are obtained by flipping the convolutional filter kernel along each spatial dimension. The final convolution result is obtained by accumulating the votes falling into each cell of the output <ref type="figure">(Fig. 2)</ref>.</p><p>This procedure can be formally stated as follows. Without loss of generality, assume we have one 3D convolutional filter with odd-valued kernel dimensions in network layer c, operating on a single input feature, with the filter weights denoted by w c ∈ R (2I+1)×(2J+1)×(2K+1) . Then, for an input grid h c−1 ∈ R L×M ×N , the convolution result at location (l, m, n) is given by:</p><formula xml:id="formula_0">z c l,m,n = I i=−I J j=−J K k=−K w c i,j,k h c−1 l+i,m+j,n+k + b c (1)</formula><p>where b c is a bias value applied to all cells in the grid. This operation needs to be applied to all L × M × N locations in the input grid for a regular dense convolution. In contrast to this, given the set of cell indices for all of the non-zero cells Φ = {(l, m, n) ∀ h c−1 l,m,n = 0}, the convolution can be recast as a feature-centric voting operation, with each input cell casting votes to increment the values in neighbouring cell locations according to:</p><formula xml:id="formula_1">z c l+i,m+j,n+k = z c l+i,m+j,n+k + w c −i,−j,−k h c−1 l,m,n<label>(2)</label></formula><p>which is repeated for all tuples (l, m, n) ∈ Φ and where</p><formula xml:id="formula_2">{i, j, k ∈ Z | i ∈ [−I, I] , j ∈ [−J, J] , k ∈ [−K, K]}.</formula><p>The voting output is passed through a ReLU non-linearity which discards non-positive features as described in the next subsection. Crucially, the biases are constrained to be nonpositive as a single positive bias would return an output grid in which almost every cell is occupied with a feature vector, hence eliminating sparsity. The bias b c therefore only needs to be added to each non-empty output cell.</p><p>With this sparse voting scheme, the filter only needs to be applied to the occupied cells in the input grid, rather than convolved over the entire grid. The algorithm is described in more detail in <ref type="bibr" target="#b5">[5]</ref>, including formal proof that feature-centric voting is equivalent to an exhaustive convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Maintaining Sparsity with ReLUs</head><p>The ability to perform fast voting in all layers is predicated on the assumption of sparsity in the input to each individual layer. While the input point cloud is sparse, the regions of non-empty cells are dilated by each successive convolutional layer, approximately by the receptive field size of the corresponding filters in the layer. It is therefore critical to select a non-linear activation function which helps to maintain sparsity in the inputs to each convolutional layer. This is achieved by applying a rectified linear unit (ReLU) as advocated in <ref type="bibr" target="#b16">[16]</ref> after a sparse convolutional layer. The ReLU activation can be written as:</p><formula xml:id="formula_3">h c = max (0, z c )<label>(3)</label></formula><p>with z c being the input to the ReLU non-linearity in layer c as computed by a sparse convolution, and h c being the output, denoting the hidden activations in the subsequent sparse intermediate representation.</p><p>In this case, only features that have a value greater than zero will be allowed to cast votes in the next sparse convolution layer. In addition to enabling a network to learn non-linear function approximations and therefore increasing its representational capacity, ReLUs effectively perform a thresholding operation by discarding negative feature values which helps to maintain sparsity in the intermediate representations. Lastly, a further advantage of ReLUs compared to other non-linearities is that they are fast to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. TRAINING</head><p>Due to the use of fixed-size bounding boxes, networks can be directly trained on 3D crops of positive and negative examples whose dimensions equal the receptive field size specified by the architecture.</p><p>Negative training examples are obtained by performing hard negative mining periodically after a fixed number of training epochs. The class-specific networks are binary classifiers and we choose a linear hinge loss for training due to its maximum margin property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Linear Hinge Loss</head><p>Given an output detection scoreŷ ∈ R, a class label y ∈ {−1, 1} distinguishing between positive and negative samples, and the parameters of the network denoted as θ, the hinge loss is formulated as:  <ref type="table" target="#tab_0">IN EACH LAYER FOR THE ARCHITECTURES USED   IN THE MODEL COMPARISON -"RF" INDICATES THAT THE RECEPTIVE   FIELD OF THE OUTPUT LAYER DEPENDS ON THE OBJECT CLASS   Model  Layer 1  Layer 2  Layer 3</ref> A</p><formula xml:id="formula_4">L (θ) = max (0, 1 −ŷ · y)<label>(4)</label></formula><formula xml:id="formula_5">RF A - - B 3 × 3 × 3 RF B - C 5 × 5 × 5 RF C - D 3 × 3 × 3 3 × 3 × 3 RF D E 5 × 5 × 5 3 × 3 × 3 RF E " ∈ ℝ %×%×%×'×(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Grid</head><p>Output Scores The loss in Eq. 4 is zero for positive samples that score over 1 and negative samples that score below −1. As such, the hinge loss drives sample scores away from the margin given by the interval [−1, 1]. As with standard CNNs, the L 1 hinge loss can be backpropagated through the network for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. L 1 Sparsity Penalty</head><p>While the ReLU non-linearity helps to maintain sparsity in the intermediate representations, we propose to include an additional regulariser to incite the network to discard uninformative features and increase sparsity throughout the entire CNN stack.</p><p>The L 1 loss has been shown to result in sparse representations with values being exactly zero <ref type="bibr" target="#b17">[17]</ref>, which is precisely the requirement for this model. Whereas the sparsity of the output layer can be tuned with a detection threshold, we encourage sparsity in the intermediate layers by incorporating a penalty term using the L 1 norm of each feature activation.</p><p>We normalise this L 1 loss with respect to the spatial dimensions of the feature map in each layer. This renders the influence of the sparsity penalty less dependent on the size of the input for a given parameter setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Dataset</head><p>We use the well-known KITTI Vision Benchmark Suite <ref type="bibr" target="#b6">[6]</ref> for training and evaluating our detection models. The dataset consists of synchronised stereo camera and lidar frames recorded from a moving vehicle with annotations for eight different object classes, showing a wide variety of road scenes with different appearances. We only use the 3D point cloud data to train and test the models.</p><p>There are 7,518 frames in the KITTI test set whose labels are not publicly available. The labelled training data consist of 7,481 frames which we split into two sets for training and validation (80% and 20% respectively). The object detection benchmark considers three classes for evaluation: cars, pedestrians and cyclists with 28,742; 4,487; and 1,627 training labels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>The benchmark evaluation on the official KITTI test set is performed in 2D image space. We therefore project our 3D detections into the 2D image plane using the provided calibration files and discard any detections that fall outside of the image.</p><p>The KITTI benchmark differentiates between easy, moderate and hard test categories depending on the bounding box size, object truncation and occlusion. The hard test case considers the largest number of positives, whereas the most difficult examples are subsequently ignored for the moderate and easy test cases. The official rankings are based on the average precision (AP) for the moderate cases.</p><p>After describing the training procedure, we present results for three experiments. Firstly, we conduct a model comparison on the validation set (Section V-D). Secondly, based on the results of the model comparison, we select one model for each class and report results on the official KITTI test set (Section V-E). Lastly, we compare the timing results of models that were trained with and without the L 1 sparsity penalty (Section V-F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>The networks are trained on 3D crops of positive and negative examples. The number of positives and negatives is initially balanced with negatives being extracted randomly from the training data at locations that do not overlap with any of the positives.</p><p>In order to improve generalisation and to compensate for the fact that the input is discretised both spatially as well as in terms of angular resolution, the training data is augmented by translating the original front-facing positive training examples by a distance smaller than the size of the 3D grid cells and randomly rotating them by an angle that is smaller than the resolution of the angular bins.</p><p>Hard negative mining is performed every ten epochs by running the current model across the full point clouds in the training set. In each round of hard negative mining, the ten highest scoring false positives per frame are added to the training set.  <ref type="table" target="#tab_0">Table I</ref>, showing the average precision for the moderate difficulty level. The non-linear models with two or three layers consistently outperform the linear baseline model our internal validation set by a considerable margin for all three classes. The performance continues to improve as the number of filters in the hidden layers is increased, but these gains are incremental compared to the large margin between the linear baseline and the smallest multi-layer models. Best viewed in colour. The filter weights are initialised as in <ref type="bibr" target="#b18">[18]</ref> and the networks are trained for 100 epochs with stochastic gradient descent with a momentum term of 0.9, a batchsize of 16, a constant learning rate of 10 −3 and L 2 weight decay of 10 −4 . The model from the epoch with the highest AP on the validation set is selected for the model comparison and the test submission.</p><p>For the timing experiments, we observed that selecting the models from the epoch with the highest AP on the validation set tends to favour models with a comparatively low sparsity in the intermediate representations. Thus, the models after the full 100 epochs of training are used for the timing experiments to enable a fair comparison.</p><p>We implemented a custom C++ library for training and testing. For the largest models, training takes about three days on a cluster CPU node with 16 cores where each example in a batch is processed in a separate thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Comparison</head><p>Fast detection speeds are particularly important in the context of robotics. As larger, more expressive models come at a higher computational cost and consequently run at slower speeds, this section investigates the trade-off between model capacity and detection performance on the validation set. Five architectures as summarised in <ref type="table" target="#tab_0">Table I</ref> with up to three layers and different filter configurations are benchmarked against each other. The "Model D" architecture is illustrated as an example in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Small 3 × 3 × 3 and 5 × 5 × 5 kernels are used in the lower layers, followed by a ReLU non-linearity. The architectures are designed so that the total receptive field is slightly larger than the class-specific bounding boxes. The network output is computed by a linear layer which is implemented as a convolutional filter whose kernel size gives the desired receptive field size for a given object class.</p><p>As can be seen in <ref type="figure" target="#fig_4">Fig. 4</ref>, the non-linear, multi-layer networks clearly outperform the linear baseline, which is comparable to <ref type="bibr" target="#b5">[5]</ref>. First and foremost, this demonstrates that increasing the complexity and expressiveness of the models is extremely helpful for detecting objects in point clouds.</p><p>The resulting gains when increasing the number of convolutional filters in the hidden layers are moderate compared to the large improvement over the baseline which is achieved with only eight filters. Similarly, increasing the receptive field of the filter kernels, while keeping the total receptive field of the networks the same, does not indicate a significant improvement in performance.</p><p>It is possible that these larger models are not sufficiently regularised. Another potential explanation is that the easy interpretability of 3D data enables even relatively small models to capture most of the variation in the input representation which is informative for solving the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Test Results</head><p>As the model comparison shows, increasing the number of filters or the kernel size does not significantly improve accuracy, while inevitably deteriorating the detection speed. Consequently, we choose to limit ourselves to eight 3 × 3 × 3 filters in each of the hidden layers for the test submission.</p><p>As the models can be run in parallel during deployment, they should ideally run at approximately the same detection speed. Due to the larger physical size of cars, compared to pedestrians and cyclists, the corresponding networks need a larger filter kernel in the output layer to achieve the required total receptive field, having a negative effect on detection speed. For the submission to the KITTI test server, we therefore select the "Model B" with two layers for cars, and the "Model D" with three layers for pedestrians and cyclists. The PR curves of these models on the KITTI test set are shown in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>The performance of Vote3Deep is compared against the other leading approaches for object detection in point clouds at the time of writing in <ref type="table" target="#tab_0">Table II</ref>. Vote3Deep establishes new state-of-the-art performance in this category for all three classes and all three difficulty levels. The performance boost is particularly significant for cyclists with a margin of almost 40% in the easy test case and more than doubling the AP in the other two test cases.</p><p>Vote3Deep currently runs on CPU and is about two times slower than <ref type="bibr" target="#b5">[5]</ref> and almost as fast as <ref type="bibr" target="#b7">[7]</ref>, with the latter relying on GPU acceleration. We expect that a GPU implementation of the sparse convolution layers will further improve the detection speed.</p><p>We also compare Vote3Deep against methods that utilise both point cloud and image data at the time of writing in <ref type="table" target="#tab_0">Table III</ref>. Despite only using point cloud data, Vote3Deep still performs better than these ( <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>) in the majority of test cases and only slightly worse in the remaining ones at a considerably faster detection speed. For all three object classes, Vote3Deep achieves the highest AP on the hard test cases, which considers the largest number of positive ground truth objects.</p><p>Overall, compared to the very deep networks used in vision (e.g. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>), these relatively shallow networks trained without any of the recently developed tricks are expressive enough to achieve significant performance gains.</p><p>Interestingly, cyclist detection benefits the most from the expressiveness of CNNs even though this class has the least number of training examples. We conjecture that cyclists have a more distinctive shape in 3D compared to pedestrians and cars, which can be more easily confused with poles or vertical planes, respectively, and that Vote3Deep models can exploit this complexity particularly well, despite the small number of positive training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Timing and Sparsity</head><p>The three models from the test submission are also trained with different values for the L 1 sparsity penalty to examine the effect of the penalty on detection speed and accuracy on the moderate test cases of the validation set in <ref type="table" target="#tab_0">Table IV</ref>. The mean and standard deviation of the detection time per frame are measured on 200 frames.</p><p>Independent of whether the sparsity penalty is employed or not, pedestrians have the fastest detection speed as the receptive field of the networks is smaller compared to the other two classes. The two-layer "Model B" for cars runs faster than the three-layer "Model D" for cyclists.</p><p>When imposing the L 1 sparsity penalty during training, the detection speed at test time is improved by almost 40% for cars at a negligible decrease in accuracy. When applying a large penalty of 10 −1 , the activations of the pedestrian and cyclists models collapse to zero during training. Yet, with a smaller penalty the detection speeds improve by about 15%.</p><p>For the fastest cyclist model, the average precision decreases by 5% compared to the baseline. For pedestrians, however, we noted that the model without a penalty starts to overfit when training for the full 100 epochs. In this case, the sparsity penalty helps to regularise the model and has a beneficial effect on the model's accuracy. Notably, the sparsity penalty proves to be most useful for increasing the detection speed for cars where a larger penalty can be applied. We conjecture that both the reduced number of intermediate layers as well as the larger receptive field help the model to learn significantly sparser, yet still highly informative, intermediate representations.</p><p>While the results clearly indicate that the L1 sparsity penalty has a beneficial effect on detection speed, a more rigorous investigation into the statistics of this gain would be useful, given the stochastic nature of the training algorithm. We leave this investigation for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>This work performs object detection in point clouds at fast speeds with CNNs constructed from sparse convolutional layers based on the voting scheme introduced in <ref type="bibr" target="#b5">[5]</ref>. With the ability to learn hierarchical representations and non-linear decision boundaries, a new state of the art is established on the KITTI benchmark for detecting objects in point clouds.</p><p>Vote3Deep also outperforms other methods that utilise information from both point clouds and images in most test cases. Possible future directions include a more low-level input representation as well as a GPU implementation of the voting algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Authors are from the Oxford Robotics Institute, University of Oxford. {firstname}@robots.ox.ac.uk Input Point Cloud Object Detections CNNs (a) 3D point cloud detection with CNNs (b) Reference image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The result of applying Vote3Deep to an unseen point cloud from the KITTI dataset, with the corresponding image for reference. The CNNs apply sparse convolutions natively in 3D via voting. The model detects cars (red), pedestrians (blue), and cyclists (magenta), even at long range, and assigns bounding boxes (green) sized by class. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2. An illustration of the voting procedure on a sparse 2D example input without a bias. The voting weights are obtained by flipping the convolutional weights along each dimension. Whereas a standard convolution applies the filter at every location in the input, the equivalent voting procedure only needs to be applied at each non-zero location to compute the same result. Instead of a 2D grid with a single feature, Vote3Deep applies the voting procedure to 3D inputs with several feature maps. For a full mathematical justification, the reader is referred to [5]. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of the "Model D" architecture from Table I. The input x (green) and the intermediate representations h c (blue) for layer c are sparse 3D grids, where each occupied spatial location holds a feature vector (solid cubes). The sparse convolutions with the filter weights w c are performed natively in 3D to compute the predictions (red). Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Model comparison for the architecture in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Precision-Recall curves for the evaluation results on the KITTI test set. "Model B" for cars and "Model D" for pedestrians and cyclists, all with eight filters in the hidden layers and trained without sparsity penalty, are used for the submission to the official test server. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I KERNEL</head><label>I</label><figDesc></figDesc><table /><note>DIMENSIONS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II AP</head><label>II</label><figDesc>IN % ON THE KITTI TEST SET FOR METHODS ONLY USING POINT CLOUDS (AT THE TIME OF WRITING) ON THE KITTI TEST SET FOR METHODS UTILISING BOTH POINT CLOUDS AND IMAGES AS INDICATED BY * (AT THE TIME OF WRITING)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cars</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pedestrians</cell><cell></cell><cell>Cyclists</cell></row><row><cell></cell><cell></cell><cell cols="2">Processor Speed</cell><cell>Easy</cell><cell cols="2">Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell cols="2">Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>Vote3Deep</cell><cell cols="2">4-core 2.5GHz CPU</cell><cell>1.1s</cell><cell>76.79</cell><cell>68.24</cell><cell></cell><cell>63.23</cell><cell>68.39</cell><cell cols="2">55.37</cell><cell>52.59</cell><cell>79.92</cell><cell>67.88</cell><cell>62.98</cell></row><row><cell>Vote3D [5]</cell><cell cols="2">4-core 2.8GHz CPU</cell><cell cols="2">0.5s 56.80</cell><cell>47.99</cell><cell></cell><cell>42.56</cell><cell>44.48</cell><cell cols="2">35.74</cell><cell>33.72</cell><cell>41.43</cell><cell>31.24</cell><cell>28.60</cell></row><row><cell>VeloFCN [7]</cell><cell></cell><cell>2.5GHz GPU</cell><cell cols="2">1.0s 60.34</cell><cell>47.51</cell><cell></cell><cell>42.74</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CSoR</cell><cell cols="2">4-core 3.5GHz CPU</cell><cell cols="2">3.5s 34.79</cell><cell>26.13</cell><cell></cell><cell>22.69</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>mBoW [19]</cell><cell cols="2">1-core 2.5GHz CPU</cell><cell cols="2">10s 36.02</cell><cell>23.76</cell><cell></cell><cell>18.44</cell><cell>44.28</cell><cell cols="2">31.37</cell><cell>30.62</cell><cell>28.00</cell><cell>21.62</cell><cell>20.93</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">AP IN % Cars</cell><cell></cell><cell cols="3">Pedestrians</cell><cell>Cyclists</cell></row><row><cell></cell><cell></cell><cell cols="3">Processor Speed</cell><cell>Easy</cell><cell cols="3">Moderate Hard</cell><cell>Easy</cell><cell cols="2">Moderate Hard</cell><cell>Easy</cell><cell>Moderate Hard</cell></row><row><cell>Vote3Deep</cell><cell></cell><cell cols="2">4-core 2.5GHz CPU</cell><cell cols="2">1.1s 76.79</cell><cell cols="2">68.24</cell><cell>63.23</cell><cell>68.39</cell><cell>55.37</cell><cell>52.59</cell><cell>79.92</cell><cell>67.88</cell><cell>62.98</cell></row><row><cell cols="4">MV-RGBD-RF* [20] 4-core 2.5GHz CPU</cell><cell cols="2">4s 76.40</cell><cell cols="2">69.92</cell><cell>57.47</cell><cell>73.30</cell><cell>56.59</cell><cell>49.63</cell><cell>52.97</cell><cell>42.61</cell><cell>37.42</cell></row><row><cell cols="2">Fusion-DPM* [21]</cell><cell cols="2">1-core 3.5GHz CPU</cell><cell>30s</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>59.51</cell><cell>46.67</cell><cell>42.05</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV DETECTION</head><label>IV</label><figDesc>SPEED IN MILLISECONDS AND AVERAGE PRECISION FOR DIFFERENT VALUES OF THE L 1 SPARSITY PENALTY</figDesc><table><row><cell></cell><cell>Cars</cell><cell></cell><cell cols="2">Pedestrians</cell><cell>Cyclists</cell><cell></cell></row><row><cell>Penalty</cell><cell>Run-time</cell><cell>AP</cell><cell>Run-time</cell><cell>AP</cell><cell>Run-time</cell><cell>AP</cell></row><row><cell>0</cell><cell cols="2">873±234 0.76</cell><cell cols="3">508±119 0.70 1055±301</cell><cell>0.86</cell></row><row><cell>10 −3</cell><cell cols="3">819±211 0.75 518±114</cell><cell>0.73</cell><cell>1090±322</cell><cell>0.86</cell></row><row><cell>10 −2</cell><cell cols="2">814±213 0.74</cell><cell>426±84</cell><cell>0.72</cell><cell>888±239</cell><cell>0.81</cell></row><row><cell>10 −1</cell><cell cols="2">553±134 0.75</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to acknowledge the support of this work by the EPSRC through grant number DFR01420, a Leadership Fellowship, a grant for Intelligent Workspace Acquisition, and a DTA Studentship; by Google through a studentship; and by the Advanced Research Computing services at the University of Oxford.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="http://arxiv.org/abs/1409.1556" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<ptr target="http://arxiv.org/pdf/1512.03385v1.pdf" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Voting for Voting in Online Point Cloud Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vehicle Detection from 3D Lidar Using Fully Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<ptr target="https://arxiv.org/abs/1608.07916" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page" from="922" to="928" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Landing Zone Detection from LiDAR</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3471" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<ptr target="http://arxiv.org/abs/1409.6070" />
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<idno type="arXiv">arXiv:1505.02890</idno>
		<ptr target="http://arxiv.org/abs/1505.02890" />
		<title level="m">Sparse 3D convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05895</idno>
		<ptr target="http://arxiv.org/abs/1608.05895" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic Detection of Cerebral Microbleeds From MR Images via 3D Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<ptr target="http://ieeexplore.ieee.org" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prasoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Sparse Rectifier Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning: A Probabilistic Perspective</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="423" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<ptr target="https://arxiv.org/abs/1502.01852" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Laser-based segment classification using a mixture of bag-of-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Steinhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4195" to="4200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiview random forest of local experts combining RGB and LIDAR data for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Villalonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium, Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="356" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pedestrian detection combining RGB and dense LIDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4112" to="4117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
