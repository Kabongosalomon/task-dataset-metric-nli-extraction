<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Gilmer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
						</author>
						<title level="a" type="main">A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, hardware systems employing GPUs and TPUs have enabled neural network training programs to process dramatically more data in parallel than ever before. The most popular way to exploit these systems is to increase the batch size in the optimization algorithm (i.e. the number of training examples processed per training step). On many workloads, modern systems can scale to larger batch sizes without significantly increasing the time per step <ref type="bibr" target="#b8">(Jouppi et al., 2017;</ref><ref type="bibr" target="#b18">Wang et al., 2019)</ref>, thus proportionally increasing the number of training examples processed per second. If researchers can use this increased throughput to reduce the time required to train each neural network, then they should achieve better results by training larger models, using larger datasets, and by exploring new ideas more rapidly.</p><p>As the capacity for data parallelism continues to increase, practitioners can take their existing, well-tuned training configurations and re-train with larger batch sizes, hoping to achieve the same performance in less training time (e.g. <ref type="bibr">Ying et al., 2018)</ref>. On an idealized data-parallel system with negligible overhead from increasing the batch size, they might hope to achieve perfect scaling, a proportional reduction in training time as the batch size increases.</p><p>However, achieving perfect scaling is not always straightforward. Changing the batch size changes the training dynamics, requiring the training hyperparameters (e.g. learning rate) to be carefully re-tuned in order to maintain the same level of validation performance. 1 In addition, smaller batch sizes provide implicit regularization from gradient noise that may need to be replaced by other forms of regularization when the batch size is increased. Finally, even with perfect tuning, increasing the batch size eventually produces diminishing returns. After a critical batch size, the number of training steps cannot be decreased in proportion to the batch size -the number of epochs must increase to match the validation performance of the smaller batch size. See <ref type="bibr" target="#b15">Shallue et al. 2019</ref> for a survey of the effects of data parallelism on neural network training. Once these effects are taken into account, there is no strong evidence that increasing the batch size degrades the maximum achievable performance on any workload. At the same time, the ever-increasing capacity for data parallelism presents opportunities for new regularization techniques that can replace the gradient noise of smaller batch sizes and new optimization algorithms that can extend perfect scaling to larger batch sizes by using more sophisticated gradient information <ref type="bibr">(Zhang et al., 2019)</ref>. <ref type="bibr">You et al. (2017)</ref> proposed the LARS optimization algorithm in the hope of speeding up neural network training by exploiting larger batch sizes. LARS is a variant of stochastic gradient descent (SGD) with momentum <ref type="bibr" target="#b13">(Polyak, 1964)</ref> that applies layer-wise normalization before applying each gradient update. Although it is difficult to draw strong con-1 Although there are heuristics for adjusting the learning rate as the batch size changes, these heuristics inevitably break down sufficiently far from the initial batch size and it is also not clear how to apply them to other training hyperparameters (e.g. momentum). arXiv:2102.06356v2 <ref type="bibr">[cs.</ref>LG] 16 Feb 2021 clusions from the results presented in the LARS paper, the MLPerf 2 Training benchmark 3 adopted LARS as one of two allowed algorithms in the closed division for ResNet-50 on ImageNet and it became the de facto standard algorithm for that benchmark task. With MLPerf entrants competing to find the fastest-training hyperparameters for LARS, the first place submissions in the two most recent MLPerf Training competitions used LARS to achieve record training speeds with batch sizes of 32,678 and 65,536, respectively. No publications or competitive submissions to MLPerf have attempted to match these results with a standard optimizer (e.g. Momentum or Adam). However, MLPerf entrants do not have a strong incentive (nor are necessarily permitted by the rules) to explore other algorithms because MLPerf Training is a systems benchmark that requires algorithmic equivalence between submissions to make fair comparisons. Thus, it has remained an open question whether LARS was necessary to achieve these training speeds instead of a traditional, generic optimizer. Moreover, since the main justification for LARS is its excellent performance on ResNet-50 at large batch sizes, more work is needed to quantify any benefit of LARS over standard algorithms at any batch size. <ref type="bibr">You et al. (2019)</ref> later proposed the LAMB optimizer to speed up pre-training for BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> using larger batch sizes after concluding that LARS was not effective across workloads. LAMB is a variant of Adam <ref type="bibr" target="#b9">(Kingma &amp; Ba, 2014)</ref> that adds a similar layer-wise normalization step to <ref type="bibr">LARS. You et al. (2019)</ref> used LAMB for BERT pre-training with batch sizes up to 65,536 and claimed that Adam cannot match the performance of LAMB beyond batch size 16,384.</p><p>In this paper, we demonstrate that standard optimizers, without any layer-wise normalization techniques, can match or improve upon the large batch size results used to justify LARS and LAMB. In Section 2, we show that Nesterov momentum <ref type="bibr" target="#b12">(Nesterov, 1983)</ref> matches the performance of LARS on the ResNet-50 benchmark with batch size 32,768. We are the first to match this result with a standard optimizer. In Section 3, contradicting the claims in <ref type="bibr">You et al. (2019)</ref>, we show that Adam obtains better BERT pre-training results than LAMB at the largest batch sizes, resulting in better downstream performance metrics after fine-tuning. In addition, we establish a new state-of-the-art for BERT pretraining speed, reaching an F1 score of 90.46 in 7,818 steps using Adam at batch size 65,536 (we report training speed in steps because our focus is algorithmic efficiency, but since we compare LARS and LAMB to simpler optimizers, fewer training steps corresponds to faster wall-time in an optimized implementation -our BERT result with Adam also improves upon the wall-time record of LAMB 2 MLPerf is a trademark of MLCommons.org. 3 https://mlperf.org/training-overview reported in <ref type="bibr">You et al. 2019)</ref>. Taken together, our results establish stronger training speed baselines for these tasks and batch sizes, which we hope will assist future work aiming to accelerate training using larger batch sizes.</p><p>In addition to the contributions mentioned above, we demonstrate several key effects that are often overlooked by studies aiming to establish the superiority of new optimization algorithms. We show that future work must carefully disentangle regularization and optimization effects when comparing a new optimizer to baselines. We also report several underdocumented details used to generate the best LARS and LAMB results, a reminder that future comparisons should document any novel tricks and include them in baselines. Finally, our results add to existing evidence in the literature on the difficulty of performing independently rigorous hyperparameter tuning for optimizers and baselines. In particular, we show that the optimal shape of the learning rate schedule is optimizer-dependent (in addition to the scale), and that differences in the schedule can dominate optimizer comparisons at smaller step budgets and become less important at larger step budgets. <ref type="bibr" target="#b15">Shallue et al. (2019)</ref> and <ref type="bibr">Zhang et al. (2019)</ref> explored the effects of data parallelism on neural network training for different optimizers, finding no evidence that larger batch sizes degrade performance and demonstrating that different optimizers can achieve perfect scaling up to different critical batch sizes. <ref type="bibr">You et al. (2017;</ref> developed the LARS and LAMB optimizers in the hope of speeding up training by achieving perfect scaling beyond standard optimizers. Many other recent papers have proposed new optimization algorithms for generic batch sizes or larger batch sizes (see <ref type="bibr" target="#b14">Schmidt et al., 2020)</ref>. <ref type="bibr" target="#b4">Choi et al. (2019)</ref> and <ref type="bibr" target="#b14">Schmidt et al. (2020)</ref> demonstrated the difficulties with fairly comparing optimizers, showing that the hyperparameter tuning protocol is a key determinant of optimizer rankings. The MLPerf Training benchmark  provides a competitive ranking of neural network training systems, but does not shed much light on the relative performance of optimizers because entrants are limited in the algorithms they can use and the hyperparameters they can tune. We are unaware of any prior studies aiming to establish stronger baselines for standard optimizers at the batch sizes considered in this paper. Optimizer baselines are typically provided by the authors of new algorithms, who have limited incentives to spend significant effort and computational resources producing the strongest possible baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Matching LARS on ImageNet</head><p>The MLPerf training benchmark for ResNet-50 v1.5 on Ima-geNet  aims to reach 75.9% validation accuracy in the shortest possible wall-clock time. In the closed division of the competition, entrants must choose between two optimizers, SGD with momentum or LARS, and are only allowed to tune a specified subset of the optimization hyperparameters, with the remaining hyperparameter values set by the competition rules. <ref type="bibr">4</ref> The winning entries in the two most recent competitions used LARS with batch size 32,768 for 72 training epochs 5 and LARS with batch size 65,536 for 88 training epochs, 6 respectively. <ref type="bibr" target="#b10">Kumar et al. (2019)</ref> later improved the training time for batch size 32,768 by reaching the target accuracy in 64 epochs. These are currently the fastest published results on the ResNet-50 benchmark. However, it has been unclear whether LARS was necessary to achieve these training speeds since no recent published results or competitive MLPerf submissions have used another optimizer. In this section, we describe how we matched the 64 epoch, 32,768 batch size result of LARS using standard <ref type="bibr">Nesterov momentum. 7</ref> A fair benchmark of training algorithms or hardware systems must account for stochasticity in individual training runs. In the MLPerf competition, the benchmark metric is the mean wall-clock time of 5 trials after the fastest and slowest trials are excluded. Only 4 out of the 5 trials need to reach the target accuracy and there is no explicit limit on the number of times an entrant can try a different set of 5 trials. Since our goal is to compare algorithms, rather than systems, we aim to match the LARS result in terms of training steps instead (but since Nesterov momentum is computationally simpler than LARS, this would also correspond to faster wall-clock time on an optimized system). Specifically, we measure the median validation accuracy over 50 training runs with a fixed budget of 2,512 training steps 8 at a batch size of 32,768. When we ran the published LARS training pipeline, 9 LARS achieved a median accuracy of 75.97% and reached the target in 35 out of 50 trials. We consider the LARS result to be matched by another optimizer if the median over 50 trials exceeds the target of 75.9%. 4 https://git.io/JtknD 5 https://mlperf.org/training-results-0-6 6 https://mlperf.org/training-results-0-7 7 The 88 epoch, 65,536 batch size result is faster in terms of wall-clock time but requires more training epochs, indicating that it is beyond LARS's perfect scaling regime. Although LARS obtains diminishing returns when increasing the batch size from 32,768 to 65,536, future work could investigate whether Nesterov momentum drops off more or less rapidly than LARS. <ref type="bibr">8</ref> Corresponding to 64 training epochs in <ref type="bibr" target="#b10">Kumar et al. (2019)</ref>. 9 https://git.io/JtsLQ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Nesterov momentum at batch size 32k</head><p>This section describes how we used the standard Nesterov momentum optimizer to train the ResNet-50 v1.5 on Ima-geNet to 75.9% validation accuracy in 2,512 update steps at a batch size of 32,768, matching the best published LARS result at this batch size. Although we implemented our own training program, the only logical changes we made to the published LARS pipeline were to the optimizer and the optimization hyperparameters. Our model implementation and data pre-processing pipeline were identical to those required under the MLPerf closed division rules (see Appendix A).</p><p>We present two Nesterov momentum hyperparameter configurations that achieve comparable performance to LARS. Configuration A achieved a median accuracy of 75.97% (the same as LARS) and reached the target accuracy in 34 out of 50 trials. Configuration B is a modified version of Configuration A designed to make as few changes as possible to the LARS hyperparameters; it achieved a median accuracy of 75.92% and reached the target in 29 out of 50 trials. See Appendix C.1 for the complete hyperparameter configurations.</p><p>To achieve these results, we tuned the hyperparameters of the training pipeline from scratch using Nesterov momentum. We ran a series of experiments, each of which searched over a hand-designed hyperparameter search space using quasi-random search <ref type="bibr">(Bousquet et al., 2017)</ref>. Between each experiment, we modified the previous search space and/or tweaked the training program to include optimization tricks and non-default hyperparameter values we discovered in the state-of-the-art LARS pipeline. The full sequence of experiments we ran, including the number of trials, hyperparameters tuned, and search space ranges, are provided in Appendix C.4. Once we had matched the LARS result with Configuration A, we tried setting each hyperparameter to its value in the LARS pipeline in order to find the minimal set of changes that still achieved the target result, producing Configuration B. The remainder of this section describes the hyperparameters we tuned and the techniques we applied on the journey to these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">NESTEROV MOMENTUM OPTIMIZER</head><p>Nesterov momentum is a variant of classical or "heavy-ball" momentum defined by the update rule</p><formula xml:id="formula_0">v t+1 = µv t + ∇ (θ t ), θ t+1 = θ t − η t (µv t+1 + ∇ (θ t )) ,</formula><p>where v 0 = 0, θ t is the vector of model parameters after t steps, ∇ (θ t ) is the gradient of the loss function (θ) averaged over a batch of training examples, µ is the momentum, and η t is the learning rate for step t. We prefer Nesterov momentum over classical momentum because it tolerates larger values of its momentum parameter <ref type="bibr" target="#b16">(Sutskever et al., 2013)</ref> and sometimes outperforms classical momentum, although the two algorithms perform similarly on many tasks <ref type="bibr" target="#b4">Choi et al., 2019)</ref>. We tuned the Nesterov momentum µ in Configurations A and B. We discuss the learning rate schedule {η t } separately in Section 2.1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">BATCH NORMALIZATION</head><p>The ResNet-50 v1.5 model uses batch normalization <ref type="bibr" target="#b7">(Ioffe &amp; Szegedy, 2015)</ref>, defined as</p><formula xml:id="formula_1">BN(x (l) ) = x (l) − mean(x (l) ) var(x (l) ) + × γ (l) + β (l) ,</formula><p>where x (l) is a vector of pre-normalization outputs from layer l, mean(·) and var(·) denote the element-wise sample mean and variance across the batch of training examples, 10 and γ (l) and β (l) are trainable model parameters.</p><p>Batch normalization introduces the following tuneable hyperparameters: , the small constant added to the sample variance; the initial values of γ (l) and β (l) ; and ρ, which governs the exponential moving averages of the scaling factors used in evaluation. The LARS pipeline uses = 10 −5 and ρ = 0.9. It sets the initial value of β (l) to 0.0 everywhere, but the initial value of γ (l) depends on the layer: it sets γ (l) to 0.0 in the final batch normalization layer of each residual block, and to 1.0 everywhere else. In Configuration A, we tuned , ρ, and γ 0 , the initial value of γ (l) in the final batch normalization layer of each residual block. In Configuration B, we used the same values as LARS for and ρ, but we found that choosing γ 0 between 0.0 and 1.0 was important for matching the LARS result with Nesterov momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">REGULARIZATION</head><p>In Configuration A, we tuned both the L2 regularization coefficient λ and label smoothing coefficient τ <ref type="bibr" target="#b17">(Szegedy et al., 2016)</ref>. The LARS pipeline uses λ = 10 −4 and τ = 0.1. Crucially, the LARS pipeline does not apply L2 regularization to the bias variables of the ResNet model nor the batch normalization parameters γ (l) and β (l) (indeed, the published LARS pipeline does not even apply LARS to these parameters -it uses Heavy-ball momentum). This detail is extremely important for both LARS and Nesterov momentum to achieve the fastest training speed. Configuration B used the same λ and τ as Configuration A. <ref type="bibr">10</ref> In a distributed training environment the mean and variance are commonly computed over a subset of the full batch. The LARS pipeline uses a "virtual batch size" of 64, which we also use to avoid changing the training objective <ref type="bibr" target="#b6">(Hoffer et al., 2017</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">LEARNING RATE SCHEDULE</head><p>The LARS pipeline uses a piecewise polynomial schedule</p><formula xml:id="formula_2">η t =    η init + (η peak − η init ) t twarmup pwarmup , t ≤ t warmup η final + (η peak − η final ) T −t T −twarmup pdecay t &gt; t warmup ,</formula><p>with η init = 0.0, η peak = 29.0, η final = 10 −4 , p warmup = 1, p decay = 2, and t warmup = 706 steps. In Configuration A, we re-tuned all of these hyperparameters with Nesterov momentum. In Configuration B, we set η init , p decay , and t warmup to the same values as LARS, changing only p warmup from 1 to 2 and re-scaling η peak and η final . <ref type="table" target="#tab_1">Table 1</ref> shows the hyperparameter values for Configuration B that differ from the state-of-the-art LARS pipeline. Aside from re-tuning the momentum, learning rate scale, and regularization hyperparameters (whose optimal values are all expected to change with the optimizer), the only changes are setting p warmup to 2 instead of 1 and re-tuning γ 0 . <ref type="figure" target="#fig_0">Figure 1</ref>   schedules are similar, we found that each optimizer had a different optimal value of the warmup polynomial power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5.">COMPARING NESTEROV MOMENTUM AND LARS</head><p>As <ref type="table" target="#tab_2">Table 2</ref> shows, Nesterov momentum performs better with p warmup = 2 instead of 1, while the opposite is true with LARS. As discussed in <ref type="bibr" target="#b1">Agarwal et al. (2020)</ref>, optimizers can induce implicit step size schedules that strongly influence their training dynamics and solution quality, and it appears from <ref type="table" target="#tab_2">Table 2</ref> that the implicit step sizes of Nesterov momentum and LARS may evolve differently, causing the shapes of their optimal learning rate schedules to differ.</p><p>Although the main concern of a practitioner is validation performance, the primary task of an optimization algorithm is to minimize training loss. <ref type="table" target="#tab_3">Table 3</ref> shows that Nesterov momentum achieves higher training accuracy than LARS, despite similar validation performance. Thus, it may be more appropriate to consider the layerwise normalization of LARS to be a regularization technique, rather than an optimization technique.</p><p>Spending even more effort tuning LARS or Nesterov momentum would likely further improve the current state-ofthe-art for that optimizer. Meaningful optimizer comparisons are only possible with independent and equally intensive tuning efforts, and we do not claim that either optimizer outperforms the other on this benchmark. That said, if the main evidence for LARS's utility as a "large-batch optimizer" is its performance on this particular benchmark, then more evidence is needed to quantify any benefit it has over traditional, generic optimizers like Nesterov momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Lessons learned</head><p>In hindsight, it was only necessary to make a few changes to the LARS pipeline to match its performance at batch size 32,768 with Nesterov momentum. However, <ref type="table" target="#tab_1">Table 1</ref> does not accurately represent the effort required when attempting to match a highly tuned training-speed benchmark.</p><p>Firstly, as described in Sections 2.1.2 and 2.1.3, the strong results of LARS depend partly on a few subtle optimization tricks and non-default values of uncommonly-tuned hyperparameters. Fortunately, in this case we could discover these tricks by examining the open-source code required for MLPerf submissions, but machine learning research papers do not always report these important details. Researchers can easily waste a lot of experiments and produce misleading results before getting all of these details right.</p><p>We demonstrate the importance of adding these tricks to our Nesterov momentum pipeline in Appendix B; without these tricks (or some new tricks), we likely would not have been able to match the LARS performance.</p><p>Secondly, the learning rate schedule really matters when trying to maximize performance with a relatively small step budget. Both LARS and Nesterov momentum are sensitive to small deviations from the optimized learning rate schedules in <ref type="figure" target="#fig_0">Figure 1</ref>, and neither schedule works as well for the other optimizer. Although relatively minor changes were sufficient to match LARS with Nesterov momentum, there is no way to know a priori how the optimal schedule will look for a new optimizer <ref type="bibr" target="#b19">(Wu et al., 2018)</ref>. Even in toy settings where the optimal learning rate schedule can be derived, it does not fit into commonly used schedule families and depends strongly on the optimizer <ref type="bibr">(Zhang et al., 2019)</ref>. Indeed, this problem applies to the other optimization hyperparameters as well: it is extremely difficult to know which are worth considering ahead of time. Finally, even when we narrowed down our hyperparemeter search spaces around the optimal point, the volume of our search spaces corresponding to near-peak performance was small, likely due to the small step budget . We investigate how these effects change with a less stringent step budget in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Stronger BERT pretraining speed baselines</head><p>You et al. <ref type="formula" target="#formula_3">(2019)</ref> developed the LAMB optimizer in the hope of speeding up training for BERT-Large (Bidirectional Encoder Representations from Transformers, <ref type="bibr" target="#b5">Devlin et al., 2018)</ref>. BERT training consists of two phases. The "pretraining" phase has two objectives: (1) predicting masked tokens based on the rest of the sequence (a masked language model), and (2) predicting whether two given sentences follow one from another. Finally, the "fine-tuning" phase refines the model for a downstream task of interest. BERT pretraining takes a considerable amount of time (up to 3 days on 16 Cloud TPU-v3 chips <ref type="bibr" target="#b8">(Jouppi et al., 2017)</ref>), whereas the fine-tuning phase is typically much faster.  <ref type="formula" target="#formula_3">(2019)</ref>, we reported the F1 score on the downstream SQuaD v1.1 task as the target metric, although this metric introduces potential confounds: optimization efficiency should be measured on the training task using training and held-out data sets. Fortunately, in this case better pretraining performance correlated a with higher F1 score after fine-tuning. See Appendix A.2 for additional experiment details. We tuned Adam hyperparameters independently for each pretraining phase, specifically learning rate η, β 1 , β 2 , the polynomial power for the learning rate warmup p warmup , and weight decay λ, using quasi-random search <ref type="bibr">(Bousquet et al., 2017)</ref>. See Appendix C.2 for the search spaces.</p><p>In addition to hyperparmeter tuning, our improved Adam results at these batch sizes are also likely due to two implementation differences. First, the Adam implementation in <ref type="bibr">You et al. (2019)</ref> comes from the BERT open source code base, in which Adam is missing the standard bias correc-tion. <ref type="bibr">12</ref> The Adam bias correction acts as an additional step size warm-up, thereby potentially improving the stability in the initial steps of training. Second, the BERT learning rate schedule had a discontinuity at the start of the decay phase due to the learning rate decay being incorrectly applied during warm-up 13 (see <ref type="figure">Figure 2</ref> in Appendix A). This peculiarity is part of the official BERT release and is present in 3000+ copies of the BERT Training code on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Investigating a less stringent step budget</head><p>Part of what makes comparing optimizers so difficult is that the hyperparameter tuning tends to dominate the comparisons <ref type="bibr" target="#b4">(Choi et al., 2019)</ref>. Moreover, tuning becomes especially difficult when we demand a fixed epoch budget even when dramatically increasing the batch size . Fixing the epoch budget as the batch size increases is equivalent to demanding perfect scaling (i.e. that the number of training steps decreases by the same factor that the batch size is increased  <ref type="table">Table 5</ref>. Validation accuracy of ResNet-50 on ImageNet trained for 6,000 steps instead of 2,512. The second column is the optimizer that was applied to the batch normalization and ResNet bias variables. We report the median over 5 random seeds of the best hyperparameter setting in a refined search space. See Appendix C.3 for more details.</p><p>so we included Adam configurations with both = 10 −6 and = 10 −8 . <ref type="table">Table 5</ref> shows the validation accuracy of these different configurations after training for 6,000 steps with batch size 32,768. In every case, we used a simple cosine decay learning rate schedule and tuned the initial learning rate and weight decay using quasi-random search. We used momentum parameters of 0.98 for Nesterov momentum and 0.929 for LARS, respectively, based on the tuned values from Section 2. We used default hyperparameters for Adam and LAMB except where specified. We set all other hyperparameters to the same values as the state-of-the-art LARS pipeline, except we set γ 0 = 1.0. See Appendix C.3 for more details. As expected, highly tuned learning rate schedules and optimizer hyperparameters are no longer necessary with a less stringent step budget. Multiple optimizer configurations in <ref type="table">Table 5</ref> exceed the MLPerf target accuracy of 75.9% at batch size 32,768 with minimal tuning. Training with larger batch sizes is not fundamentally unstable: stringent step budgets make hyperparameter tuning trickier.</p><p>In <ref type="table">Table 5</ref>, "pure LAMB" performs extremely poorly: LAMB only obtains reasonable results when it is not used on the batch normalization and ResNet bias parameters, suggesting that layerwise normalization can indeed be harmful on some parameters. "Pure LARS" and Nesterov momentum perform roughly the same at this step budget, but the MLPerf LARS pipeline, which is tuned for a more stringent step budget, does not use LARS on all parameters, at least suggesting that the optimal choice could be budgetdependent.</p><p>Many new optimizers for neural networks, including LAMB, are introduced alongside claims that the new optimizer does not require any-or at least not very much-tuning. Unfortunately, these claims require a lot of work to support, since they require trying the optimizer on new problems without using those problems during the development of the algorithm. Although our experiments here are not sufficient to determine which optimizers are easiest to tune, experiments like these that operate outside the regime of highly tuned learning rate schedules can serve as a starting point. In this experiment, LARS and LAMB do not appear to have an advantage in how easy they are to tune even on a dataset and model that were used in the development of both of those algorithms. LAMB is a variant of Adam and performs about the same as Adam with the same value of ; LARS is more analogous to Momentum and indeed Nesterov momentum and LARS have similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our results show that standard, generic optimizers suffice for achieving strong results across batch sizes. Therefore, any research program to create new optimizers for training at larger batch sizes must start from the fact that Momentum, Adam, and likely other standard methods work fine at batch sizes as large as those considered in this paper. The LARS and LAMB update rules have no more to do with the batch size (or "large" batches) than the Momentum or Adam update rules. Whether layer-wise normalization can be useful for optimization or regularization remains an open question. However, if LARS and LAMB have any advantage over standard techniques, it is not that they work dramatically better on the tasks and batch sizes in <ref type="bibr">You et al. (2017;</ref>. It should not surprise us that standard techniques continue to work as we increase the batch size -increasing the batch size should make optimization easier, not harder, as the stochastic estimate of the full batch gradient becomes more accurate. 14 This is not to suggest that there is nothing interesting about studying neural network optimization at larger batch sizes. For example, as gradient noise decreases, there may be opportunities to harness curvature information and extend the region of perfect scaling <ref type="bibr">(Zhang et al., 2019)</ref>. However, there is currently no evidence that LARS and LAMB scale better than Momentum and Adam.</p><p>Our primary concern in this paper has been matching the state of the art-and establishing new baselines-for training speed measurements of the sort used to justify new techniques and algorithms for training with larger batch sizes. In contrast, many practitioners are more concerned with obtaining the best possible validation error with a somewhat flexible training time budget. Part of the reason why match-ing LARS at batch size 32,768 was non-trivial is because getting state of the art training speed requires several tricks and implementation details that are not often discussed. It was not obvious to us a priori which ones would prove crucial. These details do not involve changes to the optimizer, but they interact with the optimizer in a regime where all hyperparameters need to be well tuned to stay competitive, making it necessary to re-tune everything for a new optimizer.</p><p>In neural network optimization research, training loss is rarely discussed in detail and evaluation centers on validation/test performance since that is what practitioners care most about. However, although we shouldn't only consider training loss, it is counter-intuitive and counter-productive to elide a careful investigation of the actual objective of the optimizer. If a new optimizer achieves better test performance, but shows no speedup on training loss, then perhaps it is not a better optimizer so much as an indirect regularizer. <ref type="bibr">15</ref> Indeed, in our experiments we found that Nesterov momentum achieves noticeably better training accuracy on ResNet-50 than the LARS configuration we used, despite reaching roughly the same validation accuracy. Properly disentangling possible regularization benefits from optimization speed-ups is crucial if we are to understand neural network training, especially at larger batch sizes where we lose some of the regularization effect of gradient noise. Hypothetically, if the primary benefit of a training procedure is regularization, then it would be better to compare the method with other regularization baselines than other optimizers.</p><p>Ultimately, we only care about batch size to the extent that higher degrees of data parallelism lead to faster training. Training with a larger batch size is a means, not the end goal. New optimizers-whether designed for generic batch sizes or larger batch sizes-have the potential to dramatically improve algorithmic efficiency across multiple workloads, but our results show that standard optimizers can match the performance of newer alternatives on the workloads we considered. Indeed, despite the legion of new update rule variants being proposed in the literature, standard Adam and Momentum remain the workhorses of practitioners and researchers alike, while independent empirical comparisons consistently find no clear winner when optimizers are compared across a variety of workloads <ref type="bibr" target="#b14">(Schmidt et al., 2020)</ref>. Meanwhile, as <ref type="bibr" target="#b4">Choi et al. (2019)</ref> and our results underscore, comparisons between optimizers crucially depend on the effort spent tuning hyperparameters for each optimizer. Given these facts, we should regard with extreme caution studies claiming to show the superiority of one particular optimizer over others. Part of the issue stems from current incentives in the research community; we overvalue the novelty of new methods and undervalue establishing strong baselines to measure progress against. This is particularly problematic in the study of optimizers, where the learning rate schedule is arguably more important than the choice of the optimizer update rule itself! As our results show, the best learning rate schedule is tightly coupled with the optimizer, meaning that tuning the learning rate schedule for a new optimizer will generally favor the new optimizer over a baseline unless the schedule of the baseline is afforded the same tuning effort. Unfortunately, these kinds of subtleties are extremely difficult to account for and must be kept in mind when interpreting empirical comparisons of new optimizers to self-reported baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we demonstrated that standard optimizers, without any layer-wise normalization techniques, can match or exceed the large batch size results used to justify LARS and LAMB. Our results did not require specialized "large batch optimizers" or any new techniques whatsoever, only hyperparameter tuning and replicating all of the essential implementation details.</p><p>Future work attempting to argue that a new algorithm is useful by comparing to baseline methods or results, including those established in this paper, faces a key challenge in showing that the gains are due to the new method and not merely due to better tuning or changes to the training pipeline (e.g. regularization tricks). Although gains from tuning will eventually saturate, we can, in principle, always invest more effort in tuning and potentially get better results for any optimizer. However, our goal should be developing optimizers that work better across many different workloads when taking into account the amount of additional tuning they require.</p><p>Moving forward, if we are to reliably make progress we need to rethink how we compare and evaluate new optimizers for neural network training. Given how sensitive optimizer performance is to the hyperparameter tuning protocol and how difficult it is to quantify hyperparameter tuning effort, we can't expect experiments with self-reported baselines to always lead to fair comparisons. Ideally, new training methods would be evaluated in a standardized competitive benchmark, where submitters of new optimizers do not have full knowledge of the evaluation workloads. Some efforts in this direction have started, for instance the MLCommons Algorithmic Efficiency Working Group 16 , but more work needs to be done to produce incentives for the community to publish well-tuned baselines and to reward researchers that conduct the most rigorous empirical comparisons.</p><p>Ying, C., Kumar, S., Chen, D., Wang, T., and Cheng, Y. Image classification at supercomputer scale. arXiv preprint arXiv:1811.06992, 2018.</p><p>You, Y., Gitman, I., and Ginsburg, B. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017.</p><p>You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C. All experiments were run on Google TPUs <ref type="bibr" target="#b8">(Jouppi et al., 2017)</ref>. The ResNet-50 experiments used Jax <ref type="bibr" target="#b3">(Bradbury et al., 2018)</ref> using the Flax library, with code to be released soon. The BERT experiments were run using Ten-sorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> version 1.15. We used the standard train/validation split from the previous literature and MLPerf competition.</p><p>For ImageNet, we used the following sequence of Tensor-Flow functions for pre-processing: 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. BERT pre-training</head><p>We used the same experimental setup as the official BERT codebase 18 and the standard train/test split from the previous literature. This matches the experimental setup of You et al.</p><p>.</p><p>We trained the two pretraining objectives on the combined Wikipedia and Books corpus (Zhu et al., 2015) datasets (2.5B and 800M words, respectively). We used sequence lengths of 128 and 512, respectively, for the pretraining tasks. We ran the fine-tuning phase on the SQuaD v1.1 question answering task. In order to match You et al. <ref type="formula" target="#formula_3">(2019)</ref>, we report the F1-score on the dev set as the target metric. We followed the fine-tuning protocol described in the LAMB optimizer setup and did not perform any additional tuning for fine-tuning.</p><p>We tuned Adam hyperparameters using quasi-random search <ref type="bibr">(Bousquet et al., 2017)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Nesterov ablations</head><p>To explore the sensitivity of our best Nesterov momentum configuration (Configuration A), we ablated several elements of the experiment pipeline, one at a time, and tested their impact on performance. <ref type="figure">Figure 4</ref> shows the results of these experiments. "Base" refers to Nesterov momentum Configuration A <ref type="table">(Table 6)</ref>. "ResNet version" is the same point as "Base" but with ResNet version 1.0 instead of version 1.5. "BN init" is the same point as "Base" but with γ 0 = 1.0 instead of 0.4138. "Virtual BN" is the same point as "Base" but with a virtual batch size of 256 instead of 64, which is the largest that fits in a single TPUv3 core. "BN &amp; LR tuning" is Configuration B <ref type="table">(Table 6)</ref>, the same point as "Base" but with p decay , t warmup , η 0 , ρ, set to their values in the LARS pipeline. Finally, "L2 variables" is the same point as "Base" but where the L2 regularization is applied to all variables. The only ablation whose median over 50 seeds continues to beat the target 75.9% accuracy (noted by the dotted red line) is "BN &amp; LR tuning", with the rest having between 0.1%-0.3% drops in median accuracy. parameters in the experiment pipeline, and Configuration B, where we reverted the less impactful hyperparameters to the same values as the LARS baseline (or in the case of p warmup , a simpler value). We included Configuration B in order to demonstrate the minimal set of changes to the baseline necessary to still reach the target accuracy. The hyperparameter values for these configurations can be found in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Adam on BERT</head><p>The search space used to tune Adam on BERT for all phases of the pipeline can be found in <ref type="table">Table 7</ref>, which yielded our best Adam results on BERT in <ref type="table">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Less stringent step budget on ResNet-50</head><p>All trials used a cosine decay learning rate schedule and tuned the initial learning rate η and L2 regularization or <ref type="table">Table 7</ref>. The search space used to tune Adam on BERT for all phases of the pipeline. λ refers to weight decay and p refers to the polynomial power in the learning rate schedule for both the warmup and decay phases.</p><formula xml:id="formula_4">Hyperparameter Range Scaling p {1, 2} Discrete η [10 −5 , 1.0] Log 1 − β 1 [10 −2 , 0.5] Log 1 − β 2 [10 −2 , 0.5] Log λ [10 −3 , 10] Log</formula><p>weight decay parameter 19 λ according to <ref type="table">Table 9</ref>. We used 50 or more trials to search in the "Initial Range" and then 25 trials to search in the refined "Final Range." Finally, we ran the best point from the latter for 5 random seeds. When LARS or LAMB were used alongside a different optimizer for the batch normalization and ResNet-50 bias parameters, we set λ = 0 on the batch normalization and ResNet-50 bias parameters. When LAMB was used all parameters, the majority of trials diverged during training -it took 67 trials to get 25 trials that did not NaN during training. Our trial budgets refer to the number of feasible trials, i.e. trials that do not diverge during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Nesterov ResNet50 search space chronology</head><p>Below we list the sequence of search spaces we used to arrive at our final values in <ref type="table">Table 6</ref>. Given that the final results reported in papers are rarely found in a single iteration of experiments, we believe that it is important to document the full journey to arriving at our results.</p><p>Note that although we tuned a wide range of hyperparameters to match the LARS result with Nesterov momentum, we later realized that many of these hyperparameters could be reverted to the values from the LARS pipeline (see <ref type="table">Table 6</ref>). We started tuning with a training budget of 2,815 steps, which is the number of steps in the MLPerf 0.6 submission. We sometimes would decrease this to 2,658 steps to test how decreasing the training budget would affect tuning performance, before eventually moving to the 2,512 steps used to generate the results in the main text.     <ref type="table" target="#tab_1">Table 13</ref>. λ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,658 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step tdecay, and then is constant at the final learning rate η0 × η decay f actor . Virtual batch size 128. We increased the max learning rate based off the larger learning rates used by LARS. We also ran two additional studies which were the same except with 250 and 977 warmup steps.  <ref type="table" target="#tab_1">Table 14</ref>. λ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step tdecay, and then is constant at the final learning rate η0 × η decay f actor . Virtual batch size 128.  <ref type="table" target="#tab_1">Table 15</ref>. λ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step tdecay, and then is constant at the final learning rate η0 × η decay f actor . Virtual batch size 128.  <ref type="table" target="#tab_1">Table 17</ref>. λ refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters from <ref type="table" target="#tab_1">Table 16</ref>, we swept over the peak learning rate in a discrete set of ten values per order of magnitude, each for three random seeds, to find the max stable learning rate.   <ref type="table" target="#tab_2">Table 20</ref>. λ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step tdecay, and then is constant at the final learning rate η0 × η decay f actor . Virtual batch size 64. Peak learning rate range was consolidated based off the results of <ref type="table" target="#tab_1">Table 17</ref>. The weight decay range was consolidated based off the results of <ref type="table" target="#tab_1">Table 18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Range</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The learning rate schedules of LARS and Nesterov momentum Configuration B. Aside from re-scaling, the only difference is setting the warmup polynomial power to 2 instead of 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>An illustration of the sudden drop in the BERT learning rate schedule in the official codebase. 6 finetuning runs starting from the same pretraining checkpoint to show the stability of our results, at each of the 32,768, mixed 65,536-32,768, and 65,536 batch size settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Equal contribution 1 Google Research, Brain Team, Mountain View, California, USA 2 Center for Astrophysics | Harvard &amp; Smithsonian, Cambridge, MA, USA. Correspondence to: Zachary Nado &lt;znado@google.com&gt;, Justin Gilmer &lt;gilmer@google.com&gt;, Christopher Shallue &lt;cshallue@cfa.harvard.edu&gt;, Rohan Anil &lt;rohananil@google.com&gt;, George Dahl &lt;gdahl@google.com&gt;.</figDesc><table><row><cell>Preprint.</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The hyperparameters of Configuration B that differ from state-of-the-art LARS at batch size 32,768.</figDesc><table><row><cell>).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>shows the LARS learning rate schedule compared to the Nesterov momentum schedule. Even though these The best warmup schedule differs for Nesterov momentum and LARS. Values are medians over 50 training runs after setting pwarmup without retuning other hyperparameters.</figDesc><table><row><cell cols="2">p warmup Nesterov</cell><cell>LARS</cell></row><row><cell>1</cell><cell cols="2">75.79% 75.97%</cell></row><row><cell>2</cell><cell cols="2">75.92% 75.69%</cell></row><row><cell cols="3">Optimizer Train Acc Test Acc</cell></row><row><cell>Nesterov</cell><cell>78.97%</cell><cell>75.93%</cell></row><row><cell>LARS</cell><cell>78.07%</cell><cell>75.97%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Median train and test accuracies over 50 training runs for Nesterov momentum Configuration B and LARS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Using Adam for pretraining exceeds the reported performance of LAMB in You et al. (2019) in terms of F1 score on the downstream SQuaD v1.1 task.batch sizes up to 65,536 and claimed that LAMB outperforms Adam batch size 16,384 and beyond. The LAMB optimizer has since appeared in several NLP toolkits, including as Microsoft DeepSpeed and NVIDIA Multi-node BERT training, and as a benchmark task in MLPerf v0.7.</figDesc><table><row><cell cols="4">Batch size Step budget LAMB Adam</cell></row><row><cell>32k</cell><cell>15,625</cell><cell>91.48</cell><cell>91.58</cell></row><row><cell>65k/32k</cell><cell>8,599</cell><cell>90.58</cell><cell>91.04</cell></row><row><cell>65k</cell><cell>7,818</cell><cell>-</cell><cell>90.46</cell></row></table><note>Model quality is typically assessed on the downstream metrics, not on pre- training loss, making BERT training a somewhat awkward benchmark for optimization research. You et al. (2019) used LAMB for BERT pretraining with11 As shown in Table 4, we trained Adam baselines that achieve better results than both the LAMB and Adam results re- ported in You et al. (2019). Our new Adam baselines obtain better F1 scores on the development set of the SQuaD v1.1 task in the same number of training steps as LAMB for both batch size 32,768 and the hybrid 65,536-then-32,768 batch size training regime in You et al. (2019). We also ran Adam at batch size 65,536 to reach nearly the same F1 score as the hybrid batch size LAMB result, but in much fewer training steps. We believe 7,818 steps is a new state-of-the- art for BERT pretraining speed (in our experiments, it also improves upon the 76-minute record claimed in You et al., 2019). Additionally, at batch size 32,768 our Adam baseline got a better pretraining loss of 1.277 compared to LAMB's 1.342. We used the same experimental setup as You et al. (2019), including two pretraining phases with max sequence lengths of 128 and then 512. In order to match You et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>in a simple search space. Hyperparameters included learning rate η, β 1 , β 2 , the polynomial power for the learning rate warmup p warmup , and weight decay λ. We fixed the in Adam to 10 −11 for all BERT experiments. See Appendix C.2 for the search spaces.</figDesc><table><row><cell>We selected the best trial using the masked language model</cell></row><row><cell>accuracy over 10k examples from the training set. The num-</cell></row><row><cell>ber of training steps for each of the phases, as well as the</cell></row><row><cell>warmup steps are identical to You et al. (2019) and are listed</cell></row><row><cell>in Appendix C.2. Each phase of pretraining used completely</cell></row><row><cell>independent Adam hyperparameters. We found the final hy-</cell></row><row><cell>perparameters within 30 trials of random search for each of</cell></row><row><cell>the phases, except for the second phase of 65,536 batch size</cell></row><row><cell>which used 130 trials.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Best hyperparameters from tuning Adam on BERT-Large pretraining. λ refers to weight decay and p refers to the polynomial power in the learning rate schedule for both the warmup and decay phases. All trials used = 10 −11 . Search spaces used for the 6,000 step, cosine learning rate schedule experiments. All hyperparameters were tuned on a logarithmic scale, except for those which define a discrete sequence of points to evaluate such as "np.logspace". Log η decay factor {10 −4 , 10 −3 , 10 −2 , 10 −1 } Discrete</figDesc><table><row><cell cols="4">Batch size Phase Seq len Warmup</cell><cell cols="2">Train</cell><cell>Learning</cell><cell>β 1</cell><cell>β 2</cell><cell>λ</cell><cell>p</cell></row><row><cell></cell><cell></cell><cell></cell><cell>steps</cell><cell cols="2">steps</cell><cell>rate</cell><cell></cell></row><row><cell>32,768</cell><cell>1</cell><cell>128</cell><cell>3,125</cell><cell cols="5">14,063 5.9415 × 10 −4 0.934271 0.989295 0.31466 1</cell></row><row><cell>32,768</cell><cell>2</cell><cell>512</cell><cell>781</cell><cell cols="5">1,562 2.8464 × 10 −4 0.963567 0.952647 0.31466 1</cell></row><row><cell>65,536</cell><cell>1</cell><cell>128</cell><cell>2,000</cell><cell cols="5">7,037 1.3653 × 10 −3 0.952378 0.86471 0.19891 2</cell></row><row><cell>32,768</cell><cell>2</cell><cell>512</cell><cell>781</cell><cell cols="5">1,562 2.8464 × 10 −4 0.952647 0.963567 0.19891 2</cell></row><row><cell>65,536</cell><cell>2</cell><cell>512</cell><cell>390</cell><cell cols="2">781</cell><cell>6.1951 × 10 −5</cell><cell>0.65322</cell><cell>0.82451 0.19891 2</cell></row><row><cell cols="5">Weights Optimizer Bias/BN Optimizer Name</cell><cell></cell><cell>Initial Range</cell><cell cols="2">Final Range</cell><cell>Best</cell></row><row><cell>Nesterov</cell><cell></cell><cell>Nesterov</cell><cell></cell><cell>η</cell><cell cols="2">np.logspace(-.5, .5, 10)</cell><cell></cell><cell>[0.8, 3]</cell><cell>1.173</cell></row><row><cell>Nesterov</cell><cell></cell><cell>Nesterov</cell><cell></cell><cell>λ</cell><cell cols="2">np.logspace(-4, -3, 10)</cell><cell cols="2">[3 × 10 −4 , 10 −3 ]</cell><cell>3.026 × 10 −4</cell></row><row><cell>LARS</cell><cell></cell><cell>Heavy-ball</cell><cell></cell><cell>η</cell><cell></cell><cell>np.logspace(0, 2, 10)</cell><cell></cell><cell>[10, 40]</cell><cell>14.49</cell></row><row><cell></cell><cell></cell><cell>momentum</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LARS</cell><cell></cell><cell>Heavy-ball</cell><cell></cell><cell>λ</cell><cell cols="4">np.logspace(-5, -2, 10) [5 × 10 −5 , 2 × 10 −4 ] 1.708 × 10 −4</cell></row><row><cell></cell><cell></cell><cell>momentum</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LARS</cell><cell></cell><cell>LARS</cell><cell></cell><cell>η</cell><cell></cell><cell>[1, 30]</cell><cell></cell><cell>[10, 30]</cell><cell>14.18</cell></row><row><cell>LARS</cell><cell></cell><cell>LARS</cell><cell></cell><cell>λ</cell><cell></cell><cell>[10 −4 , 10 −1 ]</cell><cell cols="2">[5 × 10 −5 , 5 × 10 −4 ] 5.278 × 10 −5</cell></row><row><cell>Adam ( = 10 −8 )</cell><cell></cell><cell cols="2">Adam ( = 10 −8 )</cell><cell>η</cell><cell></cell><cell>[10 −3 , 1]</cell><cell cols="2">[4 × 10 −3 , 2 × 10 −2 ]</cell><cell>0.004596</cell></row><row><cell>Adam ( = 10 −8 )</cell><cell></cell><cell cols="2">Adam ( = 10 −8 )</cell><cell>λ</cell><cell></cell><cell>[10 −2 , 4]</cell><cell cols="2">[2 × 10 −1 , 1]</cell><cell>0.6182</cell></row><row><cell>Adam ( = 10 −6 )</cell><cell></cell><cell cols="2">Adam ( = 10 −6 )</cell><cell>η</cell><cell cols="2">np.logspace(-3, 0, 10)</cell><cell cols="2">[3 × 10 −3 , 10 −2 ]</cell><cell>3.332 × 10 −3</cell></row><row><cell>Adam ( = 10 −6 )</cell><cell></cell><cell cols="2">Adam ( = 10 −6 )</cell><cell>λ</cell><cell cols="2">np.logspace(-2, 0.5, 6)</cell><cell></cell><cell>[0.5, 2]</cell><cell>1.055</cell></row><row><cell>LAMB</cell><cell></cell><cell>LAMB</cell><cell></cell><cell>η</cell><cell cols="4">np.logspace(-4, 0, 30) [4 × 10 −3 , 5 × 10 −2 ]</cell><cell>0.01134</cell></row><row><cell>LAMB</cell><cell></cell><cell>LAMB</cell><cell></cell><cell>λ</cell><cell></cell><cell>np.logspace(-5, -2, 4)</cell><cell cols="2">[1 × 10 −2 , 0.1]</cell><cell>0.02657</cell></row><row><cell>LAMB</cell><cell></cell><cell cols="2">Adam ( = 10 −8 )</cell><cell>η</cell><cell></cell><cell>[10 −3 , 1]</cell><cell cols="2">[10 −2 , 8 × 10 −2 ]</cell><cell>0.02569</cell></row><row><cell>LAMB</cell><cell></cell><cell cols="2">Adam ( = 10 −8 )</cell><cell>λ</cell><cell></cell><cell>[10 −2 , 4]</cell><cell></cell><cell>[1, 8]</cell><cell>2.500</cell></row><row><cell>LAMB</cell><cell></cell><cell cols="2">Adam ( = 10 −6 )</cell><cell>η</cell><cell cols="2">np.logspace(-3, 0, 10)</cell><cell cols="2">[10 −2 , 8 × 10 −2 ]</cell><cell>0.03378</cell></row><row><cell>LAMB</cell><cell></cell><cell cols="2">Adam ( = 10 −6 )</cell><cell>λ</cell><cell cols="2">np.logspace(-2, 0.5, 6)</cell><cell></cell><cell>[1, 8]</cell><cell>4.197</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>First search space of the Nesterov tuning journey. The search spaces were mostly by informed guesses by the authors. λ refers to weight decay, which is applied to all variables. Tuned for 251 trials. Trained for 2,815 steps ("72 epochs" as defined by MLPerf epoch calculations). We used a linear learning rate decay schedule that decays for all training steps, starting from η0 and ending at η0 × η decay f actor . Virtual batch size 128. Log η decay factor {10 −4 , 10 −3 , 10 −2 , 10 −1 } Discrete</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>η 0</cell><cell>[10 −3 , 50.0]</cell><cell></cell></row><row><cell>1 − µ</cell><cell>[10 −3 , 1.0]</cell><cell>Log</cell></row><row><cell>λ</cell><cell>[10 −5 , 10 −1 ]</cell><cell>Log</cell></row><row><cell>τ</cell><cell>[10 −2 , 2 × 10 −1 ]</cell><cell>Linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Same asTable 10but trained for 2,658 steps ("68 epochs" as defined by MLPerf epoch calculations) for 50 trials.</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>η 0</cell><cell>[10 −1 , 20.0]</cell><cell>Log</cell></row><row><cell cols="3">η decay factor {10 −5 , 10 −4 , 10 −3 } Discrete</cell></row><row><cell>t decay</cell><cell>[2392, 2.658]</cell><cell>Linear</cell></row><row><cell>1 − µ</cell><cell>[10 −3 , 1.0]</cell><cell>Log</cell></row><row><cell>λ</cell><cell>[10 −5 , 2 × 10 −1 ]</cell><cell>Log</cell></row><row><cell>τ</cell><cell>[10 −2 , 2 × 10 −1 ]</cell><cell>Linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 .</head><label>12</label><figDesc>λ refers to weight decay, which is now not applied to the bias and batch normalization variables. 50 trials. Trained for 2,658 steps. Linear learning rate decay schedule that decays for tdecay steps, starting from η0 and ending at η0 × η decay f actor . Virtual batch size 128.RangeScaling η peak [10 −1 , 32.0] Log η decay factor {10 −5 , 10 −4 , 10 −3 } Discrete</figDesc><table><row><cell>t decay</cell><cell>[2392, 2.658]</cell><cell>Linear</cell></row><row><cell>1 − µ</cell><cell>[10 −4 , 10 −1 ]</cell><cell>Log</cell></row><row><cell>λ</cell><cell>[10 −4 , 10 −1 ]</cell><cell>Log</cell></row><row><cell>τ</cell><cell>[5 × 10 −2 , 0.15]</cell><cell>Linear</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 .</head><label>16</label><figDesc>Range Scaling η peak [10 −1 , 32.0] Log η decay factor [3 × 10 −5 , 3 × 10 −4 ] Log The same as Table 15 except with virtual batch size 64. , 2 × 10 α , ..., 9 × 10 α } ∀α ∈ {−3, ...2}} + {100, }</figDesc><table><row><cell>t decay</cell><cell>[2533, 2.815]</cell><cell>Linear</cell></row><row><cell>1 − µ</cell><cell>[5 × 10 −3 , 10 −1 ]</cell><cell>Log</cell></row><row><cell>λ</cell><cell>[10 −2 , 10 −1 ]</cell><cell>Log</cell></row><row><cell>τ</cell><cell>[5 × 10 −2 , 0.15]</cell><cell>Linear</cell></row><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>η peak</cell><cell cols="2">{{10 Discrete</cell></row><row><cell>η decay factor</cell><cell>8.144 × 10 −5</cell><cell>-</cell></row><row><cell>t decay</cell><cell>2250</cell><cell>-</cell></row><row><cell>1 − µ</cell><cell>0.02397</cell><cell>-</cell></row><row><cell>λ</cell><cell>0.009992</cell><cell>-</cell></row><row><cell>τ</cell><cell>0.07786</cell><cell>-</cell></row></table><note>α</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>10 α , 10 α , ...} ∀α ∈ {−3, ...0}} + {1.0, }Table 18. λ refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters fromTable 16, we swept over the weight decay in a discrete set of twenty values per order of magnitude, to test how high the regularization has to be in this region of hyperparameter space. , 10 −6 , 10 −5 , 10 −4 , 10 −3 , 10 −2 , 10 −1 }</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell></cell><cell></cell><cell>η peak</cell><cell>4.118</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>η decay factor</cell><cell>8.144 × 10 −5</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>t decay</cell><cell>2250</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>1 − µ</cell><cell>0.02397</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>λ</cell><cell>0.009992</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>τ</cell><cell>0.07786</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>ρ</cell><cell>{0.0, 0.1, 0.3, 0.5, 0.6, 0.7,</cell><cell>Discrete</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.8, 0.9, 0.95, 0.995, 0.999}</cell></row><row><cell></cell><cell></cell><cell></cell><cell>{10 −7 Discrete</cell></row><row><cell></cell><cell></cell><cell>Scaling</cell></row><row><cell>η peak</cell><cell>4.118</cell><cell>-</cell></row><row><cell>η decay factor</cell><cell>8.144 × 10 −5</cell><cell>-</cell></row><row><cell>t decay</cell><cell>2250</cell><cell>-</cell></row><row><cell>1 − µ</cell><cell>0.02397</cell><cell>-</cell></row><row><cell>λ</cell><cell cols="2">{{0.5 × Discrete</cell></row><row><cell>τ</cell><cell>0.07786</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 19 .</head><label>19</label><figDesc>λ refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters fromTable 16, we swept over batch normalization hyperparameters.</figDesc><table><row><cell></cell><cell>Range</cell><cell>Scaling</cell></row><row><cell>η peak</cell><cell>[2.0, 8.0]</cell><cell>Log</cell></row><row><cell cols="3">η decay factor [4 × 10 −5 , 1.6 × 10 −4 ] Linear</cell></row><row><cell>t decay</cell><cell>[2100, 2400]</cell><cell>Linear</cell></row><row><cell>1 − µ</cell><cell>[0.012, 0.04]</cell><cell>Log</cell></row><row><cell>λ</cell><cell>[7 × 10 −3 , 7 × 10 −2 ]</cell><cell>Log</cell></row><row><cell>τ</cell><cell>[0.04, 0.1]</cell><cell>Linear</cell></row><row><cell>ρ</cell><cell>[0.45, 0.55]</cell><cell>Linear</cell></row><row><cell></cell><cell>[5 × 10 −6 , 5 × 10 −5 ]</cell><cell>Linear</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We do not consider the MLPerf task in this paper since it is a warm-start, partial training task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://git.io/JtY8d 13 See https://git.io/JtnQW and https://git.io/ JtnQ8.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Of course, if the number of epochs is kept fixed as the batch size increases then performance may degrade due to using fewer updates.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">Deep learning folk wisdom is that "any method to make training less effective can serve as a regularizer," whether it is a bug in gradients or a clever algorithm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://mlcommons.org/en/groups/ research-algorithms/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t f . image . s a m p l e d i s t o r t e d b o u n d i n g b o x t f . image . d e c o d e a n d c r o p j p e g t f . image . r e s i z e t f . image . r a n d o m f l i p l e f t r i g h t t f . image . c o n v e r t i m a g e d t y p e</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">Full code available at https://git.io/JtgtE 18 https://github.com/google-research/bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">As suggested inYou et al. (2019), we used L2 regularization for LARS and weight decay for LAMB. For consistency, we used L2 regularization for Nesterov momentum (which is more analogous to LARS) and weight decay for Adam (which is more analogous to LAMB).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Roy Frostig for helpful discussions and valuable feedback on the manuscript. We would also like to thank James Bradbury for encouraging us to start this project and for help with the JAX MLPerf code. Finally, we would like to thank Dehao Chen and Tao Wang for their assistance with BERT training using TensorFlow and for helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 24</ref><p>. Again we dial in more on a tighter tuning range for the L2 regularization. λ refers to L2. Trained for 2,512 steps steps. Tuned for 37 trials. Virtual batch size 64.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11803</idno>
		<title level="m">Disentangling adaptive gradient methods from learning rates</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Critical hyper-parameters: No random</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.03200" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On empirical comparisons of optimizers for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05446</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08741</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bitorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09756</idno>
		<title level="m">models on google tpu-v3 pods</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pentecost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tabaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01500</idno>
		<ptr target="https://arxiv.org/abs/1910.01500" />
		<title level="m">MLPerf training benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/kˆ2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Dokl. akad. nauk Sssr</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Descending through a crowded valley-benchmarking deep learning optimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01547</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring the effects of data parallelism on neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">112</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10701</idno>
		<title level="m">Benchmarking tpu, gpu, and cpu platforms for deep learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding short-horizon bias in stochastic meta-optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02021</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
