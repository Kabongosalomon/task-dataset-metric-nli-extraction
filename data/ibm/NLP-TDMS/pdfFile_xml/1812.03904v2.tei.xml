<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-guided Unified Network for Panoptic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
							<email>liyanwei2017@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinze</forename><surname>Chen</surname></persName>
							<email>xinze.chen@horizon.ai</email>
							<affiliation key="aff2">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<email>zhuzheng2014@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
							<email>guan.huang@horizon.ai</email>
							<affiliation key="aff2">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
							<email>dalong.du@horizon.ai</email>
							<affiliation key="aff2">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
							<email>xingang.wang@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">CAS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-guided Unified Network for Panoptic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies panoptic segmentation, a recently proposed task which segments foreground (FG) objects at the instance level as well as background (BG) contents at the semantic level. Existing methods mostly dealt with these two problems separately, but in this paper, we reveal the underlying relationship between them, in particular, FG objects provide complementary cues to assist BG understanding. Our approach, named the Attention-guided Unified Network (AUNet), is a unified framework with two branches for FG and BG segmentation simultaneously. Two sources of attentions are added to the BG branch, namely, RPN and FG segmentation mask to provide object-level and pixellevel attentions, respectively. Our approach is generalized to different backbones with consistent accuracy gain in both FG and BG segmentation, and also sets new state-of-thearts both in the MS-COCO (46.5% PQ) and Cityscapes (59.0% PQ) benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding is a fundamental yet challenging task in computer vision, which has a great impact on other applications such as autonomous driving and robotics. Classic tasks for scene understanding mainly include object detection, instance segmentation and semantic segmentation. This paper considers a recently proposed task named panoptic segmentation <ref type="bibr" target="#b22">[23]</ref>, which aims at finding all foreground (FG) objects (named things, mainly including countable targets such as people, animals, tools, etc.) at the instance level, meanwhile parsing the background (BG) contents (named stuff, mainly including amorphous regions of similar texture and/or material such as grass, sky, road, etc.) at the semantic level. The benchmark algorithm <ref type="bibr" target="#b22">[23]</ref> and MS-COCO panoptic challenge winners <ref type="bibr" target="#b0">[1]</ref> dealt with this task by directly combining FG instance segmentation  <ref type="figure">Figure 1</ref>. Given an image 1(a), the goal of panoptic segmentation 1(b) is to find FG things at the instance level 1(c) and BG stuff at the semantic level 1(d). The things of the same class share the same color family but appear in different intensities. All these results are produced by the proposed approach.</p><p>models <ref type="bibr" target="#b14">[15]</ref> and BG scene parsing <ref type="bibr" target="#b44">[45]</ref> algorithms, which ignores the underlying relationship and fails to borrow rich contextual cues between things and stuff.</p><p>In this paper, we present a conceptually simple and unified framework for panoptic segmentation. To facilitate information flow between FG things and BG stuff, we combine conventional instance segmentation and semantic segmentation networks, leading to a unified network with two branches. This strategy brings an immediate improvement in segmentation accuracy as well as higher efficiency in computation (because the network backbone can be shared). This implies that panoptic segmentation benefits from complementary information provided by FG objects and BG contents, which lays the foundation of our approach.</p><p>Going one step further, we explore the possibility of in-tegrating higher-level visual cues (i.e., beyond the features extracted from the end of the backbone) towards the more accurate segmentation. This is achieved via two attentionbased modules working at the object level and the pixel level, respectively. For the first module, we refer to the regional proposals, each of which indicates a possible FG thing, and adjusts the probability of the corresponding region to be considered as FG things and BG stuff. For the second module, we take out the FG segmentation mask, and use it to refine the boundary between FG things and BG stuff. In the context of deep networks, these two modules, named the Proposal Attention Module (PAM) and Mask Attention Module (MAM), respectively, are implemented as additional connections across FG and BG branches. Within MAM, a new layer named RoIUpsample is designed to define an accurate mapping function between pixels in the fixed-shape FG mask and the corresponding feature map. In practice, all additional connections go from the FG branch to the BG branch, mainly due to the observation that FG segmentation is often more accurate 1 . Furthermore, BG stuff, while being refined by FG things, also gives feedback via gradients. Consequently, both FG and BG segmentation accuracies are considerably improved.</p><p>The overall approach, named Attention-guided Unified Network (AUNet), can be easily instantiated to various network backbones, and optimized in an end-to-end manner. We evaluate AUNet in two popular segmentation benchmarks, namely, the MS-COCO <ref type="bibr" target="#b27">[28]</ref> and Cityscapes <ref type="bibr" target="#b7">[8]</ref> datasets, and claim the state-of-the-art performance in terms of PQ, a standard metric integrating accuracies of both things and stuff <ref type="bibr" target="#b22">[23]</ref>. In addition, the benefits brought by joint optimization and two attention-based modules are verified through an extensive ablation study 4.2.</p><p>The major contribution of this research is to present a simple and unified framework for both FG and BG segmentation, which reaches the top performance in MS-COCO <ref type="bibr" target="#b27">[28]</ref> and Cityscapes <ref type="bibr" target="#b7">[8]</ref> datasets. Furthermore, this work also investigate the complementary information delivered by FG objects and BG contents. While panoptic segmentation serves as a natural scenario of studying this topic, its application lies in a wider range of visual tasks. Our solution, AUNet, is a preliminary exploration in this field, yet we look forward to more efforts along this direction.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews related work. Section 3 elaborates the proposed AUNet, including two attention-based modules. After experiments are shown in Section 4, we conclude this work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditional deep learning based scene understanding researches often focused on foreground or background targets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref>. Recently, the rapid progress in object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> and instance segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref> made it possible to achieve object localization and segmentation at a finer level. Meanwhile, the development of semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref> boosted the performance of scene parsing. Despite their effectiveness, the separation of these tasks caused the lack of contextual cues in instance segmentation as well as the confusion brought by individuals in semantic segmentation. To bridge this gap, recently, researchers proposed a new task named panoptic segmentation <ref type="bibr" target="#b22">[23]</ref>, which aims at accomplishing both tasks (FG instance and BG semantic segmentation) simultaneously. Panoptic Segmentation: In <ref type="bibr" target="#b22">[23]</ref>, the author gave a benchmark of panopic segmentation by combining instance and semantic segmentation models. Later, a weakly-supervised method <ref type="bibr" target="#b23">[24]</ref> was proposed on top of initialized semantic results, and an end-to-end approach <ref type="bibr" target="#b10">[11]</ref> was designed to combine both FG and BG cues. However, their performance is far from the benchmark <ref type="bibr" target="#b22">[23]</ref>. Different from them, our proposed AUNet achieves the top performance in an endto-end framework. Furthermore, we also establish the bond between proposal-based instance and FCN based semantic segmentation. Most recently works include <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. Instance Segmentation: Instance segmentation aims at discriminating different instances of the same object. There are mainly two streams of methods to solve this task, namely, proposal-based methods and segmentation-based methods. Proposal-based methods, with the help of accurate regional proposals, often achieved higher performance. Recent examples include MNC <ref type="bibr" target="#b8">[9]</ref>, FCIS <ref type="bibr" target="#b24">[25]</ref>, Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> and PANet <ref type="bibr" target="#b30">[31]</ref>. Moreover, segmentation-based methods aggregated pixel-level cues to compose instances combined with semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref> or depth ordering <ref type="bibr" target="#b43">[44]</ref> results. Semantic Segmentation: With the development of socalled encoding-decoding networks such as FCN <ref type="bibr" target="#b32">[33]</ref>, rapid progress has been made in semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b44">45]</ref>. In segmentation, capturing contextual information plays a vital role, for which various approaches were proposed including ASPP used in DeepLab <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> for multi-scale contexts, DenseASPP <ref type="bibr" target="#b40">[41]</ref> for global contexts, and PSPNet <ref type="bibr" target="#b44">[45]</ref> which collected contextual priors. There were also efforts to use attention modules for spatial feature selection, such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, which will be detailed discussed next. Attention-based Modules: Attention-based modules have been widely applied in visual tasks, including image processing, video understanding, and object tracking <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. In particular, SENet <ref type="bibr" target="#b18">[19]</ref> formulated channel-wise relationships via an attention-and-gating mechanism, non-  <ref type="figure">Figure 2</ref>. The proposed network structure. We adopt FPN as our backbone and share features with three parallel branches, namely foreground branch, background branch, and RPN branch. In the training stage, the network is optimized in an end-to-end manner. In the inference stage, panoptic results are generated by things and stuff results following the method described in Section 3.4. "⊕" denotes element-wise sum and the green "⊗" represents Proposal Attention Module (PAM) or Mask Attention Module (MAM) according to its position. PAM and MAM model the complementary relation between two branches. Details of PAM and MAM are shown in <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="figure">Figure 5</ref>. The red and green arrows represent upsample and attention operations, respectively.</p><p>local network <ref type="bibr" target="#b36">[37]</ref> bridged self-attention for machine translation <ref type="bibr" target="#b35">[36]</ref> to video classification using non-local filters. In the scope of scene understanding, <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b42">[43]</ref> aggregated global contextual information as well as class-dependent features by channel-attention operations. More recently, self-attention and channel attention were adopted by <ref type="bibr" target="#b11">[12]</ref> to model long-range contexts in the spatial and channel dimensions, respectively. In this work, we establish the relationship between foreground things and background stuff in panoptic segmentation with a series of coarse-to-fine attention blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attention-guided Unified Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem and Baselines</head><p>Panoptic segmentation task aims at understanding everything visible in one view, which means each pixel of an image must be assigned a semantic label and an instance ID. To address this issue, the existing top algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> directly combined the instance and semantic results from separate models, such as Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> and PSPNet <ref type="bibr" target="#b44">[45]</ref>.</p><p>We formulate the problem of panoptic segmentation as recognizing and segmenting all FG things and understanding all BG stuff. In this way, we solve the problem from two aspects, namely foreground branch and background branch in a unified network ( <ref type="figure">Figure 2</ref>). In detail, given an input image X, our goal is to generate FG things result Y Th and BG stuff result Y St simultaneously. Thus, the panoptic result Y Pa can be generated from Y Th and Y St directly using the fusion method in Section 3.4. The performance of panoptic results is evaluated by panoptic quality (PQ) <ref type="bibr" target="#b22">[23]</ref> as described in Section 4.1. For this purpose, we firstly introduce our unified framework for panoptic segmentation in this section. Then, key elements in our designed attentionguided modules are elaborated, including proposal attention module (PAM) and mask attention module (MAM). Finally, we give our implementation details.</p><p>In this work, we view the method, in which things and stuff are generated from separate models, as our baseline. Specifically, the baseline method gives the result of things Y Th and stuff Y St from separate models M Th and M St respectively. And the FG model M Th and BG model M St are given the similar backbones (e.g., FPN <ref type="bibr" target="#b26">[27]</ref>) for the following unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unified Framework</head><p>In order to bridge the gap between FG things with BG stuff, we propose the Attention-guided Unified Network (AUNet). Comparing with the baseline approach, the proposed AUNet fuses two models (M Th and M St ) together by sharing the same backbone and generates Y Th and Y St from parallel branches. As clearly illustrated in <ref type="figure">Figure 2</ref>, the AUNet is conceptually simple: FPN is adopted as the backbone to extract discriminative features from different scales and shared by all the branches.</p><p>Different from traditional approaches, which directly combine results from M Th and M St , the proposed AUNet optimizes them using a joint loss function L (defined in Section 3.4) and facilitates both tasks in a unified framework. In detail, we adopt a proposal-based instance segmentation module to generate finer masks M in foreground branch. And for background branch, light heads are designed to aggregate scene information from shared multi-scale features. In this way, the shared backbone is supervised by FG things and BG stuff simultaneously, which promotes the connection between two branches in feature space. In order to build up the bond between FG objects and BG contents more explicitly, two sources of attention modules are added. We consider the coarse attention operation between the i-th scale BG feature map with the corresponding RPN feature map, denoted by S i and P i respectively. The attention module can be formulated as S i ⊗ P i , where "⊗" denotes attention operations, as illustrated in <ref type="figure">Figure 2</ref>. Furthermore, the finer relationship is established by the attention between the processed feature map S pam and the generated FG segmentation mask P roi , which can be formulated as S pam ⊗ P roi . Details will be investigated in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention-guided Modules</head><p>Considering the complementary relationship between FG things and BG stuff, we introduce features from foreground branch to background branch for more contextual cues. From another perspective, the attention operation connecting two branches also establishes a bond between proposal-based method and FCN-based method segmentation. To this end, two spatial attention modules are proposed, namely proposal attention module (PAM) and mask attention module (MAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Proposal Attention Module</head><p>In classic two-stage detection frameworks, region proposal network (RPN) <ref type="bibr" target="#b33">[34]</ref> is introduced to give predicted binary class labels (foreground and background) and boundingbox coordinates. This means RPN features contain rich background information which can only be obtained from</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 1x1</head><p>ReLU Conv 1x1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-Sigmoid</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPN branch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background branch</head><p>Proposal Attention</p><formula xml:id="formula_0">CsxW''xH'' Conv 3x3 CrxW''xH'' CrxW''xH'' CsxW''xH'' Conv 1x1 GAP GN Sigmoid</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 3x3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 3x3</head><p>Background Reweight stuff annotations in background branch. Therefore, we propose a new approach to establish the complementary relationship between FG elements and BG contents, called Proposal Attention Module (PAM). As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we utilize contextual cues from RPN branch for attention operation. Here, we give a detailed formulation for this process. Given an input feature map P i ∈ R Cr×W ×H from the i-th scale RPN branch, the FG weighted map M i before sigmoid activation can be formulated as:</p><formula xml:id="formula_1">M i = f (σ(f (P i , w i,1 )), w i,2 )<label>(1)</label></formula><p>where f (·, ·) denotes a convolution function, σ represents the ReLU activation function, M i ∈ R 1×W ×H means the generated FG weighted map, both w i,1 ∈ R C r ×Cr×1×1 and w i,2 ∈ R 1×C r ×1×1 indicate convolutional parameters.</p><p>To emphasize the background contents, we formulate the attention weighted map M i as 1 − sigmoid(M i ). Then, the i-th scale activated feature map S i ∈ R Cs×W ×H can be presented as:</p><formula xml:id="formula_2">S i,j = S i,j ⊗ M i ⊕ S i,j<label>(2)</label></formula><p>where ⊗ and ⊕ denotes element-wise multiplication and sum respectively, S i,j means the j-th layer of semantic feature map S i ∈ R Cs×W ×H . Motivated by <ref type="bibr" target="#b18">[19]</ref>, a simple background reweight function is designed to downweight useless background layers after attention operation. We believe it could be improved, but it is beyond the scope of this work. The reweighted feature map S i ∈ R Cs×W ×H can be generated as:</p><formula xml:id="formula_3">N i = sigmoid(GN(f (G(S i ), w i,3 )))<label>(3)</label></formula><formula xml:id="formula_4">S i,k = S i,k ⊗ N i<label>(4)</label></formula><p>where G and GN denotes global average pooling and group norm <ref type="bibr" target="#b37">[38]</ref> respectively, N i ∈ R Cs×1×1 means reweighting operator, w i,3 ∈ R Cs×Cs×1×1 represents convolutional parameter, and S i,k indicates the k-th pixel channel in S i . Based on the above formulation of PAM, we highlight the background regions in the shared feature maps via attention operation and background reweight function. It also facilitates the learning of things in turn by enhancing the weights of activated foreground regions during backpropagation (see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Mask Attention Module</head><p>With the introduction of contextual cues by PAM, background branch is encouraged to focus more on the regions of stuff. However, the predicted coarse areas from RPN branch lack enough cues for precise BG representations. Unlike RPN features, the m×m fixed-shape masks generated from foreground branch encode finer FG layouts. Thus, we propose Mask Attention Module (MAM) to further model the relationship, as illustrated in <ref type="figure">Figure 5</ref>. Consequently, the 1 × W × H shape FG segmentation mask is needed for similar attention operations as before. Now, the problem is: how to reproduce the W × H shape FG feature map from m × m masks? RoIUpsample: In order to solve the size mismatching problem, we propose a new differentiable layer called RoIUpsample. Specifically, RoIUpsample is designed similar to the inverse process of RoIAlign <ref type="bibr" target="#b14">[15]</ref>, as clearly illustrated in <ref type="figure">Figure 4</ref>. In the RoIUpsample layer, the m × m mask (m equals to 14 or 28 in Mask R-CNN) is firstly reshaped to the same size of RoIs (generated from RPN). Then we utilize the designed inverse bilinear interpolation to compute values of the output features at four regularly sampled locations (same with RoIAlign) in each mask bin, and then sum up the final results as the generated mask feature map. To meet the requirement of bilinear interpolation <ref type="bibr" target="#b20">[21]</ref>, in which near points are given more contributions, an operation for inverse bilinear interpolation is formulated:</p><formula xml:id="formula_5">           R(p 1,1 ) = (1−xp)(1−yp) valuex×valuey R(p g ) R(p 1,2 ) = (1−xp)yp valuex×valuey R(p g ) R(p 2,1 ) = xp(1−yp) valuex×valuey R(p g ) R(p 2,2 ) = xpyp valuex×valuey R(p g )<label>(5)</label></formula><p>where R(p j,k ) denotes the result of point p j,k after inverse bilinear interpolation, R(p g ) here equals to one quarter of the corresponding value in the input mask, and normalized weights value x , value y are defined as:</p><formula xml:id="formula_6">value x = x 2 p + (1 − x p ) 2 , value y = y 2 p + (1 − y p ) 2<label>(6)</label></formula><p>in which x p and y p indicate the distance between grid point p g and generated p 1,1 in two axes respectively, as presented in <ref type="figure">Figure 4</ref>(b). Note that with the Equation 5 and 6, the m× m mask can also be reverted from the generated W × H feature map with the forward bilinear interpolation. Then, the generated feature map is assigned to four different scales according to the size of RoIs, which is similar with that in FPN <ref type="bibr" target="#b26">[27]</ref>. Consequently, the generated FG feature map is achieved for the following operations. Attention Operation: Different from traditional instance segmentation tasks, the predicted FG masks are utilized to give background branch more contextual guidance in pixellevel. We firstly aggregate them together to the C m × W × H feature map using RoIUpsample, as presented in <ref type="figure">Figure 5</ref>. Then, the finer 1 × W × H activated BG regions can be produced, similar with that in PAM. With the introduction of attention, the FG masks is also supervised by semantic loss function, which enables a further improvement in scene understanding (both for things and stuff), as discussed in Section 4.2. A similar background reweight function is adopted to aggregate useful highlighted background  <ref type="figure">Figure 5</ref>. The proposed mask attention module (MAM) for a finer relationship modelling. Here, "⊗" denotes spatial element-wise multiplication and "⊕" denotes element-wise sum. The red and green arrows represent upsample and operations in MAM respectively. GAP and GN are identical with that in PAM.</p><p>features. Consequently, we model the complementary relationship between FG things and BG stuff with the proposed PAM and MAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>In this section, we give more implementation details on the training and inference stage of our proposed AUNet. Training: As well elaborated in Section 3.2, all of our proposed methods are trained in a unified framework. The whole network is optimized via a joint loss function L during training stage:</p><formula xml:id="formula_7">L = λ 1 L RPN + λ 2 L RCNN + λ 3 L Mask + λ 4 L Seg (7)</formula><p>where L RPN , L RCNN , L Mask , and L Seg denotes the loss function of RPN, RCNN, instance segmentation, and semantic segmentation, respectively. Specifically, hyperparameters are designed to balance training processes, where λ 1 to λ 4 are set to {1, 1, 1, 0.3} for MS-COCO and {1, 0.75, 1, 1} for Cityscapes.</p><p>In details, we adopt ResNet-FPN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref> as our backbone. And the hyperparameters in the foreground branch are set following Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>. The backbone is pretrained on ImageNet <ref type="bibr" target="#b34">[35]</ref>, and the remaining parameters are initialized following <ref type="bibr" target="#b15">[16]</ref>. As standard practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>, 8 GPUs are used to train all the models. Each mini-batch has 2 images per GPU for ResNet-50 and ResNet-101 based networks and 1 image per GPU for the others. The networks are optimized for several epochs (18 for MS-COCO and 100 for Cityscapes) using mini-batch stochastic gradient descent (SGD) with a weight decay of 4e-5 and a momentum of 0.9. Batch Normalization <ref type="bibr" target="#b19">[20]</ref> in the backbone is fixed and Group Normalization <ref type="bibr" target="#b37">[38]</ref> is added to all of the branches in our final results. For MS-COCO <ref type="bibr" target="#b27">[28]</ref>, the learning rate is initialized with 0.02 for the first 13 epochs and divided by 10 at 15-th and 18-th epoch respectively. Input images are horizontally flipped and reshaped to the scale with a 600 pixels short edge during training. Multi-scale testing is adopted for final results 4.3. For Cityscapes <ref type="bibr" target="#b7">[8]</ref>, the learning rate is initialized with 0.01 and divided by 10 at 68-th and 88-th epoch respectively. We construct each minibatch for training from 16 random 512×1024 image crops (2 crops per GPU) after randomly flipping and scaling each image by 0.5 to 2.0×. Multi-scale testing is dropped in 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference:</head><p>The panoptic results are produced in inference stage by fusing the results of FG things and BG stuff in a similar way with that in <ref type="bibr" target="#b22">[23]</ref>. In this stage, the overlaps of things are first resolved in a NMS-like procedure which predicts the segments with higher confidence scores. And the relationships among categories are also considered during this procedure. For example, ties should not be overlapped by person in the final result. Then, the non-overlapping instance segments are combined with stuff results by assigning instance label first in favor of the things.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, our approach is evaluated on Microsoft COCO <ref type="bibr" target="#b27">[28]</ref> and Cityscapes <ref type="bibr" target="#b7">[8]</ref> datasets. We first give description of the datasets as well as the evaluation metrics. Then we evaluate our method and give detailed analyses. Comparison with the state-of-the-art methods in panoptic segmentation are presented at last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metrics</head><p>Dataset: Due to the novelty of panoptic task itself, there are few datasets with detailed panoptic annotations as well as public evaluation metrics. Microsoft COCO <ref type="bibr" target="#b27">[28]</ref> is the most suitable and challenging one for the new panoptic segmentation task, for the detailed annotations and high data complexity. It consists of 115k images for training and 5k images for validation, as well as 20k images for test-dev and 20k images for test-challenge. MS-COCO panoptic annotations includes 80 thing categories and 53 stuff categories. We train our models on train set with no extra data and reports results on val set and test-dev set for comparison. Cityscapes <ref type="bibr" target="#b7">[8]</ref> dataset is adopted to further illustrate the effectiveness of the proposed method. In detail, it contains 2975 images for training, 500 images for validation and 1525 images for testing with fine annotations. It has another 20k coarse annotations for training, which are not used in our experiment. We report our results on val set with 19 semantic label and 8 annotated instance categories. Evaluation Metrics: We adopt the evaluation metrics introduced by <ref type="bibr" target="#b22">[23]</ref>, which computes panoptic quality (PQ) metric for evaluation. PQ can be explained as the multiplication of a segmentation quality (SQ) and a recognition quality (RQ) term:</p><formula xml:id="formula_8">PQ = (p,g)∈T P IoU (p, g) |T P | segmentation quality(SQ) × |T P | |T P | + 1 2 |F P | + 1 2 |F N | recognition quality(RQ)<label>(8)</label></formula><p>where IoU(p, g) means the intersection-over-union between predicted object p and ground truth g, true positives (T P ) denotes matched pairs of segments (IoU(p, g) &gt; 0.5), false positives (F P ) represents unmatched predicted segments, and false negatives (F N ) means unmatched ground truth segments. PQ, SQ, and RQ of both thing and stuff are also reported in our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Component-wise Analysis and Diagnosis</head><p>In this section, we will decompose our approach stepby-step to reveal the effect of each component. All experiments in this section are trained and evaluated on MS-COCO dataset in a single model with no extra data. Here, we adopt ResNet-50-FPN as our backbone. For fair comparison, we strictly follow the merging method in <ref type="bibr" target="#b22">[23]</ref> with no trick or multi-scale data augmentation in training and inference stage when doing component-wise analyses. As presented in <ref type="table" target="#tab_1">Table 1</ref>, our proposed AUNet achieve an absolute improvement of 2.4% in PQ when compared with separate training method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Unified Framework</head><p>As elaborated in Section 3.2, our proposed unified framework deals with FG things and BG stuff in parallel branches. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the unified framework boosts up the performance both in PQ St and PQ Th , which brings 1.1% absolute improvements in PQ. This can be attributed to the shared backbone and joint optimization, with which the network is supervised to focus on more discriminative features for both things and stuff. With the shared backbone, the misclassification in stuff are effectively reduced and the things are given more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Proposal Attention Module</head><p>The proposed PAM builds the complementary relationship between things and stuff from different scales. By this way, the binary-classified RPN branch is optimized under the supervision of semantic labels. With the bond between stuff and things established, the network performs consistent gain in PQ St and PQ Th , as presented in <ref type="table" target="#tab_1">Table 1</ref>. The background reweight function proves its effectiveness in PQ St . This can be resulted from the global contextual features introduced by global average pooling in Equation 3, which means it chooses to aggregate highlighted BG features under the guidance of global context. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, the activated feature map M 4 emphasize the background areas with context cues. It is worth noting that we have tried other fusion methods for FG and BG feature fusion, such as concatenation and direct summary after feature transformation. But these strategies have minor contributions, which means the attention is more appropriate for relationship establishment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Mask Attention Module</head><p>While the PAM establishes the bond between FG objects and BG contents, the MAM gives background finer representations, as elaborated in Section 3.3.2 and <ref type="figure" target="#fig_3">Figure 6</ref>. As that in PAM, MAM also achieves better performance over the raw method in both PQ St and PQ Th . However, the contribution of MAM is slightly lower than PAM. We guess this is caused by the lack of contextual cues in the generated FG segmentation mask. <ref type="bibr" target="#b1">2</ref> In fact, we also evaluate the performance when adopting different resolution masks for RoIUpsample, namely the 14 × 14 mask and the 28 × 28  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to State-of-the-arts</head><p>We compare our proposed network with other stateof-the-art methods on MS-COCO <ref type="bibr" target="#b27">[28]</ref> test-dev and Cityscapes <ref type="bibr" target="#b7">[8]</ref> val set. MS-COCO: As shown in <ref type="table">Table 2</ref>, the proposed AUNet achieves the leading PQ performance 46.5% in MS-COCO dataset without bells-and-whistles. In details, winners of COCO2018 panoptic challenge <ref type="bibr" target="#b0">[1]</ref> adopt numerous additional network enhancements during training and inference stage, e.g., abundant extra data (110k external annotated MS-COCO images), multi-scale training, model ensemble. Moreover, considering the network enhancements adopted by the winner teams, cascade R-CNN <ref type="bibr" target="#b3">[4]</ref> is adopted for things and extra blocks or label bank <ref type="bibr" target="#b17">[18]</ref> are added for stuff as well. Different from them, the proposed AUNet achieves the top performance in a unified framework with no extra data or additional network enhancements for both things and stuff. To be more specific, only one single model based on the ResNeXt-152-FPN 3 is adopted in the AUNet.</p><p>Filtering out the improvement bring by model ensemble, we compare the AUNet with "PKU 360" team who adopted a similar backbone but with additional skills. The result shows that our algorithm perform better than them especially in PQ St , for about 4.9% absolute improvements. Furthermore, the AUNet overpasses the former end-to-end method, namely JSIS-Net <ref type="bibr" target="#b10">[11]</ref>, with a 19.3% absolute gap, which proves the effectiveness of the proposed method. In <ref type="table">Table 2</ref>, it is clear that the AUNet have a great balance be- <ref type="table">Table 2</ref>. Panoptic quality (%) on MS-COCO 2018 test-dev. "extra data" here denotes using extra dataset for training, "e2e" represents using a unified framework for things and stuff prediction, and "enhance Th " and "enhanceSt" indicates using additional enhancement techniques in network heads for things and stuff respectively. PQ Th and PQ St means PQ result for things and stuff respectively. We report our single model results with no extra data or network enhancement.  tween things and stuff, even when comparing with the challenge winners (no extra data). This is due to the introduction of unified framework and attention-guided modules for complementary relationship establishment, as well elaborated in Section 4.2. <ref type="figure" target="#fig_4">Figure 7</ref> gives intuitive presentations of the top performance using our proposed AUNet. Cityscapes: We compare our proposed method with the leading bottom-up methods and Mask R-CNN in <ref type="table" target="#tab_4">Table 3</ref>. Firstly, we adopt the same training strategy with that in MS-COCO, which means all things are considered as one category in background branch, denoted as Ours equ . However, the strategy is inferior to that when using all 19 semantic labels, as illustrated in <ref type="table" target="#tab_4">Table 3</ref>. Additionally, the MAM, which is proved to decrease the PQ in Cityscapes, is disabled in the final results. We guess the decline is caused by the inconsistency with prior information 1, which means the relatively worse things prediction may give wrong cues to stuff. Overall, the proposed method surpass previous stateof-the-art <ref type="bibr" target="#b23">[24]</ref>, with a 5.2% absolute gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presents AUNet, a unified framework for panoptic segmentation. The key difference from prior approaches lies in that we unify FG (instance-level) and BG (semantic-level) segmentation into one model, so that the FG branch, often being better optimized, can assist the BG branch via two sources of attention (i.e., proposal attention module and mask attention module), which offer objectlevel and pixel-level guidance, respectively. In experiments, we observe consistent accuracy gain in MS-COCO, based on which new state-of-the-arts are achieved. Our research delivers an important message: in visual tasks, it is often beneficial to partition targets into a few subclasses according to their properties, so that complementary information can be propagated across subclasses to assist scene understanding. Panoptic segmentation, being a new task, offers a natural partition between FG things and BG stuff, yet more possibilities remain unexplored and to be studied in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The designed proposal attention module (PAM) for complementary relationship establishment. We adopt this block in each scale of shared features, i.e., W and H changes in each scale. Here, "⊗" denotes spatial element-wise multiplication and "⊕" denotes element-wise sum. The green arrows represent operations in PAM. GAP and GN indicate Global Average Pooling and Group Normalization<ref type="bibr" target="#b37">[38]</ref>, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Heatmaps of the activated BG areas in PAM (the 4th scale, M 4 ) and MAM. The red regions are assigned more weights while the blue regions less weights in the background branch. All the input images are sampled from the MS-COCO val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Example results of AUNet on the MS-COCO val set. Our performance on things 7(c) is even better than human annotations 7(b). The things of the same class share the same color family but appear in different intensities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>RoIAlign output Conv feature map Grid points of bilinear interpolation Fixed dimensional representation Variable size RoI RoIUpsample output Generated mask feature map Grid points of inverse bilinear interpolation mxm mask Variable size RoIRoIAlign output Conv feature map Grid points of bilinear interpolation Fixed dimensional representation Variable size RoI RoIUpsample output Generated mask feature map Grid points of inverse bilinear interpolation mxm mask Variable size RoIMask Attention Background Reweight RoIUpsample Feature RoIUpsample Feature Spam Smam Proi</head><label></label><figDesc></figDesc><table><row><cell>p1,1 p1,1</cell><cell></cell><cell>p1,2 p1,2</cell><cell cols="3">(a) RoIAlign process</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>yp yp yp yp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p1,1 p1,1</cell><cell></cell><cell>p1,2 p1,2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>xp xp xp xp</cell><cell>pg pg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p2,1 p2,1</cell><cell>yp yp yp yp</cell><cell>p2,2 p2,2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>xp xp xp xp</cell><cell>pg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p2,1 p2,1</cell><cell></cell><cell>p2,2 p2,2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>CmxW'xH'</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoIUpsample</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mxm mask</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Conv 1x1 ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cs'x1x1</cell></row><row><cell></cell><cell></cell><cell>Cs'xW'xH'</cell><cell>Conv 1x1 1-Sigmoid</cell><cell>1xW'xH'</cell><cell>Cs'xW'xH'</cell><cell>GAP</cell><cell>Conv 1x1</cell><cell>GN</cell><cell>Sigmoid</cell><cell>Cs'xW'xH'</cell></row><row><cell cols="2">Background branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>pg (b) RoIUpsample process Figure 4. Comparison between RoIAlign [15] and our proposed RoIUpsample. The designed RoIUpsample, which can be viewed as an inverse operation of RoIAlign, reverts the feature map from FG masks according to their accurate spatial locations. Here, we show an example of RoIAlign output and RoIUpsample input with m = 2 for an intuitive illustration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison among different settings of panoptic quality (%) on the MS-COCO dataset. "rewt" means using background reweight function in PAM and MAM. PQ Th and PQ St indicates PQ for things and stuff respectively. Method PAM MAM rewt PQ PQ Th PQ St</figDesc><table><row><cell>AP mIoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Method backbone extra data e2e enhance Th enhance St PQ SQ RQ PQ Th SQ Th RQ Th PQ St SQ St RQ St</figDesc><table><row><cell>Megvii (Face++)</cell><cell>ensemble model</cell><cell>53.2 83.2 62.9 62.2</cell><cell>85.5</cell><cell>72.5</cell><cell>39.5 79.7</cell><cell>48.5</cell></row><row><cell>Caribbean</cell><cell>ensemble model</cell><cell>46.8 80.5 57.1 54.3</cell><cell>81.8</cell><cell>65.9</cell><cell>35.5 78.5</cell><cell>43.8</cell></row><row><cell>PKU 360</cell><cell>ResNeXt-152-FPN</cell><cell>46.3 79.6 56.1 58.6</cell><cell>83.7</cell><cell>69.6</cell><cell>27.6 73.6</cell><cell>35.6</cell></row><row><cell>JSIS-Net [11]</cell><cell>ResNet-50</cell><cell>27.2 71.9 35.9 29.6</cell><cell>71.6</cell><cell>39.4</cell><cell>23.4 72.3</cell><cell>30.6</cell></row><row><cell>Ours</cell><cell>ResNet-101-FPN</cell><cell>45.2 80.6 54.7 54.4</cell><cell>83.3</cell><cell>64.8</cell><cell>31.3 76.6</cell><cell>39.4</cell></row><row><cell>Ours</cell><cell>ResNet-152-FPN</cell><cell>45.5 80.8 55.0 54.7</cell><cell>83.4</cell><cell>65.2</cell><cell>31.6 76.9</cell><cell>39.7</cell></row><row><cell>Ours</cell><cell>ResNeXt-152-FPN</cell><cell>46.5 81.0 56.1 55.8</cell><cell>83.7</cell><cell>66.3</cell><cell>32.5 77.0</cell><cell>40.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Panoptic quality (%) on the Cityscapes val set. PQ Th and PQ St denotes PQ result for things and stuff respectively. We compare our results with the bottom-up methods (the first row). Oursequ indicates all things are considered as one category in the background branch during training.</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell cols="5">PQ PQ Th PQ St AP mIoU</cell></row><row><cell>DWT [3]</cell><cell>VGG16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>21.2</cell><cell>-</cell></row><row><cell>SGN [30]</cell><cell>VGG16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.2</cell><cell>-</cell></row><row><cell>Li et. al. [24]</cell><cell>ResNet-101</cell><cell cols="2">53.8 42.5</cell><cell cols="2">62.1 28.6</cell><cell>-</cell></row><row><cell>Mask R-CNN [15]</cell><cell>ResNet-50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.5</cell><cell>-</cell></row><row><cell>Ours equ</cell><cell cols="3">ResNet-50-FPN 55.0 51.2</cell><cell cols="2">57.8 32.2</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="3">ResNet-50-FPN 56.4 52.7</cell><cell cols="3">59.0 33.6 73.6</cell></row><row><cell>Ours</cell><cell cols="3">ResNet-101-FPN 59.0 54.8</cell><cell cols="3">62.1 34.4 75.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We find the pixel accuracy of things is much higher (6.7% absolute gap) than that of stuff, when considering instance with the same semantic as one category, e.g., all individuals are evaluated as person in testing. We evaluate them on the same MS-COCO semantic evaluation metric.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We adopt zero padding for vacant areas in RoIUpsample layer, resulting in blank BG context. This needs to be investigated in the future works.Input Image Activated Mask in PAM (4th scale) Activated Mask in MAM Input Image Activated Mask in PAM (4th scale) Activated Mask in MAM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use the 64×4d variant of ResNeXt<ref type="bibr" target="#b38">[39]</ref> with deformable conv<ref type="bibr" target="#b9">[10]</ref> and non-local blocks<ref type="bibr" target="#b36">[37]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Jiagang Zhu and Yiming Hu for valuable discussions. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco: Panoptic Leaderboard</surname></persName>
		</author>
		<ptr target="http://cocodataset.org/#panoptic-leaderboard.1" />
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Panoptic segmentation with a joint semantic and instance segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Daan De Geus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubbelman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02110</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Labelbank: Revisiting global perspectives for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Tong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09891</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02446</idno>
	</analytic>
	<monogr>
		<title level="m">Panoptic feature pyramid networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07709</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An end-to-end network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05027</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Affinity derivation and graph merge for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03784</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09337</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Instancelevel segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
