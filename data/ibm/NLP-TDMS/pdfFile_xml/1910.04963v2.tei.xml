<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interaction Relational Network for Mutual Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
						</author>
						<title level="a" type="main">Interaction Relational Network for Mutual Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Interaction Recognition</term>
					<term>Pose Information</term>
					<term>Re- lational Reasoning</term>
					<term>Skeleton Based</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person-person mutual action recognition (also referred to as interaction recognition) is an important research branch of human activity analysis. Current solutions in the field -mainly dominated by CNNs, GCNs and LSTMs -often consist of complicated architectures and mechanisms to embed the relationships between the two persons on the architecture itself, to ensure the interaction patterns can be properly learned. Our main contribution with this work is by proposing a simpler yet very powerful architecture, named Interaction Relational Network, which utilizes minimal prior knowledge about the structure of the human body. We drive the network to identify by itself how to relate the body parts from the individuals interacting. In order to better represent the interaction, we define two different relationships, leading to specialized architectures and models for each. These multiple relationship models will then be fused into a single and special architecture, in order to leverage both streams of information for further enhancing the relational reasoning capability. Furthermore we define important structured pair-wise operations to extract meaningful extra information from each pair of joints -distance and motion. Ultimately, with the coupling of an LSTM, our IRN is capable of paramount sequential relational reasoning. These important extensions we made to our network can also be valuable to other problems that require sophisticated relational reasoning. Our solution is able to achieve state-of-the-art performance on the traditional interaction recognition datasets SBU and UT, and also on the mutual actions from the large-scale dataset NTU RGB+D. Furthermore, it obtains competitive performance in the NTU RGB+D 120 dataset interactions subset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECOGNITION of human interaction (mutual actions) in videos is an important computer vision task, which can help us to develop solutions for a range of applications, such as surveillance, robotics, human-computer interface, contentbased retrieval, and so on. Although there have been many works during the past decades <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref>, it is still a challenging problem, especially when the videos offer unconventional conditions, such as unusual viewpoints and cluttered background.</p><p>Most of the previous works focused on mutual action recognition using RGB videos <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. These solutions comprise approaches going from hand-crafted local features to datadriven feature extraction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Some methods try to implicitly learn the information from the poses of the persons interacting <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. These solutions often operate over the pixel M. <ref type="bibr">Perez</ref>   data, making use of manually annotated regions or poselet labels to focus on these specific regions on the frame or to add extra ad-hoc information. However, they fail to explicitly model the relationships between the interacting body parts of the involved persons, crucial information for interaction recognition.</p><p>With the development of more advanced capturing devices that can also capture depth and extract pose information from the people in the scene (e.g., Microsoft Kinect <ref type="bibr" target="#b10">[11]</ref>), a more accurate and valuable description for the human pose can be directly used by new solutions. Besides, very recently, techniques that can estimate poses from regular RGB videos have improved significantly <ref type="bibr" target="#b11">[12]</ref>. This allows us to apply reliable pose-based solutions to mutual action recognition in the RGB videos.</p><p>The availability of explicit pose representation of the humans in the videos led to a new branch of interaction recognition techniques focusing on this type of data <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b16">[17]</ref>. These solutions usually involve hand-crafted features and a classification technique that theoretically incorporates the different body-parts' relationships, and then generate a rigid architecture highly dependent on what is believed to be the prior knowledge over the human skeleton structure.</p><p>There also exists solutions, using pose information, that target general action recognition <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b25">[26]</ref>, i.e. they include actions performed by a single individual as well. Majority of more recent approaches are based on LSTMs and CNNs, usually ad-hoc network architectures, extra regularization or losses, which tries to better model the spatial and temporal dynamics of the poses during the action. Although these works achieve promising results on general actions, they lack means of leveraging the relationship information between the poses of the two persons when dealing with interactions, underperforming in those cases.</p><p>Recurrent models (RNN/LSTM) and convolutional models (CNN/GCN) with carefully designed and complicated architectures have dominated the field of pose-based (skeletonbased) action recognition for several years. Instead of using recurrent/convolutional architectures for feature learning, in our paper we propose to view the mutual action recognition task as a body-parts relation inferring and learning problem, and thus design a new architecture to automatically handle this task. Our contribution is based on the powerful Relational Network (RN) <ref type="bibr" target="#b26">[27]</ref> for implicitly reasoning over the joints relationships present at human interactions. Although simple, RNs proved to be highly efficient at learning what are the important relationships between pair of objects. Santoro et al. <ref type="bibr" target="#b26">[27]</ref> proposed RNs as a simple architecture to deal with problems that require relational reasoning, obtaining state-ofthe-art performance on tasks such as visual and text-based question answering, as well as understanding the dynamic of physical systems that included motion data. To the best of our knowledge, no one before have developed a solution based on Relational Network with the purpose of interaction recognition, nor with explicit body information.</p><p>On this work we propose Interaction Relational Network (IRN), summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>. Since our method is the first attempt inspired by the Relational Networks (RNs) for skeletonbased interaction recognition, our first contribution lies on how to logically rearrange our problem components and characteristics in terms that match the high-level concept behind the original RNs. In other words, on how to use the pose information as input, identifying the existing objects (joints) and their properties (coordinates through time), and how to adequately pair these objects, representing the potential relations relevant for interaction recognition. In our new architecture, instead of using pre-defined joints dependency structures as previous methods, we enable our network to infer by itself the existing relations among specific relationships types. We identified two types of relationships, namely, intra relations of body joints from the same person and inter relations of the joints from different persons, thus the complex relations of different body parts in the person-person interaction sequence are well represented. The relation modules from the same relationship mapping share the same weights, and will infer from the consciously paired up joints which relations exists between them, and which are important for interaction recognition. The relationship mapping, which defines how the joints are paired up, will determine the type of relationships being inferred by that Relation Module (intra or inter). The description generated from all the relation modules will then be pooled and fed to a module, which will learn how to interpret this arrangement of relations, and classify the interaction accordingly.</p><p>Given the specificity of our task and data, we propose two distinct types of relations, however the original RNs contains a single model regardless of relationship type. Thus another contribution in our work is the design of multi-relationship architectures, merging different relation models into a single end-to-end network and leveraging both types of information, therefore leading to a more accurate recognition. We demonstrate the importance of appropriately choosing a fusion architecture, and initializing it with the right models prior to training. Furthermore, we enrich our solution by equipping our new network with structured operations over the objectpair, based on domain knowledge, to extract valuable joint distance and motion information before reasoning about their relationships. In contrast, the original RNs only contain pair independent operations previously to the relational module. At last, we incorporate an LSTM to our architecture, in order to enable the IRN reasoning over the interactions relationships over a longer duration sequence as well.</p><p>We validate our approach through extensive experiments on four different datasets. Two of these are traditional datasets for human interaction recognition: SBU <ref type="bibr" target="#b5">[6]</ref> and UT <ref type="bibr" target="#b0">[1]</ref>. On which we obtain the state-of-the-art performance, close to perfect accuracy. Seeking to further validate our IRN with more challenging data, we propose to use the large-scale datasets NTU RGB + D <ref type="bibr" target="#b27">[28]</ref> and NTU RGB + D 120 <ref type="bibr" target="#b28">[29]</ref>. Although these datasets were initially created for benchmarking general human action recognition, they contain many classes of mutual actions, in fact more than SBU and UT. Additionally, they have much more samples per class and adverse evaluation protocols. Even though the NTU RGB + D datasets are more difficult, the IRN still outperforms the previous works.</p><p>Our contributions can be summarized in the following manner:</p><p>• A novel approach to Human Interaction Recognition, using the different body parts from the pose information as independent objects, and modeling their relationship pairwise. • We design effective ways to fuse different types of relationships, leveraging their complementary for an improved performance. • We extend the relational network formulation with structured pair-wise mechanisms, that allows it to selfaugment its input with relevant extra information from the input pair. • A new and efficient paradigm for interaction recognition, based on relational networks, as an alternative to the CNN/RNN/GCN architectures that currently dominate the field. These contributions constitute the novel architecture Interaction Relational Network, which is simple yet effective. We validate on four important datasets that contains human interactions under different conditions, achieving state-of-theart performance in three of them and competitive results in the fourth, demonstrating the strength of our technique. This paper is an extension of our preliminary conference work <ref type="bibr" target="#b16">[17]</ref>. For the new work in this extension a novel formulation for the relational network with a pair-wise structured input is provided, allowing our IRN to automatically extract distance and motion features. In this new work, we also propose an improved relationships fusion architecture, which takes leverage from higher-level inferred relations. Another important new addition in this paper is the coupling of the LSTM to our architecture, enabling our method to perform temporal relational reasoning, and to reason over the entire interaction sequence. To further assess our proposed method, we perform experiments in two additional datasets with challenging characteristics: UT-Interaction, on which the poses had to be estimated; and NTU RGB+D 120, which contains many classes. Finally, in this new work we perform a more thoroughly qualitative analysis, with the aid of confusion matrices and a bar chart for performance per interaction class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Action Recognition</head><p>The problem of human interaction recognition is in fact a sub-field from the more general task of action recognition, which also takes into consideration actions performed by a single person. Previous work with methods using solely RGB videos usually rely on spatio-temporal local features description <ref type="bibr" target="#b29">[30]</ref> or CNN architectures for processing appearance and motion separately <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref>. Here we will focus on the previous work that also uses explicit pose information, and have as well evaluated their methods on the SBU interaction dataset.</p><p>Most of the latest works using pose information have moved to solutions involving Deep Learning, specially LSTMs <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Du et al. <ref type="bibr" target="#b17">[18]</ref> designed a hierarchical structure using Bidirectional Recurrent Neural Networks (BRNNs), which initially process the joints separately by body part, then fusing these parts step-by-step at subsequent layers, until the whole body is covered. Zhu et al. <ref type="bibr" target="#b18">[19]</ref> introduced a regularization in their LSTM to learn the co-occurrence features from the joints -i.e. learn the discriminative joint connections related to human parts. Liu et al. <ref type="bibr" target="#b19">[20]</ref> proposed a spatio-temporal LSTM architecture, where the network would unroll not only in the temporal domain, with sequential frames, but also in the spatial, with a pre-defined traversal order of the joints following the human skeleton structure.</p><p>Since these works focus on general actions, which mostly are performed by a single person, they do not include in their solutions any strategy specifically to model the relationship between the interacting persons when it is a mutual action. Therefore they disregard the additional information coming from the interaction itself, which our experiments demonstrate to be invaluable when dealing with two-person action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Human Interaction Recognition</head><p>Most of early work on human interaction recognition is based on hand-crafted local features, following a bag-of-words paradigm, and often using SVM for the final classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. More recent and successful approaches rely on Convolutional Neural Networks for feature extraction, usually combining it with Recurrent Neural Networks (e.g., Longshort Term Memory) for more complex temporal reasoning over the interactions <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>Some of these approaches allegedly focus on pose <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, or try to embed human structural knowledge on their solution <ref type="bibr" target="#b33">[34]</ref>. Vahdat et al. <ref type="bibr" target="#b4">[5]</ref> requires pre-computed person trajectories for their solution, initially they employ a pedestrian detector to obtain bounding boxes around the individuals on the first frame, then subsequently apply a tracker to find the trajectory in the later frames. These bounding boxes are used to retrieve the input information for their solution: spatial location, time of the pose and the cropped image within this region. On the work of Rapitis et al. <ref type="bibr" target="#b1">[2]</ref>, the authors have labeled pre-defined poselets on some of the frames. They create templates from these poselets by extracting localfeatures on the cropped image around these annotations. Based on these poselets features, their max-margin framework would then select key-frames to model an action.</p><p>Another branch of works do use explicit pose information with the purpose of interaction recognition <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. For example Yun et al. <ref type="bibr" target="#b5">[6]</ref> proposed features coming from different geometric relations between the joints intra-person and inter-person, as well as intra-frames and inter-frames, then using these features as input to an SVM for classification. Ji et al. <ref type="bibr" target="#b12">[13]</ref> grouped joints that belonged to the same body part to create poselets, for describing the interaction of these body parts from both individuals, subsequently generating a dictionary representation from these poselets, which will then be fed to an SVM.</p><p>Differently from the above mentioned approaches, our proposed architecture does not need the prior knowledge about the skeleton structure. We do not use the edge information in our architecture and the input is invariant to the joint order, therefore it learns the existing and important relationships by itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relational Network</head><p>Relational networks have been recently introduced by Santoro et al. <ref type="bibr" target="#b26">[27]</ref>, targeting to improve relational reasoning over a different range of tasks. It consists of initially reducing the problem input to individual objects, then using a simple architecture that models how these objects relate to each other in a pair-wise form and subsequently uses the general view from all the inferred relationships to solve the problem at hand. The authors demonstrated the versatility of these networks by applying their method on three tasks: visual question answering (QA), text-based question answering and complex reasoning about dynamic physical systems. Which covered not only distinct purposes (question-answering and physical modeling) but also different types of input data (visual, textual and spatial state-based).</p><p>Expanding RNs to Video QA, Chowdhury et al. <ref type="bibr" target="#b34">[35]</ref> proposed Hierarchical Relational Attention. On which the authors connected attention modules over VGG and C3D features to hierarchical relational modules, nested by input question query token. Park and Kwak <ref type="bibr" target="#b35">[36]</ref> applied RNs for 3D Pose Estimation. In their work they used as input estimated 2D joints coordinates, that are grouped by body part and fed in a specific order to their proposed RN to generate the estimated 3D position. Ibrahim et al. <ref type="bibr" target="#b36">[37]</ref> also employed Hierarchical RNs, but this time for group activity recognition. They start from CNN features from each individual, then input these features to hierarchically stacked multiple layers of relational modules, each one with their own parameters and relationships graph, which dictates the pairwise relationships to be made.</p><p>As far as we know, none of the previous work have designed relational networks that use pose information for the purpose of Human Interaction Recognition, with our work being the first one to extend RNs for this domain and application.</p><formula xml:id="formula_0">t j 0 j 1 j N ... P 1 j 0 j 1 j N ... P 2 j 0 j 0 j 0 j 1 ... j N j N j N-1 j N j 0 j 1 ... j N-1 j N j 0 j 1 ... j N-1 j N g θ g Θ f ϕ f ϕ' f ϕ'' g θ g Θ g Θ g θ g θ g Θ ... ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRN inter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRN intra</head><p>Predicted Action</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted Action</head><p>Predicted Action <ref type="figure">Fig. 2</ref>. Illustration of the IRN architecture for human interaction recognition. First, we extract the information across frames from each joint separately (jn). Then we use the set of joints from both persons (Pp) as input to our different architectures, IRN inter and IRN intra . Each architecture models different relationships between the joints, and can be used independently to predict the action. Furthermore, the models can be fused as IRN inter+intra , this way using the knowledge from both types of relationships for a more accurate prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRN inter+intra</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATIONAL NETWORK OVERVIEW</head><p>The Relational Network proposed by Santoro et al. <ref type="bibr" target="#b26">[27]</ref> can be simplified through the following equation:</p><formula xml:id="formula_1">RN (O) = f φ   i,k g θ (o i , o k )   (1)</formula><p>Where O is a set of objects {o 1 , o 2 , ..., o N }, on which any i th object is represented by an arbitrary R m vector containing the properties of that object. The function g, with learnable parameters θ, is responsible by modeling the relationships for each pair of input objects independently, therefore being also referred to as Relational Module. Meanwhile function f , with trainable parameters φ, is in charge for reasoning from the merger of the relationships inferred by g θ . It is a versatile formulation which, as long as the objects are fed in pairs to the relational module, can have as input different types of data describing the objects, such as CNN/LSTM features or even physical states.</p><p>However, the original RN does not cover the case when well-distinguished relationships can be defined due to the nature of the objects and the chosen pairing up rule, leading to distinct specialized models for each type. Missing therefore good ways to train these models and properly fusing them afterward. Also, in its base formulation the Relational Network only contains an unstructured function for reasoning over the input pair, represented by g θ , lacking a pair-wise structured function that can guide the general relational module with important domain-knowledge information. In the process of tailoring the RN paradigm to our domain, we design important novel extensions to address these issues on our proposed Interaction Relational Network, detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INTERACTION RELATIONAL NETWORK</head><p>We designed an architecture, namely Interaction Relational Network, tailored for human interaction recognition using pose information, inspired by the relational network paradigm of reasoning over pair-wise relationships. <ref type="figure">Fig. 2</ref> contains a visual representation of the proposed method. First we define how to logically re-arrange the skeleton information so it can be used as input to an RN based architecture, then we formulate important relationships mapping as to comprise all relevant relations pertaining to our problem. In order to fully capture the range of relationships types present in the human interaction problem, it was necessary to extend the RN proposal to handle multiple relations models and to fuse them properly. Also, we further extend the architecture with domainknowledge pair-wise operations over the input at each relation module, allowing the network to explicitly extract information known to be valuable when dealing with pose data: distance and motion. Finally, we couple our architecture with an LSTM, allowing our method to reason over the interactions during the whole video sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skeleton Joints as Independent Objects</head><p>The power of the relational networks lies on breaking down the problem into separate interacting objects, and learning specific parameters for describing how each object relate to each other, before merging this information for the classification of the whole. In the case of pose information and action recognition, we can define each joint j i as an object, using as their low-level feature its coordinates along the frames, together with integers to identify to which joint and body part does it belong:</p><formula xml:id="formula_2">j i = (x 1 , y 1 , x 2 , y 2 , ..., x T , y T , i, b)</formula><p>, where x t and y t are the 2D coordinates of the joint i belonging to the body part b at the frame t, and T is the desired sampling of frames to be used. As previous work have done <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we considered five body parts: torso, left hand, right hand, left leg, and right leg. Each person p will therefore have a set of joints for each video, which can be defined as: P p = {j p 1 , j p 2 , ..., j p N }, where N is the total number of joints provided by the pose data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relationships Architectures</head><p>The simplest approach would be to mix and match all the joints, learning a single model with all pairs combinations.</p><p>However, besides being computationally inefficient, this naive strategy disregards the fact that the joints might come from other individuals, hence the relationship which each other carries different meaning than when the joints are from the same individual. To allow our method to distinguish between the relevant relationships, and therefore better represent human interactions, we propose exclusive relationships mappings, which will be later merged into a single architecture.</p><p>Inter-person Relationships. Since we are dealing with interaction recognition, it is desired to map the relation between the joints inter-person. Therefore, all joints from one individual will be paired with all joints from the other individual, bidirectionally, so that it can be order-invariant (in the case of active/passive actions such as kicking). For that purpose, inspired by the formulation (1), we derive the following equation:</p><formula xml:id="formula_3">IRN inter (P 1 , P 2 ) = f φ   i,k g θ j 1 i , j 2 k ⊕ i,k g θ j 2 i , j 1 k  </formula><p>(2) Where f φ and g θ can be Multi-Layer Perceptrons (MLPs), with learnable parameters φ and θ respectively. In theory and ⊕ can be any pooling operation, such as sum, max, average or concatenate, but from our experiments, we decided to use average because it gives the best results.</p><p>Intra-person Relationships. Since the intra-person relationship of the joints can be highly informative as well, we also propose another architecture, where the joints from each person will be paired with the other joints from the same person. For this case there is no need to pair bi-directionally, since the paired joints are from the same person, what would only add unnecessary redundancy to our model -in fact our preliminary experiments demonstrated that it can lead to overfitting in some cases. The pooled output from each individual is concatenated ( ) before going through function f with trainable parameters φ .</p><formula xml:id="formula_4">IRN intra (P 1 , P 2 ) = f φ N i=1 N k=i+1 g Θ j 1 i , j 1 k N i=1 N k=i+1 g Θ j 2 i , j 2 k<label>(3)</label></formula><p>Fusing Relationships. Conclusively, we propose an architecture that fuses both types of relationships under the same function f (parameters φ ), by concatenating the pooled information from each function g, each with its own parameters θ and Θ:</p><formula xml:id="formula_5">IRN inter+intra (P 1 , P 2 ) = f φ   i,k g θ j 1 i , j 2 k ⊕ i,k g θ j 2 i , j 1 k N i=1 N k=i+1 g Θ j 1 i , j 1 k N i=1 N k=i+1 g Θ j 2 i , j 2 k (4) f ϕ'' g θ g Θ ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRN inter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IRN intra</head><p>Predicted Action <ref type="figure">Fig. 3</ref>. Simplified view of IRN f c1 inter+intra , an alternative way from fusing both types of relationships. On this case, the fusion occurs after the first fully-connected layer (f c1) from f φ and f φ , which provides a higher level description for the fusion f φ .</p><formula xml:id="formula_6">f ϕ' g Θ ... ... ... ... f ϕ fc1 fc1 IRN fc1 inter+intra</formula><p>Alternatively, we also designed another fusion architecture: IRN f c1 inter+intra <ref type="figure">(Fig. 3)</ref>. At this design we concatenate the output from f first fully-connected layer (f c1), of inter and intra models (φ and φ parameters respectively), before feeding into the fusion f φ . The motivation behind this design is to allow us to transfer knowledge not only from the relational modules g θ and g Θ , but also from their exclusive f modules. The modules f φ and f φ should be specialized on the pooled descriptor for its respective type of relationships, hence contributing with a higher level description for the fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pair-wise Structured Input</head><p>One of the advantages of RNs highlighted by Santoro et al. <ref type="bibr" target="#b26">[27]</ref> is the flexibility it has to reason over relatively unstructured inputs, for example embeddings from CNNs and LSTMs, as well as it can handle raw data such as state descriptors and, in our case, joints coordinates. Nonetheless, when dealing with structured data it is advantageous to design the architecture with mechanisms to leverage the intrinsic information contained in the data structure. Although this is the case with CNNs and LSTMs used for extracting the embeddings used as input to the RN, they are applied considering only the structure per-object independently. In the case of joints coordinates, the input pair itself is organized in a manner that there is a well-defined structure among them, i.e. coordinates at the same location in the array represents the spatial location of the objects at the same particular time in the video sequence. Therefore it can also be designed architectures to take leverage of the pair-wise structure of the input.</p><p>In order to enable the RN to allow this desired feature, but also keeping its flexibility, we propose the following extension to the base formulation (1), on which function h can be carefully designed to model important domain-knowledge pair-wise information:</p><formula xml:id="formula_7">RN (O) = f φ   i,k g θ (o i , o k , h (o i , o k ))  <label>(5)</label></formula><p>Inspired by previous work that demonstrated the usefulness of explicit distance and motion features for pose-based action recognition <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, we decided to augment our relational network with modules that can automatically extract this information from each pair of joints. In other words, our h function, to be also used as extra input by g θ , will explicitly compute vectors of distances (D(j i , j k )) and motions (M (j i , j k )) from the input object-pair:</p><formula xml:id="formula_8">h (j i , j k ) = (D(j i , j k ) M (j i , j k ))<label>(6)</label></formula><p>Considering c i t the coordinates of joint i at the frame t, the vector of distances is basically the euclidean distance between the input joints at each frame.</p><formula xml:id="formula_9">D(j i , j k ) = ( c i 1 − c k 1 , c i 2 − c k 2 , ..., c i T − c k T ) (7)</formula><p>And the motion vector is defined as the distance between the joints, but at sequential frames:</p><formula xml:id="formula_10">M (j i , j k ) = ( c i 1 − c k 2 , c i 2 − c k 3 , ..., c i T −1 − c k T ) (8)</formula><p>So we can write the improved g θ , used by IRN , as:</p><formula xml:id="formula_11">g θ (j i , j k ) = g θ (j i , j k , h(j i , j k ))<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sequential Relational Reasoning</head><p>Until this point, our proposed IRN could only reason on the interactions over a short period of time. Defined by the number of frames (T ) sampled when assembling the joints information (j i ) for input to our network. In other words, the hyper-parameter T would be analogous to a temporal receptive field of our IRN. Preliminary experiments showed that using a too large value of T is detrimental to performance. It was more effective to skip some of the frames by some step size, as in dilated convolutions <ref type="bibr" target="#b37">[38]</ref>, in order to increase the temporal range of the input. However, this workaround still does not allow the IRN to use the information available in the whole video sequence, and moreover, it has no means to model the intrinsic order of movements contained in an interaction.</p><p>Seeking to enable our method to use information from all frames and reason over the evolution of the interactions over time, we incorporate an LSTM to our architecture. The LSTM module is at the end of our architecture, after f φ and before predicting the action. This architecture, hereinafter referenced as LSTM-IRN , can then leverage the information from the whole interaction sequence before classifying the video. Resulting in an architecture also suitable to model over time the relationships present in human interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS A. Datasets</head><p>SBU [6] is a dataset for two-person interaction recognition, created using Kinect, providing reliable RGBD data for each video. It consists of eight interactions (approaching, departing, pushing, kicking, punching, exchanging objects, hugging, and shaking hands), seven different participants (pairing up to 21 different permutations), on a total of 282 short videos (around 2-3 seconds each). Recording was done in a single laboratory environment and with a frame rate of 15 frames per second (FPS). Pose information is provided by means of 3D coordinates over 15 joints per person, at each frame.</p><p>Coordinates are not entirely accurate, containing noise and incorrect tracking at some cases. We followed the 5-fold cross validation protocol defined by the authors, reporting the average accuracy.</p><p>UT-Interaction <ref type="bibr" target="#b0">[1]</ref> is also a dataset that focus on interperson interaction, but differently from SBU, it only contains RGB information -i.e. no explicit pose information is provided. To overcome this issue we used OpenPose <ref type="bibr" target="#b11">[12]</ref> to extract this type of information, estimating the joints coordinates for the actors in the video. This dataset contains six different classes of actions (shake-hands, point, hug, push, kick, and punch). It was recorded under two different set of conditions, therefore it is subdivided into UT-1 and UT-2, comprising a total of 120 videos, half at each set. Set 1 was recorded on a parking lot, with static background and little camera jitter, meanwhile, Set 2 was taken on a lawn, with partial occlusion from tree branches and with interference from the wind (camera jitter and moving leaves). Videos have about 2-6 seconds each, and a frame rate of 30 FPS. Authors protocol for evaluation consists of a 10-fold cross validation per set.</p><p>NTU RGB + D <ref type="bibr" target="#b27">[28]</ref> is a dataset with a wide range of general actions. It is not a dataset exclusively for interaction recognition, however it contains 11 classes of mutual actions (punch/slap, pat on the back, giving something, walking towards, kicking, point finger, touch pocket, walking apart, pushing, hugging, handshaking), more than SBU and UT-Interaction. Moreover, this subset with interaction-only classes, contains a total of 10,347 videos, which were collected with the more precise Kinect (V2) and under more challenging conditions, with 40 different subjects and large variation in viewpoints, by using three cameras recording at the same time. The length of the videos range from 1 to 7 seconds, with a frame rate of 30 FPS. The dataset contains the 3D coordinates from 25 joints per person for all frames. For evaluation, the authors proposed two protocols: Cross Subject (CS), on which 20 pre-defined actors are used for training, and the remaining for testing; and Cross View (CV), where twocameras are reserved for training, Although this is not a dataset specifically for Human Interaction Recognition, we believe that experimenting over this dataset mutual-only classes can be highly valuable at validating our methods because of its characteristics: large scale and more challenging conditions. NTU RGB + D 120 <ref type="bibr" target="#b28">[29]</ref> is an extension from the dataset above, and it is the largest available dataset for human action recognition with RGBD data. It contains 60 additional classes, of which 15 are mutual-action classes (hit with object, wield knife, knock over, grab stuff, shoot with gun, step on foot, high-five, cheers and drink, carry object, take a photo, follow, whisper, exchange things, support somebody, rock-paperscissors). In total there are 24,794 videos from 26 interactiononly classes, captured similarly as the previous version: using Kinect (V2), with many different subjects and a diverse range of viewpoints. The evaluation protocol is slightly different: Cross-Subject, which is the same as before, with a subset of actors being used exclusively for training and another for testing; and Cross-Setup, where instead of splitting the data according to the camera, pre-defined setups are selected for comprising the training and testing splits. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>MLPs Configuration. Hyperparameters detailed here were tuned during preliminary experiments. The IRN is implemented as an MLP, where g θ consists of four fully-connected layers, the first three with 1000 units and the last with 500, and f φ contains three fully-connected layers with 500, 250 and 250 units respectively, with dropout rate of 0.25. The LSTM module contains 256 units and also a dropout of 0.25. A softmax layer is placed at the end of the architecture to generate the interactions prediction scores. Training was performed with Adam optimizer, learning rate value of 1e-4 and weight initialization following a truncated normal distribution with zero mean and 0.045 standard deviation for SBU and UT, and 0.09 standard deviation for NTU RGB+D and NTU RGB + D 120.</p><p>Training Procedure. To improve generalization during training, we randomly swap the input order between the persons' joints set (P 1 P 2 ). This was significantly beneficial for the IRN intra architecture to avoid bias on the order of the concatenated feature generated after g Θ . IRN inter+intra parameters θ and Θ are fine-tuned from the weights obtained previously by training IRN inter and IRN intra separately, meanwhile φ is randomly initialized. For IRN f c1 inter+intra , in addition to θ and Θ, we also use the f c1 layer and weights from previously trained IRN inter and IRN intra , for initializing the model before training.</p><p>Pose Estimation. OpenPose <ref type="bibr" target="#b11">[12]</ref> is a readily available toolbox for extracting pose information from RGB videos. We ran it with its default options, which were not the most accurate, but it was faster and sufficiently precise for our experiments. However, because the output is frame-based, we had to apply some post-processing steps to guarantee the consistency of the bodies from Person 1 and 2 throughout the video. It consisted mainly on correctly assigning the pose at each frame to its respective body, based on the distance of the joints between the current frame and previous frames, assuming the overall distance should not be too different between consecutive frames. This is important when there are more than two poses in the frame, due to noise or because of passersby, or when the order of the actors' pose change.</p><p>Joints and Frames Subsampling. Although the poses for UT (estimated) and NTU RGB+D (provided) contains 25 joints, we sampled only 15 of them, analogous to what is provided by SBU data. As for the parameter T, regarding the temporal receptive field, we use different values for each dataset. For SBU, since the videos are shorter, we use 8 consecutive frames as a sampling for our input feature. Since UT, NTU RGB+D and NTU RGB+D 120 have longer videos, we use 32 frames. For the experiments without an LSTM we have chosen to sample the central frames, because most likely they contain the more relevant parts of the interaction. For LSTM-IRN we build our input sequence with overlapping frames at each timestep, by sampling T frames of the video with a step of T /2 frames, i.e. 8 frames input for every 4 frames step for SBU and 32 frames every 16 for the other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation experiments</head><p>First, to better evaluate the impact of each part from our proposed methodology, we separately show our results on the SBU dataset, reported in <ref type="table" target="#tab_1">Table I.</ref> Our baseline, using only the central frames coordinates and joint indexing information, already obtains 88.7% of accuracy with IRN inter and 95.4% of accuracy with IRN intra architecture. These results indicates that our approach is able to successfully map the different types of relationships present in the problem of interaction recognition. Incorporating our IRN with mechanisms to take leverage of the pair-wise structure of the input, what allows the network to extract explicit distance and motion information, proved to be highly valuable to IRN inter and it was also helpful to IRN intra , increasing the performance on both cases.</p><p>The Naive-IRN inter+intra experiment consists on simply merging all the relationships mapping (inter and intra) into a single model relational network. This means only one set of parameters for g and pooling the outputs indiscriminately into the same global descriptor before f . The Naive-IRN inter+intra approach underperforms, obtaining only 92.0% of accuracy, lower than the specialized IRN models. Averaging the scores from these specialized models is better than naively merging the relationships, but it is still worse than using IRN intra alone. We were able to truly take leverage of the complementary between the relationships models only through our proposed fused architecture IRN inter+intra . This method maintains the specialized models not only on its design, but also by initially training them separately -random initialization of weights (Random-IRN inter+intra ) also underperforms. Furthermore, our IRN f c1 inter+intra approach seems to further correlate and benefit from the complementary of the relationships, obtaining a more significant improvement. However, our attempts to fuse at even higher-levels of the MLPs (fc2 and fc3 layers) was not beneficial, due to higher overfit. Thus fc1 is the optimal layer for fusing the relationships.</p><p>Ultimately, we extend the relational reasoning to all available frames and enable reasoning over the interaction sequence by coupling our IRN with the LSTM module. Although leading to higher over-fitting for the intra relationship type on this small-scale dataset, which obtains a slightly lower performance than the baseline, the LSTM addition is advantageous for the inter variation and, more importantly, when fusing the relationships. With LSTM-IRN f c1 inter+intra , our method achieves the best performance in this dataset: 98.2%. <ref type="figure" target="#fig_1">Fig. 4</ref> contains the confusion matrices for all LSTM-IRN architectures. It is interesting to notice how the two relationships models have confusion on different interactions, for example Inter have some confusion between Punching and Exchanging while Intra does not, and how this confusion is greatly reduced at Inter+Intra. More importantly, interaction classes getting confused by both models, such as Pushing and ShakingHands, are almost entirely distinguished with Inter+Intra. This qualitative analysis is a good indication on the capability of our proposed architecture to keep the strength of both types of relationships models, and also to leverage their complementary for distinguishing between even harder cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. State-of-the-art Comparisons</head><p>SBU. We report our best results for SBU dataset in <ref type="table" target="#tab_1">Table II</ref>, alongside previous work results. Our specialized relationship architectures, LSTM-IRN inter and LSTM-IRN intra , can already outperform or be comparable to almost all of previous work. After fusion through architecture LSTM-IRN f c1 inter+intra we obtain state-of-the-art performance, with an advantage over 1 percentage point over the second best: 97.2% with VA-LSTM <ref type="bibr" target="#b39">[40]</ref>. UT-Interaction. <ref type="table" target="#tab_1">Table III</ref> shows our results on the UT dataset, together with the results from previous works. The LSTM-IRN f c1 inter+intra obtains a performance equivalent to the current state-of-the-art on the set UT-1, an almost saturated accuracy of 98.3%. On UT-2 set, our architectures outperform the previous methods, and set the new state-ofthe-art to 96.7%, which is also very high. For UT-2 there was no improvement from using LSTM-IRN f c1 inter+intra , when in comparison to the specialized LSTM-IRN inter model, it might be the case that for this dataset the interactions are not that complex, therefore not benefiting so much from more advanced relationships modeling. To the best of our knowledge, our work is the first using explicit pose information at UT-Interaction dataset, with the previous works being based on RGB information only. Therefore, our experiments demonstrate that it is also possible to perform high performance human interaction recognition in RGB videos using estimated poses. NTU RGB+D. Our experiments results over the interaction classes of this dataset are present in <ref type="table" target="#tab_1">Table IV</ref>. Both of the specialized architectures are able to match or outperform the majority of the compared methods on the two protocols, including the graph-based method ST-GCN <ref type="bibr" target="#b44">[45]</ref>. LSTM-IRN inter by itself obtains a performance very similar to another recent graph-based method AS-GCN <ref type="bibr" target="#b45">[46]</ref>, demonstrating the importance of considering the inter-person joints relationships when dealing with mutual actions. Our LSTM-IRN f c1 inter+intra architecture can obtain an even higher performance, going beyond 90% accuracy not only on the Cross-View protocol, but also on the more challenging Cross-Subject, ultimately outmatching all the compared methods in this dataset.</p><p>NTU RGB+D 120. The new version of the NTU RGB+D dataset <ref type="bibr" target="#b28">[29]</ref> is even more challenging, as shown in <ref type="table" target="#tab_5">Table V</ref> containing our results together with previous work methods. Our proposed LSTM-IRN architectures can still outmatch most of the compared methods, demonstrating they are at some extent scalable with respect to the number of classes. Moreover, as the complexity of the interaction recognition problem grows, more benefit can be obtained with the multirelationship model LSTM-IRN f c1 inter+intra . Claim supported by the fact that at NTU RGB+D the gain in performance obtained was approximately of 1 percentage point over LSTM-IRN inter , meanwhile at NTU RGB+D 120 this gain was  <ref type="bibr" target="#b44">[45]</ref>, being slightly inferior at the Cross-Subject protocol, but superior at Cross-Setup. The graph-based method AS-GCN <ref type="bibr" target="#b45">[46]</ref> have a higher scalability than ours LSTM-IRN , however our better performance in NTU RGB+D <ref type="bibr" target="#b27">[28]</ref> indicates the compactness of our method make it more suitable when handling less classes. A more detailed examination of the performance of LSTM-IRN f c1 inter+intra is shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, which contains the accuracy per interaction class for both protocols. Some interactions are significantly more challenging than others, with a recognition rate much inferior than the average. Through the confusion matrices present in <ref type="figure" target="#fig_3">Fig. 6</ref> it can be seen that, at least for the most severe cases (around or bellow 70% accuracy), both protocols have confusion mostly between the same interactions. A few notable differences are the confusion between SupportSomebody and KnockOver for Cross-Subject, and TouchingPocket with PattingOnBack for Cross-Setup. On both protocols there is a lot of confusion between HitWithObject and WieldKnife, and these two with Punch/slapping. Also, there is some confusion between Give-Something with Handshaking and ExchangeThings, as well as between ShootWithGun and TakePhoto. These confusions are reasonable, since we are using only the body parts coordinates information, and these interactions should have similar human movements (e.g. extending the hands to each other), being mainly distinguishable by the object (or its absence) each individual holds. We believe this is a strong evidence that for more advanced interaction recognition, RGB visual information should also be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we proposed the novel Interaction Relational Network for recognition of mutual actions, through an architecture focusing at relational reasoning over the different relationships between the human joints during interactions. Current techniques dominating the field of pose-based interaction recognition are based on CNNs/LSTMs/GCNs and consist of fixed structures determining how to relate each joint.</p><p>30% 40% 50% 60% 70% 80% 90% 100% Accuracy  Therefore, given our IRN capacity to learn from the data itself how the human body parts relate to each other while two individuals interact, it stands as a highly valuable alternative to these techniques.</p><p>Our proposed method obtains state-of-the-art performance on the traditional interaction datasets SBU and UT, and it also obtains the highest performance on the mutual actions subset of the NTU RGB+D dataset, surpassing even recent graphbased methods. Meanwhile achieving competitive results in the interactions subset of NTU RGB+D 120, what indicates it can fairly scale to more classes while being compact. To accomplish such deed, important extensions had to be made over the original Relational Network proposition in order to tailor it for our problem and data. Extensions such as: multiple relationships models; fusion of these multiple relationship models; domain knowledge pair-wise structured operation over the input; and sequential relational reasoning. We hope these extensions can also be valuable to other problems, with specific data and relationship particularities that the original RN it is not capable to suitably model without it. A more complex scenario that we believe to be a strong candidate for benefiting from our architecture and extensions, after some adaptations, is the problem of Group Activity Recognition <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>Furthermore, we believe our IRN can still be improved through different means. Our relational module can benefit from other sources of data, such as high-level visual features extracted on the region around the joints coordinates, which can be appended to the current input as extra and valuable information. Another possible improvement, would be to design a more sophisticated pair-wise structured operation. Alternatively to the fixed operations for distance and motion, which are based on the knowledge that the input data represents spatial coordinates at different frames, an architecture with trainable parameters can be designed to take leverage of the same knowledge. A different potential place for improvement in our architecture is at the pooling stage. Instead of averaging all pairs features with the same weight, therefore giving the same importance to all pairs of joints, an attention mechanism can be applied, so as to adjust the contribution to the global descriptor depending of which joints does that relationship belong to.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Subject</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Interaction Relational Network (IRN) overview. Joints of both individuals are separately paired up, and then fed to independent relation modules. The pair-wise inferred relationships are then aggregated into a global description used for, at last, perform human interaction recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Confusion matrices for SBU dataset with methods: (a) LSTM-IRN inter , (b) LSTM-IRN intra and (c) LSTM-IRN f c1 inter+intra .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Performance per interaction class of method LSTM-IRN f c1 inter+intra on the NTU RGB+D 120 dataset, for both protocols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Confusion matrices for NTU RGB+D 120 dataset with method LSTM-IRN f c1 inter+intra for protocols (a) Cross-Subject and (b) Cross-Setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and A. C. Kot are with the Rapid-Rich Object Search Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798. E-mail: {mauricio001, eackot}@ntu.edu.sg</figDesc><table /><note>J. Liu is with ISTD Pillar, Singapore University of Technology and Design, Singapore. E-mail: jun liu@sutd.edu.sg Corresponding author: Jun Liu</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>EXPERIMENTS ON SBU DATASET.</figDesc><table><row><cell>Experiment</cell><cell>Acc</cell></row><row><cell>IRN inter (Baseline)</cell><cell>88.7%</cell></row><row><cell>IRN inter (Self-Augmented Input)</cell><cell>93.6%</cell></row><row><cell>LSTM-IRN inter</cell><cell>94.6%</cell></row><row><cell>IRN intra (Baseline)</cell><cell>95.4%</cell></row><row><cell>IRN intra (Self-Augmented Input)</cell><cell>95.8%</cell></row><row><cell>LSTM-IRN intra</cell><cell>95.2%</cell></row><row><cell>Naive-IRN inter+intra</cell><cell>92.0%</cell></row><row><cell>Averaging scores</cell><cell>94.3%</cell></row><row><cell>Random-IRN inter+intra</cell><cell>92.5%</cell></row><row><cell>IRN inter+intra</cell><cell>96.1%</cell></row><row><cell>IRN f c1 inter+intra</cell><cell>96.7%</cell></row><row><cell>IRN f c2 inter+intra</cell><cell>94.9%</cell></row><row><cell>IRN f c3 inter+intra</cell><cell>95.3%</cell></row><row><cell>LSTM-IRN f c1 inter+intra</cell><cell>98.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF OUR RESULTS WITH PREVIOUS WORK ON SBU.</figDesc><table><row><cell>Method</cell><cell>Acc</cell></row><row><cell>Yun et al. [6]</cell><cell>80.3%</cell></row><row><cell>Ji et al. [13]</cell><cell>86.9%</cell></row><row><cell>HBRNN [18] (reported by [19])</cell><cell>80.4%</cell></row><row><cell>CHARM [39]</cell><cell>83.9%</cell></row><row><cell>CFDM [14]</cell><cell>89.4%</cell></row><row><cell>Co-occurrence LSTM [19]</cell><cell>90.4%</cell></row><row><cell>Deep LSTM (reported by [19])</cell><cell>86.0%</cell></row><row><cell>ST-LSTM [20]</cell><cell>93.3%</cell></row><row><cell>VA-LSTM [40]</cell><cell>97.2%</cell></row><row><cell>Wu et al. [16]</cell><cell>91.0%</cell></row><row><cell>Two-stream GCA-LSTM [23]</cell><cell>94.9%</cell></row><row><cell>LSTM-IRN inter</cell><cell>94.6%</cell></row><row><cell>LSTM-IRN intra</cell><cell>95.2%</cell></row><row><cell>LSTM-IRN f c1 inter+intra</cell><cell>98.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>FROM PREVIOUS APPROACHES AND OUR PROPOSED METHODS ON THE UT DATASET PER SET.</figDesc><table><row><cell></cell><cell>Acc</cell><cell></cell></row><row><cell>Method</cell><cell>UT-1</cell><cell>UT-2</cell></row><row><cell>Ryoo et al. [1]</cell><cell>70.8%</cell><cell>-</cell></row><row><cell>Raptis et al. [2]</cell><cell>93.3%</cell><cell>-</cell></row><row><cell>Donahue et al. [9]</cell><cell>85.0%</cell><cell>-</cell></row><row><cell>Kong and Fu [41]</cell><cell>93.3%</cell><cell>91.7%</cell></row><row><cell>Wang and Ji [4]</cell><cell>95.0%</cell><cell>-</cell></row><row><cell>Aliakbarian et al. [3]</cell><cell>90.0%</cell><cell>-</cell></row><row><cell>Ke et al. [34]</cell><cell>93.3%</cell><cell>91.7%</cell></row><row><cell>Shi et al. [10]</cell><cell>97.0%</cell><cell>-</cell></row><row><cell>Shu et al. [42]</cell><cell>98.3%</cell><cell>-</cell></row><row><cell>LSTM-IRN inter</cell><cell>93.3%</cell><cell>96.7%</cell></row><row><cell>LSTM-IRN intra</cell><cell>96.7%</cell><cell>91.7%</cell></row><row><cell>LSTM-IRN f c1 inter+intra</cell><cell>98.3%</cell><cell>96.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>FROM OUR PROPOSED METHODS ON THE SUBSET OF NTU RGB+D DATASET, CONTAINING ONLY THE MUTUAL ACTION CLASSES.</figDesc><table><row><cell></cell><cell cols="2">Acc Mutual Actions</cell></row><row><cell>Method</cell><cell>Cross-Subject</cell><cell>Cross-View</cell></row><row><cell>ST-LSTM [20]</cell><cell>83.0%</cell><cell>87.3%</cell></row><row><cell>GCA-LSTM [43]</cell><cell>85.9%</cell><cell>89.0%</cell></row><row><cell>Two-stream GCA-LSTM [23]</cell><cell>87.2%</cell><cell>89.9%</cell></row><row><cell>FSNET [44]</cell><cell>74.0%</cell><cell>80.5%</cell></row><row><cell>ST-GCN [45]</cell><cell>83.3%</cell><cell>87.1%</cell></row><row><cell>AS-GCN [46]</cell><cell>89.3%</cell><cell>93.0%</cell></row><row><cell>LSTM-IRN inter</cell><cell>89.5%</cell><cell>92.8%</cell></row><row><cell>LSTM-IRN intra</cell><cell>87.3%</cell><cell>91.7%</cell></row><row><cell>LSTM-IRN f c1 inter+intra</cell><cell>90.5%</cell><cell>93.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V RESULTS</head><label>V</label><figDesc>FROM OUR PROPOSED METHODS ON THE MUTUAL-ACTIONS SUBSET OF NTU RGB+D 120 DATASET.</figDesc><table><row><cell></cell><cell cols="2">Acc Mutual Actions</cell></row><row><cell>Method</cell><cell>Cross-Subject</cell><cell>Cross-Setup</cell></row><row><cell>ST-LSTM [20]</cell><cell>63.0%</cell><cell>66.6%</cell></row><row><cell>GCA-LSTM [43]</cell><cell>70.6%</cell><cell>73.7%</cell></row><row><cell>Two-stream GCA-LSTM [23]</cell><cell>73.0%</cell><cell>73.3%</cell></row><row><cell>FSNET [44]</cell><cell>61.2%</cell><cell>69.7%</cell></row><row><cell>ST-GCN [45]</cell><cell>78.9%</cell><cell>76.1%</cell></row><row><cell>AS-GCN [46]</cell><cell>82.9%</cell><cell>83.7%</cell></row><row><cell>LSTM-IRN inter</cell><cell>74.3%</cell><cell>75.6%</cell></row><row><cell>LSTM-IRN intra</cell><cell>73.6%</cell><cell>75.2%</cell></row><row><cell>LSTM-IRN f c1 inter+intra</cell><cell>77.7%</cell><cell>79.6%</cell></row><row><cell cols="3">between 3 and 4 percentage points. Our method obtains</cell></row><row><cell>competitive results to ST-GCN</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This research was carried out at the Rapid-Rich Object Search (ROSE) Lab, Nanyang Technological University (NTU), Singapore and supported by a grant from NTU's College of Engineering (M4081746.D90). This work was partially supported by SUTD SGP-AI grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Relationship Match : Video Structure Comparison for Recognition of Complex Human Activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1593" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2650" to="2657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encouraging LSTMs to Anticipate Actions Very Early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical context modeling for video event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1770" to="1782" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A discriminative key pose sequence model for recognizing human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-person interaction detection using body-pose features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1036" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Phrases for Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="707" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-Term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action Anticipation with RBF Kernelized Feature Mapping RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft Kinect Sensor and Its Effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive body part contrast mining for human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo Workshops</title>
		<imprint>
			<publisher>ICMEW</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning contrastive feature distribution model for interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="340" to="349" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiview Skeletal Interaction Recognition Using Active Joint Interaction Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2293" to="2302" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognition and Detection of Two-Person Interactive Actions Using Automatically Selected Skeleton Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interaction Recognition Through Body Parts Relation Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-Occurrence Feature Learning for Skeleton Based Action Recognition Using Regularized Deep LSTM Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3697" to="3703" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective Active Skeleton Representation for Low Latency Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="154" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative Multi-instance Multitask Learning for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="519" to="529" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeleton-Based Human Action Recognition with Global Context-Aware Attention LSTM Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Clip Representations for Skeleton-Based 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Latent Global Network for Skeleton-based Action Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="959" to="970" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1545" to="1564" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
	<note>NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequential Deep Trajectory Descriptor for Action Recognition With Three-Stream CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1510" to="1520" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Leveraging Structural Context Models and Ranking Score Fusion for Human Interaction Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1712" to="1723" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical Relational Attention for Video Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I H</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="599" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation with Relational Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical Relational Networks for Group Activity Recognition and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Category-Blind Human Action Recognition: A Practical Recognition System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chuah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4444" to="4452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2136" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Close human interaction recognition using patchaware models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>1811.00270</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Global Context-Aware Attention LSTM Networks for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Skeleton-Based Online Action Prediction Using Scale Selection Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1453" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3590" to="3598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Hierarchical Deep Temporal Model for Group Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Singapore. He received his M.Sc. degree from University of Campinas in 2016, and his B.Sc. degree from Federal University of São Carlos in 2012, both in Brazil</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include Deep Learning, Video Analysis, Sensitive Media Detection, Medical Imaging and Computer Vision</title>
		<imprint/>
		<respStmt>
			<orgName>Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note>Mauricio Perez is currently pursuing the Ph.D. degree with the School of Electrical and Electronic Engineering</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">His research interests include video analysis, human action recognition</title>
	</analytic>
	<monogr>
		<title level="m">2011, the M.Sc. degree from Fudan University, China, in 2014, and the Ph.D. degree with the School of Electrical and Electronic Engineering</title>
		<meeting><address><addrLine>China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Jun Liu is an Assistant Professor at Singapore University of Technology and Design. He obtained the B.Eng. degree from Central South University ; Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note>and deep learning</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">He headed the Division of Information Engineering with the School of Electrical and Electronic Engineering for eight years and served as an Associate Chair (Research). He was the Vice Dean (Research) with the School of Electrical and Electronic Engineering and the Associate Dean for the College of Engineering for eight years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">F&apos;06) has been with Nanyang Technological University</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>He is currently a Professor with the School of Electrical and Electronic Engineering and the Director of the Rapid-Rich Object Search Lab</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">He was a recipient of the Best Teacher of the Year Award and co-authored several best paper awards, including for ICPR, WIFS, and IWDW. He was awarded as the IEEE Distinguished Lecturer of the Signal Processing Society</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Magazine (SPM), IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT), and IEEE Transactions on Information Forensics and Security (T-IFS)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>IES</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
