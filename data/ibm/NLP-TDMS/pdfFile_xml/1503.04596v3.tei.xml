<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
							<email>mark.mcdonnell@unisa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Telecommunications Research</orgName>
								<orgName type="department" key="dep2">School of Information Technology and Mathematical Sciences</orgName>
								<orgName type="laboratory">Computational and Theoretical Neuroscience Laboratory</orgName>
								<orgName type="institution">University of South Australia Mawson Lakes</orgName>
								<address>
									<postCode>5095</postCode>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Vladusich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Telecommunications Research</orgName>
								<orgName type="department" key="dep2">School of Information Technology and Mathematical Sciences</orgName>
								<orgName type="laboratory">Computational and Theoretical Neuroscience Laboratory</orgName>
								<orgName type="institution">University of South Australia Mawson Lakes</orgName>
								<address>
									<postCode>5095</postCode>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhanced Image Classification With a Fast-Learning Shallow Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and very few tunable parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4% error) database is also competitive with state-of-the art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>State-of-the-art performance on many image classification databases has been achieved recently using multilayered (i.e., deep) neural networks <ref type="bibr" target="#b0">[1]</ref>. Such performance generally relies on a convolutional feature extraction stage to obtain invariance to translations, rotations and scale <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Training of deep networks, however, often requires significant resources, in terms of time, memory and computing power (e.g. in the order of hours on GPU clusters). Tasks that require online learning, or periodic replacement of all network weights based on fresh data may thus not be able to benefit from deep learning techniques. It is desirable, therefore, to seek very rapid training methods, even if this is potentially at the expense of a small performance decrease.</p><p>Recent work has shown that good performance on image classification tasks can be achieved in 'shallow' convolutional networks-neural architectures containing a single training layer-provided sufficiently many features are extracted <ref type="bibr" target="#b2">[3]</ref>. Perhaps surprisingly, such performance arises even with the use of entirely random convolutional filters or filters based on randomly selected patches from training images <ref type="bibr" target="#b3">[4]</ref>. Although application of a relatively large numbers of filters is common (followed by spatial image smoothing and downsampling), good classification performance can also be obtained with a sparse feature representation (i.e. relatively few filters and minimal downsampling) <ref type="bibr" target="#b4">[5]</ref>.</p><p>Based on these insights and the goal of devising a fast training method, we introduce a method for combining several existing general techniques into what is equivalent to a five layer neural network (see <ref type="figure">Figure 1</ref>) with only a single trained layer (the output layer), and show that the method: 1) produces state-of-the-art results on well known image classification databases; 2) is trainable in times in the order of minutes (up to several hours for large training sets) on standard desktop/laptop computers; 3) is sufficiently versatile that the same hyper-parameter sets can be applied to different datasets and still produce results comparable to dataset-specific optimisation of hyper-parameters.</p><p>The fast training method we use has been developed independently several times <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> and has gained increasing recognition in recent years-see <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> for recent reviews of the different contexts and applications. The network architecture in the classification stage is that of a three layer neural network comprised from an input layer, a hidden layer of nonlinear units, and a linear output layer. The input weights are randomly chosen and untrained, and the output weights are trained in a single batch using least squares regression. Due to the convexity of the objective function, this method ensures the output weights are optimally chosen for a given set of random input weights. The rapid speed of training is due to the fact that the least squares optimisation problem an be solved using an O(KM 2 ) algorithm, where M is the number of hidden units and K the number of training points <ref type="bibr" target="#b13">[14]</ref>.</p><p>When applied to pixel-level features, these networks can be trained as discriminative classifiers and produce excellent results on simple image databases <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> but poor performance on more difficult ones. To our knowledge, however, the method has not yet been applied to convolutional features. Therefore, we have devised a network architecture (see <ref type="figure">Figure 1</ref>) that consists of three key elements that work together to ensure fast learning and good classification performance: namely, the use of (a) convolutional feature extraction, (b) random-valued input weights for classification, (c) least squares training of output weights that feed in to (d) linear output units. We apply our network to several image classification databases, including MNIST <ref type="bibr" target="#b19">[20]</ref>, CIFAR-10 <ref type="bibr" target="#b20">[21]</ref>, Google Street View House Numbers (SVHN) <ref type="bibr" target="#b21">[22]</ref> and NORB <ref type="bibr" target="#b22">[23]</ref>. The network produces state-of-the-art classification results on MNIST and NORB-small databases and near state-of-the-art performance on SVHN.</p><p>These promising results are presented in this paper to demonstrate the potential benefits of the method; clearly further innovations within the method are required if it is to be competitive on harder datasets like CIFAR-10, or Imagenet. We expect that the most likely avenues for improving our presented results for CIFAR-10, whilst retaining the method's core attributes, are (1) to introduce limited training of the Stage 1 filters by generalizing the method of <ref type="bibr" target="#b16">[17]</ref>; (2) introduction of training data augmentation. We aim to pursuing these directions in our future work.</p><p>The remainder of the paper is organized as follows. Section II contains a generic description of the network architecture and the algorithms we use for obtaining convolutional features and classifying inputs based on them. Section III describes how the generic architecture and training algorithms are specifically applied to four well-known benchmark image classification datasets. Next, Section IV describes the results we obtained for these datasets, and finally the paper concludes with discussion and remarks in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. NETWORK ARCHITECTURE AND TRAINING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHMS</head><p>The overall network is shown in <ref type="figure">Figure 1</ref>. There are three hidden layers with nonlinear units, and four layers of weights. The first layer of weights is the convolutional filter layer. The second layer is a pooling (low pass filtering) and downsampling layer. The third layer is a random projection layer. The fourth layer is the only trained layer. The output layer has linear units.</p><p>The network can be conceptually divided into two stages and two algorithms, that to our knowledge have not previously been combined. The first stage is the convolutional feature extraction stage, and largely follows that of existing approaches to image classification <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b24">25]</ref>. The second stage is the classifier stage, and largely follows the approach of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. We now describe the two stages in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stage 1 Architecture: Convolutional filtering and pooling</head><p>The algorithm we apply to extract features from images (including those with multiple channels) is summarised in Algorithm 1. Note that the details of the filters h i,c and h p described in Algorithm 1 are given in Section III-B, but here we introduce the size of these two-dimensional filters as W ×W and Q×Q. The functions g 1 (·) and g 2 (·) are nonlinear transformations applied termwise to matrix inputs to produce matrix outputs of the same size. The symbol * represents twodimensional convolution.</p><p>This sequence of steps in Algorithm 1 suggest looping over all images and channels sequentially. However, the following mathematical formulation of the algorithm indicates a standard layered neural network formulation of this algorithm is applicable, as shown in <ref type="figure">Figure 1</ref>, and therefore that computation of all features (f k , k = 1, ..K ) can be obtained in one shot from a K-column matrix containing a batch of K training points.</p><p>The key to this formulation is to note that since convolution is a linear operator, a matrix can be constructed that when multiplied by a data matrix produces the same result as Input : Set of K images, x k , each with C channels Output: Feature vectors, f k , k = 1, . . . K foreach x k do split x k into channels, x k,c , c = 1, . . . C foreach i = 1, . . . P filters do Apply filter to each channel: y i,k,c ← h i,c * x k,c Apply termwise nonlinearity: z i,k,c ← g 1 (y i,k,c ) Apply lowpass filter:</p><formula xml:id="formula_0">w i,k,c ← h p * z i,k,c Apply termwise nonlinearity: s i,k,c ← g 2 (w i,k,c ) Downsample:ŝ i,k,c ← s i,k,c Concatenate channels: r i,k ← [ŝ i,k,1 | . . . |ŝ i,k,C ] Normalize: f i,k = r i,k /(r i,k 1 ) end Concatenate over filters: f k ← [f 1,k |f 2,k | . . . |f P,k ] end</formula><p>Algorithm 1: Convolutional feature detection.</p><p>convolution applied to one instance of the data. Hence, for a total of L features per image, we introduce the following matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><p>• F be a feature matrix of size L × K;</p><p>• X be a data matrix with K columns;</p><p>• W Filter be a concatenation of the CP convolution matrices corresponding to h i,c i = 1, . . . P, c = 1, . . . C;</p><p>• W 0 be a convolution matrix corresponding to h l , that also down samples by a factor of D;</p><p>• W Pool be a block diagonal matrix containing CP copies of W 0 on the diagonals.</p><p>The entire flow described in Algorithm 1 can be written mathematically as</p><formula xml:id="formula_1">F = g 2 (W Pool g 1 (W Filter X)),<label>(1)</label></formula><p>where g 1 (·) and g 2 (·) are applied term by term to all elements of their arguments. The matrices W Filter and W Pool are sparse Toeplitz matrices. In practice we would not form them directly, but instead form one pooling matrix, and one filtering matrix for each filter, and sequential apply each filter to the entire data matrix, X.</p><p>We use a particular form for the nonlinear hidden-unit functions g 1 (·) and g 2 (·) inspired by LP-pooling <ref type="bibr" target="#b24">[25]</ref>, which is of the form g 1 (u) = u p and g 2 (v) = v 1 p . For example, with p = 2 we have</p><formula xml:id="formula_2">F = W Pool (W Filter X) 2 .<label>(2)</label></formula><p>An intuitive explanation for the use of LP-pooling is as follows. First, note that each hidden unit receives as input a linear combination of a patch of the input data, i.e. u in g 1 (u) has the form u = W 2 j=1 h i,c,j x i,j . Hence, squaring u results in a sum that contains terms proportional to x 2 i,j and terms proportional to products of each x i,j . Thus, squaring is a simple way to produce hidden layer responses that depend on the product of pairs of input data elements, i.e. interaction terms, and this is important for discriminability. Second, the</p><formula xml:id="formula_3">Hidden Layer 1 H = g1(·) X WFilter WPool F = g2(·)</formula><p>Hidden Layer 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Hidden Layer 3 Linear Output Layer</p><formula xml:id="formula_4">Win A = g(·) Wout Y = WoutA</formula><p>Stage 1: Convolutional Filtering and Pooling Stage 2: Classification Output <ref type="figure">Fig. 1</ref>. Overall network architecture. In total there are three hidden layers, plus an input layer and a linear output layer. There are two main stages: a convolutional filtering and pooling stage, and a classification stage. Only the final layer of weights, Wout is learnt, and this is achieved in a single batch using least squares regression. Of the remaining weights matrices, W Filter is specified and remains fixed, e.g. taken from Overfeat <ref type="bibr" target="#b23">[24]</ref>; W Pool describes standard average pooling and downsampling; and W in is set randomly or by using the method of <ref type="bibr" target="#b18">[19]</ref> that specifies the weights by sampling examples of the training distribution, as described in the text. Other variables shown are as follows: J 2 is the number of pixels in an image, L is the number of features extracted per image, D is a downsampling factor, M is the number of hidden units in the classifier stage and N is the number of classes. square root transforms the distribution of the hidden-unit responses; we have observed that in practice, the result of the square root operation is often a distribution that is closer to Gaussian than without it, which helps to regularise the least squares regression method of training the output weights.</p><formula xml:id="formula_5">argmax(·) Classification M units (M × L) (N × M ) N units L features D 2 L features (L × D 2 L) (D 2 L × J 2 )</formula><p>However, as will be described shortly, the classifier of Stage 2 also has a square nonlinearity. Using this nonlinearity, we have found that classification performance is generally optimised by taking the square root of the input to the random projection layer. Based on this observation, we do not strictly use LP-pooling, and instead set</p><formula xml:id="formula_6">g 1 (u) = u 2 ,<label>(3)</label></formula><p>and</p><formula xml:id="formula_7">g 2 (v) = v 0.25 .<label>(4)</label></formula><p>This effectively combines the implementation of L2-pooling, and the subsequent square root operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stage 2 Architecture: Classifier</head><p>The following descriptions are applicable whether or not raw pixels are treated as features or the input is the features extracted in stage 1. First, we introduce notation. Let:</p><p>• F train , of size L × K, contain each length L feature vector;</p><p>• Y label be an indicator matrix of size N × K, which numerically represents the labels of each training vector, where there are N classes-we set each column to have a 1 in a single row, corresponding to the label class for each training vector, and all other entries to be zero;</p><p>• W in , of size M × L be the real-valued input weights matrix for the classifier stage;</p><p>• W out , of size N × M be the real-valued output weights matrix for the classifier stage;</p><p>• the function g(·) be the activation function of each hidden-unit; for example, g(·) may be the logistic sigmoid, g(z) = 1/(1 + exp(−z)), or a squarer, g(z) = z 2 ;</p><p>• A train = g(W in F train ), of size M × K, contain the hidden-unit activations that occur due to each feature vector; g(·) is applied termwise to each element in the matrix W in F train .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stage 1 Training: Filters and Pooling</head><p>In this paper we do not employ any form of training for the filters and pooling matrices. The details of the filter weights and form of pooling used for the example classification problems presented in this paper are given Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stage 2 Training: Classifier Weights</head><p>The training approach for the classifier is that described by e.g. <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b12">13]</ref>. The default situation for these methods is that the input weights, W in , are generated randomly from a specific distribution, e.g. standard Gaussian, uniform, or bipolar. However, it is known that setting these weights non-randomly based on the training data leads to superior performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. In this paper, we use the method of <ref type="bibr" target="#b18">[19]</ref>. The input weights can also be trained iteratively, if desired, using singlebatch backpropagation <ref type="bibr" target="#b16">[17]</ref>.</p><p>Given a choice of W in , the output weights matrix is determined according to</p><formula xml:id="formula_8">W out = Y label A + train ,<label>(5)</label></formula><p>where A + train is the size K ×M Moore-Penrose pseudo inverse corresponding to A train . This solution is equivalent to least squares regression applied to an overcomplete set of linear equations, with an N -dimensional target. It is known to often be useful to regularise such problems, and instead solve the following ridge regression problem <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>:</p><formula xml:id="formula_9">W out = Y label A train (A train A train + cI) −1 ,<label>(6)</label></formula><p>where c is a hyper-parameter and I is the M × M identity matrix. In practice, it is efficient to avoid explicit calculation of the inverse in Equation <ref type="formula" target="#formula_9">(6)</ref>  <ref type="bibr" target="#b13">[14]</ref> and instead use QR factorisation to solve the following set of N M linear equations for the N M unknown variables in W out :</p><formula xml:id="formula_10">Y label A train = W out (A train A train + cI).<label>(7)</label></formula><p>Above we mentioned two algorithms, and Algorithm 2 is simply to form A train and solve Eqn. <ref type="bibr" target="#b6">(7)</ref>, followed by optimisation of c using ridge regression. For large M and K &gt; M (which is typically valid) the runtime bottleneck for this method is typically the O(KM 2 ) matrix multiplication required to obtain the Gram matrix, A train A train .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Application to Test Data</head><p>For a total of K test test images contained in a matrix X test , we first obtain a matrix F test = g 2 (W Pool g 1 (W Filter X test )), of size L × K test , by following Algorithm 1. The output of the classifier is then the N × K test matrix</p><formula xml:id="formula_11">Y test = W out g(W in F test ) (8) = W out g(W in g 2 (W Pool g 1 (W Filter X))).<label>(9)</label></formula><p>Note that we can write the response to all test images in terms of the training data:</p><formula xml:id="formula_12">Y test = Y label (g(W in F train )) + g(W in F test ) (10) where F train = g 2 (W Pool g 1 (W Filter X train )) (11) F test = g 2 (W Pool g 1 (W Filter X test )).<label>(12)</label></formula><p>Thus, since the pseudo-inverse, (·) + , can be obtained from Equation <ref type="formula" target="#formula_9">(6)</ref>, Equations (10), <ref type="bibr" target="#b10">(11)</ref> and <ref type="formula" target="#formula_1">(12)</ref> constitute a closedform solution for the entire test-data classification output, given specified matrices, W filter , W pool and W in , and hidden-unit activation functions, g 1 , g 2 , and g.</p><p>The final classification decision for each image is obtained by taking the index of the maximum value of each column of Y test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IMAGE CLASSIFICATION EXPERIMENTS: SPECIFIC DESIGN</head><p>We examined the method's performance when used as a classifier of images. <ref type="table" target="#tab_0">Table I</ref> lists the attributes of four well known databases we used. For the two databases comprised from RGB images, we used C = 4 channels, namely the raw RGB channels, and a conversion to greyscale. This approach was shown to be effective for SVHN in <ref type="bibr" target="#b25">[26]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>All raw image pixel values were scaled to the interval [0, 1]. Due to the use of quadratic nonlinearities and LPpooling, this scaling does not affect performance. The only other preprocessing done was as follows: 1) MNIST: None;</p><p>2) NORB-small: downsample from 96×96 to 32×32, for implementation efficiency reasons (this is consistent with some previous work on NORB-small, e.g. <ref type="bibr" target="#b4">[5]</ref>); 3) SVHN: convert from 3 channels to 4 by adding a conversion to greyscale from the raw RGB. We found that local and/or global contrast enhancement only diminished performance; 4) CIFAR-10: convert from 3 channels to 4 by adding a conversion to greyscale from the raw RGB; apply ZCA whitening to each channel of each image, as in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stage 1 Design: Filters and Pooling</head><p>Since our objective here was to train only a single layer of the network, we did not seek to train the network to find filters optimised for the training set. Instead, for the size W ×W twodimension filters, h i,c , we considered the following options: 1) simple rotated bar and corner filters, and square uniform centre-surround filters; 2) filters trained on Imagenet and made available in Overfeat <ref type="bibr" target="#b23">[24]</ref>; we used only the 96 stage-1 'accurate' 7×7 filters; 3) patches obtained from the central W × W region of randomly selected training images, with P/N training images from each class.</p><p>The filters from Overfeat 1 are RGB filters. Hence, for the databases with RGB images, we applied each channel of the filter to the corresponding channel of each image. When applied to greyscale channels, we converted the Overfeat filter to greyscale. For NORB, we applied the same filter to both stereo channels. For all filters, we subtract the mean value over all W 2 dimensions in each channel, in order to ensure a mean of zero in each channel.</p><p>In implementing the two-dimensional convolution operation required for filtering the raw images using h i,c , we obtained only the central 'valid' region, i.e. for images of size J × J, the total dimension of the valid region is (J − W + 1) 2 . Consequently, the total number of features per image obtained prior to pooling, from P filters, and images with C channels is L = CP (J − W + 1) 2 .</p><p>In previous work, e.g. <ref type="bibr" target="#b24">[25]</ref>, the form of the Q × Q twodimension filter, h p is a normalised Gaussian. Instead, we used a simple summing filter, equivalent to a kernel with all entries equal to the same value, i.e.</p><formula xml:id="formula_13">h p,u,v = 1 Q 2 , u = 1, . . . Q, v = 1, . . . Q.<label>(13)</label></formula><p>In implementing the two-dimensional convolution operation required for filtering using h p , we obtained the 'full' convolutional region, which for images of size J ×J is (J −W +Q) 2 , given the 'valid' convolution first applied using h i,c , as described above.</p><p>The remaining part of the pooling step is to downsample each image dimension by a factor of D, resulting in a total ofL = L/D 2 features per image. In choosing D, we experimented with a variety of scales before settling on the value shown in <ref type="table" target="#tab_0">Table II</ref>. We note there exists an interesting tradeoff between the number of filters P , and the downsampling factor, D. For example, in <ref type="bibr" target="#b2">[3]</ref>, D = L/2, whereas in [5] D = 1. We found that, up to a point, smaller D enables a smaller number of filters, P , for comparable performance.</p><p>The hyper-parameters we used for each dataset are shown in <ref type="table" target="#tab_0">Table II</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stage 2 Design: Classifier projection weights</head><p>To construct the matrix W in we use the method proposed by <ref type="bibr" target="#b17">[18]</ref>. In this method, each row of the matrix W in is chosen to be a normalized difference between the data vectors corresponding to randomly chosen examples from distinct classes of the training set. This method has previously been shown to be superior to setting the weights to values chosen from random distributions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>For the nonlinearity in the classifier stage hidden units, g(z), the typical choice in other work <ref type="bibr" target="#b12">[13]</ref> is a sigmoid. However, we found it sufficient (and much faster in an implementation) to use the quadratic nonlinearity. This suggests that good image classification is strongly dependent on the presence of interaction terms-see the discussion about this in Section II-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stage 2 Design: Ridge Regression parameter</head><p>With these choices, there remains only two hyperparameters for the Classifier stage: the regression parameter, c, and the number of hidden-units, M . In our experiments, we examined classification error rates as a function of varying M . For each M , we can optimize c using cross-validation. However, we also found that a good generic heuristic for setting c was</p><formula xml:id="formula_14">c = N 2 M 2 min(diag(A train A train )),<label>(14)</label></formula><p>and this reduces the number of hyper-parameters for the classification stage to just one: the number of hidden-units, M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Stage 1 and 2 Design: Nonlinearities</head><p>For the hidden-layer nonlinearities, to reiterate, we use:</p><formula xml:id="formula_15">g 1 (u) = u 2 , g 2 (v) = v 0.25 , g(z) = z 2 .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>We examined the performance of the network on classifying the test images in the four chosen databases, as a function of the number of filters, P , the downsampling rate D, and the number of hidden units in the classifier stage, M . We use the maximum number of channels, C, available in each dataset (recall from above that we convert RGB images to greyscale, as a fourth channel).</p><p>We considered the three kinds of untuned filters described in Section III-B, as well as combinations of them. We did not exhaustively consider all options, but settled on the Overfeat filters as being marginally superior for NORB, SVHN and CIFAR-10 (in the order of 1% in comparison with other options), while hand-designed filters were superior for MNIST, but only marginally compared to randomly selected patches from the training data. There is clearly more that can be investigated to determine whether hand-designed filters can match trained filters when using the method of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Summary of best performance attained</head><p>The best performance we achieved is summarised in Table III. Results for various databases. The state-of-the-art result listed for MNIST and CIFAR-10 can be improved by augmenting the training set with distortions and other methods <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>; we have not done so here, and report state-of-the-art only for methods not doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Trend with increasing M</head><p>We now use MNIST as an example to indicate how classification performance scales with the number of hidden units in the classifier stage, M . The remain parameters were W = 7, D = 3 and P = 43, which included hand-designed filters comprised from 20 rotated bars (width of one pixel), 20 rotated corners (dimension 4 pixels) and 3 centred squares (dimensions 3, 4 and 5 pixels), all with zero mean. The rotations were of binary filters and used standard pixel value interpolation. <ref type="figure" target="#fig_0">Figure 2</ref> shows a power law-like decrease in error rate as M increases, with a linear trend on the log-log axes. The best error rate shown on this figure is 0.40%. As shown in <ref type="table" target="#tab_0">Table III</ref>, we have attained a best repeatable rate of 0.37% using 60 filters and D = 2. When we combined Overfeat filters with hand-designed filters and randomly selected patches from the training data, we obtained up to 0.32% error on MNIST, but this was an outlier since it was not repeatedly obtained by different samples of W in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Indicative training times</head><p>For an implementation in Matlab on a PC with 4 cores and 32 GB of RAM, for MNIST (60000 training points) the total time required to generate all features for all 60000 training images from one filter is approximately 2 seconds. The largest number of filters we used to date was 384 (96 RGB+greyscale), and when applied to SVHN (∼600000 training points), the total run time for feature extraction is then about two hours (in this case we used batches of size 100000 images).</p><p>The runtime we achieve for feature generation benefits from carrying out convolutions using matrix multiplication applied to large batches simultaneously; if instead we iterate over all training images individually, but still carry out convolutions using matrix multiplication, the time for generating features approximately doubles. Note also that we employ Matlab's sparse matrix data structure functionality to represent W Filter and W Pool , which also provides a speed boost when multiplying these matrices to carry out the convolutions. If we do not use the matrix-multiplication method for convolution, and instead apply two-dimensional convolutions to each individual image, the feature generation is slowed even more.</p><p>For the classifier stage, on MNIST with M = 6400, the runtime is approximately 150 seconds for D = 3 (there is a small time penalty for smaller D, due to the larger dimension of the input to the classifier stage). Hence, the total run time for MNIST with 40 filters and M = 6400 is in the order of 4 minutes to achieve a correct classification rate above 99.5%. With fewer filters and smaller M , it is simple to achieve over 99.2% in a minute or less.</p><p>For SVHN and CIFAR-10 where we scaled up to M = 40000, the run time bottleneck is the classifier, due to the O(KM 2 ) runtime complexity. We found it necessary to use a PC with more RAM (peak usage was approximately 70 GB) for M &gt; 20000. In the case of M = 40000, the network was trained in under an hour on CIFAR-10, while SVHN took about 8-9 hours. Results within a few percent of our best, however, can be obtained in far less time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND CONCLUSIONS</head><p>As stated in the introduction, the purpose of this paper is to highlight the potential benefits of the method presented, namely that it can attain excellent results with a rapid training speed and low implementation complexity, whilst only suffering from reduced performance relative to state-of-the-art on particularly hard problems.</p><p>In terms of efficacy on classification tasks, as shown in <ref type="table" target="#tab_0">Table III</ref>, our best result (0.37% error rate) surpasses the best ever reported performance for classification of the MNIST test set when no augmentation of the training set is done. We have also achieved, to our knowledge, the best performance reported in the literature for the NORB-small database, surpassing the previous best <ref type="bibr" target="#b28">[29]</ref> by about 0.3%.</p><p>For SVHN, our best result is within ∼ 2% of state-of-theart. It is highly likely that using filters trained on the SVHN database rather than on Imagenet would reduce this gap, given the structured nature of digits, as opposed to the more complex nature of Imagenet images. Another avenue for closing the gap on state-of-the-art using the same filters would be to increase M and decrease D, thus resulting in more features and more classifier hidden units. Although we increased M to 40000, we did not observe saturation in the error rate as we increased M to this point.</p><p>For CIFAR-10, it is less clear what is lacking in our method in comparison with the gap of about 14% to state-of-the-art methods. We note that CIFAR-10 has relatively few training points, and we observed that the gap between classification performance on the actual training set, in comparison with the test set, can be up to 20%. This suggests that designing enhanced methods of regularisation (e.g. methods similar to dropout in the convolutional stage, or data augmentation) are necessary to ensure our method can achieve good performance on CIFAR-10. Another possibility is to use a nonlinearity in the classifier stage that ensures the hidden-layer responses reflect higher order correlations than possible from the squaring function we used. However, we expect that training the convolutional filters in Stage 1 so that they extract features that are more discriminative for the specific dataset will be the most likely enhancement for improving results on CIFAR-10.</p><p>Finally, we note that there exist iterative approaches for training the classifier component of Stage 2 using least squares regression, and without training the input weightssee, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. These methods can be easily adapted for use with the convolutional front-end, if, for example, additional batches of training data become available, or if the problem involves online learning.</p><p>In closing, following acceptance of this paper, we became aware of a newly published paper that combines convolutional feature extraction with least squares regression training of classifier weights to obtain good results for the NORB dataset <ref type="bibr" target="#b34">[35]</ref>. The three main differences between the method of the current paper and the method of <ref type="bibr" target="#b34">[35]</ref> are as follows. First, we used a hidden layer in our classifier stage, whereas <ref type="bibr" target="#b34">[35]</ref> solves for output weights using least squares regression applied to the output of the pooling stage. Second, we used a variety of methods for the convolutional filter weights, whereas <ref type="bibr" target="#b34">[35]</ref> uses orthogonalised random weights only. Third, we downsample following pooling, whereas <ref type="bibr" target="#b34">[35]</ref> does not do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Mark D. McDonnell's contribution was by supported by an Australian Research Fellowship from the Australian Research Council (project number DP1093425). We gratefully acknowledge Prof David Kearney and Dr Victor Stamatescu from University of South Australia and Dr Sebastien Wong of DSTO, Australia, for useful discussions and provision of computing resources. We also acknowledge discussions with Prof Philip De Chazal of University of Sydney.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Example set of error percentage value on the 10000 MNIST test images, for ten repetitions of the selection W in . The best result shown is 40 errors out of 10000. Increasing M above 6400 saturates in performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>Image Databases. Note that the NORB-small database consists of images of size 96 × 96 pixels, but we first downsampled all training and test images to 32 × 32 pixels, as in<ref type="bibr" target="#b4">[5]</ref>.</figDesc><table><row><cell>Database</cell><cell cols="2">Classes Training</cell><cell>Test</cell><cell>Channels</cell><cell>Pixels</cell></row><row><cell>MNIST [20]</cell><cell>10</cell><cell>60000</cell><cell>10000</cell><cell>1</cell><cell>28×28</cell></row><row><cell>NORB-small [23]</cell><cell>5</cell><cell>24300</cell><cell cols="3">24300 2 (stereo) 32×32</cell></row><row><cell>SVHN [22]</cell><cell>10</cell><cell>604308</cell><cell>26032</cell><cell>3 (RGB)</cell><cell>32×32</cell></row><row><cell>CIFAR-10 [21]</cell><cell>10</cell><cell>50000</cell><cell>10000</cell><cell>3 (RGB)</cell><cell>32×32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell cols="2">Hyper-parameter</cell><cell cols="4">MNIST NORB SVHN CIFAR-10</cell></row><row><cell cols="2">Filter size, W</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell></row><row><cell cols="2">Pooling size, Q</cell><cell>8</cell><cell>10</cell><cell>7</cell><cell>7</cell></row><row><cell cols="2">Downsample factor, D</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>3</cell></row><row><cell>TABLE II.</cell><cell cols="5">Stage 1 Hyper-parameters (Convolutional Feature</cell></row><row><cell></cell><cell></cell><cell cols="2">Extraction).</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available from http://cilvr.nyu.edu/doku.php?id=software:overfeat:start</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>JMLR:W&amp;CP 15</idno>
	</analytic>
	<monogr>
		<title level="m">Proc.14th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>.14th International Conference on Artificial Intelligence and Statistics (AISTATS)<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)<address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1279" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feed forward neural networks with random weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kraaijveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. B: Pattern Recognition Methodology and Systems</title>
		<meeting><address><addrLine>The Hague; Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1992-09-03" />
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
	<note>Proc. 11th IAPR Int. Conf. on Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A rapid supervised learning neural network for function interpolation and approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1220" to="1230" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extreme learning machine: A new learning scheme of feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Neural Networks (IJCNN&apos;2004)</title>
		<meeting>International Joint Conference on Neural Networks (IJCNN&apos;2004)<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A large-scale model of the functioning brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bekolay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dewolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="page" from="1202" to="1205" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale synthesis of functional spiking neural circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="881" to="898" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extreme learning machine for regression and multiclass classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="513" to="529" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An insight into extreme learning machines: Random neurons, random features and kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="376" to="390" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the &apos;extreme learning machine&apos; algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Tissera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vladusich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="134254" to="134255" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online and adaptive pseudoinverse solutions for ELM weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="233" to="238" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explicit computation of input weights in extreme learning machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Chazal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2889</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ELM2014 conference</title>
		<meeting>ELM2014 conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient and effective algorithms for training single-hidden-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="554" to="558" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constrained extreme learning machine: a novel highly discriminative random feedforward neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">p. XXX</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Constrained extreme learning machines: A study on classification cases</title>
		<idno type="arXiv">arXiv:1501.06115</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Dept of CS, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://ufldl.stanford.edu/housenumbers" />
	</analytic>
	<monogr>
		<title level="m">2011, nIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>CBLS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural networks applied to house numbers digit classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A deep learning pipeline for image understanding and acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, New York University.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2627" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Representation Learning Workshop, NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003)</title>
		<meeting>the Seventh International Conference on Document Analysis and Recognition (ICDAR 2003)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning the pseudoinverse solution to network weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The No-Prop algorithm: A new learning algorithm for multilayer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenblatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="182" to="188" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Local receptive fields based extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L C</forename><surname>Kasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Vong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
