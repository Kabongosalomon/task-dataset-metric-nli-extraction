<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-05-19">19 May 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<email>07wanglimin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen key lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-05-19">19 May 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deepconvolutional descriptor (TDD), which shares the merits of both hand-crafted features <ref type="bibr" target="#b30">[31]</ref> and deep-learned features <ref type="bibr" target="#b23">[24]</ref>. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectoryconstrained sampling and pooling for aggregating deeplearned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features <ref type="bibr" target="#b30">[31]</ref> and deep-learned features <ref type="bibr" target="#b23">[24]</ref>. Our method also achieves superior performance to the state of the art on these datasets 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> in videos attracts increasing research interests in computer vision community due to its potential applications in video surveillance, human computer interaction, and video content analysis. However, action recognition remains as a difficult problem when focusing on realistic datasets collected from movies <ref type="bibr" target="#b16">[17]</ref>, web videos <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26]</ref>, and TV shows <ref type="bibr" target="#b19">[20]</ref>. There are large intra-class variations in the same <ref type="bibr">Figure 1</ref>. There are mainly two types of features in action recognition: hand-crafted features and deep-learned features. For hand-crafted features, improved trajectories <ref type="bibr" target="#b30">[31]</ref> combined with Fisher vector are most successful. For deep-learned features, Convolutional Networks (ConvNets) <ref type="bibr" target="#b17">[18]</ref> are popular deep architectures, which contain a sequence of convolutional and pooling layers. They aims to automatically learn features with a deep discriminatively trained neural network. action class, which may be caused by background clutter, viewpoint change, and various motion speeds and styles. Meanwhile, the high dimension and low resolution of video further increases the difficulty to design efficient and robust recognition method. Visual representations from action videos are crucial for dealing with these issues and designing effective recognition systems. Currently, there are mainly two types of video features available for action recognition, as illustrated in <ref type="figure">Figure 1</ref>.</p><p>The first type of representations are the hand-crafted local features, and typical local features include Space Time Interest Points <ref type="bibr" target="#b15">[16]</ref>, Cuboids <ref type="bibr" target="#b6">[7]</ref>, Dense Trajectories <ref type="bibr" target="#b29">[30]</ref>, and Improved Trajectories <ref type="bibr" target="#b30">[31]</ref>. Calculation of these local features can be usually decomposed into two phrases: detector, which aims to discover the salient and informative regions for action understanding, and descriptor, whose goal is to describe the visual patterns of extracted regions. Among these local features, improved trajectories with rich descriptors of HOG, HOF, MBH have shown to be successful on a number of challenging datasets (e.g. HMDB51 <ref type="bibr" target="#b14">[15]</ref>, UCF101 <ref type="bibr" target="#b25">[26]</ref>) and contests (e.g. THUMOS <ref type="bibr" target="#b10">[11]</ref>). Improved trajectories include several important ingredients in their extraction process. Firstly, these extracted trajectories are mainly located at regions with high motion salience, which contain rich and discriminative information for action recognition. Secondly, these local descriptors of the corresponding regions in several successive frames, are aligned and pooled along the trajectories. This trajectoryconstrained sampling strategy also takes account of the temporal continuity of human action, and is effective to deal with the variations of motion speed. However, these hand-crafted descriptors are not optimized for visual representation and may lack discriminative capacity for action recognition.</p><p>The second type of representations are the deep-learned features, and typical methods include Convolutional RBMs <ref type="bibr" target="#b28">[29]</ref>, 3D ConvNets <ref type="bibr" target="#b8">[9]</ref>, Deep ConvNets <ref type="bibr" target="#b11">[12]</ref>, and Two-Stream ConvNets <ref type="bibr" target="#b23">[24]</ref>. These deep learning methods aim to automatically learn the semantic representation from raw video by using a deep neural network discriminatively trained from a large number of labeled data. Two-Stream ConvNets <ref type="bibr" target="#b23">[24]</ref> are probably the most successful architecture at present, and they match the state-of-the-art performance of improved trajectories <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> on UCF101 and HMDB51. They are composed of two neural networks, namely spatial nets and temporal nets. Spatial nets mainly capture the discriminative appearance features for action understanding, while temporal nets aim to learn the effective motion features. However, unlike image classification tasks <ref type="bibr" target="#b13">[14]</ref>, these deep learning based methods fail to outperform previous hand-crafted features. One problem of deep learning methods is that they require a large number of labeled videos for training, while most available datasets are relatively small. Meanwhile, most of current deep learning based action recognition methods largely ignore the intrinsic difference between temporal domain and spatial domain, and just treat temporal dimension as feature channels when adapting the architectures of ConvNets to model videos.</p><p>Motivated by the above analysis, this paper proposes a new kind of video feature, called trajectory-pooled deepconvolutional descriptor (TDD). The design of TDD aims to combine the benefits of both hand-crafted and deeplearned features. To achieve this goal, our approach integrates the key factors from two successful video representations, namely improved trajectories <ref type="bibr" target="#b30">[31]</ref> and twostream ConvNets <ref type="bibr" target="#b23">[24]</ref>. We utilize deep architecture to learn multi-scale convolutional feature maps, and introduce the strategies of trajectory-constrained sampling and pooling to encode deep features into effective descriptors.</p><p>Specifically, we first train two-stream ConvNets on a relatively large dataset, while more labeled action videos will make ConvNet training more stable and robust. Then, we treat the learned two-stream ConvNets as generic feature extractors, and use them to obtain multi-scale convolutional feature maps for each video. Meanwhile, we detect a set of point trajectories with the method of improved trajectories. Based on convolutional feature maps and improved trajectories, we pool the local ConvNet responses over the spatiotemporal tubes centered at the trajectories, where the resulting descriptor is called TDD. Finally, we choose Fisher vector representation to aggregate these local TDDs over the whole video into a global super vector, and use linear SVM as the classifier to perform action recognition. We conduct experiments on two public action datasets: the HMDB51 dataset <ref type="bibr" target="#b14">[15]</ref> and the UCF101 dataset <ref type="bibr" target="#b25">[26]</ref>. We show that our TDDs obtain the state-of-the-art performance for action recognition on these challenging datasets. Meanwhile, our results demonstrate that our TDDs are complementary to those hand-crafted features (HOG, HOF, and MBH) and the fusion of them is able to further boost the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Hand-crafted features. Local features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref> have become popular and effective representations in action recognition, as these local features do not require algorithms to detect human body and are robust to background clutter, illumination changes, and video noise. Space Time Interest Points <ref type="bibr" target="#b15">[16]</ref> proposed Harris3D detector to extract informative regions, while Cuboid <ref type="bibr" target="#b6">[7]</ref> detector relied on temporal Gabor filters. Willems et al. <ref type="bibr" target="#b38">[39]</ref> proposed a Hessian detector, which is a spatio-temporal extension of Hessian saliency measure used for blob detection in images.</p><p>Meanwhile several local descriptors have been proposed to represent the 3D volumes extracted around these interest points, such as Histogram of Gradient (HOG), Histogram of Optical Flow (HOF) <ref type="bibr" target="#b16">[17]</ref>, 3D Histogram of Gradient (HOG3D) <ref type="bibr" target="#b12">[13]</ref>, and Extended SURF (ESURF) <ref type="bibr" target="#b38">[39]</ref>. Recent works made use of point trajectories <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> to extract and align 3D volumes, and resorted to more rich low level descriptors for constructing effective video representations, including HOG, HOF, and Motion Boundary Histogram (MBH).</p><p>One limitation of these local features is that they lack semantics and discriminative capacity. To overcome this issue, several mid-level and high-level video representations have been proposed such as Action Bank <ref type="bibr" target="#b21">[22]</ref>, Dynamic-Poselets <ref type="bibr" target="#b36">[37]</ref>, Motionlets <ref type="bibr" target="#b34">[35]</ref>, Motion Atoms and Phrases <ref type="bibr" target="#b33">[34]</ref>, and Actons <ref type="bibr" target="#b41">[42]</ref>. They usually resorted to some heuristic mining methods to select discriminative visual elements as feature units. Instead, this paper takes a different view of this problem and replace these local hand-crafted descriptors with deep-learned representations. Our deep representations deliver high level semantic information, and are learned automatically from training data without using these heuristic rules. Deep-learned features. Deep learning techniques have achieved great success in image based tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref> and there have been a number of attempts to develop deep architectures for video action recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>. Taylor et al. <ref type="bibr" target="#b28">[29]</ref> used Gated Restricted Boltzmann Machines (GRBMs) to learn the motion features in an unsupervised manner and then resorted to convolutional learning to fine tune the parameters. Ji et al. <ref type="bibr" target="#b8">[9]</ref> extended 2D ConvNet to video domain for action recognition on relatively small datasets, and recently Karpathy et al. <ref type="bibr" target="#b11">[12]</ref> tested ConvNets with deep structures on a large dataset, called Sports-1M. However, these deep models achieved lower performance compared with shallow hand-crafted representation <ref type="bibr" target="#b30">[31]</ref>, which might be ascribed to two facts: firstly, available action datasets are relatively small for deep learning; secondly, learning complex motion patterns is more challenging. Simonyan et al. <ref type="bibr" target="#b23">[24]</ref> designed two-stream ConvNets containing spatial and temporal net by exploiting large ImageNet dataset for pre-training and explicitly calculating optical flow for capturing motion information, and finally it matched the state-of-the-art performance.</p><p>However, these deep models lacked considerations of temporal characteristics of video data and relied on large training datasets. We incorporate video temporal characteristics into deep architectures by using strategy of trajectory-constrained sampling and pooling, and propose a new descriptor. Meanwhile, our descriptors can be easily adapted to the datasets of smaller size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Improved Trajectories Revisited</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our proposed representation (TDD) is based on low level trajectory extraction and we choose improved trajectories <ref type="bibr" target="#b30">[31]</ref>. In this section, we briefly review the extraction process of improved trajectories. It is worth noting that our TDD is independent of the method of extracting trajectories, and we use improved trajectories due to its good performance.</p><p>Improved trajectories are extended from dense trajectories <ref type="bibr" target="#b29">[30]</ref>. To compute dense trajectories, the first step is to densely sample a set of points on 8 spatial scales on a grid with step size of 5 pixels. Points in homogeneous areas are eliminated by setting a threshold for the smaller eigenvalue of their autocorrelation matrices. Then these sampled points are tracked by media filtering of dense flow field.</p><formula xml:id="formula_0">P t+1 = (x t+1 , y t+1 ) = (x t , y t ) + (M * ω t )| (xt,y t ) , (1)</formula><p>where M is the median filter kernel, * is convolutional operation, ω t = (u t , v t ) is the dense optical flow field of the t th frame, and (x t , y t ) is the rounded position of (x t , y t ).</p><p>To avoid the drifting problem of tracking, the maximum length of trajectory is set as 15-frame. Finally, those static trajectories are removed as they lack motion information, and other trajectories with suddenly large displacement are also ignored, since they are obviously incorrect due to inaccurate optical flow.</p><p>Improved trajectories boost the recognition performance of dense trajectories by taking camera motion into account. It assumes that the background motion of two consecutive frames can be characterized by a homography matrix. To estimate the homography matrix, the first step is to find the correspondence between two consecutive frames. They resort to SURF <ref type="bibr" target="#b1">[2]</ref> feature matching and optical flow based matching, as these two kinds of matching scheme are complementary to each other. Then, they use the RANSAC <ref type="bibr" target="#b7">[8]</ref> algorithm to estimate homography matrix. Based on the homography, they rectify the frame image to remove the camera motion and re-calculate the optical flow, called warped flow. Warped flow brings advantages to the descriptors calculated from optical flows, in particular for HOF, and trajectories corresponding to camera motion can be removed too.</p><p>We adopt improved trajectories for the task of TDD extraction, but make a modification. Unlike dense trajectories or improved trajectories, we only track points on its original spatial scale, and extract multi-scale TDDs around the extracted trajectories (see <ref type="bibr">Section 4)</ref>. We observe that tracking on a single scale is fast for implementation. In summary, given a video V , we obtain a set of trajectories</p><formula xml:id="formula_1">T(V ) = {T 1 , T 2 , · · · , T K },<label>(2)</label></formula><p>where K is the number of trajectories, and T k denotes the k th trajectory in the original spatial scale:</p><formula xml:id="formula_2">T k = {(x k 1 , y k 1 , z k 1 ), (x k 2 , y k 2 , z k 2 ), · · · , (x k P , y k P , z k P )},<label>(3)</label></formula><p>where (x k p , y k p , z k p ) is the pixel position of the p th point in trajectory T k , and P is the length of trajectory (P = 15). These trajectories will be used for trajectory-constrained sampling and pooling in the process of TDD extraction, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Convolutional Descriptors</head><p>In this section, we describe a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the benefits of both hand-crafted and deep-learned features. We first introduce the architectures of convolutional networks (ConvNets) we used. Then, we show how to adapt the ConvNets trained on large datasets to extract multi-scale convolutional feature maps. Finally, based on improved trajectories and convolutional feature maps, we describe the details of how to calculate TDDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Convolutional networks</head><p>Our TDD starts with designing deep ConvNets for extracting convolutional feature maps. In principle, any kind of ConvNet architecture can be adopted for TDD extraction. In our implementation, we choose the twostream ConvNets <ref type="bibr" target="#b23">[24]</ref> due to their good performance on the datasets of UCF101 and HMDB51.</p><p>The two-stream ConvNets contain two separate Con-vNets, namely spatial nets and temporal nets. Spatial nets are designed for capturing static appearance cues, which are trained on single frame images (224 × 224 × 3), while temporal nets aim to describe the dynamic motion information, whose input are volumes of stacking optical flow fields (224 × 224 × 2F , F is the number of stacking flows). Meanwhile, decoupling the spatial and temporal nets also allows to exploit the available images by pretraining spatial nets on the ImageNet challenge dataset <ref type="bibr" target="#b5">[6]</ref>, and explicitly handle motion information with optical flow algorithms for temporal nets</p><p>The details about ConvNets are shown in <ref type="table">Table 1</ref>. This ConvNet architecture is original from the Clarifai networks <ref type="bibr" target="#b40">[41]</ref> and adapted to the task of action recognition with less filters in conv4 layer and lower-dimensional full7 layer. But we make a small modification. We use the same network architecture for both spatial and temporal net in addition to the input data layer, while the original two-stream ConvNets <ref type="bibr" target="#b23">[24]</ref> ignore the second local response normalized (LRN) layer in the temporal net due to memory consumption problem. The implementation and training details can be found in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Convolutional feature maps</head><p>Once the training of two-stream ConvNets is complete, we treat them as generic feature extractors to obtain the convolutional feature maps of videos. In general, for each video, we obtain these feature maps of spatial and temporal net in a frame-by-frame and volume-by-volume manner, respectively. In order to make the feature maps with equal temporal duration with input video, we pad the optical flow fields at the beginning with F − 1 copies of the optical flow field from the first frame, where F is the number of stacking optical flow.</p><p>For each frame or volume, we take it as the input for spatial or temporal nets. We make two modifications about the spatial and temporal nets. The first one is that we remove the layers after the target layer for feature extraction. For example, to extract feature maps of conv4, we will remove the layers from conv5 to full8. Therefore, the output of spatial and temporal net will be the convolutional feature maps, which will be used for extracting TDD in the next subsection.</p><p>The second modification is that before each convolutional or pooling layer, with kernel size k, we conduct zero padding of the layer's input with size ⌊k/2⌋. This padding allows the input and output maps of these layers to have the same spatial extent. With this padding, it will be straightforward to map the positions of trajectory points in video to the coordinates of convolutional feature maps. A trajectory point with video coordinates (x p , y p , z p ) in Equation (3) will be centered on (r × x p , r × y p , z p ) in convolutional map, where r is map size ratio with respective to input size, as listed in <ref type="table">Table 1</ref>.</p><p>ConvNets are bottom-up architectures with a sequence of alternating convolutional and pooling layers. Different <ref type="table">Layer   conv1  pool1  conv2  pool2  conv3  conv4  conv5  pool5  full6 full7 full8</ref>  <ref type="table">stride  2  2  2  2  1  1  1  2  --channel  96  96  256  256  512  512  512  512  4096 2048  101  map size</ref>   <ref type="table">Table 1</ref>. ConvNet Architectures. We use similar architectures to two-stream ConvNets <ref type="bibr" target="#b23">[24]</ref>, which are adapted to the task of action recognition from the Clarifai networks <ref type="bibr" target="#b40">[41]</ref>, with less filters in conv4 layer (512 vs. 1024) and lower-dimensional full7 layer (2048 vs. 4096). For layers of conv1 and conv2, local response normalized (LRN) is applied with parameters settings: n = 5, α = 5 × 10 −4 , β = 0.75. The layers of full6 and full7 are regularised by using dropout and the full8 layer acts as a soft-max classifier. The activation function for all weight layers is the rectification linear unit (RELU). The size ratios of feature maps with respect to input data range from 1/2 to 1/32, and the feature receptive fields vary from 7 × 7 to 171 × 171, for different convolutional and pooling layers (conv1 to pool5). <ref type="table">Table 1</ref>, ranging from 7 × 7 to 171 × 171. As described in paper <ref type="bibr" target="#b40">[41]</ref>, these different layers capture patterns from simple visual elements such as edges, to complex visual concepts such as parts and objects. The higher layers have larger receptive fields and obtain more invariant and discriminative features. Intuitively, these different layers describe the visual content at different levels, each of which is complementary to each other for the task of recognition. We will exploit this complimentary property of different layers during the extraction of TDD. Given a video V , we obtain a set of convolutional feature maps:</p><formula xml:id="formula_3">size 7 × 7 3 × 3 5 × 5 3 × 3 3 × 3 3 × 3 3 × 3 3 × 3 - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>layers of ConvNets have various receptive fields as shown in</head><formula xml:id="formula_4">C(V ) = {C s 1 , C s 2 , · · · , C s M , C t 1 , C t 2 , · · · , C t M },<label>(4)</label></formula><p>where C s m ∈ R Hm×Wm×L×Nm is the m th feature map of spatial net, H m is its height, W m is its width, L is the video duration, and N m is the number of channels. C t m ∈ R Hm×Wm×L×Nm is the m th feature map of temporal net, M is the number of layers for extracting TDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Trajectory-pooled descriptors</head><p>We will describe the method for extracting trajectorypooled deep-convolutional descriptors (TDDs) from a set of improved trajectories T(V ) and convolutional feature maps C(V ) for a given video V . In essence, TDD is a kind of local trajectory-aligned descriptor computed in a 3D volume around the trajectory. TDDs from the spatial and temporal nets capture the appearance and motion information of this 3D volume, respectively. The size of the volume is N × N pixels and P frames, where N is the receptive field size and P is the trajectory length. The extraction of TDD is composed of two steps: feature map normalization and trajectory pooling.</p><p>Normalization proves to be an effective strategy in designing features partially because it can reduce the influence of illumination. It has been widely exploited in local descriptors such as SIFT <ref type="bibr" target="#b18">[19]</ref>, HOG <ref type="bibr" target="#b4">[5]</ref>, and HOF <ref type="bibr" target="#b16">[17]</ref>, and in deep learning such as local response normalization <ref type="bibr" target="#b13">[14]</ref>. We apply the normalization strategy to the convolutional feature maps of two-stream ConvNets to suppress the activation burstiness of some neurons. We design two kinds of normalization methods:</p><p>• Spatiotemporal Normalization. For spatiotemporal normalization, we normalize the feature map for each channel independently across the video spatiotemporal extent. Given a feature map C ∈ R H×W ×L×N of Equation <ref type="formula" target="#formula_4">(4)</ref>, we normalize the convolutional feature value as follows:</p><formula xml:id="formula_5">C st (x, y, z, n) = C(x, y, z, n)/maxV n st ,<label>(5)</label></formula><p>where maxV n st is the maximum value of n th feature maps over the whole video spatiotemporal extent, which means maxV n st = max x,y,z C(x, y, z, n). The spatiotemporal normalization method ensures that each convolutional feature channel ranges in the same interval, and thus contributes equally to final TDD recognition performance.</p><p>• Channel Normalization. For channel normalization, we normalize the feature map for each pixel independently across the feature channels. We conduct channel normalization for feature map C ∈ R H×W ×L×N as follows:</p><p>C ch (x, y, z, n) = C(x, y, z, n)/maxV x,y,z ch , <ref type="bibr" target="#b5">(6)</ref> where maxV x,y,z ch is the maximum value of different feature channels at pixel position (x, y, z), that is maxV x,y,z ch = max n C(x, y, z, n). This channel normalization is able to make sure that the feature value of each pixel range in the same interval, and let each pixel make the equal contribution in the final representation.</p><p>After the step of feature normalization, we will extract TDDs based on trajectories and normalized convolutional feature maps by using trajectory pooling. Specifically, given a trajectory T k and a normalized feature map C a m , which is the m th -layer feature map after either spatiotemporal normalization or channel normalization from spatial net or temporal net (a ∈ {s, t}), we conduct sum-pooling of the normalized feature maps over the 3D volume centered at the trajectory as follows:</p><formula xml:id="formula_6">D(T k , C a m ) = P p=1</formula><p>C a m ((r m × x k p ), (r m × y k p ), z k p ), <ref type="bibr" target="#b6">(7)</ref> where (x k p , y k p , z k p ) is the p th point position of video coordinates in trajectory T k , r m is the m th -layer map size ratio with respective to input size as listed in <ref type="table">Table 1</ref>, (·) is the rounding operation. D(T k , C a m ) is called trajectory-pooled deep convolutional descriptor, and is a new kind of feature combing the merits of both improved dense trajectories and two-stream ConvNets.</p><p>Multi-scale TDD extension. The above description on TDD extraction is about the single scale, we will present the multi-scale extension of TDD. For improved trajectory, it samples points and tracks them on multi-scale videos, while fixes the spatial extent of HOG, HOF, and MBH descriptors as 32 × 32. The original method needs to conduct point tracking and descriptor calculation in multi-scale settings. In our implementation, we try a more efficient multiscale strategy. Specifically, we calculate optical flow and track point in a single scale. Then we construct multiscale pyramid representations of video frames and optical flow fields. These pyramid representations are fed into the two stream ConvNets and transformed into multi-scale convolutional feature maps as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Based on multi-scale convolutional maps and single-scale improved trajectories, we are able to compute multi-scale TDDs efficiently, by applying trajectory pooling to multi-scale convolutional feature maps as described above. The only modification to different scales is to replace feature map size ratio r m in Equation <ref type="formula">(7)</ref> with r m × s, where s is the scale of current feature map. In practice, compared with improved trajectories, we use less scales with s = 1/2, 1/ √ 2, 1, √ 2, 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first present the details of datasets and their evaluation scheme. Then, we describe the details of our method. Finally, we give the experimental results and compare TDD with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>In order to verify the effectiveness of TDDs, we conduct experiments on two public large datasets, namely HMDB51 <ref type="bibr" target="#b14">[15]</ref> and UCF101 <ref type="bibr" target="#b25">[26]</ref>. The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6, 766 video clips from 51 action categories, with each category containing at least 100 clips. Our experiments follow the original evaluation scheme using three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.</p><p>The UCF101 dataset contains 101 action classes and there are at least 100 video clips for each class. The whole dataset contains 13, 320 video clips, which are divided into 25 groups for each action category. We follow the evaluation scheme of the THUMOS13 challenge <ref type="bibr" target="#b10">[11]</ref> and adopt the three training/testing splits for evaluation. As UCF101 is larger than HMDB51, we use the UCF101 dataset to train two-stream ConvNets initially, and transfer this learned model for TDD extraction on the HMDB51 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>Two-stream ConvNets training. Training deep Con-vNets is more challenging for action recognition as action is more complex than object and the available dataset is extremely small compared with the ImageNet dataset <ref type="bibr" target="#b5">[6]</ref>. We choose the training dataset of UCF101 split1 for learning two-stream ConvNets as it is probably the largest public available dataset. We use the Caffe toolbox <ref type="bibr" target="#b9">[10]</ref> for ConvNet implementation. The network weights are learnt using the mini-batch (set to 256) stochastic gradient descent with momentum (set to 0.9). For spatial net, we first resize the frame to make the smaller side as 256, and then randomly crop a 224 × 224 region from the frame. It then undergoes random horizontal flipping. We pre-train the network with the public available model <ref type="bibr" target="#b3">[4]</ref>. Finally, we fine tune the model parameters on the UCF101 dataset, where the learning rate is set as 10 −2 , decreased to 10 −3 after 14K iterations, and training stopped at 20K iterations.</p><p>For temporal net, its input is 3D volume of stacking optical flows fields. We choose the TVL1 optical flow algorithm <ref type="bibr" target="#b39">[40]</ref> and use the OpenCV implementation, due to its balance between accuracy and efficiency. For fast computation, we discretize the values of optical flow fields into integers and set their range as 0-255 just like images. Specifically, we choose to stack 10 frames of optical flow fields to keep a balance between performance and efficiency. We train temporal net on UCF101 from scratch. As the dataset is relatively small, we use high dropout ratio to improve the generalization capacity of trained model. We set dropout 0.9 for full6 layer and dropout 0.8 for full7 layer. The training procedure of temporal net is similar to spatial net and a 224 × 224 × 20 sub-volume is randomly cropped and flipped from training video. The learning rate is initially set as 10 −2 and decreases to 10 −3 after 50K iterations. It is then reduced to 10 −4 after 70K iterations and training is stopped at 90K iterations.</p><p>Results of two-stream ConvNets. To evaluate the trained model, as in <ref type="bibr" target="#b23">[24]</ref>, we select 25 frames for each video clip and obtain 10 crops for each frame. The final recognition result is the average across these crops and frames. We obtain 71.2% recognition accuracy with spatial net and 80.1% with temporal net. The performance of our implemented two-stream ConvNets is 84.7%, which is similar to that of two-stream ConvNets <ref type="bibr" target="#b23">[24]</ref> (85.6%). However, obtaining ConvNets with high performance is not the final goal of this paper, and we aim to verify the effectiveness of TDDs.</p><p>Feature encoding. We choose Fisher vector <ref type="bibr" target="#b22">[23]</ref> to encode the TDDs of a video clip into high dimensional representation as its effectiveness for action recognition has been verified in previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27]</ref>, and then use a linear SVM as the classifer (C = 100). In order to train GMMs, we first de-correlate TDD with PCA and reduce its dimension to D. Then, we train a GMM with K (K = 256) mixtures, and finally the video is represented with a 2KDdimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Exploration experiments</head><p>Dimension reduction. To specify the PCA dimension of TDD for GMM training and Fisher vector encoding, we first explore different dimensions reduced by PCA on the HMDB51 dataset, with conv4 descriptors from spatial net. In this exploration experiment, we use the spatiotemporal normalization method for TDD and the results are shown in the left of <ref type="figure" target="#fig_1">Figure 3</ref>. We vary the dimension from 32 to 256 and the results show that dimension 64 achieves the high performance, and higher dimension may cause performance degradation. Thus, we fix the dimension as 64 for TDDs in the remainder of this section.</p><p>Normalization method. Another important component in TDD design is the normalization method and we have presented two normalization methods: spatiotemporal normalization (ST. Norm.) and channel normalization (Cha. Norm.) in Section 4.3 . We conduct experiments to investigate the effectiveness of normalization methods by using conv4 descriptors from spatial net on the HMDB51 dataset, and the results are shown in the right of <ref type="figure" target="#fig_1">Figure  3</ref>. We see that normalization is important for improving Algorithm HMDB51 UCF101 HOG <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> 40.2% 72.4% HOF <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> 48.9% 76.0% MBH <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> 52.1% 80.8% HOF+MBH <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> 54.7% 82.2% iDT <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> 57.2% 84.7% Spatial net <ref type="bibr" target="#b23">[24]</ref> 40.5% 73.0% Temporal net <ref type="bibr" target="#b23">[24]</ref> 54.6% 83.7% Two-stream ConvNets <ref type="bibr" target="#b23">[24]</ref> 59  <ref type="table">Table 3</ref>. Performance of TDD on the HMDB51 dataset and UCF101 dataset. We compare our proposed TDD with iDT features <ref type="bibr" target="#b30">[31]</ref> and two-stream ConvNets <ref type="bibr" target="#b23">[24]</ref>. We also explore the complementary properties TDD features and iDT features. The combination of them can further boost the performance.</p><p>performance and spatiotemporal normalization is the best choice. We also explore the complementary property of these two normalization methods by fusing the Fisher vectors of them, and observe that it can further improve the performance. Therefore, in the remainder of this section, we will use the combined representation obtained from these two normalization methods for TDDs. Different layers. Finally we investigate the performance of TDDs from different layers of spatial and temporal nets on the HMDB51 dataset, and the results are summarized in <ref type="table">Table 2</ref>. For layers of conv5, conv4, and conv3, we use the outputs of RELU activations, and for layers of conv2 and conv1, we choose the outputs of max pooling layers after convolution operations. We see that descriptors of layers conv4 and conv5 obtain highest recognition performance for spatial net, while the ones of layers conv3 and conv4 are top performers for temporal net. Therefore, in the following evaluation of TDD, we choose the descriptors from conv4 and conv5 layers for spatial nets, and conv3 and conv4 layers for temporal nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation of TDDs</head><p>In this section, we evaluate the performance of our proposed TDDs on the HMDB51 and UCF101 dataset, and the experimental results are summarized in <ref type="table">Table 3</ref>. We first compare the performance of TDDs with that of improved trajectories. The convolutional descriptors of spatial net are much better than HOG descriptors, which indicates that deep-learned features contains more discriminative capacity than hand-crafted features. For convolutional descriptors  <ref type="table">Table 2</ref>. The performance of different layers of spatial nets and temporal nets on the HMDB51 dataset.  of temporal net, they are better than or comparable to the descriptors of HOF and MBH, but the improvement is not so evident as spatial convolutional descriptors. The reason may be that HOF and MBH calculation is based on warped optical flow instead of original optical flow, which has been proved to be pretty effective for HOF descriptor <ref type="bibr" target="#b30">[31]</ref>. We consider using warped flow for TDDs extraction in the future.</p><p>We also compare the performance of TDDs with the two-stream ConvNets. Although our trained two-stream ConvNets obtain slightly lower performance than theirs, we see that our spatial TDDs outperform spatial nets by a large margin and temporal TDD is comparable to their temporal net. These results indicate the fact that trajectoryconstrained sampling and pooling is an effective strategy for improving recognition performance, in particular for spatial TDDs. We also notice that the combined TDDs from spatial and temporal nets outperform two-stream ConvNets by around 4% and 2% on the two datasets, respectively. We also show some examples of video frames, optical flow fields, and their corresponding feature maps in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>From these examples, we see that the convolutional feature maps are relatively sparse and exhibit high correlation with the action areas.</p><p>Finally, we explore a practical way to improve the recognition performance of action recognition system by combining TDDs with iDTs, using early fusion of Fisher vector representation. The recognition results are shown in <ref type="table">Table 3</ref>, and the fusion of them can further boost the performance. This further improvement indicates our TDDs are complementary to those low-level local features.</p><p>Computational costs. Compared with iDT, we only track points on a single scale and extract original flow HMDB51 UCF101 STIP+BoVW <ref type="bibr" target="#b14">[15]</ref> 23.0% STIP+BoVW <ref type="bibr" target="#b25">[26]</ref> 43.9% Motionlets <ref type="bibr" target="#b34">[35]</ref> 42.1% Deep Net <ref type="bibr" target="#b11">[12]</ref> 63.3% DT+BoVW <ref type="bibr" target="#b29">[30]</ref> 46.6% DT+VLAD <ref type="bibr" target="#b2">[3]</ref> 79.9% DT+MVSV <ref type="bibr" target="#b2">[3]</ref> 55.9% DT+MVSV <ref type="bibr" target="#b2">[3]</ref> 83.5% iDT+FV <ref type="bibr" target="#b30">[31]</ref> 57.2% iDT+FV <ref type="bibr" target="#b31">[32]</ref> 85.9% iDT+HSV <ref type="bibr" target="#b20">[21]</ref> 61.1% iDT+HSV <ref type="bibr" target="#b20">[21]</ref> 87.9% Two Stream <ref type="bibr" target="#b23">[24]</ref> 59.4% Two Stream <ref type="bibr" target="#b23">[24]</ref> 88.0% TDD+FV 63.2% TDD+FV 90.3% Our best result 65.9% Our best result 91.5% <ref type="table">Table 4</ref>. Comparison of TDD to the state of the art. We separately present the results of TDDs and our best results obtained with early fusion of TDDs and iDTs.</p><p>instead of warped flow. The ConvNets are implemented by Cuda and computing is very efficient. <ref type="table">Table 4</ref> compares our recognition results with several recently published methods on the dataset of HMDB51 and UCF101. The performance of TDDs outperforms previous methods on both datasets. On the HMDB51 dataset, our best result outperforms other methods by 4.8%, and on the UCF101 dataset, our best result outperforms by 3.5%. This superior performance of TDDs indicates the effectiveness of introducing trajectory-constrained sampling and pooling into deep-learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison to the state of the art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper has proposed an effective video presentation, called trajectory-pooled deep-convolutional descriptor (TDD), which integrates the advantages of hand-crafted and deep-learned features. Deep architectures are utilized to learn discriminative convolutional feature maps, and then the strategies of trajectory-constrained sampling and pooling are adopted to aggregate these convolutional features into TDDs. Our features achieve superior performance on two datasets for action recognition, as evidenced by comparison with the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Pipeline of TDD. The whole process of extracting TDD is composed of three steps: (i) extracting trajectories, (ii) extracting multiscale convolutional feature maps, and (iii) calculating TDD. We effectively exploit two available state-of-the-art video representations, namely improved trajectories and two-stream ConvNets. Grounded on them, we conduct trajectory-constrained sampling and pooling over convolutional feature maps to obtain trajectory-pooled deep convolutional descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Exploration of different settings in TDD on the HMDB51 dataset. Left: Performance trend with varying PCA reduced dimension. Right: Comparison of different normalization methods. "Combine" means the fusion of spatiotemporal normalization and channel normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) RGB (b) Flow-x (c) Flow-y (d) S-conv4 (e) S-conv5 (f) T-conv3 (g) T-conv4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Examples of video frames, optical flow fields, and their corresponding feature maps of spatial nets and temporal nets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>× 7 11 × 11 27 × 27 43 × 43 75 × 75 107 × 107 139 × 139 171 × 171 ---</figDesc><table><row><cell>ratio</cell><cell>1/2</cell><cell>1/4</cell><cell>1/8</cell><cell>1/16</cell><cell>1/16</cell><cell>1/16</cell><cell>1/16</cell><cell>1/32</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">receptive field 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Recognition accuracy 24.1% 33.9% 41.9% 48.5% 47.2% 39.2% 50.7% 54.5% 51.2% 46.1%</figDesc><table><row><cell></cell><cell>Spatial ConvNets</cell><cell cols="2">Temporal ConvNets</cell></row><row><cell>Convolutional layer</cell><cell>conv1 conv2 conv3 conv4</cell><cell>conv5 conv1 conv2 conv3</cell><cell>conv4 conv5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The TDD code and learned two-stream ConvNet models are available at https://wanglimin.github.io/tdd/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by a donation of Tesla K40 GPU from NVIDIA Corporation. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SURF: speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS-PETS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<idno>1981. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093. 6</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3D-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISP</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured learning of human interactions in TV shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patron-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>2012. 1</idno>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice. CoRR, abs/1405</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4506</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image classification with the Fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">IJCV</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale web video event classification by use of Fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LEAR-INRIA submission for the thumos workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on THUMOS Challenge</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining motion atoms and phrases for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3D parts for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent hierarchical model of temporal structure for complex activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A comparative study of encoding, pooling and normalization methods for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-L 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th DAGM Symposium on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action recognition with actons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
