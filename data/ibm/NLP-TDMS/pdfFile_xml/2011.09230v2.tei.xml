<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Na</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ajou University</orgName>
								<address>
									<settlement>Korea</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
							<email>heechul@knu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Kyungpook National University</orgName>
								<address>
									<settlement>Korea</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung</forename><forename type="middle">Jin</forename><surname>Chang</surname></persName>
							<email>h.j.chang@bham.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Hwang</surname></persName>
							<email>wjhwang@ajou.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Ajou University</orgName>
								<address>
									<settlement>Korea</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) methods for learning domain invariant representations have achieved remarkable progress. However, most of the studies were based on direct adaptation from the source domain to the target domain and have suffered from large domain discrepancies. In this paper, we propose a UDA method that effectively handles such large domain discrepancies. We introduce a fixed ratio-based mixup to augment multiple intermediate domains between the source and target domain. From the augmented-domains, we train the source-dominant model and the target-dominant model that have complementary characteristics. Using our confidencebased learning methodologies, e.g., bidirectional matching with high-confidence predictions and self-penalization using low-confidence predictions, the models can learn from each other or from its own results. Through our proposed methods, the models gradually transfer domain knowledge from the source to the target domain. Extensive experiments demonstrate the superiority of our proposed method on three public benchmarks: Office-31, Office-Home, and VisDA-2017. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, we have seen considerable improvements in several computer vision applications using deep learning; however, this success has been limited to supervised learning methods with abundant labeled data. Collecting and labeling data from various domains is an expensive and timeconsuming task. To address this problem, semi-supervised learning <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34]</ref> and unsupervised learning <ref type="bibr" target="#b8">[9]</ref> have been studied; however, in most cases, it was assumed that learning of the model occurred in a similar domain.</p><p>UDA refers to a set of transfer learning methods for transferring knowledge learned from the source domain to the target domain under the assumption of domain discrepancy. Moreover, it is useful when the source domain con- <ref type="bibr" target="#b0">1</ref> Our code is available at https://github.com/NaJaeMin92/FixBi. <ref type="figure">Figure 1</ref>. Comparison of previous domain adaptation methods and our proposed method. Top: Previous methods try to adapt directly without any consideration of large domain discrepancies. Bottom: Our proposed method utilize augmented domains between the source and target domain for efficient domain adaptation. tains enough labeled data to learn, but not much labeled data are present in the target domain. Domain adaptation (DA) generally assumes that the two domains have the same conditional distribution, but different marginal distributions. Under these assumptions, effective knowledge transfer is difficult when the two domains have large marginal distribution gaps. This becomes much more challenging in a scenario where the target domain has no labeled data at all.</p><p>In previous UDA methods, a domain discriminator <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37]</ref> was introduced to encourage domain confusion through domain-adversarial objectives and minimize the gap between the source and target distributions. In <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>, domain discrepancy based approaches used metrics such as maximum mean discrepancy (MMD) and joint MMD (JMMD) to reduce the difference between two feature spaces. Moreover, inspired by the generative adversarial network (GAN), GAN-based DA methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> have attempted to generate transferable representations to minimize domain discrepancy. Most of these studies have directly adapted the knowledge learned from the source domain to the target domain. However, fundamentally, this does not take into account the case where the distance between the source and target domain is large, as shown in <ref type="figure">Figure 1</ref>.</p><p>In this paper, our goal is to compensate efficiently for the large domain discrepancies. To address this challenge, we construct multiple intermediate augmented domains, whose characteristics are different and complementary to each other. To achieve this, we propose a fixed ratio-based mixup. Our proposed mixup approach minimizes the domain randomness of <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> between the source and target samples and generates multiple intermediate domains, as shown in <ref type="figure">Figure 1</ref>. For example, an augmented domain close to the source domain has more reliable label information, but it has a lower correlation with the target domain. By contrast, label information in an augmented domain close to the target domain is relatively inaccurate, but the similarity to the target domain is much higher.</p><p>In these augmented domains, we train the complementary models that teach each other to bridge between the source and target domain. Specifically, we introduce a bidirectional matching based on the high-confidence predictions of each model for the target samples, moving the intermediate domains to the target domain. We also apply selfpenalization, which penalizes its own model to improve performance through self-training. Moreover, to properly impose the characteristics of models that change with each iteration, we use an adaptive threshold by the confidence distribution of each mini-batch, not a predefined one <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44]</ref>. Finally, to prevent divergence of the augmented models generated in different domains, we propose a consistency regularization using an augmented domain with the same ratio of source and target samples.</p><p>We conduct extensive ablation studies for a detailed analysis of our proposed method and achieve comparable performance to state-of-the-art methods in standard DA benchmarks such as Office-31 <ref type="bibr" target="#b28">[29]</ref>, Office-Home <ref type="bibr" target="#b39">[40]</ref>, and VisDA-2017 <ref type="bibr" target="#b26">[27]</ref>. The main contributions of this paper are summarized as follows.</p><p>• We propose a fixed ratio-based mixup to efficiently bridges the source and target domains utilizing the intermediate domains.</p><p>• We propose confidence-based learning methodologies: a bidirectional matching and a self-penalization using positive and negative pseudo-labels, respectively.</p><p>• We empirically validate the superiority of our method to UDA with extensive ablation studies and evaluations on three standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semi-supervised Learning. Semi-supervised learning (SSL) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref> leverages unlabeled data to improve a model's performance when limited labeled data is provided, which alleviates the expensive labeling process efficiently. Many recently proposed semi-supervised learning methods, such as MixMatch <ref type="bibr" target="#b2">[3]</ref>, FixMatch <ref type="bibr" target="#b33">[34]</ref>, and ReMixMatch <ref type="bibr" target="#b1">[2]</ref>, based on augmentation viewpoints. MixMatch <ref type="bibr" target="#b2">[3]</ref> used low-entropy labels for data-augmented unlabeled instances and mixed labeled and unlabeled data for semi-supervised learning. On the basis of consistency regularization and pseudo-labeling, FixMatch <ref type="bibr" target="#b33">[34]</ref> generates pseudo-labels using the model's predictions on weakly augmented unlabeled images. Then, when the examples have high-confidence predictions, they train the model using strong-augmented images. Note that in general, they assumed that labeled and unlabeled data have similar domains or feature distributions.</p><p>Basically, semi-supervised domain adaptation has more information about some target labels compared with UDA, and some related works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref> have been proposed leveraging semi-supervised signals. Specifically, in <ref type="bibr" target="#b29">[30]</ref>, a minimax entropy approach was proposed that adversarially optimizes an adaptive few-shot model. In <ref type="bibr" target="#b27">[28]</ref>, the learning of opposite structures was unified whereby it consists of a generator and two classifiers trained with opposite forms of losses for a unified object.</p><p>Meanwhile, <ref type="bibr" target="#b43">[44]</ref> addresses semi-supervised domain adaptation by breaking it down into SSL and UDA problems. Two models are in charge of each sub-problem and are trained based on co-teaching. One model is trained with labeled source samples and labeled target samples, and the other model is trained with unlabeled target samples and labeled target samples. In this way, by using different combinations of data, it provides two different perspectives. By contrast, in this paper, we guarantee two different perspectives with the two types of our fixed ratio-based mixup. In addition, <ref type="bibr" target="#b43">[44]</ref> used co-teaching <ref type="bibr" target="#b11">[12]</ref> concepts to train the mixup objectives between the source and target domain, whereas we use this concept to train the pseudo-labels in the target domain with bidirectional matching. Furthermore, when applying the mixup operation, <ref type="bibr" target="#b43">[44]</ref> uses only selected target samples, whereas we use all target samples.</p><p>Unsupervised Domain Adaptation. Recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref> have focused on UDA based on domain alignment and discriminative domain-invariant feature learning methods. For example, a deep adaptation network (DAN) <ref type="bibr" target="#b20">[21]</ref> minimized MMD over domain-specific layers, and joint adaptation networks <ref type="bibr" target="#b23">[24]</ref> aligned the joint distributions of domain-specific layers across different domains <ref type="figure">Figure 2</ref>. An overview of the proposed method. The proposed method consists of (a) fixed ratio-based mixup, (b) confidence-based learning, e.g., bidirectional matching with the positive pseudo-labels and self-penalization with the negative pseudo-labels, and (c) consistency regularization. Best viewed in color.</p><p>based on a JMMD. Deep domain confusion (DCC) <ref type="bibr" target="#b37">[38]</ref> made use of MMD metrics in the fully connected layer for learning both discriminative and transferable domains. A domain adversarial neural network (DANN) <ref type="bibr" target="#b7">[8]</ref> learned a domain invariant representation by back-propagating the reverse gradients of the domain classifier. Adversarial discriminative domain adaptation (ADDA) <ref type="bibr" target="#b36">[37]</ref> learned a discriminative representation using the source labels, and then, a separate encoding that maps the target data to the same space based on a domain-adversarial loss is used. Maximum classification discrepancy (MCD) <ref type="bibr" target="#b31">[32]</ref> tried to align the distribution of a target domain by considering taskspecific decision boundaries by maximizing the discrepancy on the target samples and then generating features that minimize this discrepancy. A contrastive adaptation network (CAN) <ref type="bibr" target="#b16">[17]</ref> optimized the metric for minimizing the domain discrepancy, which explicitly models the intra-class domain discrepancy and the inter-class domain discrepancy.</p><p>Robust spherical domain adaptation (RSDA) <ref type="bibr" target="#b10">[11]</ref> used a spherical classifier for label prediction and a spherical domain discriminator for discriminating domain labels and utilized robust pseudo-label loss in the spherical feature space. Structurally regularized deep clustering (SRDC) <ref type="bibr" target="#b35">[36]</ref> enhanced target discrimination by clustering intermediate network features and structural regularization with soft selection of less divergent source examples. Dual mixup regularized learning (DMRL) <ref type="bibr" target="#b40">[41]</ref> guided the classifier to enhance consistent predictions between samples and enriched the intrinsic structures of the latent space. For mixing the source and target domains, they proposed two mixup regularizations based on randomness.</p><p>Note that in this study, we create bridges between the target and source domain by augmenting multiple intermediate domains. For this purpose, unlike <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref>, the scope of the augmented domain was not expanded simply by relying on randomness. However, two fixed ratio-based mixups are used to create a source-closed augmented domain, which has a clear label but is at a distance from the target domain, and a target-closed augmented domain, which has the opposite properties. Then, they teach each other in order to transfer domain knowledge to the target side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In UDA, we are given labeled data</p><formula xml:id="formula_0">X s = {(x s i , y s i )} Ns i=1</formula><p>from the source domain and unlabeled data</p><formula xml:id="formula_1">X t = {(x t i )} Nt i=1</formula><p>from the target domain where the N s and N t denote the sizes of X s and X t , respectively. The large distribution gap between P (X s ) and P (X t ) is one of the major obstacles for the UDA problem. Our goal is to ensure that the knowledge learned from the source domain is well generalized in the target domain. In this section, we introduce our FixBi algorithm and the detailed ideas it builds on, as shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fixed Ratio-based Mixup</head><p>In general, the mixup <ref type="bibr" target="#b45">[46]</ref> is a kind of data augmentation method to increase the robustness of neural networks when learning from corrupt labels. Recent studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref> have utilized the mixup to construct virtual samples with convex combinations between labeled and unlabeled data. In this context, most domain adaptation methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41]</ref> based on the mixup use mixup-ratio λ with randomly sampled values from the beta distribution: λ ∼ Beta(α, α) where α is a hyperparameter. It is because they have tried to generate training samples that exist somewhere between the source and target domain without any consideration of the domain gap. However, we propose to use two fixed mixup ratios λ sd and λ td to provide more clarity and less random-ness.</p><p>Given a pair of input samples and their corresponding one-hot labels in the source and target domain: (x s i , y s i ) and (x t i ,ŷ t i ), our mixup settings are defined as follows:</p><formula xml:id="formula_2">x st i = λx s i + (1 − λ)x t ĩ y st i = λy s i + (1 − λ)ŷ t i ,<label>(1)</label></formula><p>where λ ∈ {λ sd , λ td } s.t. λ sd + λ td = 1. Note that y t i is the pseudo-labels obtained by the baseline model, e.g., DANN <ref type="bibr" target="#b7">[8]</ref> or MSTN <ref type="bibr" target="#b41">[42]</ref>, for the unlabeled target samples. The detailed analysis of our fixed ratio-based mixup is covered in Section 4.2.</p><p>Taking advantage of the fixed ratio-based mixup, we construct two network models that act as bridges between the source and target domain. The key point here is to obtain two networks with different perspectives through our mixup strategies. For this purpose, we leverage two different models made by the proposed mixup ratios λ sd and λ td : "source-dominant model" (SDM) and "target-dominant model" (TDM). The source-dominant model has strong supervision for the source domain but relatively weak supervision for the target domain. By contrast, the target-dominant model has strong target supervision but weak source supervision. As both types of mixups are not confined to a single domain, they can serve as bridges between the two different domains.</p><p>Consequently, we apply two fixed ratios λ sd for SDM and λ td for TDM. Let p(y|x st i ) denote the predicted class distribution produced by the model for an inputx st i . Then the objective of our fixed ratio-based mixup is defined as follows:</p><formula xml:id="formula_3">L f m = 1 B B i=1ŷ st i log(p(y|x st i )),<label>(2)</label></formula><p>whereŷ st i = argmax p(y|x st i ) and B is a mini-batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Confidence-based Learning</head><p>Through our fixed ratio-based mixup, the two networks have different characteristics and can develop with mutually complementary learning. To utilize the two models as bridges from the source domain to the target domain, we propose a confidence-based learning where one model teaches the other model using the positive pseudo-labels or teach itself using the negative pseudo-labels.</p><p>Bidirectional Matching with positive pseudo-labels. Inspired by <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4]</ref>, when one network assigns the class probability of input above a certain threshold τ , we assume that this predicted label as a pseudo-label. Here, we refer to these labels as positive pseudo-labels. Then we train the peer network to make its predictions match these positive pseudo-labels via a standard cross-entropy loss. Let us denote probability distributions p and q of two models. Then the objective of our bidirectional matching is defined as follows:</p><formula xml:id="formula_4">L bim = 1 B B i=1 1(max(p(y|x t i ) &gt; τ )ŷ t i log(q(y|x t i )),<label>(3)</label></formula><p>whereŷ t i = argmax p(y|x t i ). Note that in <ref type="bibr" target="#b33">[34]</ref>, only oneway matching is used according to input augmentations. However, since our method derives the results from both networks for the same input, bidirectional matching is available.</p><p>Self-penalization with negative pseudo-labels. As well as the bidirectional matching that matches the positive pseudo-labels to the predictions of the peer network, each network learns through the self-penalization using the negative pseudo-labels. Here, the negative pseudo-label indicates the most confident label (top-1 label) predicted by the network with a confidence lower than the threshold τ . Since the negative pseudo-label is unlikely to be a correct label, we need to increase the probability values of all other classes except for this negative pseudo-label. Therefore, we optimize the output probability corresponding to the negative pseudo-label to be close to zero. The objective of selfpenalization is defined as follows:</p><formula xml:id="formula_5">L sp = 1 B B i=1 1(max(p(y|x t i ) &lt; τ )ŷ t i log(1 − p(y|x t i )).</formula><p>(4) Unlike the recent studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44</ref>] that ignore the lowconfidence predictions (or large loss samples), it is worth noting that we utilize the low-confidence predictions as the meaningful knowledges for learning the models. Furthermore, we apply the learnable temperature of the softmax to adjust the output distributions.</p><p>Looking back to the confidence threshold τ , the basic strategy is to set a fixed value as a hyperparameter. However, a deep neural network (DNN) tends to start at a low level of confidence value and its value gradually increases as the network learns. The fixed threshold cannot properly reflect the confidence which is changed constantly during training, therefore the number of positive and negative pseudo-labels can be biased to one side. To overcome this problem, we adopt an adaptive threshold which is changed adaptively by the sample mean and standard deviation of a mini-batch, not a fixed one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Consistency Regularization</head><p>Through our confidence-based learning, the two models with different characteristics gradually get closer to the target domain because they are trained with more reliable pseudo-labels of the target samples. We introduced a new consistency regularization to ensure a stable convergence of training both models. Here, we assume that the well-trained models should be regularized to have consistent results in the same space. It helps to construct the domain bridging by ensuring that the two models trained from the different domain spaces maintain consistency in the same area between the source and target domain. For the intermediate space, both fixed mixup-ratios λ sd and λ td are set to 0.5. The consistency regularization loss can be defined as follows: </p><formula xml:id="formula_6">L cr = 1 B B i=1 p(y|x st i ) − q(y|x st i ) 2 2 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Procedure</head><p>The training process of our FixBi is summarized in Algorithm 1. First, we start to train our networks with pretrained baseline weights, similar to <ref type="bibr" target="#b10">[11]</ref>. Then, we copy the pre-trained weights to w sd and w td . In each iteration, we generate two types of samples with different mixup ratios λ sd and λ td . Initially, to ensure that the two networks have independent characteristics, we apply a warm-up period of k epochs where each network is independently trained only with the fixed ratio-based mixup and the self-penalization. After enough training, we begin to train with bidirectional matching that can teach each other. Note that one network is trained with pseudo-labels from the peer network which satisfies the confidence threshold constraint. At the same time, we also apply the consistency regularization loss to guarantee stable convergence in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed method on three domain adaptation benchmarks such as Office-31, Office-Home and VisDA-2017, compared with state-of-the-art domain adaptation methods. In addition, we validate the contributions of the proposed method through extensive ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setups</head><p>Datasets. We evaluated our method in the following three standard benchmarks for UDA.</p><p>Office-31 <ref type="bibr" target="#b28">[29]</ref> is the most popular dataset for real-world domain adaptation. It contains 4,110 images of 31 categories in three domains: Amazon (A), Webcam (W), DSLR (D). We evaluated all methods on six domain adaptation tasks.</p><p>Office-Home <ref type="bibr" target="#b39">[40]</ref> is a more challenging benchmark than Office-31. It consists of images of everyday objects organized into four domains: artistic images (Ar), clip art (Cl), product images (Pr), and real-world images (Rw). It contains 15,500 images of 65 classes.</p><p>VisDA-2017 <ref type="bibr" target="#b26">[27]</ref> is a large-scale dataset for syntheticto-real domain adaptation. It contains 152,397 synthetic images for the source domain and 55,388 real-world images for the target domain.</p><p>Baselines. Since our proposed method can be flexibly applied in any UDA methods, we use DANN <ref type="bibr" target="#b7">[8]</ref> as a baseline for a detail analysis of our contributions and MSTN <ref type="bibr" target="#b41">[42]</ref> for performance comparisons with the state-ofthe-art methods.</p><p>Implementation details. Following the standard protocol for UDA, we use all labeled source data and all unlabeled target data. For Office-31 and Office-Home, we use ResNet-50 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> as the backbone network. We use minibatch stochastic gradient descent (SGD) with a momentum of 0.9, an initial learning rate of 0.001, and a weight decay of 0.005. We follow the same learning rate schedule as in <ref type="bibr" target="#b7">[8]</ref>. The mixup ratios are set up with λ sd = 0.7 and λ td = 0.3. The confidence threshold τ is calculated as (mean − 2 × std) across all mini-batches. We train the model for a total of 200 epochs and set the warm-up epochs to 100. For VisDA-2017, we use ResNet-101 as the backbone architecture. We use the SGD optimizer with a momentum of 0.9, an initial learning rate of 0.0001, and a weight decay of 0.005. We train the model for 25 epochs with warm-up epochs of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies and discussions</head><p>For a more detailed analysis of our proposed method, we conducted ablation studies on the Office-31 dataset with DANN <ref type="bibr" target="#b7">[8]</ref> as the baseline model.</p><p>Comparison of different mixup-ratio rules. We compare our fixed-ratio mixup with two existing general ratios in <ref type="table" target="#tab_1">Table 1</ref>. The "Random" refers to the ratio randomly sampled from the beta distribution, as in the traditional mixup approaches <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43]</ref>. The "Range" refers to the ratio randomly sampled from the beta distribution and limited to a specific range. In this case, the mixup ratio λ is determined by λ ∼ max(λ, 1 − λ), where λ ∼ Beta(α, α). In this experiment, SDM and TDM are trained with λ and 1 − λ , respectively, intending to give them different perspectives. We set the hyperparameter α of the beta distribution to 1.0 for "Random" and "Range", as used in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b25">26]</ref>. Lastly, the "Fixed" refers to using two fixed mixup ratios (λ sd , λ td ). We set λ sd = 0.7 and λ td = 0.3, which satisfies λ sd + λ td = 1. Note that our final accuracy is the result of an ensemble of the output probabilities of both models.</p><p>The left side of <ref type="table" target="#tab_1">Table 1</ref> shows the accuracy when only a fixed ratio mixup is applied without the bidirectional matching. In the case of "Random", the accuracy is similar to that of "Fixed". However, in the case of "Range", extreme accuracy degradation is noticeable in TDM. It shows that a mixup which is too target-biased negatively affects learning when the target labels are incorrect. The right side of Table 1 presents the accuracy when applying the bidirectional matching with the fixed ratio mixup. In the case of "Ran-   <ref type="figure">Figure 5</ref>. The visualization of embedded features on the task A→W. Blue and orange points denote the source and target domains, respectively. Best viewed in color.</p><p>dom" and "Range", it is difficult to expect performance improvement through the bidirectional matching. By contrast, we observe that the networks trained through the fixed ratiobased mixup can benefit from the bidirectional matching. Why is it better to use a fixed ratio? We claim that this difference occurs because the two networks have different perspectives through our fixed ratio-based mixup. To verify this, we analyzed the class-wise accuracy of SDM and TDM. We apply L f m with fixed ratios λ sd = 0.7 and λ td = 0.3 to the models, respectively. We pick the top-10 accuracies with the largest difference between classwise accuracies for the two network models. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, we observe that these two models have different strengths and weaknesses from the viewpoint of class-wise accuracy. Note that the performances of these two models are similar at 86.3% and 86.0%.</p><p>Comparison with simple ensemble models. To show that our two perspectives have a different meaning to a simple ensemble of a single-perspective, we compare the ensemble of a single-perspective with our proposed method. For the single-perspective, we train models with the mixup ratios of (0.3, 0.3) and (0.7, 0.7), respectively. For a fair comparison, we apply only L f m and L bim . As shown in Table 2, the accuracy of our two-perspective model is higher than that of the other single-perspective models. Effects of the components of our FixBi. We conduct ablation studies to investigate the effectiveness of the components of our proposed method. In <ref type="table">Table 3</ref>, our fixed ratiobased mixup improves the baseline DANN [8] on average by 3.3%. The bidirectional matching provides an additional 1.5% improvement on average. Especially, in the task of A→W and A→D, we observe that the bidirectional matching has an impressive impact on performance improvement. We also observe that self-penalization has a significant impact on both task D→A and W→A. In addition, our consistency regularization loss helps to improve performance. Overall, FixBi improves the baseline DANN by an average of 7.1%. This shows that each component is effective in improving performance.</p><p>Analysis of the confidence threshold. We visualize our confidence threshold τ in <ref type="figure" target="#fig_1">Figure 4</ref>. Note that our confidence threshold τ is changed adaptively within each minibatch, and gradually increased with learning iterations. We observe a sharp increase in the confidence of TDM at 100 epoch, the point at which L bim starts to be applied. It can be seen more clearly, especially when the difference in the mixup ratio between SDM and TDM is large.</p><p>Feature visualization. <ref type="figure">Figure 5</ref> visualizes the embedded features on the task A→W by t-SNE <ref type="bibr" target="#b38">[39]</ref>. For the baseline (e.g, DANN <ref type="bibr" target="#b7">[8]</ref>), the target domain features are embedded around the clusters of the source domain features, but it fails to form the clusters of the target domain features. On the other hand, our FixBi constructs the compact clusters of the target domain features close to the source domain features. From this result, we confirm that our proposed method works successfully for an unsupervised domain adaptation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state-of-the-art methods</head><p>We compare our method with the various state-of-the-art methods on three public benchmarks. The results of Office-31, Office-Home, and VisDA-2017 are reported in <ref type="table" target="#tab_3">Tables  4, 5</ref>, and 6, respectively. We use MSTN <ref type="bibr" target="#b41">[42]</ref> as a baseline network. Note that our final accuracy is obtained by the sum of the softmax results of two network models. Office-31. <ref type="table">Table 4</ref> shows the comparative performance on the Office-31 dataset based on ResNet-50. The average accuracy of our method is 91.4%, which outperforms the other methods such as SRDC <ref type="bibr" target="#b35">[36]</ref> and RSDA-MSTN <ref type="bibr" target="#b10">[11]</ref>. Our method shows a significant performance improvement over the baseline MSTN <ref type="bibr" target="#b41">[42]</ref> method in situations with very large domain shifts, e.g., A→W, W→A, A→D, and D→A tasks. In particular, compared to baseline, the task with the most improved accuracy was W→A, achieving a performance improvement of 13.8%. Compared with the mixupbased method DMRL <ref type="bibr" target="#b40">[41]</ref>, a large performance improvement is also observed.</p><p>Office-Home. In <ref type="table" target="#tab_3">Table 5</ref>, we compare our method with recent UDA methods on the Office-Home dataset based on ResNet-50. Our FixBi shows particularly strong performances in tasks with the large domain discrepancy between the real-world domain and the artificial domain, e.g., Rw→Ar, Rw→Cl, Rw→Pr, and Cl→Rw tasks. Our method has an average accuracy of 72.7%, which outperforms the state-of-the-art results achieved by SRDC <ref type="bibr" target="#b35">[36]</ref>. Note that our method achieves approximately 2% better accuracy compared with RSDA-MSTN <ref type="bibr" target="#b10">[11]</ref> on Office-Home.</p><p>VisDA-2017. <ref type="table">Table 6</ref> presents the classification accuracy for the VisDA-2017 dataset based on ResNet-101. Our FixBi achieves 22.2% performance improvement on an av-erage compared with the baseline method <ref type="bibr" target="#b41">[42]</ref>. Above all, our method shows about 12% better accuracy than other mixup-based methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> and achieves comparable performance compared to the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a FixBi algorithm that bridging the domain spaces to deal with the large domain discrepancy problem in an unsupervised domain adaptation scenario. Our main methodology is to construct an intermediate domain with different characteristics between the source domain and the target domain. We completed this through our fixed ratio-based mixup with different mixupratios, and further proposed bidirectional matching, selfpenalization, and consistency regularization for efficient use of intermediate space. Extensive ablation studies demonstrate the effectiveness of our proposed algorithm and experiments on the three standard benchmarks show that our proposed method achieves competitive performance to the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Class-wise accuracy (%) on the task A→W of the Office-31. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the confidence threshold τ on the task A→W of the Office-31. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>; w sd ) &gt; τ sd then Update L bim (x t ; w td ); end ObtainM cr using Eq. (1) with λ cr ; Update L cr (M cr ; w sd ); Update L cr (M cr ; w td ); Learned model parameters w sd and w td .</figDesc><table><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>Output:</cell></row></table><note>Algorithm 1: FixBi Training Procedure Input : Network weights w sd and w td , total epochs E, mini batch B, warm-up epochs k, mixup-ratios λ sd , λ td , and λ cr (= 0.5), source samples x s , target samples x t , and mixup samplesM . for e=1 to E do for i=1 to B do ObtainM sd using Eq. (1) with λ sd ; ObtainM td using Eq. (1) with λ td ; Update L f m (M sd ; w sd ) and L sp (x t ; w sd ); Update L f m (M td ; w td ) and L sp (x t ; w td ); if e &gt; k then if max(y|x t ; w td ) &gt; τ td then Update L bim (x t ; w sd ); end if max(y|xt</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of three different mixup-ratio rules on the task A→W.</figDesc><table><row><cell>Type</cell><cell>w/o L bim SDM TDM</cell><cell>w/ L bim SDM TDM</cell></row><row><cell>Random</cell><cell cols="2">86.5±1.0 85.3±0.9 86.7±0.8 85.6±0.7</cell></row><row><cell>Range</cell><cell cols="2">86.0±1.7 29.6±6.8 83.3±6.2 81.0±5.4</cell></row><row><cell cols="3">Fixed (Ours) 86.3±0.6 86.0±0.7 89.3±0.4 90.1±0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of ensemble networks on Office-31. Method (λ sd , λ td ) A→W D→W W→D A→D D→A W→A AvgTable 3. Ablation results (%) of investigating the effects of our components on Office-31. L DAN N L f m L bim L sp L cr A→W D→W W→D A→D D→A W→A Avg</figDesc><table><row><cell>Single-perspective</cell><cell cols="2">(0.3, 0.3) (0.7, 0.7)</cell><cell>88.6 89.2</cell><cell>96.5 96.5</cell><cell>100.0 100.0</cell><cell>85.6 85.5</cell><cell>69.4 69.1</cell><cell>65.1 67.8</cell><cell>84.2 84.7</cell></row><row><cell cols="3">Two-perspective (Ours) (0.7, 0.3)</cell><cell>90.1</cell><cell>98.5</cell><cell>100.0</cell><cell>88.4</cell><cell>72.5</cell><cell>72.5</cell><cell>87.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>82.0</cell><cell>96.9</cell><cell>99.1</cell><cell>79.7</cell><cell>68.2</cell><cell>67.4</cell><cell>82.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>86.5</cell><cell>98.4</cell><cell>100.0</cell><cell>85.5</cell><cell>71.4</cell><cell>71.5</cell><cell>85.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>90.1</cell><cell>98.5</cell><cell>100.0</cell><cell>88.4</cell><cell>72.5</cell><cell>72.5</cell><cell>87.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>92.3</cell><cell>98.6</cell><cell>100.0</cell><cell>90.4</cell><cell>76.3</cell><cell>74.1</cell><cell>88.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>94.2</cell><cell>99.3</cell><cell>100.0</cell><cell>91.3</cell><cell>76.5</cell><cell>74.3</cell><cell>89.3</cell></row><row><cell cols="10">Table 4. Accuracy (%) on Office-31 for unsupervised domain adaptation (ResNet-50). The best accuracy is indicated in bold</cell></row><row><cell cols="4">and the second best one is underlined. * Reproduced by [5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>A→W</cell><cell cols="2">D→W</cell><cell>W→D</cell><cell>A→D</cell><cell cols="2">D→A</cell><cell>W→A</cell><cell>Avg</cell></row><row><cell>ResNet-50 [13]</cell><cell cols="9">68.4±0.2 96.7±0.1 99.3±0.1 68.9±0.2 62.5±0.3 60.7±0.3 76.1</cell></row><row><cell>DANN [8]</cell><cell cols="9">82.0±0.4 96.9±0.2 99.1±0.1 79.7±0.4 68.2±0.4 67.4±0.5 82.2</cell></row><row><cell>MSTN* [42]</cell><cell>91.3</cell><cell></cell><cell>98.9</cell><cell>100.0</cell><cell>90.4</cell><cell>72.7</cell><cell></cell><cell>65.6</cell><cell>86.5</cell></row><row><cell>CDAN+E [23]</cell><cell cols="9">94.1±0.1 98.6±0.1 100.0±0.0 92.9±0.2 71.0±0.3 69.3±0.3 87.7</cell></row><row><cell>DMRL [41]</cell><cell cols="9">90.8±0.3 99.0±0.2 100.0±0.0 93.4±0.5 73.0±0.3 71.2±0.3 87.9</cell></row><row><cell>SymNets [47]</cell><cell cols="9">90.8±0.1 98.8±0.3 100.0±0.0 93.9±0.5 74.6±0.6 72.5±0.5 88.4</cell></row><row><cell>GSDA [16]</cell><cell>95.7</cell><cell></cell><cell>99.1</cell><cell>100</cell><cell>94.8</cell><cell>73.5</cell><cell></cell><cell>74.9</cell><cell>89.7</cell></row><row><cell>CAN [17]</cell><cell cols="9">94.5±0.3 99.1±0.2 99.8±0.2 95.0±0.3 78.0±0.3 77.0±0.3 90.6</cell></row><row><cell>SRDC [36]</cell><cell cols="9">95.7±0.2 99.2±0.1 100.0±0.0 95.8±0.2 76.7±0.3 77.1±0.1 90.8</cell></row><row><cell cols="10">RSDA-MSTN [11] 96.1±0.2 99.3±0.2 100.0±0.0 95.8±0.3 77.4±0.8 78.9±0.3 91.1</cell></row><row><cell>FixBi (Ours)</cell><cell cols="9">96.1±0.2 99.3±0.2 100.0±0.0 95.0±0.4 78.7±0.5 79.4±0.3 91.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Accuracy (%) on Office-Home for unsupervised domain adaptation (ResNet-50). The best accuracy is indicated in bold and the second best one is underlined. * Reproduced by<ref type="bibr" target="#b10">[11]</ref> 94.2 90.9 25.7 87.2</figDesc><table><row><cell>Method</cell><cell cols="13">Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg</cell></row><row><cell>ResNet-50 [13]</cell><cell>34.9</cell><cell>50</cell><cell>58</cell><cell>37.4</cell><cell>41.9</cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9</cell><cell>46.1</cell></row><row><cell>DANN [8]</cell><cell>45.6</cell><cell>59.3</cell><cell>70.1</cell><cell>47</cell><cell>58.5</cell><cell>60.9</cell><cell>46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8</cell><cell>57.6</cell></row><row><cell>CDAN [23]</cell><cell>49</cell><cell>69.3</cell><cell>74.5</cell><cell>54.4</cell><cell>66</cell><cell>68.4</cell><cell>55.6</cell><cell>48.3</cell><cell>75.9</cell><cell>68.4</cell><cell>55.4</cell><cell>80.5</cell><cell>63.8</cell></row><row><cell>MSTN* [42]</cell><cell>49.8</cell><cell>70.3</cell><cell>76.3</cell><cell>60.4</cell><cell>68.5</cell><cell>69.6</cell><cell>61.4</cell><cell>48.9</cell><cell>75.7</cell><cell>70.9</cell><cell>55</cell><cell>81.1</cell><cell>65.7</cell></row><row><cell>SymNets [47]</cell><cell>47.7</cell><cell>72.9</cell><cell>78.5</cell><cell>64.2</cell><cell>71.3</cell><cell>74.2</cell><cell>63.6</cell><cell>47.6</cell><cell>79.4</cell><cell>73.8</cell><cell>50.8</cell><cell>82.6</cell><cell>67.2</cell></row><row><cell>GSDA [16]</cell><cell>61.3</cell><cell>76.1</cell><cell>79.4</cell><cell>65.4</cell><cell>73.3</cell><cell>74.3</cell><cell>65</cell><cell>53.2</cell><cell>80</cell><cell>72.2</cell><cell>60.6</cell><cell>83.1</cell><cell>70.3</cell></row><row><cell>GVB-GD [7]</cell><cell>57</cell><cell>74.7</cell><cell>79.8</cell><cell>64.6</cell><cell>74.1</cell><cell>74.6</cell><cell>65.2</cell><cell>55.1</cell><cell>81</cell><cell>74.6</cell><cell>59.7</cell><cell>84.3</cell><cell>70.4</cell></row><row><cell>RSDA-MSTN [11]</cell><cell>53.2</cell><cell>77.7</cell><cell>81.3</cell><cell>66.4</cell><cell>74</cell><cell>76.5</cell><cell>67.9</cell><cell>53</cell><cell>82</cell><cell>75.8</cell><cell>57.8</cell><cell>85.4</cell><cell>70.9</cell></row><row><cell>SRDC [36]</cell><cell>52.3</cell><cell>76.3</cell><cell>81</cell><cell>69.5</cell><cell>76.2</cell><cell>78</cell><cell>68.7</cell><cell>53.8</cell><cell>81.7</cell><cell>76.3</cell><cell>57.1</cell><cell>85</cell><cell>71.3</cell></row><row><cell>FixBi (Ours)</cell><cell>58.1</cell><cell>77.3</cell><cell>80.4</cell><cell>67.7</cell><cell>79.5</cell><cell>78.1</cell><cell>65.8</cell><cell>57.9</cell><cell>81.7</cell><cell>76.4</cell><cell>62.9</cell><cell>86.7</cell><cell>72.7</cell></row><row><cell cols="14">Table 6. Accuracy (%) on VisDA-2017 for unsupervised domain adaptation (ResNet-101). The best accuracy is indicated in</cell></row><row><cell cols="7">bold and the second best one is underlined. * Reproduced by [5]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">aero bicycle bus</cell><cell cols="10">car horse knife motor person plant skate train truck Avg</cell></row><row><cell cols="2">ResNet-101 [13] 72.3</cell><cell>6.1</cell><cell cols="3">63.4 91.7 52.7</cell><cell>7.9</cell><cell>80.1</cell><cell>5.6</cell><cell cols="5">90.1 18.5 78.1 25.9 49.4</cell></row><row><cell>DANN [8]</cell><cell>81.9</cell><cell>77.7</cell><cell cols="3">82.8 44.3 81.2</cell><cell>29.5</cell><cell>65.1</cell><cell>28.6</cell><cell cols="3">51.9 54.6 82.8</cell><cell>7.8</cell><cell>57.4</cell></row><row><cell>DAN [22]</cell><cell>68.1</cell><cell>15.4</cell><cell>76.5</cell><cell>87</cell><cell>71.1</cell><cell>48.9</cell><cell>82.3</cell><cell>51.5</cell><cell cols="5">88.7 33.2 88.9 42.2 61.1</cell></row><row><cell>MSTN* [42]</cell><cell>89.3</cell><cell>49.5</cell><cell cols="3">74.3 67.6 90.1</cell><cell>16.6</cell><cell>93.6</cell><cell>70.1</cell><cell cols="5">86.5 40.4 83.2 18.5 65.0</cell></row><row><cell>JAN [24]</cell><cell>75.7</cell><cell>18.7</cell><cell cols="3">82.3 86.3 70.2</cell><cell>56.9</cell><cell>80.5</cell><cell>53.8</cell><cell cols="5">92.5 32.2 84.5 54.5 65.7</cell></row><row><cell>DM-ADA [43]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.6</cell></row><row><cell>DMRL [41]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.5</cell></row><row><cell>MODEL [20]</cell><cell>94.8</cell><cell>73.4</cell><cell cols="3">68.8 74.8 93.1</cell><cell>95.4</cell><cell>88.6</cell><cell>84.7</cell><cell cols="5">89.1 84.7 83.5 48.1 81.6</cell></row><row><cell>STAR [25]</cell><cell>95</cell><cell>84</cell><cell>84.6</cell><cell>73</cell><cell>91.6</cell><cell>91.8</cell><cell>85.9</cell><cell>78.4</cell><cell cols="2">94.4 84.7</cell><cell>87</cell><cell cols="2">42.2 82.7</cell></row><row><cell>CAN [17]</cell><cell>97</cell><cell>87.2</cell><cell cols="3">82.5 74.3 97.8</cell><cell>96.2</cell><cell>90.8</cell><cell>80.7</cell><cell cols="5">96.6 96.3 87.5 59.9 87.2</cell></row><row><cell>FixBi (Ours)</cell><cell>96.1</cell><cell>87.8</cell><cell cols="3">90.5 90.3 96.8</cell><cell>95.3</cell><cell>92.8</cell><cell>88.7</cell><cell>97.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast generalized distillation for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-third Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>1998. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference on Computational Learning Theory (COLT</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domainspecific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Negative sampling in semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrillidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradually vanishing bridge for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by back propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spherical space domain adaptation with robust pseudo-label loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9101" to="9110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cycada: Cycleconsistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1711.03213</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with hierarchical gradient synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Online meta-learning for multisource and semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04398</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model adaptation: Unsupervised domain adaptation without source data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1502.0279*1</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fourth International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic classifiers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Virtual mixup training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04215</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visda: The visual domain adaptation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Opposite structure learning for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2002.02545</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and pertuations for dep semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via structurally regularized deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dual mixup regularized learning for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Roby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mico: Mixup co-training for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12684</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Domain-symmetric networks for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
