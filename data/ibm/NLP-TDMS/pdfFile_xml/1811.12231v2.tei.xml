<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-01-14">14 Jan 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
							<email>robert.geirhos@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
							<email>p.rubisch@sms.ed.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
							<email>claudio.michaelis@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<email>matthias.bethge@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
							<email>felix.wichmann@uni-tuebingen.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
							<email>wieland.brendel@bethgelab.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen &amp; IMPRS-IS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Tübingen &amp; U. of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Tübingen &amp; IMPRS-IS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-01-14">14 Jan 2019</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019 * Joint senior authors 1 Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on 'Stylized-ImageNet', a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Classification of a standard ResNet-50 of (a) a texture image (elephant skin: only texture cues); (b) a normal image of a cat (with both shape and texture cues), and (c) an image with a texture-shape cue conflict, generated by style transfer between the first two images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>How are Convolutional Neural Networks (CNNs) able to reach impressive performance on complex perceptual tasks such as object recognition <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref> and semantic segmentation <ref type="bibr" target="#b25">(Long et al., 2015)</ref>? One widely accepted intuition is that CNNs combine low-level features (e.g. edges) to increasingly complex shapes (such as wheels, car windows) until the object (e.g. car) can be readily classified. As <ref type="bibr" target="#b19">Kriegeskorte (2015)</ref> puts it, "the network acquires complex knowledge about the kinds of shapes associated with each category. <ref type="bibr">[...]</ref> High-level units appear to learn representations of shapes occurring in natural images" <ref type="bibr">(p. 429)</ref>. This notion also appears in other explanations, such as in <ref type="bibr" target="#b24">LeCun et al. (2015)</ref>: Intermediate CNN layers recognise "parts of familiar objects, and subsequent layers [...] detect objects as combinations of these parts" (p. 436). We term this explanation the shape hypothesis.</p><p>This hypothesis is supported by a number of empirical findings. Visualisation techniques like <ref type="bibr">Deconvolutional Networks (Zeiler &amp; Fergus, 2014)</ref> often highlight object parts in high-level CNN features. 1 Moreover, CNNs have been proposed as computational models of human shape perception by <ref type="bibr" target="#b21">Kubilius et al. (2016)</ref>, who conducted an impressive number of experiments comparing human and CNN shape representations and concluded that CNNs "implicitly learn representations of shape that reflect human shape perception" (p. 15). <ref type="bibr" target="#b29">Ritter et al. (2017)</ref> discovered that CNNs develop a so-called "shape bias" just like children, i.e. that object shape is more important than colour for object classification (although see <ref type="bibr" target="#b15">Hosseini et al. (2018)</ref> for contrary evidence). Furthermore, CNNs are currently the most predictive models for human ventral stream object recognition (e.g. ; and it is well-known that object shape is the single most important cue for human object recognition <ref type="bibr" target="#b22">(Landau et al., 1988)</ref>, much more than other cues like size or texture (which may explain the ease at which humans recognise line drawings or millennia-old cave paintings).</p><p>On the other hand, some rather disconnected findings point to an important role of object textures for CNN object recognition. CNNs can still classify texturised images perfectly well, even if the global shape structure is completely destroyed <ref type="bibr" target="#b1">Brendel &amp; Bethge, 2019)</ref>. <ref type="bibr">Conversely, standard</ref> CNNs are bad at recognising object sketches where object shapes are preserved yet all texture cues are missing <ref type="bibr" target="#b0">(Ballester &amp; de Araújo, 2016)</ref>. Additionally, two studies suggest that local information such as textures may actually be sufficient to "solve" ImageNet object recognition: <ref type="bibr" target="#b6">Gatys et al. (2015)</ref> discovered that a linear classifier on top of a CNN's texture representation (Gram matrix) achieves hardly any classification performance loss compared to original network performance. More recently, <ref type="bibr" target="#b1">Brendel &amp; Bethge (2019)</ref> demonstrated that CNNs with explicitly constrained receptive field sizes throughout all layers are able to reach surprisingly high accuracies on ImageNet, even though this effectively limits a model to recognising small local patches rather than integrating object parts for shape recognition. Taken together, it seems that local textures indeed provide sufficient information about object classes-ImageNet object recognition could, in principle, be achieved through texture recognition alone. In the light of these findings, we believe that it is time to consider a second explanation, which we term the texture hypothesis: in contrast to the common assumption, object textures are more important than global object shapes for CNN object recognition.</p><p>Resolving these two contradictory hypotheses is important both for the deep learning community (to increase our understanding of neural network decisions) as well as for the human vision and neuroscience communities (where CNNs are being used as computational models of human object recognition and shape perception). In this work we aim to shed light on this debate with a number of carefully designed yet relatively straightforward experiments. Utilising style transfer <ref type="bibr" target="#b7">(Gatys et al., 2016)</ref>, we created images with a texture-shape cue conflict such as the cat shape with elephant texture depicted in <ref type="figure">Figure 1c</ref>. This enables us to quantify texture and shape biases in both humans and CNNs. To this end, we perform nine comprehensive and careful psychophysical experiments comparing humans against CNNs on exactly the same images, totalling 48,560 psychophysical trials across 97 observers. These experiments provide behavioural evidence in favour of the texture hypothesis: A cat with an elephant texture is an elephant to CNNs, and still a cat to humans. Beyond quantifying existing biases, we subsequently present results for our two other main contributions:</p><p>AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet AlexNet 100 GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet GoogLeNet   changing biases, and discovering emergent benefits of changed biases. We show that the texture bias in standard CNNs can be overcome and changed towards a shape bias if trained on a suitable data set. Remarkably, networks with a higher shape bias are inherently more robust to many different image distortions (for some even reaching or surpassing human performance, despite never being trained on any of them) and reach higher performance on classification and object recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>In this section we outline the core elements of paradigm and procedure. Extensive details to facilitate replication are provided in the Appendix. Data, code and materials are available from this repository: https://github.com/rgeirhos/texture-vs-shape</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PSYCHOPHYSICAL EXPERIMENTS</head><p>All psychophysical experiments were conducted in a well-controlled psychophysical lab setting and follow the paradigm of <ref type="bibr" target="#b10">Geirhos et al. (2018)</ref>, which allows for direct comparisons between human and CNN classification performance on exactly the same images. Briefly, in each trial participants were presented a fixation square for 300 ms, followed by a 300 ms presentation of the stimulus image. After the stimulus image we presented a full-contrast pink noise mask (1/f spectral shape) for 200 ms to minimise feedback processing in the human visual system and to thereby make the comparison to feedforward CNNs as fair as possible. Subsequently, participants had to choose one of 16 entry-level categories by clicking on a response screen shown for 1500 ms. On this screen, icons of all 16 categories were arranged in a 4 × 4 grid. Those categories were airplane, bear, bicycle, bird, boat, bottle, car, cat, chair, clock, dog, elephant, keyboard, knife, oven and truck. Those are the so-called "16-class-ImageNet" categories introduced in <ref type="bibr" target="#b10">Geirhos et al. (2018)</ref>.</p><p>The same images were fed to four CNNs pre-trained on standard ImageNet, namely AlexNet <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref>, GoogLeNet <ref type="bibr" target="#b31">(Szegedy et al., 2015)</ref>, VGG-16 <ref type="bibr" target="#b30">(Simonyan &amp; Zisserman, 2015)</ref> and ResNet-50 <ref type="bibr" target="#b13">(He et al., 2015)</ref>. The 1,000 ImageNet class predictions were mapped to the 16 categories using the WordNet hierarchy (Miller, 1995)-e.g. ImageNet category tabby cat would be mapped to cat. In total, the results presented in this study are based on 48,560 psychophysical trials and 97 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DATA SETS (PSYCHOPHYSICS)</head><p>In order to assess texture and shape biases, we conducted six major experiments along with three control experiments, which are described in the Appendix. The first five experiments (samples visualised in <ref type="figure" target="#fig_0">Figure 2</ref>) are simple object recognition tasks with the only difference being the image features available to the participant:</p><p>Original 160 natural colour images of objects (10 per category) with white background. Right: ten examples of images with content/shape of left image and style/texture from different paintings. After applying AdaIN style transfer, local texture cues are no longer highly predictive of the target class, while the global shape tends to be retained. Note that within SIN, every source image is stylized only once.</p><p>Greyscale Images from Original data set converted to greyscale using skimage.color.rgb2gray. For CNNs, greyscale images were stacked along the colour channel.</p><p>Silhouette Images from Original data set converted to silhouette images showing an entirely black object on a white background (see Appendix A.6 for procedure).</p><p>Edges Images from Original data set converted to an edge-based representation using Canny edge extractor implemented in MATLAB.</p><p>Texture 48 natural colour images of textures (3 per category). Typically the textures consist of full-width patches of an animal (e.g. skin or fur) or, in particular for man-made objects, of images with many repetitions of the same objects (e.g. many bottles next to each other, see <ref type="figure" target="#fig_6">Figure 7</ref> in the Appendix).</p><p>It is important to note that we only selected object and texture images that were correctly classified by all four networks. This was made to ensure that our results in the sixth experiment on cue conflicts, which is most decisive in terms of the shape vs texture hypothesis, are fully interpretable.</p><p>In the cue conflict experiment we present images with contradictory features (see <ref type="figure">Figure 1</ref>) but still ask the participant to assign a single class. Note that the instructions to human observers were entirely neutral w.r.t. shape or texture ("click on the object category that you see in the presented image; guess if unsure. There is no right or wrong answer, we are interested in your subjective impression").</p><p>Cue conflict Images generated using iterative style transfer <ref type="bibr" target="#b7">(Gatys et al., 2016)</ref> between an image of the Texture data set (as style) and an image from the Original data set (as content). We generated a total of 1280 cue conflict images (80 per category), which allows for presentation to human observers within a single experimental session.</p><p>We define "silhouette" as the bounding contour of an object in 2D (i.e., the outline of object segmentation). When mentioning "object shape", we use a definition that is broader than just the silhouette of an object: we refer to the set of contours that describe the 3D form of an object, i.e. including those contours that are not part of the silhouette. Following Gatys et al. (2017), we define "texture" as an image (region) with spatially stationary statistics. Note that on a very local level, textures (according to this definition) can have non-stationary elements (such as a local shape): e.g. a single bottle clearly has non-stationary statistics, but many bottles next to each other are perceived as a texture: "things" become "stuff" <ref type="bibr">(Gatys et al., 2017, p. 178)</ref>. For an example of a "bottle texture" see <ref type="figure" target="#fig_6">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">STYLIZED-IMAGENET</head><p>Starting from ImageNet we constructed a new data set (termed Stylized-ImageNet or SIN) by stripping every single image of its original texture and replacing it with the style of a randomly selected painting through AdaIN style transfer <ref type="bibr" target="#b16">(Huang &amp; Belongie, 2017)</ref> (see examples in <ref type="figure" target="#fig_1">Figure 3</ref>) with a stylization coefficient of α = 1.0. We used Kaggle's Painter by Numbers data set 2 as a style source due to its large style variety and size (79,434 paintings). We used AdaIN fast style transfer rather than iterative stylization (e.g. Gatys et al., 2016) for two reasons: Firstly, to ensure that training on SIN and testing on cue conflict stimuli is done using different stylization techniques, such that the results do not rely on a single stylization method. Secondly, to enable stylizing entire Ima-geNet, which would take prohibitively long with an iterative approach. We provide code to create Stylized-ImageNet here: https://github.com/rgeirhos/Stylized-ImageNet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TEXTURE VS SHAPE BIAS IN HUMANS AND IMAGENET-TRAINED CNNS</head><p>Almost all object and texture images (Original and Texture data set) were recognised correctly by both CNNs and humans ( <ref type="figure" target="#fig_0">Figure 2</ref>). Greyscale versions of the objects, which still contain both shape and texture, were recognised equally well. When object outlines were filled in with black colour to generate a silhouette, CNN recognition accuracies were much lower than human accuracies. This was even more pronounced for edge stimuli, indicating that human observers cope much better with images that have little to no texture information. One confound in these experiments is that CNNs tend not to cope well with domain shifts, i.e. the large change in image statistics from natural images (on which the networks have been trained) to sketches (which the networks have never seen before).</p><p>We thus devised a cue conflict experiment that is based on images with a natural statistic but contradicting texture and shape evidence (see Methods). Participants and CNNs have to classify the images based on the features (shape or texture) that they most rely on. The results of this experiment are visualised in <ref type="figure" target="#fig_2">Figure 4</ref>. Human observers show a striking bias towards responding with the shape category (95.9% of correct decisions). 3 This pattern is reversed for CNNs, which show a clear bias towards responding with the texture category (VGG-16: 17.2% shape vs. 82.8% texture; GoogLeNet: 31.2% vs. 68.8%; AlexNet: 42.9% vs. 57.1%; ResNet-50: 22.1% vs. 77.9%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OVERCOMING THE TEXTURE BIAS OF CNNS</head><p>The psychophysical experiments suggest that ImageNet-trained CNNs, but not humans, exhibit a strong texture bias. One reason might be the training task itself: from <ref type="bibr" target="#b1">Brendel &amp; Bethge (2019)</ref> we know that ImageNet can be solved to high accuracy using only local information. In other words, it might simply suffice to integrate evidence from many local texture features rather than going through the process of integrating and classifying global shapes. In order to test this hypothesis we train a ResNet-50 on our Stylized-ImageNet (SIN) data set in which we replaced the object-related local texture information with the uninformative style of randomly selected artistic paintings.</p><p>A standard ResNet-50 trained and evaluated on Stylized-ImageNet (SIN) achieves 79.0% top-5 accuracy (see <ref type="table">Table 1</ref>). In comparison, the same architecture trained and evaluated on ImageNet (IN) achieves 92.9% top-5 accuracy. This performance difference indicates that SIN is a much harder task than IN since textures are no longer predictive, but instead a nuisance factor (as desired). Intriguingly, ImageNet features generalise poorly to SIN (only 16.4% top-5 accuracy); yet features learned on SIN generalise very well to ImageNet (82.6% top-5 accuracy without any fine-tuning).</p><p>In order to test wheter local texture features are still sufficient to "solve" SIN we evaluate the performance of so-called BagNets. Introduced recently by <ref type="bibr" target="#b1">Brendel &amp; Bethge (2019)</ref>, BagNets have a ResNet-50 architecture but their maximum receptive field size is limited to 9 × 9, 17 × 17 or 33 × 33  <ref type="table">(red circles)</ref> and ImageNet-trained networks AlexNet (purple diamonds), VGG-16 (blue triangles), GoogLeNet (turquoise circles) and ResNet-50 (grey squares). Shape vs. texture biases for stimuli with cue conflict (sorted by human shape bias). Within the responses that corresponded to either the correct texture or correct shape category, the fractions of texture and shape decisions are depicted in the main plot (averages visualised by vertical lines). On the right side, small barplots display the proportion of correct decisions (either texture or shape correctly recognised) as a fraction of all trials. Similar results for ResNet-152, DenseNet-121 and Squeezenet1 1 are reported in the Appendix, <ref type="figure" target="#fig_1">Figure 13</ref>. pixels. This precludes BagNets from learning or using any long-range spatial relationships for classification. While these restricted networks can reach high accuracies on ImageNet, they are unable to achieve the same on SIN, showing dramatically reduced performance with smaller receptive field sizes (such as 10.0% top-5 accuracy on SIN compared to 70.0% on ImageNet for a BagNet with receptive field size of 9 × 9 pixels). This is a clear indication that the SIN data set we propose does actually remove local texture cues, forcing a network to integrate long-range spatial information.</p><p>Most importantly, the SIN-trained ResNet-50 shows a much stronger shape bias in our cue conflict experiment ( <ref type="figure" target="#fig_4">Figure 5</ref>), which increases from 22% for a IN-trained model to 81%. In many categories the shape bias is almost as strong as for humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ROBUSTNESS AND ACCURACY OF SHAPE-BASED REPRESENTATIONS</head><p>Does the increased shape bias, and thus the shifted representations, also affect the performance or robustness of CNNs? In addition to the IN-and SIN-trained ResNet-50 architecture we here additionally analyse two joint training schemes:</p><p>•     <ref type="table" target="#tab_5">Table 2</ref>. This is in line with the intuition that for object detection, a shape-based representation is more beneficial than a texture-based representation, since the ground truth rectangles encompassing an object are by design aligned with global object shape.  Furthermore, we provide robustness results for our models tested on ImageNet-C, a comprehensive benchmark of 15 different image corruptions <ref type="bibr" target="#b14">(Hendrycks &amp; Dietterich, 2019)</ref>, in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness against distortions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>As noted in the Introduction, there seems to be a large discrepancy between the common assumption that CNNs use increasingly complex shape features to recognise objects and recent empirical findings which suggest a crucial role of object textures instead. In order to explicitly probe this question, we utilised style transfer <ref type="bibr" target="#b7">(Gatys et al., 2016)</ref> to generate images with conflicting shape and texture information. On the basis of extensive experiments on both CNNs and human observers in a controlled psychophysical lab setting, we provide evidence that unlike humans, ImageNet-trained CNNs tend to classify objects according to local textures instead of global object shapes. In combination with previous work which showed that changing other major object dimensions such as colour <ref type="bibr" target="#b10">(Geirhos et al., 2018)</ref>    all produce excellent results using standard CNNs, while CNNbased shape transfer seems to be very difficult <ref type="bibr" target="#b11">(Gokaslan et al., 2018)</ref>. CNNs can still recognise images with scrambled shapes <ref type="bibr" target="#b1">Brendel &amp; Bethge, 2019)</ref>, but they have much more difficulties recognising objects with missing texture information <ref type="bibr" target="#b0">(Ballester &amp; de Araújo, 2016;</ref><ref type="bibr" target="#b35">Yu et al., 2017)</ref>. Our hypothesis might also explain why an image segmentation model trained on a database of synthetic texture images transfers to natural images and videos <ref type="bibr" target="#b32">(Ustyuzhaninov et al., 2018)</ref>. Beyond that, our results show marked behavioural differences between ImageNet-trained CNNs and human observers. While both human and machine vision systems achieve similarly high accuracies on standard images <ref type="bibr" target="#b10">(Geirhos et al., 2018)</ref>, our findings suggest that the underlying classification strategies might actually be very different. This is problematic, since CNNs are being used as computational models for human object recognition (e.g. .</p><p>In order to reduce the texture bias of CNNs we introduced Stylized-ImageNet (SIN), a data set that removes local cues through style transfer and thereby forces networks to go beyond texture recognition. Using this data set, we demonstrated that a ResNet-50 architecture can indeed learn to recognise objects based on object shape, revealing that the texture bias in current CNNs is not by design but induced by ImageNet training data. This indicates that standard ImageNet-trained models may be taking a "shortcut" by focusing on local textures, which could be seen as a version of Occam's razor: If textures are sufficient, why should a CNN learn much else? While texture classification may be easier than shape recognition, we found that shape-based features trained on SIN generalise well to natural images.</p><p>Our results indicate that a more shape-based representation can be beneficial for recognition tasks that rely on pre-trained ImageNet CNNs. Furthermore, while ImageNet-trained CNNs generalise poorly towards a wide range of image distortions (e.g. <ref type="bibr" target="#b3">Dodge &amp; Karam, 2017;</ref><ref type="bibr" target="#b9">Geirhos et al., 2017;</ref>, our ResNet-50 trained on Stylized-ImageNet often reaches or even surpasses human-level robustness (without ever being trained on the specific image degradations). This is exciting because <ref type="bibr" target="#b10">Geirhos et al. (2018)</ref> showed that networks trained on specific distortions in general do not acquire robustness against other unseen image manipulations. This emergent behaviour highlights the usefulness of a shape-based representation: While local textures are easily distorted by all sorts of noise (including those in the real world, such as rain and snow), the object shape remains relatively stable. Furthermore, this finding offers a compellingly simple explanation for the incredible robustness of humans when coping with distortions: a shape-based representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In summary, we provided evidence that machine recognition today overly relies on object textures rather than global object shapes as commonly assumed. We demonstrated the advantages of a shapebased representation for robust inference (using our Stylized-ImageNet data set 5 to induce such a representation in neural networks). We envision our findings as well as our openly available model weights, code and behavioural data set (49K trials across 97 observers) 6 to achieve three goals: Firstly, an improved understanding of CNN representations and biases. Secondly, a step towards more plausible models of human visual object recognition. Thirdly, a useful starting point for future undertakings where domain knowledge suggests that a shape-based representation may be more beneficial than a texture-based one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 REPRODUCIBILITY &amp; ACCESS TO CODE / MODELS / DATA</head><p>In this Appendix, we report experimental details for human and CNN experiments. All trained model weights reported in this paper as well as our human behavioural data set (48,560 psychophysical trials across 97 observers) are openly available from this repository: https://github.com/rgeirhos/texture-vs-shape</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROCEDURE</head><p>We followed the paradigm of <ref type="bibr" target="#b10">Geirhos et al. (2018)</ref> for maximal comparability. A trial consisted of 300 ms presentation of a fixation square and a 200 ms presentation of the stimulus image, which was followed by a full-contrast pink noise mask (1/f spectral shape) of the same size lasting for 200 ms. Participants had to choose one of 16 entry-level categories by clicking on a response screen shown for 1500 ms. On this screen, icons of all 16 categories were arranged in a 4 × 4 grid. The experiments were not self-paced and therefore one trial always lasted 2200 ms (300 ms + 200 ms + 200 ms + 1500 ms = 2200 ms). The necessary time to complete an experiment with 1280 stimuli was 47 minutes, for 160 stimuli six minutes, and for 48 stimuli two minutes. In the experiments with 1280 trials, observers were given the possibility of taking a brief break after every block of 256 trials (five blocks in total).</p><p>As preparation, participants were shown the response screen prior to an experiment and were asked to name all 16 categories in order to get an overview over the possible stimuli categories and to make sure that all categories were clear from the beginning. They were instructed to click on the category they believed was presented. Responses through clicking on a response screen could be changed within the 1500 ms response interval, only the last entered response was counted as the answer. Prior to the real experiment a practice session was performed for the participants to get used to the time course of the experiment and the position of category items on the response screen. This screen was shown for an additional 300 ms in order to provide feedback and indicate whether the entered answer was incorrect. In that case, a short low beep sound occurred and the correct category was highlighted by setting its background to white. The practice session consisted of 320 trials. After 160 trials the participants had the chance to take a short break. In the break, their performance of the first block was shown on the screen along the percentage of trials where no answer was entered. After the practice blocks, observers were shown an example image of the manipulation (not used in the experiment) to minimise surprise. Images used in the practice session were natural images from 16-class-ImageNet <ref type="bibr" target="#b10">(Geirhos et al., 2018)</ref>, hence there was no overlap with images or manipulations used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 APPARATUS</head><p>Observers were shown the 224×224 pixels stimuli in a dark cabin on a 22", 120 Hz VIEWPixx LCD monitor (VPixx Technologies, Saint-Bruno, Canada). The screen of size 484 × 302 mm corresponds to 1920 × 1200 pixels, although stimuli were only presented foveally at the center of the screen (3 × 3 degrees of visual angle at a viewing distance of 107 cm) while the background was set to a grey value of 0.7614 in the  experiments with a cue conflict manipulation were paid e 10 per hour or gained course credit. Observers measured in all other experiments (with a clear ground truth category) were able to earn an additional bonus up to e 5 or equivalent further course credit based on their performance. This motivation scheme was applied to ensure reliable answer rates, and explained to observers in advance. Participant bonus, in these cases, was calculated as follows: The base level with a bonus of e 0 was set to 50% accuracy. For every additional 5% of accuracy, participants gained a e 0.50 bonus. This means that with a performance above 95%, an observer was able to gain the full bonus of e 5 or equivalent course credit. Overall, we took the following steps to prevent low quality human data: 1., using a controlled lab environment instead of an online crowdsourcing platform; 2. the payment motivation scheme as explained above; 3. displaying observer performance on the screen at regular intervals during the practice session; and 4. splitting longer experiments into five blocks, where participants could take a break in between blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 CNN MODELS &amp; TRAINING DETAILS</head><p>ResNet-50 We used a standard ResNet-50 architecture from PyTorch <ref type="bibr" target="#b27">(Paszke et al., 2017)</ref>, the torchvision.models.resnet50 implementation. For the comparison against BagNets reported in <ref type="table">Table 1</ref>, results for IN training correspond to a ResNet-50 pre-trained on ImageNet without any modifications (model weights from torchvision.models). Reported results for SIN training correspond to the same architecture trained on SIN for 60 epochs with Stochastic Gradient Descent (torch.optim.SGD) using a momentum term of 0.9, weight decay (1e-4) and a learning rate of 0.1 which was multiplied by a factor of 0.1 after 20 and 40 epochs of training. We used a batch size of 256. This SIN-trained model is the same model that is reported in <ref type="figure" target="#fig_4">Figures 5 and 6</ref> as well as in <ref type="table" target="#tab_5">Table 2</ref>. In the latter, this corresponds to the second row (training performed on SIN, no fine-tuning on ImageNet). For the model reported in the third row, training was jointly performed on SIN and on IN. This means that both training data sets were treated as one big data set (exactly twice the size of the IN training data set), on which training was performed for 45 epochs with identical hyperparameters as described above, except that the initial learning rate of 0.1 was multiplied by 0.1 after 15 and 30 epochs. The weights of this model were then used to initialise the model reported in the fourth row of <ref type="table" target="#tab_5">Table 2</ref>, which was fine-tuned for 60 epochs on ImageNet (identical hyperparameters except that the initial learning rate of 0.01 was multiplied by 0.1 after 30 epochs). We compared training models from scratch versus starting from an ImageNet-pretrained model. Empirically, using features pre-trained on ImageNet led to better results across experiments, which is why we used ImageNet pre-training throughout experiments and models (for both ResNet-50 and restricted ResNet-50 models).</p><p>BagNets Model weights (pre-trained on ImageNet) and architectures for BagNets (results reported in <ref type="table">Table 1</ref>) were kindly provided by <ref type="bibr" target="#b1">Brendel &amp; Bethge (2019)</ref>. For SIN training, identical settings as for the SIN-trained ResNet-50 were used to ensure comparability (training for 60 epochs with SGD and identical hyperparameters as reported above).</p><p>Faster R-CNN We used the Faster R-CNN implementation from https://github.com/ jwyang/faster-rcnn.pytorch (commit 21f289867d09f410631f1b9b7b46ce8c1e81ae07) with all hyperparameters kept at default. The only change we made to the model is replacing the encoder with a ResNet-50 and applying custom input whitening. We trained this model on Pascal VOC 2007 for 7 epochs with a batch size of 1, a learning rate of 0.001 and a learning rate decay step after epoch 5.</p><p>Pre-trained AlexNet, GoogLeNet, VGG-16 We used AlexNet <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref>, GoogLeNet <ref type="bibr" target="#b31">(Szegedy et al., 2015)</ref> and VGG-16 <ref type="bibr" target="#b30">(Simonyan &amp; Zisserman, 2015)</ref> for the evaluation reported in <ref type="figure" target="#fig_2">Figure 4</ref>. Evaluation was performed using Caffe <ref type="bibr" target="#b17">(Jia et al., 2014)</ref>. Network weights (training on ImageNet) were obtained from https://github.com/BVLC/caffe/ wiki/Model-Zoo (AlexNet &amp; GoogLeNet) and http://www.robots.ox.ac.uk/ (VGG-16).</p><p>ResNet-101 pre-trained on Open Images V2 For our comparison of biases in ImageNet vs. OpenImages <ref type="figure" target="#fig_1">(Figure 13 right)</ref> the ResNet-101 pretrained on Open Images V2 <ref type="bibr" target="#b18">(Krasin et al., 2017)</ref> was used. It was obtained from https://github.com/openimages/dataset/blob/ master/READMEV2.md along with the inference code provided by the authors. In order to map predictions to the 16 classes, we used the parameters top k = 100000 and score threshold = 0.0 to obtain as all predictions, and then mapped the responses to our 16 classes using the provided label map. 15 out of our 16 classes are classes in Open Images as well; the remaining class keyboard was mapped to Open Images class computer keyboard (in this case, Open Images makes a finer distinction to separate musical keyboards from computer keyboards).</p><p>ResNet-101, ResNet-152, DenseNet-121, SqueezeNet1 1 For the comparison to other models pre-trained on ImageNet <ref type="figure" target="#fig_1">(Figure 13 left)</ref>, we evaluated the pre-trained networks provided by torchvision.models.</p><p>Training AlexNet, VGG-16 on SIN For the evaluation of model biases after training on SIN <ref type="figure">(Figure 11)</ref>, we obtained the model architectures from torchvision.models and trained the networks under identical circumstances as ResNet-50. This includes identical hyperparameter settings, except for the learning rate. The learning rate for AlexNet was set to 0.001 and for VGG-16 to 0.01 initially; both learning rates were multiplied by 0.1 after 20 and 40 epochs of training (60 epochs in total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 IMAGE MANIPULATIONS AND IMAGE DATABASE</head><p>In total, we conducted nine different experiments. Here is an overview of the images and / or image manipulations for all of them. All images were saved in the png format and had a size of 224 × 224 pixels. Original, texture and cue conflict images are visualised in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>Original experiment This experiment consisted of 160 coloured images, 10 per category. All of them had a single, unmanipulated object (belonging to one category) in front of a white background. This white background was especially important since these stimuli were being used as content images for style transfer, and we thus made sure that the background was neutral to produce better style transfer results. The images for this experiment as well as for the texture experiment described below were carefully selected using Google advanced image search with the criteria "labelled for noncommercial reuse with modification (free to use, share and modify)" and the search term "&lt;entity&gt; white background" (original) or "&lt;entity&gt; texture" (texture). In some cases where this did not lead to sufficient results, we used images from the ImageNet validation data set which were manually modified to have a white background if necessary. We made sure that both the images from this experiment as well as the texture images were all correctly recognised by all four pre-trained CNNs (if an image was not correctly recognised, we replaced it by another one). This was used to ensure that our results for cue conflict experiments are fully interpretable: if, e.g., a texture image was not correctly recognised by CNNs, there would be no point in using it as a texture (style) source for style transfer.</p><p>Greyscale experiment This experiment used the same images as the original experiment with the difference that they were converted to greyscale using skimage.color.rgb2gray. For CNNs, greyscale images were stacked three times along the colour channel.</p><p>Silhouette experiment The images from the original experiment were transformed into silhouette images showing an entirely black object on a white background. We used the following transformation procedure: First, images were converted to bmp using command line utility (convert). They were then converted to svg using potrace, and then to png using convert again. Since an entirely automatic binarization pipeline is not feasible (it takes domain knowledge to understand that a car wheel should, but a doughnut should not be filled with black colour), we then manually checked every single image and adapted the silhouette using GIMP if necessary.</p><p>Edge experiment The stimuli shown in this condition were generated by applying the "Canny" edge extractor implemented in MATLAB (Release 2016a, The MathWorks, Inc., Natick, Massachusetts, United States) to the images used in the original experiment. No further manipulations were performed on this data set. This line of code was used to detect edges and generate the stimuli used in this experiment: imwrite(1-edge(imgaussfilt(rgb2gray(imread(filename)), 2), 'Canny'), targetFilename);</p><p>Texture experiment Images were selected using the procedure outlined above for the original experiment. Some objects have a fairly stationary texture (e.g. animals), which makes it easy to find texture images for them. For the more difficult case (e.g. man-made objects), we made use of the fact that every object can become a texture if it is used not in isolation, but rather in a clutter of many objects of the same kind (e.g. . That is, for a bottle texture we used images with many bottles next to each other (as visualised in <ref type="figure" target="#fig_6">Figure 7)</ref>.</p><p>Cue conflict experiment This experiment used images with a texture-shape cue conflict. They were generated using iterative style transfer <ref type="bibr" target="#b7">(Gatys et al., 2016)</ref> between a texture image (from the texture experiment described above) and a content image (from the original experiment) each. While 48 texture images and 160 content images would allow for a total of 48 × 160 = 7680 cue conflict images (480 per category), we used a balanced subset of 1280 images instead (80 per category), which allows for presentation to human observers within a single experimental session. The procedure for selecting the style and content images was done as follows. For all possible 16 × 16 combinations of style and texture categories, exactly five cue conflict images were generated by randomly sampling style and content images from their respective categories. Sampling was performed without replacement for as long as possible, and then without replacement for the remaining images. The same stimuli acquired with this method were used for the cue conflict control experiments, where participants saw exactly these images but with different instructions biased towards shape and towards texture (results described later). For our analysis of texture vs. shape biases <ref type="figure" target="#fig_2">(Figure 4)</ref>, we excluded trials for which no cue conflict was present (i.e., those trials where a bicycle content image was fused with a bicycle texture image, hence no texture-shape cue conflict present).</p><p>Filled silhouette experiment Style transfer is not the only possibility to generate a texture-shape cue conflict, and we here aimed at testing one other method to generate such stimuli: cropping texture images with a shape mask, such that the silhouette of an object and its texture constitute a cue conflict (visualised in <ref type="figure" target="#fig_6">Figure 7</ref>). Stimuli were generated by using the silhouette images from the silhouette experiment as a mask for texture images. If the silhouette image at a certain location has a black pixel, the texture was used at this location, and for white pixels the resulting target image pixel was white. In order to have a larger variety of textures than the 48 textures used in the texture experiment, the texture database was augmented by rotating all textures with ten different previously chosen angles uniformly distributed between 0 and 360 degrees, resulting in a texture database of 480 images. Results for this control experiment, not part of the main paper, are reported later. We ensured that no silhouette was seen more than once per observer.</p><p>Robustness experiment (distorted images) For this experiment, human accuracies for reference were provided by <ref type="bibr" target="#b10">Geirhos et al. (2018)</ref>  images as outlined in the paper. For maximal comparability, we also used the same images. For a description of the parametric distortion we kindly refer the reader to <ref type="bibr" target="#b10">Geirhos et al. (2018)</ref>. In <ref type="figure" target="#fig_7">Figure 8</ref>, we plot one example image across manipulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 STYLIZED-IMAGENET (SIN)</head><p>We used AdaIN style transfer <ref type="bibr" target="#b16">(Huang &amp; Belongie, 2017)</ref>  We investigated the effect of different instructions to human observers. The results presented in the main paper for cue conflict stimuli correspond all to a neutral instruction, not biased w.r.t. texture or shape. In two separate experiments, participants were explicitly instructed to ignore the textures and click on the shape category of cue conflict stimuli, and vice versa. The results, presented in <ref type="figure">Figure 10</ref>, indicate that for a shape bias instruction, human data are almost exactly the same as for the neutral instruction reported earlier (indicating that human observers are indeed using shapes per default); and if they are instructed to ignore the shapes and click on the texture category, they still show a substantial shape bias (indicating that even if they seek to ignore shapes, they find it extremely difficult to do so).</p><p>A.9 RESULTS: FILLED SILHOUETTE EXPERIMENT This experiment was conducted as a control experiment to make sure that the strong differences between humans and CNNs when presented with cue conflict images are not merely an artefact of the particular setup that we employed. Stimuli are visualised in <ref type="figure" target="#fig_6">Figure 7</ref>; results in <ref type="figure" target="#fig_0">Figure 12</ref>. In a    nutshell, we also find a shape bias in humans when stimuli are not generated via style transfer but instead through cropping texture images with a shape mask, such that the silhouette of an object and its texture constitute a cue conflict. CNNs have a less pronounced texture bias in these experiments; ResNet-50 trained on SIN still responds with the shape category more than ResNet-50 trained on IN. Overall, these results are much more difficult to interpret since the texture-silhouette cue conflict stimuli, visualised in <ref type="figure" target="#fig_6">Figure 7</ref>, do not have a clear-cut texture-shape distinction like the cue conflict stimuli generated via style transfer. Still, they are largely in accord with the style transfer results presented in the main paper.  <ref type="figure" target="#fig_1">Figure 13</ref>: The texture bias on cue conflict stimuli is not specific to ImageNet-trained networks (left) and also occurs in very deep, wide and compressed networks (right). Left: The texture bias is not specific to ImageNet-trained networks. Comparison of texture-shape biases on cue conflict stimuli generated with style transfer for ResNet-101 trained on ImageNet (grey squares) and ResNet-101 trained on the Open Images Dataset V2 (green squares) along with human data for comparison (red circles). Both networks have a qualitatively similar texture bias. We use a ResNet-101 architecture here since Open Images has released a pre-trained ResNet-101. Right: The texture bias also appears in a very deep network (ResNet-152, grey squares), a very wide one (DenseNet-121, purple trianlges), and a very compact one (SqueezeNet1 1, brown diamonds). Human data for comparison (red circles). All networks are pre-trained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans Humans</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Accuracies and example stimuli for five different experiments without cue conflict.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualisation of Stylized-ImageNet (SIN), created by applying AdaIN style transfer to ImageNet images. Left: randomly selected ImageNet image of class ring-tailed lemur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Classification results for human observers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Shape vs. texture biases for stimuli with a texture-shape cue conflict after training ResNet-50 on Stylized-ImageNet (orange squares) and on ImageNet (grey squares). Plotting conventions and human data (red circles) for comparison are identical toFigure 4. Similar results for other networks are reported in the Appendix, Figure 11. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Classification accuracy on parametrically distorted images. ResNet-50 trained on Stylized-ImageNet (SIN) is more robust towards distortions than the same network trained on ImageNet (IN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>. Human 'error bars' indicate the full range of results for human observers. CNNs were then evaluated on different image manipulations applied to natural original content images original texture images cue conflict (filled silhouettes) cue conflict (style transfer) Visualisation of stimuli in data sets. Top two rows: content and texture images. Bottom rows: cue conflict stimuli generated from the texture and content images above (silhouettes filled with rotated textures; style transfer stimuli).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Visualisation of image distortions. One exemplary image (class bird, original image in colour at the top left) is manipulated as follows. From left to right: additive uniform noise, low contrast, high-pass filtering, low-pass filtering. In the row below, a greyscale version for comparison; the other manipulations from left to right are: Eidolon manipulations I, II and III as well as phase noise. Figure adapted from Geirhos et al. (2018) with the authors' permission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :Figure 11 :Figure 12 :</head><label>9101112</label><figDesc>Accuracies and example stimuli for five different experiments without cue conflict, comparing training on ImageNet (IN) to training on Stylized-ImageNet (SIN). Classification results for human observers (red circles) and ImageNet-trained networks AlexNet (purple diamonds), VGG-16 (blue triangles), GoogLeNet (turquoise circles) and ResNet-50 (grey squares) on stimuli with a texture-shape cue conflict generated with style transfer, and biased rather than neutral instructions to human observers. Plotting conventions and CNN data as inFigure 4. Texture vs shape biases on of AlexNet and VGG-16 after training on Stylized-ImageNet. Plotting conventions as in Figures 4 and 5. Plot shows biases for AlexNet (purple diamonds), VGG-16 (blue triangles) and human observers (red circles) for comparison. For GoogLeNet, no data is available since network training was performed in PyTorch and torchvision.models unfortunately does not provide a GoogLeNet (inception v1) architecture. Classification results for human observers and CNNs on stimuli with a texture-silhouette cue conflict (filled silhouette experiment). Plotting conventions as in Figures 4 and 5. Left: Human observers (red circles) and ImageNet-trained networks AlexNet (purple diamonds), VGG-16 (blue triangles), GoogLeNet (turquoise circles) and ResNet-50 (grey squares). Right: Human observers (red circles, data identical to the left) and ResNet-50 trained on ImageNet (grey squares) vs. ResNet-50 trained on Stylized-ImageNet (orange squares).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>A</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>100 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 VGG−16 100 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 ResNet−50 100</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Training jointly on SIN and IN. • Training jointly on SIN and IN with fine-tuning on IN. We refer to this model as Shape-ResNet.</figDesc><table><row><cell>architecture</cell><cell cols="4">IN→IN IN→SIN SIN→SIN SIN→IN</cell></row><row><cell>ResNet-50</cell><cell>92.9</cell><cell>16.4</cell><cell>79.0</cell><cell>82.6</cell></row><row><cell>BagNet-33 (mod. ResNet-50)</cell><cell>86.4</cell><cell>4.2</cell><cell>48.9</cell><cell>53.0</cell></row><row><cell>BagNet-17 (mod. ResNet-50)</cell><cell>80.3</cell><cell>2.5</cell><cell>29.3</cell><cell>32.6</cell></row><row><cell>BagNet-9 (mod. ResNet-50)</cell><cell>70.0</cell><cell>1.4</cell><cell>10.0</cell><cell>10.9</cell></row></table><note>Table 1: Stylized-ImageNet cannot be solved with texture features alone. Accuracy comparison (in percent; top-5 on validation data set) of a standard ResNet-50 with Bag of Feature networks (BagNets) with restricted receptive field sizes of 33×33, 17×17 and 9×9 pixels. Arrows indicate: train data→test data, e.g. IN→SIN means training on ImageNet and testing on Stylized-ImageNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Accuracy comparison on the ImageNet (IN) validation data set as well as object detection performance (mAP50) on PASCAL VOC 2007. All models have an identical ResNet-50 architecture. Method details reported in the Appendix. We then compared these models with a vanilla ResNet-50 on three experiments: (1) classification performance on IN, (2) transfer to Pascal VOC 2007 and (3) robustness against image perturbations.Classification performance Shape-ResNet surpasses the vanilla ResNet in terms of top-1 and top-5 ImageNet validation accuracy as reported inTable 2. This indicates that SIN may be a useful data augmentation on ImageNet that can improve model performance without any architectural changes.Transfer learning We tested the representations of each model as backbone features for Faster R-CNN (Ren et al., 2017) on Pascal VOC 2007. Incorporating SIN in the training data substantially improves object detection performance from 70.7 to 75.1 mAP50 as shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>We systematically tested how model accuracies degrade if images are distorted by uniform or phase noise, contrast changes, high-and low-pass filtering or eidolon perturbations.4  The results of this comparison, including human data for reference, are visualised inFigure 6. While lacking a few percent accuracy on undistorted images, the SIN-trained network outperforms the IN-trained CNN on almost all image manipulations. (Low-pass filtering / blurring is the only distortion type on which SIN-trained networks are more susceptible, which might be due to the over-representation of high frequency signals in SIN through paintings and the reliance on sharp edges.) The SIN-trained ResNet-50 approaches human-level distortion robustness-despite never seeing any of the distortions during training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>of the Appendix. Training jointly on SIN and IN leads to strong improvements for 13 corruption types (Gaussian, Shot and Impulse noise; Defocus, Glas and Motion blur; Snow, Frost and Fog weather types; Contrast, Elastic, Pixelate and JPEG digital corruptions). This substantially reduces overall corruption error from 76.7 for a vanilla ResNet-50 to 69.3. Again, none of these corruption types were explicitly part of the training data, reinforcing that incorporating SIN in the training regime improves model robustness in a very general way.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>and object size relative to the context<ref type="bibr" target="#b4">(Eckstein et al., 2017)</ref> do not have a strong detrimental impact on CNN recognition performance, this highlights the special role that local cues such as textures seem to play in CNN object recognition.Intriguingly, this offers an explanation for a number of rather disconnected findings: CNNs match texture appearance for humans<ref type="bibr" target="#b33">(Wallis et al., 2017)</ref>, and their predictive power for neural responses along the human ventral stream appears to be largely due to human-like texture representations, but not human-like contour representations<ref type="bibr" target="#b23">(Laskar et al., 2018;</ref> Long &amp; Konkle, 2018). Furthermore, texture-based generative modelling approaches such as style transfer<ref type="bibr" target="#b7">(Gatys et al., 2016)</ref>, single image super-resolution<ref type="bibr" target="#b12">(Gondal et al., 2018)</ref> as well as static and dynamic texture synthesis (Gatys</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Characteristics of human participants (p.) across experiments. The symbol '#' refers to</cell></row><row><cell>"number of"; 'rt' stands for "median reaction time (ms)" in an experiment.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>ImageNet training and validation data sets. Style transfer was performed once per ImageNet image. As a style source, we used images from Kaggle's Painter by Numbers data set (https://www.kaggle.com/c/painter-by-numbers/, accessed on March 1, 2018). Style selection was performed randomly with replacement. Every ImageNet image was stylized once and only once. Paintings from the Kaggle data set were used if at least 224 × 224 pixels in size; the largest possible square crop was then downsampled to this size prior to using it as a style image. All accuracies are reported on the respective validation data sets. Code to generate Stylized-ImageNet from ImageNet (and the Kaggle paintings) is available on github in this repository: https://github.com/rgeirhos/Stylized-ImageNet A.8 RESULTS: CUE CONFLICT CONTROL EXPERIMENTS(DIFFERENT INSTRUCTIONS)</figDesc><table><row><cell>to generate Stylized-ImageNet.</cell></row><row><cell>More specifically, the AdaIN implementation from https://github.com/naoto0804/</cell></row><row><cell>pytorch-AdaIN (commit 31e769c159d4c8639019f7db7e035a7f938a6a46) was employed to</cell></row><row><cell>stylize the entire</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 4 :</head><label>4</label><figDesc>Corruption error (lower=better) on ImageNet-C (Hendrycks &amp; Dietterich, 2019), consisting of different types of noise, blur, weather and digital corruptions. Abbreviations: mCE = mean Corruption Error (average of the 15 individual corruption error values); SIN = Stylized-ImageNet; IN = ImageNet; ft = fine-tuning. Results kindly provided by Dan Hendrycks.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To avoid any confusion caused by different meanings of the term 'feature', we consistently use it to refer to properties of CNNs (learned features) rather than to object properties (such as colour). When referring to physical objects, we use the term 'cue' instead.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/painter-by-numbers/ (accessed on March 1, 2018). 3 It is important to note that a substantial fraction of the images (automatically generated with style transfer between randomly selected object image and texture image) seemed hard to recognise for both humans and CNNs, as depicted by the fraction of incorrect classification choices inFigure 4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Our comparison encompasses all distortions reported by<ref type="bibr" target="#b10">Geirhos et al. (2018)</ref> with more than five different levels of signal strength. Data from human observers included with permission from the authors (see appendix).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Available from https://github.com/rgeirhos/Stylized-ImageNet 6 Available from https://github.com/rgeirhos/texture-vs-shape</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work has been funded, in part, by the German Research Foundation (DFG; Sachbeihilfe Wi 2103/4-1 and SFB 1233 on "Robust Vision"). The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting R.G. and C.M.; M.B. acknowledges support by the Centre for Integrative Neuroscience Tübingen (EXC 307) and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003.</p><p>We would like to thank Dan Hendrycks for providing the results of <ref type="table">Table 4</ref> (corruption robustness of our models on ImageNet-C). Furthermore, we would like to express our gratitude towards Alexander Ecker, Leon Gatys, Tina Gauger, Silke Gramer, Heike König, Jonas Rauber, Steffen Schneider, Heiko Schütt, Tom Wallis and Uli Wannek for support and/or useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the performance of GoogLeNet and AlexNet applied to sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Matsumura De Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1124" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks rival the representation of primate IT cortex for core visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles F Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D L K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E A</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J J</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Karam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02498</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Humans, but not deep neural networks, often miss giant targets in scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Miguel P Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><forename type="middle">E</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Welbourne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2827" to="2832" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Christina M Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07006</idno>
		<title level="m">Synthesising dynamic textures using convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture and art with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="178" to="186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06969</idno>
		<title level="m">Comparing deep neural networks against humans: object recognition when the signal gets weaker</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">M</forename><surname>Medina Temme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08750</idno>
		<title level="m">Generalisation in humans and deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving shape deformation in unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04325</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of texture transfer for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Muhammad W Gondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00043</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Assessing shape bias property of Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baicen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayoore</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07739</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">OpenImages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhyanesh</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://github.com/openimages" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep neural networks: A new framework for modeling biological vision and brain information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Vision Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="417" to="446" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep neural networks as a computational model for human shape sensitivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefania</forename><surname>Bracci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans P Op De</forename><surname>Beeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1004896</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The importance of shape in early lexical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Development</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="321" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Nasir Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis G Sanchez</forename><surname>Laskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odelia</forename><surname>Giraldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02888</idno>
		<title level="m">Correspondence of deep neural networks and the brain for visual textures</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bria Long and Talia Konkle. The role of textural statistics vs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://konklab.fas.harvard.edu/ConferenceProceedings/Long_2018_CCN.pdf" />
	</analytic>
	<monogr>
		<title level="m">deep CNN and neural responses to objects</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
	<note>Deep learning</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">M</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08606</idno>
		<title level="m">Cognitive psychology for deep neural networks: A shape bias case study</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">One-shot texture segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ustyuzhaninov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02654</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A parametric texture model based on deep convolutional features closely matches texture appearance for humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><forename type="middle">M</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5" to="5" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sketcha-net: A deep neural network that beats humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="425" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ResNet−</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>SIN</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">/wiki/File:Liquor_bottles.jpg, released under the CC BY 2.0 license by user scottfeldstein as indicated on the website. The CC BY 2</title>
		<ptr target="https://creativecommons.org/licenses/by/2.0/legalcode" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
