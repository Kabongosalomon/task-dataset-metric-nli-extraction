<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-granularity Generator for Temporal Action Proposal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent AI Lab Southeast University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-granularity Generator for Temporal Action Proposal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action proposal generation is an important task, aiming to localize the video segments containing human actions in an untrimmed video. In this paper, we propose a multi-granularity generator (MGG) to perform the temporal action proposal from different granularity perspectives, relying on the video visual features equipped with the position embedding information. First, we propose to use a bilinear matching model to exploit the rich local information within the video sequence. Afterwards, two components, namely segment proposal producer (SPP) and frame actionness producer (FAP), are combined to perform the task of temporal action proposal at two distinct granularities. SPP considers the whole video in the form of feature pyramid and generates segment proposals from one coarse perspective, while FAP carries out a finer actionness evaluation for each video frame. Our proposed MGG can be trained in an end-to-end fashion. By temporally adjusting the segment proposals with fine-grained frame actionness information, MGG achieves the superior performance over state-of-the-art methods on the public THUMOS-14 and ActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to perform the classification of the proposals generated by MGG, leading to significant improvements compared against the competing methods for the video detection task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action proposal <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> aims at capturing video temporal intervals that are likely to contain an action in an untrimmed video. This task plays an important role in video analysis and can thus be applied in many areas, such as action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, summarization <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>, grounding <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and captioning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. Many methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref> have been proposed to handle this task, and have shown that, akin to object proposals for object detec- <ref type="figure">Figure 1</ref>: Our proposed MGG can generate segment proposals and frame actionness simultaneously, which helps discover information about possible actions at both the coarse and fine levels. By temporally adjusting the boundaries of the segment within the search space determined by the computed frame actionness, MGG can yield refined action proposals with both high recall and precision. tion <ref type="bibr" target="#b29">[30]</ref>, temporal action proposal has a crucial impact on the quality of action detection.</p><p>High-quality action proposal methods should capture temporal action instances with both high recall and high temporal overlapping with ground-truths, meanwhile producing proposals without many false alarms. One type of existing methods focuses on generating segment proposals <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>, where the initial segments are regularly distributed or manually defined over the video sequence. A binary classier is thereafter trained to evaluate the confidence scores of the segments. Such methods are able to generate proposals of various temporal spans. However, since the segments are regularly distributed or manually defined, the generated proposals naturally have imprecise boundary information, even though boundary regressors are further applied. Another thread of work, like <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref>, tackles the action proposal task in the form of evaluating frame actionness. These methods densely evaluate the confidence score for each frame and group consecutive frames together as candidate proposals. The whole video sequence is analyzed at a finer level, in contrast with the segment proposal based methods. As a result, the boundaries of the generated proposals are of high precision. However, such methods often produce low confidence scores for long video segments, resulting in misses of true action segments and thus low recalls.</p><p>Obviously, these two types of methods are complementary to each other. Boundary sensitive network (BSN) <ref type="bibr" target="#b25">[26]</ref> adopts a "local to global" scheme for action proposal, which locally detects the boudary information and globally ranks the candidate proposals. Complementary temporal action proposal (CTAP) <ref type="bibr" target="#b12">[13]</ref> consists of three stages, which are initial proposal generation, complementary proposal collection, and boundary adjustment and proposal ranking, respectively. However, both of these two methods are multistage models with the modules in different stages trained independently, without overall optimization of the models. Another drawback is the neglect of the temporal position information, which conveys the temporal ordering information of the video sequence and is thereby expected to be helpful for precisely localizing the proposal boundary.</p><p>In order to address the aforementioned drawbacks, we propose a multi-granularity generator (MGG) by taking full advantage of both segment proposal and frame actionness based methods. At the beginning, the frame position embedding, realized with cosine and sine functions of different wavelengths, is combined with the video frame features. The combined features are then fed to MGG to perform the temporal action proposal. Specifically, a bilinear matching model is first proposed to exploit the rich local information of the video sequence. Afterwards, two components, namely segment proposal producer (SPP) and frame actionness producer (FAP), are coupling together and responsible for generating coarse segment proposals and evaluating fine frame actionness, respectively. SPP uses a U-shape architecture with lateral connections to generate candidate proposals of different temporal spans with high recall. For FAP, we densely evaluate the probabilities of each frame being the starting point, ending point, and inside a correct proposal (middle point). During the inference, MGG can further temporally adjust the segment boundaries with respect to the frame actionness information as shown in <ref type="figure">Fig. 1</ref>, and consequently produce refined action proposals.</p><p>In summary, the main contributions of our work are fourfold:</p><p>• We propose an end-to-end multi-granularity generator (MGG) for temporal action proposal, using a novel representation integrating video features and the position embedding information. MGG simultaneously generates coarse segment proposals by perceiving the whole video sequence, and predicts the frame actionness by densely evaluating each video frame.</p><p>• A bilinear matching model is proposed to exploit the rich local information within the video sequence, which is thereafter harnessed by the following SPP and FAP.</p><p>• SPP is realized in a U-shape architecture with lateral connections, capturing temporal proposals of various spans with high recall, while FAP evaluates the probabilities of each frame being the stating point, ending point, and middle point.</p><p>• Through temporally adjusting the segment proposal boundaries using the complementary information in the frame actionness, our proposed MGG achieves the state-of-the-art performances on the THUMOS-14 and ActivityNet-1.3 datasets for the temporal action proposal task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A large number of existing approaches have been proposed to tackle the problem of temporal action detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>. Inspired by the success of two-stage detectors like RCNN <ref type="bibr" target="#b16">[17]</ref>, many recent methods adopt a proposal-plus-classification framework <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref>, where classifiers are applied on a smaller number of class agnostic segment proposals for detection. The proposal stage and classification stage can be trained separately <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref> or jointly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref>, and demonstrate very competitive results. Regarding temporal action proposal, DAP <ref type="bibr" target="#b9">[10]</ref> and SST <ref type="bibr" target="#b0">[1]</ref> introduce RNNs to process video sequences in a single pass. However, LSTM <ref type="bibr" target="#b17">[18]</ref> and GRU <ref type="bibr" target="#b7">[8]</ref> fail to handle video segments with long time spans. Alternatively, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref> directly generate proposals from sliding windows. R-C3D <ref type="bibr" target="#b43">[44]</ref> and TAL-Net <ref type="bibr" target="#b4">[5]</ref> follow the Faster R-CNN <ref type="bibr" target="#b29">[30]</ref> paradigm to predict locations of temporal proposals and the corresponding categories. These methods perceive the whole videos in a coarser level, while the pre-defined temporal intervals may limit the accuracy of generated proposals. Methods like temporal action grouping (TAG) <ref type="bibr" target="#b42">[43]</ref> and CDC <ref type="bibr" target="#b32">[33]</ref> produce final proposals by densely giving evaluation to each frame. Analyzing videos in a finer level, the generated proposals are quite accurate in boundaries. In our work, MGG tackles the problem of temporal action proposal in both coarse and fine perspectives, being better at both recall and overlapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Given an untrimmed video sequence s = {s n } ls n=1 with its length as l s , temporal action proposal aims at detecting action instances ϕ p = {ξ n = [t s,n , t e,n ]} Ms n=1 , where M s is the total number of action instances, and [t s,n , t e,n ] denote the starting and ending points of an action instance ξ n , respectively.</p><p>We propose one novel neural network, namely MGG shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which analyzes the video and performs  The subsequent BaseNet relies on a blinear model to exploit the rich local information within the sequential video representations. Afterwards, SPP and FAP are used to produce the action proposals from the coarse (segment) and fine (frame) perspectives, respectively. Finally, the temporal boundary adjustment (TBA) module adjusts the segment proposal boundaries regarding the frame actionness and therefore generates action proposals of both high recall and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Representation</head><p>First, we need to encode the video sequence and generate the corresponding representations. Same as the previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>, one convolutional neural network (CNN) is used to convert one video sequence s = {s n } ls n=1 into one visual feature sequence f = {f n } ls n=1 with f n ∈ R d f . d f is the dimension of each feature representation. However, the temporal ordering information of the video sequence is not considered. Inspired by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38]</ref>, we embed the position information to explicitly characterize the ordering information of each visual feature, which is believed to benefit the action proposal generation. The position information of the n-th (n ∈ [1, l s ]) visual feature f n is embedded into a feature p n with a dimension d p by computing cosine and sine functions of different wavelengths: p n (2i) = sin(n/10000 2i/dp ), p n (2i + 1) = cos(n/10000 2i/dp ),</p><p>where i is the index of the dimension. The generated position embedding p n will be equipped with the visual feature representation f n via concatenation, denoted by l n = [f n , p n ]. As such, the final video representations L = {l n } ls n=1 ∈ R ls×d l are obtained, where d l = d f + d p denotes the dimension of the fused representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BaseNet</head><p>Based on the video representations, we propose a novel BaseNet to exploit the rich local behaviors within the video sequence. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, two temporal convolutional layers are first stacked to exploit video temporal relationships. A typical temporal convolutional layer is denoted as Conv(n f , n k , Ω), where n f , n k , and Ω are filter numbers, kernel size, and activation function, respectively. In our proposed BaseNet, the two convolutional layers are of the same architecture, specifically Conv(d h , k, ReLU), where d h is set to 512, k is set to 5, and ReLU refers to the activation of rectified linear units <ref type="bibr" target="#b28">[29]</ref>. The outputs of these two temporal convolutional layers are denoted as H 1 and H 2 , respectively.</p><p>The intermediate representations H 1 and H 2 express the semantic information of the video sequence at different levels, which are rich in characterizing the local information. We propose a bilinear matching model <ref type="bibr" target="#b27">[28]</ref> to capture the interaction behaviors between H 1 and H 2 . Due to a large number of parameters contained in a traditional bilinear matching model, which result in an increased computational complexity and a higher convergence difficulty, we turn to pursue a factorized bilinear matching model <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>:</p><formula xml:id="formula_1">H n 1 =H n 1 W i + b i , H n 2 =H n 2 W i + b i , T n i =Ĥ n 1Ĥ n 2 ,<label>(2)</label></formula><p>where H n 1 ∈ R 1×d h and H n 2 ∈ R 1×d h denote the corresponding representations at the n-th location of H 1 and H 2 , respectively. W i ∈ R d h ×g and b i ∈ R 1×g are the parameters to be learned, with g denoting a hyperparameter and being much smaller than d h . Due to the smaller value of g, fewer parameters are introduced, which are easier for training. As such, the matching video representations T = [T 1 , .., T ls ], with T n = [T n 1 , T n 2 , .., T n d h ] denoting the n-th feature, is obtained and used as the input to the following SPP and FAP for proposal generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Segment Proposal Producer</head><p>Due to large variations of action duration, capturing proposals of different temporal lengths with high recall is a big  challenge. Xu et al. <ref type="bibr" target="#b43">[44]</ref> used one feature map to locate proposals of various temporal spans, yielding low average recall. SSAD <ref type="bibr" target="#b23">[24]</ref> and TAL-Net <ref type="bibr" target="#b4">[5]</ref> use a feature pyramid network, with each layer being responsible for proposal localization with specific time spans. However, each pyramid layer, especially the lower ones being unaware of high-level semantic information, is unable to localize temporal proposals accurately. To deal with this issue, we adopt a U-shape architecture with lateral connections between the convolutional and deconvolutional layers, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>With yielded matching video representations T as input, SPP first stacks three layers, specifically one temporal convolutional layer and two max-pooling layers, to reduce the temporal dimension and hence increase the size of the receptive field accordingly. As a result, the temporal feature T c with temporal dimension l s /8 is taken as the input of the U-shape architecture.</p><p>Same as the previous work, such as Unet <ref type="bibr" target="#b31">[32]</ref>, FPN <ref type="bibr" target="#b26">[27]</ref>, and DSSD <ref type="bibr" target="#b11">[12]</ref>, our U-shape architecture also consists of a contracting path and an expansive path as well as the lateral connections. Regarding the contracting path, with repeated temporal convolutions with stride 2 for downsampling, the feature pyramid (FP)</p><formula xml:id="formula_2">F L = {f (0) L , f (1) L , ...f (M −1) L } is ob- tained, where f (n)</formula><p>L is the n-th level feature map of F L with temporal dimension ls 8 * 2 n . M denotes the total number of pyramid levels. For the expansive path, temporal deconvolutions are adopted on multiple layers with an upscaling factor of 2. Via lateral connections, high-level features from the expansive path are combined with the corresponding low-level features, with the fused features denoted as f (n) H . Repeating this operation, the fused feature pyramid is</p><formula xml:id="formula_3">defined as F H = {f (0) H , f (1) H , ...f (M −1) H }.</formula><p>Different levels of feature pyramids are of different receptive fields, which are responsible for locating proposals of different temporal spans.</p><p>A set of anchors are regularly distributed over each level of feature pyramid F H , based on which segment proposals are produced. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, each f H is followed by two branches, with each branch realized by stacking two layers of temporal convolutions. Specifically, one branch is the classification module to predict the probability of a ground-truth proposal being present at each temporal location for each of the ρ anchors, where ρ is the number of anchors per location of the feature pyramid. The other branch is the boundary regression module to yield the relative offset between the anchor and the ground-truth proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Frame Actionness Producer</head><p>Based on the yielded matching video representations T , the frame actionness producer (FAP) is proposed to evaluate the actionness of each frame. Specifically, three two-layer temporal convolutional networks are used to generate the starting point, ending point, and middle point probabilities for each frame, respectively. Please note that two-layer temporal convolutional networks share the same configuration, where the first one is defined as Conv(d f , k, ReLU) and the second one is Conv(1, k, Sigmoid). d f is set to 64, while k, as the kernel size, is set to 3. And their weights are not shared. As a result, we obtain three probability sequences, namely the starting probability sequence P s = {p s n } ls n=1 , the ending probability sequence P e = {p e n } ls n=1 , and the middle probability sequence P m = {p m n } ls n=1 , with p s n , p e n , and p m n denoting the starting, ending, and middle probabilities of the n-th feature, respectively. Compared with the generated segment proposals by SPP, the frame actionness yielded by FAP densely evaluates each frame in a finer manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Inference</head><p>In this section, we will first introduce how to train our proposed MGG network, which can subsequently generate segment proposals and frame actionness. During the inference, we propose one novel fusion strategy by temporally adjusting the segment boundary information with respect to the frame actionness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training</head><p>As introduced in Sec. 3, our proposed MGG considers both the SPP and FAP together with a shared BaseNet. During the training process, these three components cooperate with each other and are jointly trained in an end-to-end fashion. Specifically, the objective function of our proposed MGG is defined as:</p><formula xml:id="formula_4">L M GG = L SP P + βL F AP ,<label>(3)</label></formula><p>where L SP P and L F AP are the objective functions defined for SPP and FAP, respectively. β is a parameter to adjust their relative contributions, which is empirically set to 0.1.</p><p>Detailed information about L SP P and L F AP will be introduced in what follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">SPP Training</head><p>Our proposed SPP produces a set of anchor segments for each level of the fused feature pyramids F H . We first introduce how to assign labels to the corresponding anchor segments. Subsequently, the objective function by referring to the assigned labels is introduced. Label Assignment. Same as Faster RCNN <ref type="bibr" target="#b29">[30]</ref>, we assign a binary class label to each anchor segment. A positive label is assigned if it overlaps with some ground-truth proposals with temporal Intersection-over-Union (tIoU) higher than 0.7, or has the highest tIoU with a ground-truth proposal. Anchors are regarded as negative if the maximum tIoU with all ground-truth proposals is lower than 0.3. Anchors that are neither positive nor negative are filtered out. To ease the issue of class imbalance, we sample the positive and negative examples with a ratio of 1:1 for training.</p><p>Objective Function. As shown in <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>, we perform a multi-task training for SPP, which not only predicts the actionness of each anchor segment but also regresses its boundary information. For actionness prediction, the crossentropy function is used, while the smooth L 1 loss function, as introduced in <ref type="bibr" target="#b15">[16]</ref>, is used for boundary regression. Specifically, the objective function is defined as:</p><formula xml:id="formula_5">L SP P = 1 N cls i L cls (p i , p * i )+ γ 1 N reg i [p * i 1]L reg (W i , W * i ),<label>(4)</label></formula><p>where γ is a trade-off parameter, which is set to 0.001 empirically. N cls is the total number of training examples. p i stands for the yielded score. p * i is the label, 1 for positive samples and 0 for negative samples. L cls is the cross-entropy loss function between p i and p * i . The smooth L 1 loss function L reg is activated only when the groundtruth label p * i is positive, and disabled otherwise. N reg is the number of training examples whose p * i is positive. W i = {t c , t l } represents the predicted relative offsets of anchor segments. W * i = {t * c , t * l } indicates the relative offsets between ground-truth proposals and the anchors, which can be computed:</p><formula xml:id="formula_6">t * c = (c * i − c i )/l i , t * l = log(l * i /l i ),<label>(5)</label></formula><p>where c i and l i indicate the center and length of anchor segments, respectively. c * i and l * i represent the center and length of the ground-truth action instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">FAP Training</head><p>FAP takes the matching video representations with their length as l s as input and outputs three probability sequences, namely the starting probability sequence P s = {p s n } ls n=1 , the ending probability sequence P e = {p e n } ls n=1 , and the middle probability sequence P m = {p m n } ls n=1 . Label Assignment.</p><p>The ground-truth annotations of temporal action proposals are denoted as π = {ψ n = [t s,n , t e,n ]} Ma n=1 , where M a is the total number of annotations. For each action instance ψ n ∈ π, we define the starting, ending, and middle regions as [t s,n −d d,n /η, t s,n + d d,n /η], [t e,n − d d,n /η, t e,n + d d,n /η], and [t s,n , t e,n ], respectively, where d d,n = t e,n − t s,n is the duration of the annotated action instance and η is set to 10 empirically. For each visual feature, if it lies in the starting, ending, or middle regions of any action instances, its corresponding starting, ending, or middle label will be set to 1, otherwise 0. In this way, we obtain the ground-truth label for the three sequences, which are denoted as G s = {g s n } ls n=1 , G e = {g e n } ls n=1 , and G m = {g m n } ls n=1 , respectively. Objective Function. Given the predicted probability sequences and ground-truth labels, the objective function for FAP is defined as:</p><formula xml:id="formula_7">L all F AP = λ s L s F AP + λ e L e F AP + λ m L m F AP .<label>(6)</label></formula><p>The cross-entropy loss function is used for calculating all the three losses L s F AP , L e F AP , and L m F AP , where a weighting factor set by an inverse class frequency is introduced to address class imbalance. L all F AP is the sum of the starting loss L s F AP , ending loss L e F AP , and middle loss L m F AP , where λ s , λ e , and λ m are the weights specifying the relative importance of each part. In our experiments, we set λ s = λ e = λ m = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inference</head><p>As aforementioned, SPP aims to locate segment proposals of various temporal spans, thus yielding segment proposals with inaccurate boundary information. On the contrary, FAP gives an evaluation of each video frame in a finer level, which makes it sensitive to boundaries of action proposals. Obviously, SPP and FAP are complementary to each other. Therefore, during the inference phase, we propose the temporal boundary adjustment (TBA) module realized in a two-stage fusion strategy to improve the boundary accuracy of segment proposals with respect to the frame actionness.</p><p>Stage I. We first use non-maximum suppression (NMS) to post-process the segment-level action instances detected by SPP. The generated results are denoted as ϕ p = {ξ n = [t s,n , t e,n ]} Ms n=1 , where M s is the total number of the detected action instances, and t s,n and t e,n denote the corresponding starting and ending times of an action instance ξ n , respectively. We will adjust t s,n and t e,n by referring to the starting and ending scores detected in FAP. Firstly, we set two context regions ξ s n and ξ e n , which are named as the searching space: ξ s n = [t s,n − d d,n /ε, t s,n + d d,n /ε], ξ e n = [t e,n − d d,n /ε, t e,n + d d,n /ε], <ref type="bibr" target="#b6">(7)</ref> where d d,n = t e,n − t s,n is the duration of ξ n . ε which controls the size of the searching space is set to 5 . The max starting score and the corresponding time in the region of ξ s n are defined as c s n and t max s,n , respectively , and the max ending score and the corresponding time in the region of ξ e n are defined as c e n and t max e,n , respectively. If c s n or c e n is higher than a threshold σ ∈ [0, 1], which is set manually for each specific dataset, we adjust the starting or ending point of ξ n with a weighting factor δ to control the contribution of t max s,n and t max e,n and yield the refined action instance ξ n . As such, the new segment-level action instance set is refined to be ϕ p = {ξ n } Ms n=1 . Stage II. The middle probability sequence illustrates the probability of each frame whether it is inside one action proposal or not. We use the grouping scheme similar to TAG <ref type="bibr" target="#b42">[43]</ref> to group the consecutive frames with high middle probability into regions as the candidate action instances. Such generated action instances are denoted by ϕ tag = {φ n } Mt n=1 with M t indicating the total number of grouped action instances. We propose to make a further position adjustment by considering both ϕ tag and ϕ p . Specifically, for each action instance ξ n in ϕ p , its tIoU with all the action instances in ϕ tag are computed. If the maximum tIoU is higher than 0.8, the boundaries of ξ n will be replaced by the corresponding action instance φ n in ϕ tag . Via such an operation, the substituted proposals are sensitive to boundaries and the overall boundary accuracy is improved accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>THUMOS-14 <ref type="bibr" target="#b21">[22]</ref>. It includes 1,010 videos and 1,574 videos with 20 action classes in the validation and test sets, respectively. There are 200 and 212 videos with temporal annotations of actions labeled in the validation and testing sets, respectively. We conduct the experiments on the same public split as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>ActivityNet-1.3 <ref type="bibr" target="#b1">[2]</ref>. The whole dataset consists of 19,994 videos with 200 classes annotated, with 50% for training, 25% for validation, and the rest 25% for testing.  <ref type="bibr" target="#b34">[35]</ref>, SST <ref type="bibr" target="#b0">[1]</ref>, TURN <ref type="bibr" target="#b13">[14]</ref>, BSN <ref type="bibr" target="#b25">[26]</ref>, TAG <ref type="bibr" target="#b42">[43]</ref>, and CTAP <ref type="bibr" target="#b12">[13]</ref> on THUMOS-14 in terms of AR@AN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Method @50 @100 @200 @500 @1000 We train our model on the training set and perform evaluations on the validation and testing sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Temporal Proposal Generation</head><p>In this section, we compare our proposed MGG against the existing state-of-the-art methods on both THUMOS-14 and ActivityNet-1.3 datasets.</p><p>For temporal action proposal, Average Recall (AR) computed with different tIoUs is usually adopted for performance evaluation. Following traditional practice, tIoU thresholds set from 0.5 to 0.95 with a step size of 0.05 are used on ActivityNet-1.3, while tIoU thresholds set from 0.5 to 1.0 with a step size of 0.05 are used on THUMOS-14. We also measure AR with different Average Numbers (ANs) of proposals, denoted as AR@AN. Moreover, the area under the AR-AN curve (AUC) is also used as one metric on ActivityNet-1.3, where AN ranges from 0 to 100. <ref type="table" target="#tab_1">Table 1</ref> illustrates the performance comparisons on the testing set of THUMOS-14. Different feature representations will significantly affect the performances. As such, we adopt the two-stream <ref type="bibr" target="#b35">[36]</ref> and C3D <ref type="bibr" target="#b36">[37]</ref> features for fair comparisons. Taking the two-stream features as input, the AR@AN performances are consistently improved for AN ranging from 50 to 500, while BSN+NMS achieves a better performance with AN equal to 1000. While the C3D features are adopted, the AR@AN of MGG is higher than those of the other methods, with AN ranging from 50 to 1000. Such experiments clearly indicate the effectiveness of MGG in temporal proposal generation.</p><p>Furthermore, <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates the AR-AN and recall@100-tIoU curves of different models on the testing split of THUMOS-14. It can be observed that our proposed MGG outperforms the other methods in terms of AR-AN curves. Specifically, when AN equals 40, MGG significantly improves the performance from 33.02% to 37.01%. For recall@100-tIoU, MGG gains a significantly higher recall when tIoU ranges from 0.5 to 1, indicating high accuracy of our proposal results.  Performance comparisons with TCN <ref type="bibr" target="#b8">[9]</ref>, MSRA <ref type="bibr" target="#b45">[46]</ref>, Prop-SSAD <ref type="bibr" target="#b24">[25]</ref>, CTAP <ref type="bibr" target="#b12">[13]</ref>, and BSN <ref type="bibr" target="#b25">[26]</ref> on the validation and testing splits of ActivityNet-1.3.  <ref type="table" target="#tab_3">Table 2</ref> illustrates the performance comparisons on the ActivityNet-1.3 dataset, where a two-stream Inflated 3D ConvNet (I3D) model <ref type="bibr" target="#b3">[4]</ref> is used to extract features. Specifically, we compare our proposed MGG with the stateof-the-art methods, namely TCN <ref type="bibr" target="#b8">[9]</ref>, MSRA <ref type="bibr" target="#b45">[46]</ref>, Prop-SSAD <ref type="bibr" target="#b24">[25]</ref>, CTAP <ref type="bibr" target="#b12">[13]</ref>, and BSN <ref type="bibr" target="#b25">[26]</ref>, in terms of AUC and AR@100. It can be observed that the proposed MGG outperforms the other methods on both the validation and testing sets. Specifically, MGG improves AR@100 on the validation set from 74.16 of the state-of-the-art method BSN to 74.54. <ref type="figure">Fig. 5</ref> illustrates some qualitative results of the generated proposals by MGG on ActivityNet-1.3 and THUMOS-14. Each is composed of a sequence of frames sampled from a full video. By analyzing videos from both coarse and fine perspectives, MGG generates the refined proposals, with high overlapping with ground-truth proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>In this subsection, the effect of each component in MGG is studied in detail. We ablate the studies on the validation set of ActivityNet-1.3. Specifically, in order to verify the component effectiveness of MGG: position embedding, bilinear matching, U-shape architecture in SPP, FAP, and SPP, we perform the ablation studies as follows: MGG-P: We discard the position information of the input video sequence and directly feed the visual feature representations into MGG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MGG-B:</head><p>We discard the bilinear matching model which exploits the interactions between the two temporal convolutions within BaseNet, and instead feed the output of the Ground-Truth Segment Proposals Refined Proposals <ref type="figure">Figure 5</ref>: Qualitative results of the proposals generated by MGG on ActivityNet-1.3 (top and middle) and THUMOS-14 (bottom). It can be observed that the boundary information of the segment proposals generated by SPP is further adjusted using FAP, resulting in more precise proposals. second convolutional layer to the following SPP and FAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MGG-U:</head><p>We discard the U-shape architecture which is proposed in SPP to increase semantic information of the lower layers. Correspondingly, only the expansive path of the feature pyramid is used. MGG-F: We only consider SPP to generate the final proposals, without considering FAP and the following TBA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MGG-S:</head><p>We only consider FAP to generate the final proposals, without considering SPP and the following TBA module. As shown in <ref type="table" target="#tab_6">Table 3</ref>, our full model MGG outperforms all its variants, namely MGG-P, MGG-B, MGG-U, MGG-F, and MGG-S, which verifies the effectiveness of the components. In order to examine the detailed effectiveness of the U-shape architecture, we compare the recall rate of generated proposals in different lengths. As shown in <ref type="table" target="#tab_7">Table 4</ref>, the recall rate of short proposals drops dramatically, when the U-shape architecture is removed. The reason is that the U-shape architecture transfers higher semantic information to the lower layers, which can perceive global information of the video sequence, and is thus helpful for capturing proposals with short temporal extents.  Moreover, it can be observed that MGG-F and MGG-S both perform inferiorly to our full MGG. The main reason is that SPP and FAP generate proposals at different granularities. Our proposed TBA can exploit their complementary behaviors and fuse them together to produce proposals with more precise boundary information. As introduced in Sec. 4.2, TBA performs in two stages: Stage I: The starting and ending probability sequences generated by FAP are used to adjust boundaries of segment proposals from SPP. Stage II: The middle probability sequence is grouped into proposals with the method similar to <ref type="bibr" target="#b42">[43]</ref> and gives a final adjustment to boundaries of proposals from Stage I. <ref type="table" target="#tab_8">Table 5</ref> illustrates the effectiveness of each stage in TBA. It can be observed that the two stages of TBA can both refine boundaries of segment proposals, thus consistently improving the performances, with AUC increasing from 64.31% to 66.43%.</p><p>Training: Stagewise v.s. End-to-end. MGG is designed to jointly optimize SPP and FAP in an end-to-end fashion. It is also possible to train SPP and FAP separately, in which they do not work together. Such a training scheme is referred to as the stagewise training. <ref type="table" target="#tab_8">Table 5</ref> illustrates the performance comparisons between endto-end training and stagewise training. It can be observed that models trained in an end-to-end fashion can outperform those learned with stagewise training under the same settings. It clearly demonstrates the importance of jointly optimizing SPP and FAP with BaseNet as a shared block to provide intermediate video representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Action Detection</head><p>In order to further examine the quality of generated proposals by MGG, we feed the detected proposals into the state-of-the-art action classifiers, including SCNN <ref type="bibr" target="#b34">[35]</ref> and UntrimmedNet <ref type="bibr" target="#b41">[42]</ref>. For fair comparisons, the same classifiers are also used for other proposal generation methods, including SST <ref type="bibr" target="#b0">[1]</ref>, TURN <ref type="bibr" target="#b13">[14]</ref>, CTAP, and BSN. We adopt the conventional mean Average Precision (mAP) metric, where Average Precision (AP) reports the performance of each activity category. Specifically, mAP with tIoU thresholds {0.3, 0.4, 0.5, 0.6, 0.7} is used on THUMOS-14. <ref type="table" target="#tab_9">Table 6</ref> illustrates the performance comparisons, which are evaluated on the testing set of THUMOS-14. With the same classifier, MGG achieves better performance than the other proposal generators, and outperforms the state-of-theart proposal methods, namely CTAP <ref type="bibr" target="#b12">[13]</ref> and BSN <ref type="bibr" target="#b25">[26]</ref>, thus demonstrating the effectiveness of our proposed MGG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a novel architecture, namely MGG, for the temporal action proposal generation. MGG holds two branches: one is SPP perceiving the whole video in a coarse level and the other is FAP working in a finer level. SPP and FAP couple together and integrate into MGG, which can be trained in an end-to-end fashion. By analyzing whole videos from both coarse and fine perspectives, MGG generates proposals with high recall and more precise boundary information. As such, MGG achieves better performance than the other state-of-the-art methods on the THUMOS-14 and ActivityNet-1.3 datasets. The superior performance of video detection relying on the generated proposals further demonstrates the effectiveness of the proposed MGG. Ground-Truth Segment Proposals Refined Proposals <ref type="figure">Figure 6</ref>: Qualitative results of proposals generated by MGG. First four rows represent temporal proposals on ActivityNet-1.3. Last two rows represent temporal proposals on THUMOS-14. After TBA adopted to adjust proposal boundaries generated by segment proposal generator (SPG), the refined proposals will have high overlap with the ground-truth proposals. Ground-Truth Predicted Proposals <ref type="figure">Figure 7</ref>: Failure cases generated by MGG on THUMOS-14. For ground-truths with short temporal spans (first two rows), it is challenging for MGG to locate them. While quality of video frames is poorer (the last row), the performance will be reduced further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our proposed MGG. The video visual features are first combined with the position embedding information to form the video representations. The proposed BaseNet relies on a blinear model to exploit the rich local information within the sequential video representations. Segment proposal producer (SPP) is realized by using a U-shape architecture with lateral connections to generate proposals of different temporal lengths, while frame actionness producer (FAP) evaluates each frame whether it is the starting point, ending point, or middle point. With the temporal boundary adjustment (TBA) module, boundaries of the segment proposals are temporally adjusted based on computed frame actionness, and the refined accurate action proposals are therefore generated.temporal action proposal at different granularities. Specifically, our proposed MGG consists of four components. The video visual features are first combined with the position embedding information to yield the video representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Overview of SPP with pyramid levels M = 3. With a U-shape architecture and lateral connections, the generated feature pyramid F H is helpful for capturing proposals with different temporal durations. (b) The anchor predict module has two branches which are used for classification and boundary regression, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>AR-AN and recall@AN=100 curves of different temporal action proposal methods on the testing set of THUMOS-14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons with DAPs [10], SCNNprop</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on the validation set of ActivityNet-1.3 in terms of AUC and AR@AN.</figDesc><table><row><cell>Method</cell><cell cols="2">AUC (val) @30</cell><cell>@50</cell><cell>@80 @100</cell></row><row><cell>MGG-P</cell><cell>65.59</cell><cell cols="3">65.21 69.93 72.88 73.92</cell></row><row><cell>MGG-B</cell><cell>65.88</cell><cell cols="3">65.56 70.41 73.19 73.89</cell></row><row><cell>MGG-U</cell><cell>65.02</cell><cell cols="3">64.85 69.41 72.95 73.71</cell></row><row><cell>MGG-F</cell><cell>64.31</cell><cell cols="3">63.76 67.91 71.04 72.24</cell></row><row><cell>MGG-S</cell><cell>59.91</cell><cell cols="3">59.53 63.05 67.18 68.96</cell></row><row><cell>MGG</cell><cell>66.43</cell><cell cols="3">66.21 70.97 73.87 74.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Recall rates of MGG-U and MGG on generated proposals of different temporal extents on the validation set of ActivityNet-1.3, where AN and tIoU thresholds are set to 100 and 0.75, respectively.</figDesc><table><row><cell>Method</cell><cell>0-5s</cell><cell>5-10s</cell><cell>10-15s</cell><cell>15-20s</cell><cell>25-30s</cell><cell>35-40s</cell><cell>40-45s</cell></row><row><cell>MGG-U</cell><cell>0.15</cell><cell>0.63</cell><cell>0.73</cell><cell>0.80</cell><cell>0.91</cell><cell>0.93</cell><cell>0.94</cell></row><row><cell>MGG</cell><cell>0.21</cell><cell>0.73</cell><cell>0.82</cell><cell>0.90</cell><cell>0.93</cell><cell>0.93</cell><cell>0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons of the two-stage TBA on the validation set of ActivityNet-1.3 in both end-to-end training and stagewise training manners. ) 64.12 65.40 66.28 64.31 65.54 66.43 AR@100 72.05 73.41 74.19 72.24 73.48 74.54</figDesc><table><row><cell>Stagewise</cell><cell>End-to-end</cell></row><row><cell>MGG-F</cell><cell></cell></row><row><cell>Stage I</cell><cell></cell></row><row><cell>Stage II</cell><cell></cell></row><row><cell>AUC(val</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons between MGG and the other proposal generation methods in terms of video detection on the testing set of THUMOS-14, where mAP is reported with tIoU set from 0.3 to 0.7.</figDesc><table><row><cell>Proposal Method</cell><cell>Classifier</cell><cell>0.7</cell><cell>0.6</cell><cell>0.5</cell><cell>0.4</cell><cell>0.3</cell></row><row><cell>SST [1]</cell><cell>SCNN-cls</cell><cell>-</cell><cell>-</cell><cell>23.0</cell><cell>-</cell><cell>-</cell></row><row><cell>TURN [14]</cell><cell>SCNN-cls</cell><cell>7.7</cell><cell>14.6</cell><cell>25.6</cell><cell>34.9</cell><cell>44.1</cell></row><row><cell>CTAP [13]</cell><cell>SCNN-cls</cell><cell>-</cell><cell>-</cell><cell>26.9</cell><cell>-</cell><cell>-</cell></row><row><cell>BSN [26]</cell><cell>SCNN-cls</cell><cell>15.0</cell><cell>22.4</cell><cell>29.4</cell><cell>36.6</cell><cell>43.1</cell></row><row><cell>MGG</cell><cell>SCNN-cls</cell><cell>15.8</cell><cell>23.6</cell><cell>29.9</cell><cell>37.8</cell><cell>44.9</cell></row><row><cell>SST [1]</cell><cell>UNet</cell><cell>4.7</cell><cell>10.9</cell><cell>20.0</cell><cell>31.5</cell><cell>41.2</cell></row><row><cell>TURN [14]</cell><cell>UNet</cell><cell>6.3</cell><cell>14.1</cell><cell>24.5</cell><cell>35.3</cell><cell>46.3</cell></row><row><cell>BSN [26]</cell><cell>UNet</cell><cell>20.0</cell><cell>28.4</cell><cell>36.9</cell><cell>45.0</cell><cell>53.5</cell></row><row><cell>MGG</cell><cell>UNet</cell><cell>21.3</cell><cell>29.5</cell><cell>37.4</cell><cell>46.8</cell><cell>53.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>This supplementary material includes additional experiments that are not presented in the main paper and more qualitative results to demonstrate the performances of our proposed MGG.</p><p>Recall Rates. Ground-truth proposals with short temporal spans are hard to capture, which mainly dues to that short proposals are of less semantic information. In the main paper, we illustrate the improvement of recall rates in short proposals with the U-shape architecture. Here, we demonstrate the short proposal recall rates of different methods including DAP <ref type="bibr" target="#b9">[10]</ref>, TURN <ref type="bibr" target="#b13">[14]</ref>, CTAP <ref type="bibr" target="#b12">[13]</ref>, BSN <ref type="bibr" target="#b25">[26]</ref> and our proposed MGG on the testing set of THUMOS-14. The temporal spans ranges from 1 frame to 60 frames, and the recall rates are computed with AN and tIoU set to 100 and 0.75, respectively. As shown in <ref type="table">Table 7</ref>, the recall rates of MGG outperform the other competitor methods. One reason is that the temporal boundary adjustment (TBA) module is helpful for the proposal to be accurate in boundaries. Thus the generated proposals will have high overlap with ground-truths. Another reason is the U-shape architecture, which provides high-level semantic information for lower layers and helpful for the capture of proposals with short temporal durations.</p><p>Qualitative Results. More qualitative results are illustrated in <ref type="figure">Fig. 6</ref>. The first four rows are videos from the validation set of ActivityNet-1.3 <ref type="bibr" target="#b1">[2]</ref> and the last two rows are from the testing set of THUMOS-14 <ref type="bibr" target="#b21">[22]</ref>. It can be observed that the refined proposals are of higher accuracy, which demonstrates the effectiveness of the proposed MGG. Some failure cases are shown in <ref type="figure">Fig. 7</ref>. For ground-truth proposals with short temporal durations, false negatives are produced. Moreover, if the videos are of low quality, it will be hard to capture the corresponding semantic meanings and thereby result in wrong proposals.  <ref type="bibr" target="#b9">[10]</ref> 0.000 0.000 0.025 0.047 0.097 0.106 TURN <ref type="bibr" target="#b13">[14]</ref> 0.000 0.000 0.043 0.117 0.174 0.370 CTAP <ref type="bibr" target="#b12">[13]</ref> 0.000 0.000 0.043 0.126 0.267 0.357 BSN+NMS <ref type="bibr" target="#b25">[26]</ref> 0.000 0.048 0.168 0.237 0.339 0.400 MGG 0.000 0.081 0.183 0.296 0.364 0.431</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining spatiotemporal video patterns towards robust action retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Localizing natural language in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5727" to="5736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04821</idno>
		<title level="m">Ctap: Complementary temporal action proposal generation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards efficient action recognition: Principal backpropagation for training two-stream networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1773" to="1782" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition in unconstrained videos by explicit motion modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3781" to="3795" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trajectorybased modeling of human actions with motion reference points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="425" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Factorized bilinear models for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06750</idno>
		<title level="m">Temporal convolution based action proposal: Submission to activitynet 2017</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02964</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1417" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Autoloc: Weaklysupervised temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="162" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reconstruction network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional attentive fusion with context gating for dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THU-MOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A pursuit of temporal accuracy in general activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02716</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">R-c3d: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5794" to="5803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="199" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Msr asia msm at activitynet challenge 2017: Trimmed action recognition, temporal action proposals and densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Highlight detection with pairwise deep ranking for first-person video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="982" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
