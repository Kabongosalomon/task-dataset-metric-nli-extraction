<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kindling the Darkness: A Practical Low-light Image Enhancer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">Kindling the Darkness: A Practical Low-light Image Enhancer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Low light enhancement</term>
					<term>image decomposition</term>
					<term>image restoration !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images captured under low-light conditions often suffer from (partially) poor visibility. Besides unsatisfactory lightings, multiple types of degradations, such as noise and color distortion due to the limited quality of cameras, hide in the dark. In other words, solely turning up the brightness of dark regions will inevitably amplify hidden artifacts. This work builds a simple yet effective network for Kindling the Darkness (denoted as KinD), which, inspired by Retinex theory, decomposes images into two components. One component (illumination) is responsible for light adjustment, while the other (reflectance) for degradation removal. In such a way, the original space is decoupled into two smaller subspaces, expecting to be better regularized/learned. It is worth to note that our network is trained with paired images shot under different exposure conditions, instead of using any ground-truth reflectance and illumination information. Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over state-of-the-art alternatives. Our KinD is robust against severe visual defects, and user-friendly to arbitrarily adjust light levels. In addition, our model spends less than 50ms to process an image in VGA resolution on a 2080Ti GPU. All the above merits make our KinD attractive for practical use.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>V ERY often, capturing high-quality images in dim light conditions is challenging. Though a few operations, such as setting high ISO, long exposure, and flash, can be applied under the circumstances, they suffer from different drawbacks. For instance, high ISO increases the sensitivity of an image sensor to light, but the noise is also amplified, thus leading to the low (signal-to-noise ratio) SNR. Long exposure is limited to shoot static scenes, otherwise it highly likely gets in trouble of blurry results. Using flash can somehow brighten the environment, which however frequently introduces unexpected highlights and unbalanced lighting into photos, making them visually unpleasant. In practice, typical users may even not have the above options with limited photographing tools, e.g. cameras embedded in portable devices. Although the low-light image enhancement has been a long-standing problem in the community with a great progress made over the past years, developing a practical low-light image enhancer remains challenging, since flexibly lightening the darkness, effectively removing the degradations, and being efficient should all be concerned. <ref type="figure" target="#fig_0">Figure 1</ref> provides three natural images captured under challenging light conditions. Concretely, the first case is with extremely low light. Severe noise and color distortion are hidden in the dark. By simply amplifying the intensity of the image, the degradations show up as given on the topright corner. The second image is photographed at sunset (weak ambient light), most objects in which suffer from backlighting. Imaging at noon facing to the light source (the sun) also hardly gets rid of the issue like the second case Notice that the first image is with extremely low light, we show its x20 version on the top-right corner. exhibits, although the ambient light is stronger and the scene is more visible. Note that those relatively bright regions of the last two photos will be saturated by direct amplification.</p><p>Deep learning-based methods have revealed their superior performance in numerical low-level vision tasks, such as denoising and super-resolution, most of which need the training data with ground truth. For the target problem, say low-light image enhancement, no ground-truth real data exists, although the order of light intensity can be determined. Because, from the viewpoint of users, the favorite light levels for different people/requirements could be much diverse. In other words, one cannot say what light condition is the best/ground-truth. Therefore, it is not so felicitous to map an image only to a version with a specific level of light.</p><p>Based on the above analysis, we summarize challenges in low-light image enhancement as follows:</p><p>• How to effectively estimate the illumination component from a single image, and flexibly adjust light levels?</p><p>• How to remove the degradations like noise and color distortion previously hidden in the darkness after lightening up dark regions?</p><p>• How to train a model without well-defined ground-truth light conditions for low-light image enhancement by only looking at two/several different examples?</p><p>In this paper, we propose a deep neural network to take the above concerns into account simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous Arts</head><p>A large number of low-light image enhancement schemes have been proposed. In what follows, we briefly review classic and contemporary works closely related to ours.</p><p>Plain Methods. Intuitively, for an image with the globally low light, the visibility can be enhanced by directly amplifying it. But, as shown in the first case of <ref type="figure" target="#fig_0">Figure 1</ref>, the visual defects including noise and color distortion show up along the details. For images containing bright regions, e.g. the last two pictures in <ref type="figure" target="#fig_0">Figure 1</ref>, this operation easily results in (partial) saturation/over-exposure. One technical line, with histogram equalization (HE) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and its follow-ups <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> as representatives, tries to map the value range into [0, 1] and balance the histogram of outputs for avoiding the truncation problem. These methods de facto aim to increase the contrast of image. Another mapping manner is gamma correction (GC), which is carried out on each pixel individually in a non-linear way. Although GC can promote the brightness especially of dark pixels, it does not consider the relationship of a certain pixel with its neighbors. The main drawback of the plain approaches is that they barely consider real illumination factors, usually making enhanced results visually vulnerable and inconsistent with real scenes.</p><p>Traditional Illumination-based Methods. Different from the plain methods, strategies in this category are aware of the concept of illumination. The key assumption, inspired by Retinex theory <ref type="bibr" target="#b5">[6]</ref>, is that the (color) image can be decomposed into two components, i.e. reflectance and illumination. Early attempts include single-scale Retinex (SSR) <ref type="bibr" target="#b6">[7]</ref> and multi-scale Retinex (MSR) <ref type="bibr" target="#b7">[8]</ref>. Limited to the manner of producing the final result, the output often looks unnatural and somewhere over-enhanced. Wang et al. proposed a method called NPE <ref type="bibr" target="#b8">[9]</ref>, which jointly enhances contrast and preserves naturalness of illumination. Fu et al. developed a method <ref type="bibr" target="#b9">[10]</ref>, which adjusts the illumination through fusing multiple derivations of the initially estimated illumination map. However, this method sometimes sacrifices the realism of those regions containing rich textures. Guo et al. focused on estimating the structured illumination map from an initial one <ref type="bibr" target="#b10">[11]</ref>. These methods generally assume that the images are noise-and color distortion-free, and do not explicitly consider the degradations. In <ref type="bibr" target="#b11">[12]</ref>, a weighted variational model for simultaneous reflectance and illumination estimation (SRIE) was designed to obtain better reflectance and illumination layers, then the target image is generated by manipulating the illumination. Following <ref type="bibr" target="#b10">[11]</ref>, Li et al. further introduced an extra term to host noise <ref type="bibr" target="#b12">[13]</ref>. Although both <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref> can reject slight noise in images, they are short of abilities in handling color distortion and heavy noise.</p><p>Deep Learning-based Methods. With the emergence of deep learning, a number of low-level vision tasks have been benefited from deep models, such as <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> for denoising, <ref type="bibr" target="#b15">[16]</ref> for super-resolution, <ref type="bibr" target="#b16">[17]</ref> for compression artifact removal and <ref type="bibr" target="#b17">[18]</ref> for dehazing. Regarding the target mission of this paper, the low-light net (LLNet) proposed in <ref type="bibr" target="#b18">[19]</ref> builds a deep network that performs as a simultaneous contrast enhancement and denoising module. Shen et al. deemed that multi-scale Retinex is equivalent to a feed-forward convolutional neural network with different Gaussian convolution kernels. Motivated by this, they constructed a convolutional neural network (MSR-net) <ref type="bibr" target="#b19">[20]</ref> to learn an end-to-end mapping between dark and bright images. Wei et al. designed a deep network, called Retinex-Net <ref type="bibr" target="#b20">[21]</ref>, that integrates image decomposition and illumination mapping. Please notice that Retinex-Net additionally employs an off-the-shelf denoising tool (BM3D <ref type="bibr" target="#b21">[22]</ref>) to clean the reflectance component. These strategies all assume that there exist images with "ground-truth" lights, without considering that the noise differently affects regions with various lights. Simply speaking, after extracting the illumination factor, the noise level of dark regions is (much) higher than that of bright ones in the reflectance. In such a situation, adopting/training a denoiser with a uniform ability over an image (reflectance) is no longer suitable. In addition, the above methods do not explicitly cope with the degradation of color distortion, which is not uncommon in real images. More recently, Chen et al. proposed a pipeline for processing low-light images based on end-to-end training of a fully convolutional network <ref type="bibr" target="#b22">[23]</ref>, which can jointly deal with noise and color distortion. However, this work is specific to data in RAW format, limiting its applicable scenarios. As stated in <ref type="bibr" target="#b22">[23]</ref>, if modifying the network to accept data in JPEG format, the performance significantly drops.</p><p>Most existing methods manipulate the illumination by gamma correction, appointing a level existing in carefully constructed training data, or fusion. For gamma correction, it may be unable to reflect the relationship between different light (exposure) levels. As for the second manner, it is heavily restricted to whether the appointed level is contained in the training data. While for the last one, it even does not provide a manipulation option. Therefore, it is desired to learn a mapping function to arbitrarily convert one light (exposure) level to another for offering users the flexibility of adjustment.</p><p>Image Denoising Methods. In the fields of image processing, multimedia, and computer vision, image denoising has been a hot topic for a long time, with numerous techniques proposed over past decades. Classic ones model/regularize the problem by utilizing some specific priors of natural clean images, like non-local self-similarity, From the perspective of functionality, it also can be divided into three modules, including layer decomposition, reflectance restoration, and illumination adjustment. piecewise smoothness, signal (representation) sparsity, etc. The most popular schemes arguably go to BM3D <ref type="bibr" target="#b21">[22]</ref> and WNNM <ref type="bibr" target="#b23">[24]</ref>. Due to the high complexity of optimization procedure in the testing, and the large searching space of proper parameters, these traditional methods often show the unsatisfactory performance in real situations. Lately, deep learning based denoisers exhibit the superiority on the task. The representative works, such as SSDA using stacked sparse denoising auto-encoders <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, TNRD by trainable nonlinear reaction diffusion <ref type="bibr" target="#b26">[27]</ref>, DnCNN with residual learning and batch normalization <ref type="bibr" target="#b14">[15]</ref>, can save computational expense thanks to only feed-forward convolution operations involved in the testing phase. However, these deep models still have the difficulty for blind image denoising. One may train multiple models for varied levels or one model with a large number of parameters, which is obviously inflexible in practice. By taking the recurrent thought into the task, this issue is mitigated <ref type="bibr" target="#b27">[28]</ref>. But, none of the mentioned approaches considers that different regions of a light-enhanced image host different levels of noise. Same problem happens to color distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our Contributions</head><p>This study presents a deep network for practically solving the low-light enhancement problem. The main contributions of this work can be summarized in the following aspects.</p><p>• Inspired by Retinex theory, the proposed network decomposes images into two components, i.e. reflectance and illumination, which decouples the original space into two smaller ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The network is trained with paired images captured under different light/exposure conditions, instead of using any ground-truth reflectance and illumination information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Our designed model provides a mapping function for flexibly adjusting light levels according to different demands from users.</p><p>• The proposed network also contains a module, which is capable to effectively remove visual defects amplified through lightening dark regions.</p><p>• Extensive experiments are conducted to demonstrate the efficacy of our design and its superiority over state-of-theart alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>A desired low-light image enhancer should be capable to effectively remove the degradations hidden in the darkness, and flexibly adjust light/exposure conditions. We build a deep network, denoted as KinD, to achieve the goal. As schematically illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the network is composed of two branches for handling the reflectance and illumination components, respectively. From the perspective of functionality, it also can be divided into three modules, including layer decomposition, reflectance restoration, and illumination adjustment. In the next subsections, we shall explain the details about the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Consideration &amp; Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Layer Decomposition</head><p>As discussed in Sec. 1.1, the main drawback of plain methods comes from the blindness of illumination. Thus, it is key to obtain the illumination information. If having the illumination well-extracted from the input, the rest hosts the details and possible degradations, where the restoration (or degradation removal) can be executed on. In Retinex theory, an image I can be viewed as a composition of two components, i.e. reflectance R and illumination L, in the fashion of I = R • L, where • designates the element-wise product. Further, decomposing images in the Retinex manner consequently decouples the space of mapping a degraded low-light image to a desired one into two smaller subspaces, expecting to be better and easier regularized/learned. Moreover, the illumination map is core to flexibly adjusting light/exposure conditions. Based on the above, the Retinexbased layer decomposition is suitable and necessary for the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Data Usage &amp; Priors</head><p>There is no well-defined ground-truth for light conditions. Furthermore, no/few ground-truth reflectance and illumination maps for real images are available. The layer decomposition problem is in nature under-determined, thus additional priors/regularizers matter. Suppose that the images are degradation-free, different shots of a certain scene should share the same reflectance. While the illumination maps, though could be intensively varied, are of simple and mutually consistent structure. In real situations, the degradations embodied in low-light images are often worse than those in brighter ones, which will be diverted into the reflectance component. This inspires us that the reflectance from the image in bright light can perform as the reference (ground-truth) for that from the degraded low-light one to learn restorers. One may ask that why not use synthetic data? Because it is hard to synthesize. The degradations are not in a simple form, and change with respect to different sensors. Please notice that the usage of reflectance (well-defined) totally differs from using images in (relatively) bright light as the reference of low light ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Illumination Guided Reflectance Restoration</head><p>In the decomposed reflectance, the pollution of regions corresponding to darker illumination is heavier than that to brighter one. Mathematically, a degraded low-light image can be naturally modeled as I = R • L + E, where E designates the pollution component. By taking simple algebra steps, we have:</p><formula xml:id="formula_0">I = R • L + E =R • L = (R +Ẽ) • L = R • L +Ẽ • L,<label>(1)</label></formula><p>whereR stands for the polluted reflectance, andẼ is the degradation having the illumination decoupled. The relationship E =Ẽ • L holds. Taking the additive white Gaussian noise E ∼ N (0, σ 2 ) for an example, the distribution ofẼ becomes much more complex and strongly relates to L, i.e. σ <ref type="bibr" target="#b1">2</ref> Li for each position i. This is to say, the reflectance restoration cannot be uniformly processed over an entire image, and the illumination map can be a good guider. One may wonder what if directly removing E from the input I? For one thing, the unbalance issue still remains. By viewing from another point, the intrinsic details will be unequally confounded with the noise. For another thing, different from the reflectance, we no longer have proper references for degradation removal in this manner, since L varies. Analogous analysis serves other types of degradation, like color-distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Arbitrary Illumination Manipulation</head><p>The favorite illumination strengths of different persons/applications may be pretty diverse. Therefore, a  practical system needs to provide an interface for arbitrary illumination manipulation. In the literature, three main ways for enhancing light conditions are fusion, light level appointment, and gamma correction. The fusionbased methods, due to the fixed fusion mode, lack in the functionality of light adjustment. If adopting the second option, the training dataset has to contain images with target levels, limiting its flexibility. For gamma correction, although it can achieve the goal by setting different γ values, it may be unable to reflect the relationship between different light (exposure) levels. This paper advocates to learn a flexible mapping function from real data, which accepts users to appoint arbitrary levels of light/exposure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">KinD Network</head><p>Inspired by the consideration and motivation, we build a deep neural network, denoted as KinD, for kindling the darkness. Below, we describe the three subnets in details from the functional perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Operator   </p><note type="other">Kernel Output Channels Stride Output Name RGB Conv&amp;ReLU</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Layer Decomposition Net</head><p>Recovering two components from one image is a highly ill-posed problem. Having no ground-truth information guided, a loss with well-designed constraints is important. Fortunately, we have paired images with different light/exposure configurations [I l , I h ]. Recall that the reflectance of a certain scene should be shared across different images, we regularize the decomposed reflectance pair [R l , R h ] to be close (ideally the same if degradationfree). Furthermore, the illumination maps [L l , L h ] should be piece-wise smooth and mutually consistent. The following terms are adopted. We simply use L LD rs := R l − R h 2 2 to regularize the reflectance similarity, where · 2 means the 2 norm (MSE). The illumination smoothness is constrained by</p><formula xml:id="formula_1">L LD is := ∇L l max(|∇I l |, ) 1 + ∇L h max(|∇I h |, ) 1 ,</formula><p>where ∇ stands for the first order derivative operator containing ∇ x (horizontal) and ∇ y (vertical) directions, and · 1 means the 1 norm. In addition, is a small positive constant (0.01 in this work) for avoiding zero denominator, and | · | means the absolute value operator. This smoothness term measures the relative structure of the illumination with respect to the input. For a location on an edge in I, the penalty on L is small; while for a location in a flat region in I, the penalty turns to be large. As for the mutual consistency, we employ L LD mc := M • exp(−c · M) 1 with M := |∇L l | + |∇L h |. <ref type="figure" target="#fig_3">Figure 4</ref> depicts the function behavior of u · • exp(−c · u), where c is the parameter controlling the shape of function. As can be seen from <ref type="figure" target="#fig_3">Figure 4</ref>, the penalty first goes up but then drops towards 0 as u increases. This characteristic well fits the mutual consistency, i.e. strong mutual edges should be preserved while weak ones depressed. We notice that setting c = 0 leads to a simple 1 loss on M. Besides, the decomposed two layers should reproduce the input, which is constrained by the reconstruction error, say</p><formula xml:id="formula_2">L LD rec := I l − R l • L l 1 + I h − R h • L h 1 .</formula><p>As a result, the loss function of layer decomposition net is as follows:</p><formula xml:id="formula_3">L LD := L LD rec + 0.01L LD rs + 0.08L LD is + 0.1L LD mc .</formula><p>(2) The layer decomposition network contains two branches corresponding to the reflectance and illumination, respectively. The reflectance branch adopts a typical 5-layer U-Net <ref type="bibr" target="#b28">[29]</ref>, followed by a convolutional (conv) layer and a Sigmoid layer. While the illumination branch is composed of two conv+ReLU layers and a conv layer on concatenated feature maps from the reflectance branch (for possibly excluding textures from the illumination), finally followed by a Sigmoid layer. The detailed layer decomposition network configuration is provided in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Reflectance Restoration Net</head><p>The reflectance maps from low-light images, as shown in <ref type="figure" target="#fig_2">Figures 3 and 5</ref>, are more interfered by degradations than those from bright-light ones. Employing the clearer reflectance to act as the reference (informal ground-truth) for the messy one is our principle. For seeking a restoration function, the objective turns to be simple as follows:</p><formula xml:id="formula_4">L RR := R − R h 2 2 − SSIM(R, R h ) + ∇R − ∇R h 2 2 , (3)</formula><p>where SSIM(·, ·) is the structural similarity measurement, andR corresponds to the restored reflectance. The third   This subnet is similar to the reflectance branch in the layer decomposition subnet, but deeper. The schematic configuration is given in <ref type="figure" target="#fig_1">Figure 2</ref> and detailed in Appendix. We recall that the degradation distributes in the reflectance complexly, which strongly depends on the illumination distribution. Thus, we bring the illumination information into the restoration net together with the degraded reflectance. The effectiveness of this operation can be observed in <ref type="figure" target="#fig_4">Figure 5</ref>. In the two reflectance maps with different degradation (light) levels, the results by BM3D can fairly remove noise (without regarding the color distortion in nature). The blur effect exists almost everywhere. In our results, the textures (the dust/waterbased stains for example) of the window region, which is originally bright and barely polluted, keeps clear and sharp, while the degradations in the dark region get largely removed with details (e.g. the characters on the bottles) very well maintained. Besides, the color distortion is also cured by our method. The detailed reflectance restoration network configuration is provided in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Illumination Adjustment Net</head><p>There does not exist a ground-truth light level for images. Therefore, for fulfilling diverse requirements, we need a mechanism to flexibly convert one light condition to another. We have paired illumination maps. Even though without knowing the exact relationship between the paired illuminations, we can roughly calculate their ratio of strength, i.e. α by mean(L t /L s ) where the division is element-wise. This ratio can be used as an indicator to train an adjustment function from a source light L s to a target one L t . If adjusting a lower level of light to a higher one, α &gt; 1, otherwise α ≤ 1. In the testing phase, α can be specified by users. The network is lightweight, containing 3 conv layers (two conv+ReLu, and one conv) and 1 Sigmoid layer. We notice that the indicator α is expanded to a feature map, acting as a part of input for the net. The following is the loss for illumination adjustment net:</p><formula xml:id="formula_5">L IA := L − L t 2 2 + |∇L| − |∇L t | 2 2 ,<label>(4)</label></formula><p>where L t can be L h or L l , andL is the adjusted illumination map from the source light (L h or L l ) towards the target one. <ref type="figure" target="#fig_5">Figure 6</ref> shows the difference between our learned adjustment function and gamma correction. For comparison fairness, we tune the parameter γ for gamma correction to reach a similar overall light strength with ours via γ = log(L) 1 log(Ls) 1 . We consider two adjustments without loss of generality, including one light down and one light up.      the adjusted results by gamma correction, while (c) and (e) are ours. To more clearly show the difference, we plot the 1D intensity curves at x = 100, 200, 400. As for the lightdown case, our learned manner decreases more than gamma correction in intensity on relatively bright regions, while less or about the same on dark regions. Regarding the lightup case, the opposite trend appears. In other words, our method increases less the light on relatively dark regions, while more or about the same on bright regions. The learned manner is more corroborative with actual situations. Furthermore, the α fashion is more convenient than the γ way for users to manipulate. For instance, setting α to 2 means turns the light 2x up. The detailed illumination adjustment network configuration is provided in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL VALIDATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>We use the LOL dataset as the training dataset, which includes 500 low/normal-light image pairs. In the training, we merely employ 450 image pairs, and no synthetic images are used. For the layer decomposition net, batch size is set to be 10 and patch-size to be 48x48. While for the reflectance restoration net and illumination adjustment net, batch size is set to be 4 and patch-size to be 384x384. We use the stochastic gradient descent (SGD) technique for optimization. The entire network is trained on a Nvidia GTX 2080Ti GPU and Intel Core i7-8700 3.20GHz CPU using the Tensorflow framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Evaluation</head><p>We evaluate our method on widely-adopted datasets, including LOL <ref type="bibr" target="#b20">[21]</ref>, LIME <ref type="bibr" target="#b10">[11]</ref>, NPE <ref type="bibr" target="#b8">[9]</ref>, and MEF <ref type="bibr" target="#b29">[30]</ref>. Four metrics are adopted for quantitative comparison, which are PSNR, SSIM, LOE <ref type="bibr" target="#b8">[9]</ref>, and NIQE <ref type="bibr" target="#b30">[31]</ref>. A higher value in terms of PSNR and SSIM indicates better quality, while, in LOE and NIQE, the lower the better. The state-of-the-art methods of BIMEF <ref type="bibr" target="#b31">[32]</ref>, SRIE <ref type="bibr" target="#b11">[12]</ref>, CRM <ref type="bibr" target="#b32">[33]</ref>, Dong <ref type="bibr" target="#b33">[34]</ref>, LIME <ref type="bibr" target="#b10">[11]</ref>, MF <ref type="bibr" target="#b34">[35]</ref>, RRM <ref type="bibr" target="#b12">[13]</ref>, Retinex-Net <ref type="bibr" target="#b20">[21]</ref>, GLAD <ref type="bibr" target="#b35">[36]</ref>, MSR <ref type="bibr" target="#b7">[8]</ref> and NPE <ref type="bibr" target="#b8">[9]</ref> are involved as the competitors. <ref type="table" target="#tab_6">Table 4</ref> reports the numerical results among the competitors on LOL dataset. For each testing low-light image, there is a "normal"-light correspondence. Thus, the correspondence can be taken as the reference to measure PSNR and SSIM. From the numbers, we see that our KinD significantly outperforms all the other methods. In terms of the nonreference metric NIQE, our KinD also takes the first place by a large margin. But, in LOE, our method seems falling behind many methods. As the authors of <ref type="bibr" target="#b10">[11]</ref> stated, using the low-light input itself to compute LOE is problematic. One should choose a reliable reference. Similar to computing PSNR and SSIM, we again employ the correspondence image as the reference (denoted as LOE ref ). In this way, our KinD comes up to the 3rd place, slightly inferior to CRM (977.3 vs. 926.1). Regarding the LIME, NPE, and MEF datasets, no reference images are available. Thus, we only adopt the NIQE to evaluate the performance difference among the involved methods. In this comparison, as given in Tab. 5, our KinD shows its clear advantage against the others. Specifically, KinD outperforms all the competitors     In addition, <ref type="figure" target="#fig_0">Figures 7-13</ref> give a number of visual comparisons on the images with different light conditions. From the results, we can see that, although most of methods can somehow brighten the inputs, severe visual defects caused by unsatisfactory adjustment of light and/or obstinate noise and color distortion remain. Our KinD works well in these cases with the light properly adjusted and degradations clearly removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>In this work, we have proposed a deep network, named KinD, for low-light enhancement. Inspired by Retinex theory, the proposed network decomposes images into the reflectance and illumination layers. The decomposition consequently decouples the original space into two smaller subspaces. As ground-truth reflectance and illumination information is in short, the network is alternatively trained using paired images captured under different light/exposure conditions. To remove the degradations previously hidden in the darkness, the proposed KinD builds a restoration module. A mapping function has also been learned in KinD, which better fits the actual situations than the traditional gamma correction, and flexibly adjusts light levels. Extensive experiments demonstrated the clear advantages of our design over the state-of-the-art alternatives. In the current version, KinD takes less than 50ms to handle an image in VGA resolution on a Nvidia 2080Ti GPU. By applying techniques like MobileNet or quantization, our KinD can be further accelerated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Left column: three natural images captured under different light conditions. Right column: our enhanced results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The architecture of our KinD network. Two branches correspond to the reflectance and illumination, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Left column: Lower light input and its decomposed illumination and (degraded) reflectance maps. Right column: Brighter input and its corresponding maps. Three rows respectively correspond to inputs, illumination maps, and reflectance maps. These are testing images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The behavior of function v = u · exp (−c · u). The parameter c controls the shape of function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The polluted reflectance maps (top), and their results by BM3D (middle) and our reflectance restoration net (bottom). The right column corresponds to a heavier degradation (a lower light) level than the left. These are testing images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison between Gamma correction and our illumination adjustment manner. (a) shows the original/source illumination map. Two cases, including 1) turning the light down with γ = 1.34 (b) and α = 0.7 (c), and 2) turning the light up with γ = 0.53 (d) and α = 1.5 (e), are provided. (f)-(k) give the 1D curves at x = 100, 200, 400 corresponding to the red, green, and blue lines in (a), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 (</head><label>6</label><figDesc>a) depicts the source illumination, (b) and (d) are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Visual comparison with state-of-the-art low-light image enhancement methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Visual comparison with state-of-the-art low-light image enhancement methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>8 Fig. 9 :</head><label>89</label><figDesc>Visual Comparison with state-of-the-art low-light image enhancement methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Visual Comparison with state-of-the-art low-light image enhancement methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Visual Comparison with state-of-the-art low-light image enhancement methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 :</head><label>12</label><figDesc>Visual Comparison with state-of-the-art low-light image enhancement methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 :</head><label>13</label><figDesc>Visual Comparison with state-of-the-art low-light image enhancement methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Layer decomposition network</figDesc><table><row><cell>Inputs</cell><cell>Operator</cell><cell cols="4">Kernel Output Channels Stride Output Name</cell></row><row><cell>Decom i conv3, Decom conv5</cell><cell>Concat</cell><cell>-</cell><cell>33</cell><cell>-</cell><cell>RE concat1</cell></row><row><cell>RE concat1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>32</cell><cell>1</cell><cell>RE conv1 1</cell></row><row><cell>RE conv1 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>32</cell><cell>1</cell><cell>RE conv1 2</cell></row><row><cell>RE conv1 2</cell><cell>Max Pooling</cell><cell>2 × 2</cell><cell>32</cell><cell>2</cell><cell>RE pool1</cell></row><row><cell>RE pool1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>64</cell><cell>1</cell><cell>RE conv2 1</cell></row><row><cell>RE conv2 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>64</cell><cell>1</cell><cell>RE conv2 2</cell></row><row><cell>RE conv2 2</cell><cell>Max Pooling</cell><cell>2 × 2</cell><cell>64</cell><cell>2</cell><cell>RE pool2</cell></row><row><cell>RE pool2</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>128</cell><cell>1</cell><cell>RE conv3 1</cell></row><row><cell>RE conv3 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>128</cell><cell>1</cell><cell>RE conv3 2</cell></row><row><cell>RE conv3 2</cell><cell>Max Pooling</cell><cell>2 × 2</cell><cell>128</cell><cell>2</cell><cell>RE pool3</cell></row><row><cell>RE pool3</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>256</cell><cell>1</cell><cell>RE conv4 1</cell></row><row><cell>RE conv4 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>256</cell><cell>1</cell><cell>RE conv4 2</cell></row><row><cell>RE conv4 2</cell><cell>Max Pooling</cell><cell>2 × 2</cell><cell>256</cell><cell>2</cell><cell>RE pool4</cell></row><row><cell>RE pool4</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>512</cell><cell>1</cell><cell>RE conv5 1</cell></row><row><cell>RE conv5 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>512</cell><cell>1</cell><cell>RE conv5 2</cell></row><row><cell>RE conv5 2</cell><cell>Deconv</cell><cell>2 × 2</cell><cell>256</cell><cell>2</cell><cell>RE up1</cell></row><row><cell>RE up1, RE conv4 2</cell><cell>Concat</cell><cell>-</cell><cell>512</cell><cell>-</cell><cell>RE concat2</cell></row><row><cell>RE concat2</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>256</cell><cell>1</cell><cell>RE conv6 1</cell></row><row><cell>RE conv6 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>256</cell><cell>1</cell><cell>RE conv6 2</cell></row><row><cell>RE conv6 2</cell><cell>Deconv</cell><cell>2 × 2</cell><cell>128</cell><cell>2</cell><cell>RE up2</cell></row><row><cell>RE up2, RE conv3 2</cell><cell>Concat</cell><cell>-</cell><cell>256</cell><cell>-</cell><cell>RE concat3</cell></row><row><cell>RE concat3</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>128</cell><cell>1</cell><cell>RE conv7 1</cell></row><row><cell>RE conv7 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>128</cell><cell>1</cell><cell>RE conv7 2</cell></row><row><cell>RE conv7 2</cell><cell>Deconv</cell><cell>2 × 2</cell><cell>64</cell><cell>2</cell><cell>RE up3</cell></row><row><cell>RE up3, RE conv2 2</cell><cell>Concat</cell><cell>-</cell><cell>128</cell><cell>-</cell><cell>RE concat4</cell></row><row><cell>RE concat4</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>64</cell><cell>1</cell><cell>RE conv8 1</cell></row><row><cell>RE conv8 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>64</cell><cell>1</cell><cell>RE conv8 2</cell></row><row><cell>RE conv8 2</cell><cell>Deconv</cell><cell>2 × 2</cell><cell>32</cell><cell>2</cell><cell>RE up4</cell></row><row><cell>RE up4, RE conv1 2</cell><cell>Concat</cell><cell>-</cell><cell>64</cell><cell>-</cell><cell>RE concat5</cell></row><row><cell>RE concat5</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>32</cell><cell>1</cell><cell>RE conv9 1</cell></row><row><cell>RE conv9 1</cell><cell>Conv&amp;ReLU</cell><cell>3 × 3</cell><cell>256</cell><cell>1</cell><cell>RE conv9 2</cell></row><row><cell>RE conv9 2</cell><cell>Conv</cell><cell>3 × 3</cell><cell>3</cell><cell>1</cell><cell>RE conv10</cell></row><row><cell>RE conv10</cell><cell>Sigmoid</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>RE refletance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Reflectance restoration network</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Illumination adjustment network term concentrates on the closeness in terms of textures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Quantitative comparison on LOL dataset in terms of PSNR, SSIM, LOE, LOE ref , and NIQE. The best results are highlighted in bold.</figDesc><table><row><cell>Metric</cell><cell></cell><cell>NIQE</cell><cell></cell></row><row><cell>Datasets</cell><cell cols="3">LIME-data NPE-data MEF-data</cell></row><row><cell>BIMEF [32]</cell><cell>3.8169</cell><cell>4.1963</cell><cell>3.4237</cell></row><row><cell>CRM [33]</cell><cell>3.8546</cell><cell>3.9220</cell><cell>3.2708</cell></row><row><cell>Dong [34]</cell><cell>4.0516</cell><cell>4.1263</cell><cell>4.1094</cell></row><row><cell>LIME [11]</cell><cell>4.1549</cell><cell>4.2629</cell><cell>3.7159</cell></row><row><cell>MF [35]</cell><cell>4.0689</cell><cell>4.1096</cell><cell>3.4773</cell></row><row><cell>RRM [13]</cell><cell>4.6426</cell><cell>4.8452</cell><cell>4.1535</cell></row><row><cell>SRIE [12]</cell><cell>3.7863</cell><cell>3.9795</cell><cell>3.4577</cell></row><row><cell>Retinex [21]</cell><cell>4.5977</cell><cell>4.5674</cell><cell>4.4755</cell></row><row><cell>MSR [8]</cell><cell>3.7642</cell><cell>4.3663</cell><cell>3.6096</cell></row><row><cell>NPE [9]</cell><cell>3.9048</cell><cell>3.9520</cell><cell>3.5378</cell></row><row><cell>GLAD [36]</cell><cell>4.1280</cell><cell>3.9699</cell><cell>3.3435</cell></row><row><cell>KinD</cell><cell>3.7236</cell><cell>3.8826</cell><cell>3.3429</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Quantitative comparison on LIME, NPE, and MEF datasets in terms of NIQE. The best results are highlighted in bold. on the LIME and NPE datasets. For the MEF data, it is only behind CRM by a small difference (3.34 vs. 3.27).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pisano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hemminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deluca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braeuning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="200" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple and effective histogram equalization approach to image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="170" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A dynamic histogram equalization for image contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdullah-Al-Wadud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Consum. Electron</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="593" to="600" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contextual and variational contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Turgay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3431" to="3441" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2d histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="108" to="128" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="62" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3538" to="3548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fusionbased enhancing method for weakly illuminated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lime: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2782" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structure-revealing low-light image enhancement via robust retinex model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2828" to="2841" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="341" to="349" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dehazenet: An endto-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Llnet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Lore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akintayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Msr-net:lowlight image enhancement using deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3291" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adaptive multicolumn deep neural networks with application to robust image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamically unfolding recurrent restorer: A moving endpoint control method for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Powerconstrained contrast enhancement for emissive displays based on histogram equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chulwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Young-Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang-Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="93" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A bio-inspired multi-exposure fusion framework for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A new low-light image enhancement algorithm using camera response model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast efficient algorithm for enhancement of low lighting video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICME</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A fusionbased enhancing method for weakly illuminated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gladnet: Low-light enhancement network with global awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
