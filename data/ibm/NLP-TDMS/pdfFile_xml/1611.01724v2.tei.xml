<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS? FINE-GRAINED GATING FOR READING COMPREHENSION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<email>zhiliny@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2017 WORDS OR CHARACTERS? FINE-GRAINED GATING FOR READING COMPREHENSION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test and Who Did What datasets. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Finding semantically meaningful representations of the words (also called tokens) in a document is necessary for strong performance in Natural Language Processing tasks. In neural networks, tokens are mainly represented in two ways, either using word-level representations or character-level representations. Word-level representations are obtained from a lookup table, where each unique token is represented as a vector. Character-level representations are usually obtained by applying recurrent neural networks (RNNs) or convolutional neural networks (CNNs) on the character sequence of the token, and their hidden states are combined to form the representation. Word-level representations are good at memorizing the semantics of the tokens while character-level representations are more suitable for modeling sub-word morphologies <ref type="bibr" target="#b12">(Ling et al., 2015;</ref>. For example, considering "cat" and "cats", word-level representations can only learn the similarities between the two tokens by training on a large amount of training data, while character-level representations, by design, can easily capture the similarities. Character-level representations are also used to alleviate the difficulties of modeling out-of-vocabulary (OOV) tokens <ref type="bibr" target="#b13">(Luong &amp; Manning, 2016)</ref>.</p><p>Hybrid word-character models have been proposed to leverage the advantages of both word-level and character-level representations. The most commonly used method is to concatenate these two representations . However, concatenating word-level and character-level representations is technically problematic. For frequent tokens, the word-level representations are usually accurately estimated during the training process, and thus introducing character-level representations can potentially bias the entire representations. For infrequent tokens, the estimation of wordlevel representations have high variance, which will have negative effects when combined with the character-level representations. To address this issue, recently <ref type="bibr" target="#b14">Miyamoto &amp; Cho (2016)</ref> introduced a scalar gate conditioned on the word-level representations to control the ratio of the two representations. However, for the task of reading comprehension, preliminary experiments showed that this method was not able to improve the performance over concatenation. There are two possible reasons. First, word-level representations might not contain sufficient information to support the decisions of selecting between the two representations. Second, using a scalar gate means applying the same ratio for each of the dimensions, which can be suboptimal.</p><p>In this work, we present a fine-grained gating mechanism to combine the word-level and characterlevel representations. We compute a vector gate as a linear projection of the token features followed by a sigmoid activation. We then multiplicatively apply the gate to the character-level and wordlevel representations. Each dimension of the gate controls how much information is flowed from the word-level and character-level representations respectively. We use named entity tags, part-ofspeech tags, document frequencies, and word-level representations as the features for token properties which determine the gate. More generally, our fine-grained gating mechanism can be used to model multiple levels of structure in language, including words, characters, phrases, sentences and paragraphs. In this work we focus on studying the effects on word-character gating.</p><p>To better tackle the problem of reading comprehension, we also extend the idea of fine-grained gating for modeling the interaction between documents and queries. Previous work has shown the importance of modeling interactions between document and query tokens by introducing various attention architectures for the task <ref type="bibr" target="#b7">(Hermann et al., 2015;</ref><ref type="bibr" target="#b10">Kadlec et al., 2016)</ref>. Most of these use an inner product between the two representations to compute the relative importance of document tokens. The Gated-Attention Reader <ref type="bibr" target="#b5">(Dhingra et al., 2016a)</ref> showed improved performance by replacing the inner-product with an element-wise product to allow for better matching at the semantic level. However, they use aggregated representations of the query which may lead to loss of information. In this work we use a fine-grained gating mechanism for each token in the paragraph and each token in the query. The fine-grained gating mechanism applies an element-wise multiplication of the two representations.</p><p>We show improved performance on reading comprehension datasets, including Children's Book Test (CBT), Who Did What, and SQuAD. On CBT, our approach achieves new state-of-the-art results without using an ensemble. Our model also improves over state-of-the-art results on the Who Did What dataset. To demonstrate the generality of our method, we apply our word-character finegrained gating mechanism to a social media tag prediction task and show improved performance over previous methods.</p><p>Our contributions are two-fold. First, we present a fine-grained word-character gating mechanism and show improved performance on a variety of tasks including reading comprehension. Second, to better tackle the reading comprehension tasks, we extend our fine-grained gating approach to modeling the interaction between documents and queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Hybrid word-character models have been proposed to take advantages of both word-level and character-level representations. <ref type="bibr" target="#b12">Ling et al. (2015)</ref> introduce a compositional character to word (C2W) model based on bidirectional LSTMs. <ref type="bibr" target="#b11">Kim et al. (2016)</ref> describe a model that employs a convolutional neural network (CNN) and a highway network over characters for language modeling. <ref type="bibr" target="#b14">Miyamoto &amp; Cho (2016)</ref> use a gate to adaptively find the optimal mixture of the character-level and word-level inputs.  employ deep gated recurrent units on both character and word levels to encode morphology and context information. Concurrent to our work, ? employed a similar gating idea to combine word-level and character-level representations, but their focus is on low-level sequence tagging tasks and the gate is not conditioned on linguistic features.</p><p>The gating mechanism is widely used in sequence modeling. Long short-term memory (LSTM) networks <ref type="bibr" target="#b9">(Hochreiter &amp; Schmidhuber, 1997)</ref> are designed to deal with vanishing gradients through the gating mechanism. Similar to LSTM, Gated Recurrent Unit (GRU) was proposed by <ref type="bibr" target="#b2">Cho et al. (2014)</ref>, which also uses gating units to modulate the flow of information. The gating mechanism can also be viewed as a form of attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b24">Yang et al., 2016b)</ref> over two inputs.</p><p>Similar to the idea of gating, multiplicative integration has also been shown to provide a benefit in various settings. <ref type="bibr" target="#b22">Yang et al. (2014)</ref> find that multiplicative operations are superior to additive operations in modeling relations. <ref type="bibr" target="#b21">Wu et al. (2016)</ref> propose to use Hadamard product to replace sum operation in recurrent networks, which gives a significant performance boost over existing RNN models. <ref type="bibr" target="#b5">Dhingra et al. (2016a)</ref> use a multiplicative gating mechanism to achieve state-of-the-art results on question answering benchmarks.</p><p>Reading comprehension is a challenging task for machines. A variety of models have been proposed to extract answers from given text <ref type="bibr" target="#b8">(Hill et al., 2016;</ref><ref type="bibr" target="#b10">Kadlec et al., 2016;</ref><ref type="bibr" target="#b18">Trischler et al., 2016;</ref><ref type="bibr" target="#b17">Sordoni et al., 2016;</ref><ref type="bibr" target="#b4">Cui et al., 2016)</ref>.  propose a dynamic chunk reader to extract and rank a set of answer candidates from a given document to answer questions. <ref type="bibr" target="#b20">Wang &amp; Jiang (2016)</ref> introduce an end-to-end neural architecture which incorporates match-LSTM and pointer networks <ref type="bibr" target="#b19">(Vinyals et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FINE-GRAINED GATING</head><p>In this section, we will describe our fine-grained gating approach in the context of reading comprehension. We first introduce the settings of reading comprehension tasks and a general neural network architecture. We will then describe our word-character gating and document-query gating approaches respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">READING COMPREHENSION SETTING</head><p>The reading comprehension task involves a document P = (p 1 , p 2 , · · · , p M ) and a query Q = (q 1 , q 2 , · · · , q N ), where M and N are the lengths of the document and the query respectively. Each token p i is denoted as (w i , C i ), where w i is a one-hot encoding of the token in the vocabulary and C i is a matrix with each row representing a one-hot encoding of a character. Each token in the query q j is similarly defined. We use i as a subscript for documents and j for queries. The output of the problem is an answer a, which can either be an index or a span of indices in the document. Now we describe a general architecture used in this work, which is a generalization of the gated attention reader <ref type="bibr" target="#b5">(Dhingra et al., 2016a)</ref>. For each token in the document and the query, we compute a vector representation using a function f . More specifically, for each token p i in the document,</p><formula xml:id="formula_0">we have h 0 i = f (w i , C i ).</formula><p>The same function f is also applied to the tokens in the query. Let H 0 p and H q denote the vector representations computed by f for tokens in documents and queries respectively. In Section 3.2, we will discuss the "word-character" fine-grained gating used to define the function f .</p><p>Suppose that we have a network of K layers. At the k-th layer, we apply RNNs on H k−1 p and H q to obtain hidden states P k and Q k , where P k is a M × d matrix and Q k is a N × d matrix with d being the number of hidden units in the RNNs. Then we use a function r to compute a new representation for the document H k p = r(P k , Q k ). In Section 3.3, we will introduce the "document-query" finegrained gating used to define the function r.</p><p>After going through K layers, we predict the answer index a using a softmax layer over hidden states H k p . For datasets where the answer is a span of text, we use two softmax layers for the start and end indices respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WORD-CHARACTER FINE-GRAINED GATING</head><p>Given a one-hot encoding w i and a character sequence C i , we now describe how to compute the vector representation h i = f (w i , C i ) for the token. In the rest of the section, we will drop the subscript i for notation simplicity.</p><p>We first apply an RNN on C and take the hidden state in the last time step c as the character-level representation . Let E denote the token embedding lookup table. We perform a matrix-vector multiplication Ew to obtain a word-level representation. We assume c and Ew have the same length d e in this work.</p><p>Previous methods defined f using the word-level representation Ew <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>, the character-level representation c <ref type="bibr" target="#b12">(Ling et al., 2015)</ref>, or the concatenation [Ew; c] . Unlike these methods, we propose to use a gate to dynamically choose between the word-level and character-level representations based on the properties of the token. Let v denote a feature vector that encodes these properties. In this work, we use the concatenation of named entity tags, partof-speech tags, binned document frequency vectors, and the word-level representations to form the feature vector v. Let d v denote the length of v.</p><p>The gate is computed as follows: where W g and b g are the model parameters with shapes d e × d v and d e , and σ denotes an elementwise sigmoid function.</p><formula xml:id="formula_1">g = σ(W g v + b g )</formula><p>The final representation is computed using a fine-grained gating mechanism,</p><formula xml:id="formula_2">h = f (c, w) = g c + (1 − g) (Ew)</formula><p>where denotes element-wise product between two vectors.</p><p>An illustration of our fine-grained gating mechanism is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Intuitively speaking, when the gate g has high values, more information flows from the character-level representation to the final representation; when the gate g has low values, the final representation is dominated by the word-level representation.</p><p>Though <ref type="bibr" target="#b14">Miyamoto &amp; Cho (2016)</ref> also use a gate to choose between word-level and character-level representations, our method is different in two ways. First, we use a more fine-grained gating mechanism, i.e., vector gates rather than scalar gates. Second, we condition the gate on features that better reflect the properties of the token. For example, for noun phrases and entities, we would expect the gate to bias towards character-level representations because noun phrases and entities are usually less common and display richer morphological structure. Experiments show that these changes are key to the performance improvements for reading comprehension tasks.</p><p>Our approach can be further generalized to a setting of multi-level networks so that we can combine multiple levels of representations using fine-grained gating mechanisms, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DOCUMENT-QUERY FINE-GRAINED GATING</head><p>Given the hidden states P k and Q k , we now describe how to compute a representation H k that encodes the interactions between the document and the query. In this section, we drop the superscript k (the layer number) for notation simplicity. Let p i denote the i-th row of P and q j denote the j-row of Q. Let d h denote the lengths of p i and q j .</p><p>Attention-over-attention (AoA) <ref type="bibr" target="#b4">(Cui et al., 2016)</ref> defines a dot product between each pair of tokens in the document and the query, i.e., p T i q j , followed by row-wise and column-wise softmax nonlinearities. AoA imposes pair-wise interactions between the document and the query, but using a dot product is potentially not expressive enough and hard to generalize to multi-layer networks. The gated attention (GA) reader <ref type="bibr" target="#b5">(Dhingra et al., 2016a)</ref> defines an element-wise product as p i g i where g i is a gate computed by attention mechanism on the token p i and the entire query. The intuition for the gate g i is to attend to important information in the document. However, there is no direct pair-wise interaction between each token pair. We present a fine-grained gating method that combines the advantages of the above methods (i.e., both pairwise and element-wise). We compute the pairwise element-wise product between the hidden states in the document and the query, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. More specifically, for p i and q j , we have</p><formula xml:id="formula_3">I ij = tanh(p i q j )</formula><p>where q j can be viewed as a gate to filter the information in p i . We then use an attention mechanism over I ij to output hidden states h i as follows</p><formula xml:id="formula_4">h i = j softmax(u T h I ij + w T i w j b h1 + b h2 )I ij</formula><p>where u h is a d v -dimensional model parameter, b h1 and b h2 are scalar model parameters, w i and w j are one-hot encodings for p i and q j respectively. We additionally use one-hot encodings in the attention mechanism to reinforce the matching between the same tokens since such information is not fully preserved in I ij when k is large. The softmax nonlinearity is applied over all j's. The final hidden states H are formed by concatenating the h i 's for each token p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We first present experimental results on the Twitter dataset where we can rule out the effects of different choices of network architectures, to demonstrate the effectiveness of our word-character fine-grained gating approach. Later we show experiments on more challenging datasets on reading comprehension to further show that our approach can be used to improve the performance on highlevel NLP tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EVALUATING WORD-CHARACTER GATING ON TWITTER</head><p>We evaluate the effectiveness of our word-character fine-grained gating mechanism on a social media tag prediction task. We use the Twitter dataset and follow the experimental settings in <ref type="bibr" target="#b6">Dhingra et al. (2016b)</ref>. We also use the same network architecture upon the token representations, which is an LSTM layer followed by a softmax classification layer <ref type="bibr" target="#b6">(Dhingra et al., 2016b)</ref>. The Twitter dataset consists of English tweets with at least one hashtag from Twitter. Hashtags and HTML tags have been removed from the body of the tweet, and user names and URLs are replaced with special tokens. The dataset contains 2 million tweets for training, 10K for validation and 50K for testing, with a total of 2,039 distinct hashtags. The task is to predict the hashtags of each tweet.</p><p>We compare several different methods as follows. Word char concat uses the concatenation of word-level and character-level representations as in ; word char feat concat concatenates the word-level and character-level representations along with the features described in   <ref type="bibr" target="#b14">Miyamoto &amp; Cho (2016)</ref> but is conditioned on the features; fine-grained gate is our method described in Section 3.2. We include word char feat concat for a fair comparison because our fine-grained gating approach also uses the token features.</p><p>The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. We report three evaluation metrics including precision@1, re-call@10, and mean rank. Our method outperforms character-level models used in <ref type="bibr" target="#b6">Dhingra et al. (2016b)</ref> by 2.29%, 2.69%, and 2.5 points in terms of precision, recall and mean rank respectively. We can observe that scalar gating approach <ref type="bibr" target="#b14">(Miyamoto &amp; Cho, 2016)</ref> can only marginally improve over the baseline methods, while fine-grained gating methods can substantially improve model performance. Note that directly concatenating the token features with the character-level and word-level representations does not boost the performance, but using the token features to compute a gate (as done in fine-grained gating) leads to better results. This indicates that the benefit of fine-grained gating mainly comes from better modeling rather than using additional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PERFORMANCE ON READING COMPREHENSION</head><p>After investigating the effectiveness of the word-character fine-grained gating mechanism on the Twitter dataset, we now move on to a more challenging task, reading comprehension. In this section, we experiment with two datasets, the Children's Book Test dataset <ref type="bibr" target="#b8">(Hill et al., 2016)</ref> and the SQuAD dataset <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">CLOZE-STYLE QUESTIONS</head><p>We evaluate our model on cloze-style question answering benchmarks.    0.712 (0.710) 0.625 (0.625) <ref type="bibr" target="#b20">Wang &amp; Jiang (2016)</ref> 0.700 (0.703) 0.591 (0.595)</p><p>The Children's Book Test (CBT) dataset is built from children's books. The whole dataset has 669,343 questions for training, 8,000 for validation and 10,000 for testing. We closely follow the setting in <ref type="bibr" target="#b5">Dhingra et al. (2016a)</ref> and incrementally add different components to see the changes in performance. For the fine-grained gating approach, we use the same hyper-parameters as in <ref type="bibr" target="#b5">Dhingra et al. (2016a)</ref> except that we use a character-level GRU with 100 units to be of the same size as the word lookup table. The word embeddings are updated during training.</p><p>In addition to different ways of combining word-level and character-level representations, we also compare two different ways of integrating documents and queries: GA refers to the gated attention reader <ref type="bibr" target="#b5">(Dhingra et al., 2016a)</ref> and FG refers to our fine-grained gating described in Section 3.3.</p><p>The results are reported in <ref type="table" target="#tab_1">Table 2</ref>. We report the results on common noun (CN) questions and named entity (NE) questions, which are two widely used question categories in CBT. Our finegrained gating approach achieves new state-of-the-art performance on both settings and outperforms the current state-of-the-art results by up to 1.76% without using ensembles. Our method outperforms the baseline GA reader by up to 2.4%, which indicates the effectiveness of the fine-grained gating mechanism. Consistent with the results on the Twitter dataset, using word-character fine-grained gating can substantially improve the performance over concatenation or scalar gating. Furthermore, we can see that document-query fine-grained gating also contributes significantly to the final results.</p><p>We also apply our fine-grained gating model to the Who Did What (WDW) dataset (?). As shown in <ref type="table" target="#tab_2">Table 3</ref>, our model achieves state-of-the-art results compared to strong baselines. We fix the word embeddings during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">SQUAD</head><p>The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset collected recently <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>. It contains 23,215 paragraphs come from 536 Wikipedia articles. Unlike other reading comprehension datasets such as CBT, the answers are a span of text rather than a single word. The dataset is partitioned into a training set (80%, 87,636 question-answer pairs), a development set (10%, 10,600 question-answer pairs) and a test set which is not released.  We report our results in <ref type="table" target="#tab_3">Table 4</ref>. "Exact match" computes the ratio of questions that are answered correctly by strict string comparison, and the F1 score is computed on the token level. We can observe that both word-character fine-grained gating and document-query fine-grained gating can substantially improve the performance, leading to state-of-the-art results among published papers. Note that at the time of submission, the best score on the leaderboard is 0.716 in exact match and 0.804 in F1 without published papers. A gap exists because our architecture described in Section 3.1 does not specifically model the answer span structure that is unique to SQuAD. In this work, we focus on this general architecture to study the effectiveness of fine-grained gating mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VISUALIZATION AND ANALYSIS</head><p>We visualize the model parameter W g as described in Section 3.2. For each feature, we average the corresponding weight vector in W g . The results are described in <ref type="figure" target="#fig_2">Figure 3</ref>. We can see that named entities like "Organization" and noun phrases (with tags "NNP" or "NNPS") tend to use characterlevel representations, which is consistent with human intuition because those tokens are usually infrequent or display rich morphologies. Also, DOCLEN-4, WH-adverb ("WRB"), and conjunction ("IN" and "CC") tokens tend to use word-level representations because they appear frequently.</p><p>We also sample random span of text from the SQuAD dataset, and visualize the average gate values in <ref type="figure" target="#fig_3">Figure 4</ref>. The results are consistent with our observations in <ref type="figure" target="#fig_2">Figure 3</ref>. Rare tokens, noun phrases, and named entities tend to use character-level representations, while others tend to use word-level representations. To further justify this argument, we also list the tokens with highest and lowest gate values in <ref type="table" target="#tab_4">Table 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We present a fine-grained gating mechanism that dynamically combines word-level and characterlevel representations based on word properties. Experiments on the Twitter tag prediction dataset show that fine-grained gating substantially outperforms scalar gating and concatenation. Our method also improves the performance on reading comprehension and achieves new state-of-the-art results on CBT and WDW. In our future work, we plan to to apply the fine-grained gating mechanism for combining other levels of representations, such as phrases and sentences. It will also be intriguing to integrate NER and POS networks and learn the token representation in an end-to-end manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Word-character fine-grained gating. The two lookup tables are shared. "NER", "POS", "frequency" refer to named entity tags, part-of-speech tags, document frequency features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Paragraph-question fine-grained gating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the weight matrix Wg. Weights for each features are averaged. Red means high and yellow means low. High weight values favor character-level representations, and low weight values favor wordlevel representations. "Organization", "'Person", "Location", and "O" are named entity tags; "DOCLEN-n" are document frequency features (larger n means higher frequency, n from 0 to 4); others are POS tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of gate values in the text. Red means high and yellow means low. High gate values favor character-level representations, and low gate values favor word-level representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance on the Twitter dataset. "word" and "char" means using word-level and character-level representations respectively.</figDesc><table><row><cell>Model</cell><cell cols="3">Precision@1 Recall@10 Mean Rank</cell></row><row><cell cols="2">word (Dhingra et al., 2016b) 0.241</cell><cell>0.428</cell><cell>133</cell></row><row><cell>char (Dhingra et al., 2016b)</cell><cell>0.284</cell><cell>0.485</cell><cell>104</cell></row><row><cell>word char concat</cell><cell>0.2961</cell><cell>0.4959</cell><cell>105.8</cell></row><row><cell>word char feat concat</cell><cell>0.2951</cell><cell>0.4974</cell><cell>106.2</cell></row><row><cell>scalar gate</cell><cell>0.2974</cell><cell>0.4982</cell><cell>104.2</cell></row><row><cell>fine-grained gate</cell><cell>0.3069</cell><cell>0.5119</cell><cell>101.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance on the CBT dataset. The "GA word char concat" results are extracted from Dhingra et al. (2016a). Our results on fine-grained gating are based on a single model. "CN" and "NE" are two widely used question categories. "dev" means development set, and "test" means test set.</figDesc><table><row><cell>Model</cell><cell cols="4">CN dev CN test NE dev NE test</cell></row><row><cell>GA word char concat</cell><cell>0.731</cell><cell>0.696</cell><cell>0.768</cell><cell>0.725</cell></row><row><cell>GA word char feat concat</cell><cell>0.7250</cell><cell cols="3">0.6928 0.7815 0.7256</cell></row><row><cell>GA scalar gate</cell><cell>0.7240</cell><cell cols="3">0.6908 0.7810 0.7260</cell></row><row><cell>GA fine-grained gate</cell><cell>0.7425</cell><cell cols="3">0.7084 0.7890 0.7464</cell></row><row><cell>FG fine-grained gate</cell><cell>0.7530</cell><cell cols="3">0.7204 0.7910 0.7496</cell></row><row><cell>Sordoni et al. (2016)</cell><cell>0.721</cell><cell>0.692</cell><cell>0.752</cell><cell>0.686</cell></row><row><cell>Trischler et al. (2016)</cell><cell>0.715</cell><cell>0.674</cell><cell>0.753</cell><cell>0.697</cell></row><row><cell>Cui et al. (2016)</cell><cell>0.722</cell><cell>0.694</cell><cell>0.778</cell><cell>0.720</cell></row><row><cell>Munkhdalai &amp; Yu (2016)</cell><cell>0.743</cell><cell>0.719</cell><cell>0.782</cell><cell>0.732</cell></row><row><cell>Kadlec et al. (2016) ensemble</cell><cell>0.711</cell><cell>0.689</cell><cell>0.762</cell><cell>0.710</cell></row><row><cell>Sordoni et al. (2016) ensemble</cell><cell>0.741</cell><cell>0.710</cell><cell>0.769</cell><cell>0.720</cell></row><row><cell cols="2">Trischler et al. (2016) ensemble 0.736</cell><cell>0.706</cell><cell>0.766</cell><cell>0.718</cell></row><row><cell cols="2">Section 3.2; scalar gate uses a scalar gate similar to</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance on the Who Did What dataset. "dev" means development set, and "test" means test set."WDW-R" is the relaxed version of WDW.</figDesc><table><row><cell>Model</cell><cell cols="4">WDW dev WDW test WDW-R dev WDW-R test</cell></row><row><cell>Kadlec et al. (2016)</cell><cell>-</cell><cell>0.570</cell><cell>-</cell><cell>0.590</cell></row><row><cell>Chen et al. (2016)</cell><cell>-</cell><cell>0.640</cell><cell>-</cell><cell>0.650</cell></row><row><cell cols="2">Munkhdalai &amp; Yu (2016) 0.665</cell><cell>0.662</cell><cell>0.670</cell><cell>0.667</cell></row><row><cell>Dhingra et al. (2016a)</cell><cell>0.716</cell><cell>0.712</cell><cell>0.726</cell><cell>0.726</cell></row><row><cell>this paper</cell><cell>0.723</cell><cell>0.717</cell><cell>0.731</cell><cell>0.726</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance on the SQuAD dev set. Test set results are included in the brackets.</figDesc><table><row><cell>Model</cell><cell>F1</cell><cell>Exact Match</cell></row><row><cell>GA word</cell><cell>0.6695</cell><cell>0.5492</cell></row><row><cell>GA word char concat</cell><cell>0.6857</cell><cell>0.5639</cell></row><row><cell>GA word char feat concat</cell><cell>0.6904</cell><cell>0.5711</cell></row><row><cell>GA scalar gate</cell><cell>0.6850</cell><cell>0.5620</cell></row><row><cell>GA fine-grained gate</cell><cell>0.6983</cell><cell>0.5804</cell></row><row><cell>FG fine-grained gate</cell><cell>0.7125</cell><cell>0.5995</cell></row><row><cell cols="3">FG fine-grained gate + ensemble 0.7341 (0.733) 0.6238 (0.625)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Word tokens with highest and lowest gate values. High gate values favor character-level representations, and low gate values favor word-level representations. Gate values Word tokens Lowest or but But These these However however among Among that when When although Although because Because until many Many than though Though this This Since since date where Where have That and And Such such number so which by By how before Before with With between Between even Even if Highest Sweetgum Untersee Jianlong Floresta Chlorella Obersee PhT Doctorin Jumonville WFTS WTSP Boven Pharm Nederrijn Otrar Rhin Magicicada WBKB Tanzler KMBC WPLG Mainau Merwede RMJM Kleitman Scheur Bodensee Kromme Horenbout Vorderrhein Chlamydomonas Scantlebury Qingshui Funchess</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was funded by NVIDIA, the Office of Naval Research Scene Understanding grant N000141310721, the NSF grant IIS1250956, and Google Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention-over-attention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tweet2vec: Character-based distributed representations for social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Muehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gated word-character recurrent language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04315</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Iterative alternating neural attention for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02245</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural language comprehension with the epireader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On multiplicative integration with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning multi-relational semantics using neural-embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06270</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end answer chunk extraction and ranking for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
