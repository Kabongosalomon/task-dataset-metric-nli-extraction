<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunde</forename><surname>Jia</surname></persName>
							<email>jiayunde@bit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
							<email>xtong@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code available at https://github.com/ Microsoft/Deep3DFaceReconstruction</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep learning based 3D face reconstruction methods have shown promising results in both quality and efficiency. However, training deep neural networks typically requires a large volume of data, whereas face images with ground-truth 3D face shapes are scarce. In this paper, we propose a novel deep 3D face reconstruction approach that 1) leverages a robust, hybrid loss function for weakly-supervised learning which takes into account both low-level and perception-level information for supervision, and 2) performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation. Our method is fast, accurate, and robust to occlusion and large pose. We provide comprehensive experiments on three datasets, systematically comparing our method with fifteen recent methods and demonstrating its state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Faithfully recovering the 3D shapes of human faces from unconstrained 2D images is a challenging task and has numerous applications such as face recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref>, face media manipulation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50]</ref>, and face animation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. Recently, there is a surge of interest in 3D face reconstruction from a single image using deep Convolutional Neutral Networks (CNN) in lieu of the complex and costly optimization used by traditional methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. Since ground truth 3D face data is scarce, many previous approaches resort to synthetic data or using 3D shapes fitted by traditional methods as surrogate shape labels <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. However, their accuracy may be jeopardized by the domain gap issue or the * This work was done when Yu Deng was an intern at MSRA. imperfect training labels.</p><p>To circumvent these issues, methods have been proposed to train networks without shape labels in an unsupervised or weakly-supervised fashion and promising results have been obtained <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b15">16]</ref>. The crux of unsupervised learning is a differentiable image formation procedure which renders a face image with the network predictions, and the supervision signal stems from the discrepancy between the input image and the rendered counterpart. For example, Tewari et al. <ref type="bibr" target="#b48">[49]</ref> and Sengupta et al. <ref type="bibr" target="#b45">[46]</ref> use pixelwise photometric difference as training loss. To improve robustness and expressiveness, Tewari et al. <ref type="bibr" target="#b47">[48]</ref> proposed a two-step reconstruction scheme where the second step produces a shape and texture correction with a neural network. Genova et al. <ref type="bibr" target="#b15">[16]</ref> proposed to measure face image discrepancy on the perception level by using the distances between deep features extracted from a face recognition network.</p><p>Our goal in this paper is to obtain accurate 3D face reconstruction with weakly-supervised learning. We identified that using low-level information of pixel-wise color alone may suffer from local minimum issue where low error can be obtained with unsatisfactory face shapes. On the other hand, using only perceptual loss also lead to suboptimal results since it ignores the pixel-wise consistency with raw image signal. In light of this, we propose a hybridlevel loss function which integrates both of them, giving rise to accurate results. We also propose a novel skin color based photometric error attention strategy, granting our method further robustness to occlusion and other challenging appearance variations such as beard and heavy make-up. We train an off-the-shelf deep CNN to predict 3D Morphable Model (3DMM) <ref type="bibr" target="#b4">[5]</ref> coefficients, and accurate reconstruction is achieved on multiple datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>With a strong CNN model for single-image 3D face reconstruction, we take a further step and consider the problem of CNN-based face reconstruction aggregation with a set of images. Given multiple face images of a subject (e.g., from a personal album) captured in the wild under disparate conditions, it is natural to leverage all the images to build a better 3D face shape. To apply the deep neural networks on arbitrary number of images, one solution would be aggregating the single-image reconstruction results, and perhaps the simplest strategy is naively averaging the recovered shapes. However, such a native strategy did not consider the quality of the input images (e.g., if some samples contain severe occlusion). Nor does it take full advantage of pose differences to improve the shape prediction.</p><p>In this paper, we propose to learn 3D face aggregation from multiple images, also in an unsupervised fashion. We train a simple auxiliary network to produce "confidence scores" of the regressed identity-bearing 3D model coefficients, and obtain final identity coefficients via confidencebased aggregation. Despite no explicit confidence label is used, our method automatically learns to favor high-quality (especially high-visibility) photos. Moreover, it can exploit pose difference to better fuse the complementary information, learning to more accurate 3D shapes.</p><p>To summarize, this paper makes the following two main contributions:</p><p>• We propose a CNN-based single-image face reconstruction method which exploits hybrid-level image information for weakly-supervised learning. Our loss consists of a robustified image-level loss and a perception-level loss. We demonstrate the benefit of combing them, and show the state-of-the-art accuracy of our method on multiple datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b55">56]</ref>, significantly outperforming previous methods trained in a fully supervised fashion <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b50">51]</ref>. Moreover, we show that with a low-dimensional 3DMM subspace, we are still able to outperform prior art with "unrestricted" 3D representations <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b13">14]</ref> by an appreciable margin.</p><p>• We propose a novel shape confidence learning scheme for multi-image face reconstruction aggregation. Our confidence prediction subnet is also trained in a weakly-supervised fashion without ground-truth label. We show that our method clearly outperforms naive aggregation (e.g., shape averaging) and some heuristic strategies <ref type="bibr" target="#b33">[34]</ref>. To our knowledge, this is the first attempt towards CNN-based 3D face reconstruction and aggregation from an unconstrained image set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D face reconstruction has been a longstanding task in computer vision and computer graphics. In the literature, 3D Morphable Models (3DMM) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b6">7]</ref> have played a paramount role for 3D face modelling. With a 3DMM, reconstruction can be performed by an analysis-by-synthesis scheme using image intensity <ref type="bibr" target="#b4">[5]</ref> and other features such as edges <ref type="bibr" target="#b38">[39]</ref>. More recently, model fitting using facial landmarks has gained much popularity with the growth of face alignment techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b2">3]</ref>. However, sparse landmarks cannot well capture the dense facial geometry so these methods are poor at preserving facial fidelity. Beyond 3DMM, another popular 3D face model is the multilinear tensor model <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b42">43]</ref>. A few model-free single-image reconstruction methods have been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b18">19]</ref>; most of them require some reference 3D face shapes. For example, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> estimate image depth by building correspondences between the input image and one or a set of reference 3D faces. In <ref type="bibr" target="#b26">[27]</ref>, a shape-from-shading approach is proposed with a reference 3D face as prior.</p><p>The aforementioned approaches usually involve costly optimization process to recover a high quality 3D face. Recently, numerous methods are proposed which employ CNNs to achieve efficient face reconstruction. Some of them apply CNNs to regress 3DMM coefficients directly <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b15">16]</ref>, some use multi-step schemes to add correction or details onto coarse 3DMM predictions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b17">18]</ref>, while others advocate direct modelfree reconstruction <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>For all these CNN-based methods, one great hurdle is the lack of training data. To alleviate the problem, many methods resort to synthetic data or using 3D shapes fitted by traditional methods as surrogate training labels <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>. Others have attempted unsupervised or weakly-supervised training <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b15">16]</ref>. Our method is also based on weakly-supervised learning, for which our findings in this paper are threefold: 1) the loss function is important for weakly-supervised learning and both low-level and perception-level information should be leveraged; 2) the results obtained with weak supervision can be significantly better than those trained with synthetic data or pseudo ground truth shapes, and 3) somewhat surprisingly, the results confined in the low-dimensional 3DMM subspace can still be much better than state-of-the-art results with "unrestricted" representations.</p><p>We also studied the problem of reconstruction aggregation from multiple images. One related work is <ref type="bibr" target="#b33">[34]</ref> which investigated the reconstruction quality measurement closest to human ratings and used it to fuse the reconstructions obtained with 3DMM fitting. We however show that their method is deficient in our case. Our method is also related to traditional methods working on unconstrained photo collections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. While excellent results have been obtained by these methods, they typically consist of multiple steps such as face frontalization, photometric stereo, and local normal refinement. The whole pipeline is complex and may break down under severe occlusion and extreme pose. Our goal in this paper is not to replace these traditional methods, but to study the shape aggregation problem (similar to <ref type="bibr" target="#b33">[34]</ref>) with a CNN and provide an extremely fast and robust alternative learned end-to-end. </p><formula xml:id="formula_0">a -identity β -expression δ -texture p -pose γ -lighting Confidence: c - a i | c i β i δ i p i γ i β i δ i p i γ i a aggr R-Net C-Net Analytic image generation R-Net Cosine distance</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skin Estimation</head><p>Deep identity feature loss </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception-level loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries: Models and Outputs</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>, we use a CNN to regress coefficients of a 3DMM face model. For unsupervised/weaklysupervised training <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref>, we also regress the illumination and face pose to enable analytic image regeneration. We detail our models and CNN outputs as follows.</p><p>3D Face Model. With a 3DMM, the face shape S and the texture T can be represented by an affine model:</p><formula xml:id="formula_1">S = S(α, β) =S + B id α + B exp β T = T(δ) =T + B t δ<label>(1)</label></formula><p>whereS andT are the average face shape and texture; B id , B exp , and B t are the PCA bases of identity, expression, and texture respectively, which are all scaled with standard deviations; α, β, and δ are the corresponding coefficient vectors for generating a 3D face. We adopt the popular 2009 Basel Face Model <ref type="bibr" target="#b32">[33]</ref> forS, B id ,T, and B t , and use the expression bases B exp of <ref type="bibr" target="#b17">[18]</ref> which are built from Face-Warehouse <ref type="bibr" target="#b10">[11]</ref>. A subset of the bases is selected, resulting in α ∈ R 80 , β ∈ R 64 and δ ∈ R 80 . We exclude the ear and neck region, and our final model contains 36K vertices.</p><p>Illumination Model. We assume a Lambertian surface for face and approximate the scene illumination with Spherical Harmonics (SH) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. The radiosity of a vertex s i with surface normal n i and skin texture t i can then be computed</p><formula xml:id="formula_2">as C(n i , t i |γ) = t i · B 2 b=1 γ b Φ b (n i ) where Φ b : R 3 → R<label>are</label></formula><p>SH basis functions and γ b are the corresponding SH coefficients. We choose B = 3 bands following <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref> and assume monochromatic lights such that γ ∈ R 9 .</p><p>Camera Model. We use the perspective camera model with an empirically-selected focal length for the 3D-2D projection geometry. The 3D face pose p is represented by rotation R ∈ SO(3) and translation t ∈ R 3 .</p><p>In summary, the unknowns to be predicted can be represented by a vector x = (α, β, δ, γ, p) ∈ R 239 . In this paper, we use a ResNet-50 network <ref type="bibr" target="#b21">[22]</ref> to regress these coefficients by modifying the last fully-connected layer to 239 neurons. For brevity, we denote this modified ResNet-50 network for single image reconstruction as R-Net. We present how we train it in the following section. in <ref type="figure" target="#fig_1">Fig. 1</ref>. Our R-Net is trained without any ground truth labels, but via evaluating a hybrid-level loss on I and backpropagate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image-Level Losses</head><p>We first introduce our loss functions on low-level information including per-pixel color and sparse 2D landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Robust Photometric Loss</head><p>First, it is straightforward to measure the dense photometric discrepancy between the raw image and the reconstructed one <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>. In this paper, we propose a robust, skinaware photometric loss instead of a naive one, defined as:</p><formula xml:id="formula_3">L photo (x) = i∈M A i · I i − I i (x) 2 i∈M A i<label>(2)</label></formula><p>where i denotes pixel index, M is reprojected face region which can be readily obtained, · denotes the l 2 norm, and A is a skin color based attention mask for the training image which is described as follows.</p><p>Skin Attention. To gain robustness to occlusions and other challenging appearance variations such as beard and heavy make-up, we compute a skin-color probability P i for each pixel. We train a naive Bayes classifier with Gaussian Mixture Models on a skin image dataset from <ref type="bibr" target="#b25">[26]</ref> . For</p><formula xml:id="formula_4">each pixel i, we set A i = 1, if P i &gt; 0.5 P i , otherwise .</formula><p>We find that such a simple skin-aware loss function works remarkably well in practice without the need for a face segmentation method <ref type="bibr" target="#b42">[43]</ref>. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the benefit of using our skin attention mask.</p><p>It is also worth mentioning that our loss in Eq. 2 integrates over 2D image pixels as opposed to 3D shape vertices in <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref>. It enables us to easily identify self-occlusion via z-buffering thus our trained model can handle large poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Landmark Loss</head><p>We also use landmark locations on the 2D image domain as weak supervision to train the network. We run the state-ofthe art 3D face alignment method of <ref type="bibr" target="#b7">[8]</ref> to detect 68 landmarks {q n } of the training images. During training, we project the 3D landmark vertices of our reconstructed shape onto the image obtaining {q n }, and compute the loss as:</p><formula xml:id="formula_5">L lan (x) = 1 N N n=1 ω n q n − q n (x) 2<label>(3)</label></formula><p>where ω n is the landmark weight which we experimentally set to 20 for inner mouth and nose points and others to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Perception-Level Loss</head><p>While using the low-level information to measure image discrepancy can generally yields decent results <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>, we find using them alone can lead to local minimum issue for CNN-based 3D face reconstruction. <ref type="figure" target="#fig_3">Figure 3</ref> shows that our R-net trained with only image-level losses generates smoother textures and lower photometric errors than the compared opponents, but the resultant 3D shapes are less accurate by visual inspection.</p><p>To tackle this issue, we introduce a perception-level loss to further guide the training. Inspired by <ref type="bibr" target="#b15">[16]</ref>, we seek for the weak-supervision signal from a pre-trained deep face recognition network. Specifically, we extract the deep features of the images and compute the cosine distance:</p><formula xml:id="formula_6">L per (x) = 1 − &lt; f (I), f (I (x)) &gt; f (I) · f (I (x))<label>(4)</label></formula><p>where f (·) denotes deep feature encoding and &lt; ·, · &gt; vector inner product. In this work, we train a FaceNet <ref type="bibr" target="#b43">[44]</ref> structure using an in-house face recognition dataset with 3M face images of 50K identities crawled from the Internet, and use it as our deep feature extractor. <ref type="figure" target="#fig_3">Figure 3</ref> shows that with the perceptual loss, the textures are sharper and the shapes are more faithful. Quantitative results in the experiment section also show the benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Regularization</head><p>To prevent face shape and texture degeneration, we add a commonly-used loss on the regressed 3DMM coefficients:</p><formula xml:id="formula_7">L coef (x) = ω α α 2 + ω β β 2 + ω γ δ 2<label>(5)</label></formula><p>which enforces a prior distribution towards the mean face. The balancing weights are empirically set to ω α = 1.0, ω β = 0.8 and ω γ = 1.7e−3.</p><p>Although the face textures in the Basel 2009 3DMM <ref type="bibr" target="#b32">[33]</ref> were obtained with special devices, they still contain some baked-in shading (e.g., ambient occlusion). To favor a constant skin albedo similar to <ref type="bibr" target="#b47">[48]</ref>, we add a flattening constrain to penalize the texture map variance:</p><formula xml:id="formula_8">L tex (x) = c∈{r,g,b} var(T c,R (x))<label>(6)</label></formula><p>where R is a pre-defined skin region covering cheek, noise, and forehead.</p><p>In summary, our loss function L(x) for R-Net is composed of two image-level losses, a perceptual loss and two regularization loss. Their weights are set to w photo = 1.9, w lan = 1.6e−3, w per = 0.2, w coef = 3e−4 and w tex = 5 respectively in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Weakly-supervised Neural Aggregation for Multi-Image Reconstruction</head><p>Given multiple face images of a subject (e.g., a photo album), it is natural to leverage all the images to build a better 3D face shape. Images captured under different conditions should contain information complementary to each other due to change of pose, lighting etc. Moreover, using an image set for reconstruction can gain further robustness to occlusion and bad lighting in some individual images.</p><p>Applying deep neural networks on an arbitrary number of orderless images is not straightforward. In this work, we use a network to learn a measurement of confidence or quality of the single-image reconstruction results, and use it to aggregate the individual shapes. Specifically, we seek to generate a vector c ∈ R 80 with positive elements measuring the confidence of the identity-bearing shape coefficients α ∈ R 80 . We do not consider other coefficients such as expression, pose, and lighting as they vary across images and fusion is unnecessary. We also bypass texture as we found the skin color of a subject can vary significantly across inthe-wild images. Let I := {I j |j = 1, . . . , M } be an image collection of a person, x j = (α j , β j , δ j , p j , γ j ) the output coefficient vector from R-Net for each image j, and c j the confidence vector for each α j , we obtain the final shape via element-wise shape coefficient aggregation:</p><formula xml:id="formula_9">α aggr = ( j c j α j ) ( j c j )<label>(7)</label></formula><p>where and denote Hadamard product and division, respectively. Next, we present how we train a network, denoted as C-Net, to predict c in a weakly-supervised fashion without labels. The structure of C-Net will be presented afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Label-Free Training</head><p>To train C-Net on image sets, we generate the reconstructed image set {I j } of {I j } with {x j }, wherex j = (α aggr , β j , δ j , p j , γ j ). We define the training loss as</p><formula xml:id="formula_10">L({x j }) = 1 M M j=1 L(x j )<label>(8)</label></formula><p>where L(·) is our hybrid-level loss function defined in Section 4 evaluated with I j of I j . This way, the error can be backpropagated to α aggr thus further to c and C-Net weights, since Eq. 7 is differentiable. C-Net will be trained to produce confidences that lead to an aggregated 3D face shape consistent with the face image set as much as possible. The pipeline is illustrated in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>. In the multi-image training stage, the loss weights ω lm , ω photo and ω id are set to 1.6e−3, 1.9, and 0.1 respectively.</p><p>Our aggregation design and training scheme are inspired by the set-based face recognition work of <ref type="bibr" target="#b54">[55]</ref>. However, <ref type="bibr" target="#b54">[55]</ref> used a scalar quality score for feature vector aggregation, whereas we produce element-wise scores for 3DMM coefficients. In Section 6.2.1, we show element-wise scores yield superior results and analyze how our network exploits face pose difference for better shape aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Confidence-Net Structure</head><p>Our C-Net is designed to be light-weight. Since R-Net is able to predict high-level information such as pose and lighting, it is natural to reuse its feature maps for C-Net. In practice, we take both shallow and deep features from R-Net, as illustrated in <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>. The shallow feature can be used to measure image corruptions such as occlusion.</p><p>Specifically, we take the features after the first residual block F b1 ∈ R 28×28×256 and after global pooling F g ∈ R 2048 of R-Net as the input to C-Net. We apply three 3 × 3 convolution layers 256 channels and stride 2, followed by a global pooling layer on F b1 to get F b1 ∈ R 256 . We then concatenate F b1 and F g , and apply two fully-connected layers with 512 and 80 neurons respectively. At last, we apply sigmoid function to make the confidence predictions c ∈ R 80 positive. Our C-Net has 3M parameters in total, which is about 1/8 size of R-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Implementation Details. To train our R-Net, we collected in-the-wild images from multiple sources such as CelebA <ref type="bibr" target="#b31">[32]</ref>, 300W-LP <ref type="bibr" target="#b56">[57]</ref>, I-JBA <ref type="bibr" target="#b29">[30]</ref>, LFW <ref type="bibr" target="#b23">[24]</ref> and LS3D <ref type="bibr" target="#b7">[8]</ref>. We balanced the pose and race distributions and get ∼260K face images as our training set. We use the method of <ref type="bibr" target="#b11">[12]</ref> to detect and align the images. The input image size is 224×224. We take the weights pre-trained in ImageNet <ref type="bibr" target="#b41">[42]</ref> as initialization, and train R-Net using Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with batch size of 5, initial learning rate of 1e−4, and 500K total iterations. To train C-Net, we construct an image corpus using 300W-LP <ref type="bibr" target="#b56">[57]</ref>, Multi-PIE <ref type="bibr" target="#b16">[17]</ref> and part of our in-house face recognition dataset. For 300W-LP and Multi-PIE, we choose 5 images with rotation angles evenly distributed for each person. For the face recognition dataset, we randomly select 5 images for each person. The whole training set contains ∼50K images of ∼10K identities. We freeze the trained R-Net, and randomly initialize C-Net except for its last fully-connected layer which is initialized to zero (so that we start from average pooling). We train it using Adam <ref type="bibr" target="#b28">[29]</ref> with batch size of 5, initial learning rate of 2e−5 and 10K total iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results on Single-Image Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Ablation Study</head><p>To validate the efficacy of our proposed hybrid-level loss function, we conduct ablation study on two datasets: the MICC Florence 3D Face dataset <ref type="bibr" target="#b0">[1]</ref> and the FaceWarehouse dataset <ref type="bibr" target="#b10">[11]</ref>. MICC contains 53 subjects, each associated with a ground truth scan in neutral expression and three video sequences captured in cooperative, indoor, and outdoor scenarios. For FaceWarehouse, we use 9 subjects each with 20 expressions for evaluation. <ref type="table" target="#tab_0">Table 1</ref> presents the reconstruction errors with various loss combinations. It shows that jointly considering imageand perception-level information gives rise to significantly higher accuracy than using them separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Comparison with Prior Art</head><p>Comparison on MICC Florence with <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b30">31]</ref>. We first compare with the methods of Tran et al. <ref type="bibr" target="#b50">[51]</ref> and Genova et al. <ref type="bibr" target="#b15">[16]</ref>. For <ref type="bibr" target="#b50">[51]</ref> and ours, we evaluate the  <ref type="bibr" target="#b15">[16]</ref>. Our texture and shape exhibit larger variance and are more consistent with the inputs. The images are from <ref type="bibr" target="#b15">[16]</ref>. error with the average shape from a sequence. <ref type="bibr" target="#b15">[16]</ref> averaged their encoder embeddings from all frames before reconstruction and produce a single shape per sequence. Following <ref type="bibr" target="#b15">[16]</ref>, we crop the ground truth mesh to 95mm around the nose tip and run ICP with isotropic scale for alignment. The results of <ref type="bibr" target="#b50">[51]</ref> only contains part of the forehead region, thus we further cut the ground truth meshes accordingly for fair comparison. <ref type="table">Table 2</ref> shows that our method significantly outperforms <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b15">[16]</ref> on all three sequences. The qualitative comparison in <ref type="figure" target="#fig_4">Fig. 4</ref> and <ref type="figure" target="#fig_5">Fig. 5</ref> also demonstrates the superiority of our results. Note that <ref type="bibr" target="#b15">[16]</ref> uses a perceptual loss similar to ours, but they ignores the lowlevel information such as photometric similarity.</p><p>We then compare with PRN <ref type="bibr" target="#b13">[14]</ref>, a recent CNN method with supervised learning that predicts unrestricted face shapes. Following <ref type="bibr" target="#b13">[14]</ref>, we render face images with 20 poses for each subject using pitch angles of −15, 20, and 25 degrees and yaw angles of −80, 40, 0, 40, 80 degrees. <ref type="figure">Figure 6</ref> shows the point-to-plane RMSE averaged across subjects and pitch angles. Our method has a much lower error than PRN for all yaw angles. Also note that PRN has a larger model size than ours (160MB vs. 92MB). We further qualitatively compare with several learningbased methods including VRN <ref type="bibr" target="#b24">[25]</ref>, 3DDFA <ref type="bibr" target="#b56">[57]</ref>, and Liu et al. <ref type="bibr" target="#b30">[31]</ref>. <ref type="figure" target="#fig_5">Figure 5</ref> shows that our method can well-recover both identity and expression, whereas the results of other methods have very low shape variance.</p><p>Comparison on Facewarehouse with <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15]</ref>. We compare our results on the 9 Facewarehouse subjects selected by <ref type="bibr" target="#b47">[48]</ref>, with three learning-based approaches of Tewari et al. <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref>, Kim et al. <ref type="bibr" target="#b27">[28]</ref> and an optimizationbased approach of Garrido et al. <ref type="bibr" target="#b14">[15]</ref>. The evaluation protocol of <ref type="bibr" target="#b47">[48]</ref> is used.</p><p>We evaluate two face regions: a smaller one same as <ref type="bibr">Figure 7.</ref> in <ref type="bibr" target="#b47">[48]</ref>, and a larger one with more cheek areas included (see <ref type="figure">Fig. 7</ref>). The pointto-point errors are presented in <ref type="table">Table 3</ref>. Our method achieved the lowest reconstruction error among all learning-based methods. Note that <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b47">[48]</ref>-C (coarse results), <ref type="bibr" target="#b27">[28]</ref>, and our method are all based on 3DMM representation, and we show significant improvement upon theirs. Our method is even better than <ref type="bibr" target="#b47">[48]</ref>-F which uses a corrective space to refine the 3DMM shape. Our accuracy gets closer to the optimization-based approach of <ref type="bibr" target="#b14">[15]</ref> while our method can be orders of magnitude faster. We further compare with <ref type="bibr" target="#b47">[48]</ref> qualitatively in <ref type="figure" target="#fig_6">Fig 8.</ref> Our recovered shapes are of higher fidelity. Moreover, some artifacts from <ref type="bibr" target="#b47">[48]</ref> can be observed under occlusion while our results are much more pleasing. Also note that our method can handle profile faces (see, e.g., <ref type="figure" target="#fig_7">Fig. 9</ref>), while the largepose robustness of the above methods are unclear to us.</p><p>Comparison on BU-3DFE with <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b13">14]</ref>. The BU-3DFE dataset <ref type="bibr" target="#b55">[56]</ref> contains 100 subjects across different races and each with different expressions. Here we use the frontal images of 100 neutral faces for our evaluation. Again, we perform ICP alignment and compute point-to-plane distance. <ref type="table">Table 4</ref> shows that our method significantly outperforms <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b13">[14]</ref>. The BU-3DFE images are captured under controlled settings. <ref type="figure" target="#fig_7">Figure 9</ref> further shows that our method also outperforms <ref type="bibr" target="#b44">[45]</ref> under large pose and challenging appearance variance of in-the-wild images.</p><p>Comparison with other methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b52">53]</ref>. <ref type="table" target="#tab_0">Figure 10   Table 3</ref>. Mean reconstruction error (mm) on 180 meshes of 9 subjects from FaceWarehouse. "-F" and "-C" denote the "fine" and "coarse" results of <ref type="bibr" target="#b47">[48]</ref>. The face regions "S" (Smaller) and "L" (Larger) are shown in <ref type="figure">Fig. 7</ref>. Our error is lowest among the learning-based methods. *: due to the GPU parallel computing scheme, one forward pass of our R-Net takes 20ms with both batch-size 1 and batch-size 10 (evaluated with an NVIDIA TITAN Xp GPU). The times of other methods are quoted from <ref type="bibr" target="#b47">[48]</ref>.</p><p>Learning Optimization Ours <ref type="bibr" target="#b47">[48]</ref>   compares our results with Richardson et al. <ref type="bibr" target="#b36">[37]</ref>, Tran and Liu <ref type="bibr" target="#b52">[53]</ref> and Tewari et al. <ref type="bibr" target="#b48">[49]</ref>. By visual inspection, our method produces better results. , Tewari et al. <ref type="bibr" target="#b48">[49]</ref>, and Tran and Liu <ref type="bibr" target="#b52">[53]</ref>. Images are from <ref type="bibr" target="#b52">[53]</ref>. (Best viewed with zoom) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Ablation Study and Analysis</head><p>To test our multi-image shape aggregation method, we first conduct ablation study on render images of MICC. We render 20 poses for each of the 53 subjects as in Sec. 6.1.2. <ref type="table" target="#tab_3">Table 5</ref> presents the shape error of different aggregation strategies (S1 to S5). For S1, we train a C-Net similar to that described in Sec. 5.1 but modify the final FC layer to output a global confidence score c j ∈ R + , and we aggregate the identity coefficients via α aggr = j c j · α j / j c j . For S2, we sum all elements in the confidence vector c j ∈ R 80 to get a global confidence for aggregation. For S3, we simply choose a single shape with largest confidence vector summation for error computation. For S4, we use our element-wise coefficient aggregation described in Sec. 5.1. <ref type="table" target="#tab_3">Table 5</ref> shows that all shape aggregation methods including the naive shape averaging have a lower error than the mean of per-frame shape errors. Nevertheless, all our aggregation strategies yield better results than naive shape averaging, demonstrating the efficacy of our learning-based aggregation method. Among them, the element-wise coefficient aggregation (S4) performs best.</p><p>Analysis. We further analyze confidence score statistics to see whether they are affected by face pose. We compute the average relative confidence scores for profile and frontal images respectively. As shown in <ref type="figure" target="#fig_1">Fig 11 (left)</ref>, for profile faces the confidence is lower in general, but higher on a few dimensions. <ref type="figure" target="#fig_1">Figure 11</ref> (right) further shows that the coefficient entries having larger influence on face depth (Z-direction in the our 3D face coordinate system) tend to get relatively larger confidence scores on profile faces than on frontal ones. This is consistent with our intuition, and suggests that with element-wise confidences, the network can exploit view difference for better reconstruction.  <ref type="figure" target="#fig_1">Figure 11</ref>. Confidence statistics on frontal and profile images. We show the first 20 entries with largest PCA energy (standard derivation). Left: average relative confidence scores of 53 subjects. Right: Z-direction shape influence w.r.t. profile-to-frontal confidence ratio. Each dot represents a coefficient vector entry. Entries having larger influence on face depth (Z-direction) tend to get relatively larger confidence scores on profile faces than on frontal ones (linear regression R 2 = 0.423).  <ref type="table">Table 6</ref>. Multi-image reconstruction error on the MICC dataset. We use same inputs and evaluation metric as in <ref type="table">Table 2</ref>. "[34]-G" and "[34]-S" denote global and segment-based aggregation of our predicted shapes using the strategy of <ref type="bibr" target="#b33">[34]</ref>. Method Cooperative Indoor Outdoor All Shape averaging 1.66±0.52 1.66±0. <ref type="bibr" target="#b45">46</ref>   <ref type="figure" target="#fig_1">Figure 12</ref> presents some examples of our confidence prediction on our test set (to ease presentation we show the confidence vector summation i c j i for each image). Our C-Net generally favors quality face images with frontal pose, high visibility, natural lighting etc. Occlusions like sunglasses, hat and hair decrease the confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Comparison with Prior Art</head><p>To our knowledge, our method is the first one applying neutral networks for face reconstruction confidence prediction and aggregation. So here we compare with a heuristic strategy of Piotraschke and Blanz <ref type="bibr" target="#b33">[34]</ref>. <ref type="table">Table 6</ref> shows that our method produced better results than shape averaging and <ref type="bibr" target="#b33">[34]</ref> on the MICC dataset (we treat a sequence as an image set). The method of <ref type="bibr" target="#b33">[34]</ref> underperformed. Its results are even slightly worse than shape averaging. We conjecture this is because <ref type="bibr" target="#b33">[34]</ref> rely on the surface normal discrepancy with mean face to eliminate deficient reconstructions, yet our R-Net always produces a smooth, plausible face shape which renders their quality measurement ineffective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have proposed a CNN-based single-image face reconstruction method which exploits hybrid-level image information for weakly-supervised learning without groundtruth 3D shapes. Comprehensive experiments have shown that our method outperforms previous methods by a large margin in terms of both accuracy and robustness. We have also proposed a novel multi-image face reconstruction aggregation method using CNNs. Without any explicit label, our method can learn to measure image quality and exploit the complementary information in different images to reconstruct 3D faces more accurately.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Overview of our approach. (a) The framework of our method, which consists of a reconstruction network for end-to-end single image 3D reconstruction and a confidence measurement subnet designed for multi-image based reconstruction. (b) The training pipeline for single images with our proposed hybrid-level loss functions. Our method does not require any ground-truth 3D shapes for training. It only leverages some weak supervision signals such as facial landmarks, skin mask and a pre-trained face recognition CNN. (c) The training pipeline for multi-image based reconstruction. Our confidence subnet learns to measure the reconstruction confidence for aggregation with out any explicit label. The dashed arrows denote error backpropagration for network training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of the results without (top row) and with (bottom row) using our skin attention mask for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of the results with only image-level losses (top row) and with both image-level and perceptual losses (bottom row) for training. The numbers are the evaluated photometric errors. A lower photometric error does not guarantee a better shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison with Genova et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Comparison with VRN [25], 3DDFA [57], Tran et al. [51], Liu et al. [31] on three MICC subjects. Our results show largest variance and are visually most faithful among all methods. The input images and results of other methods are from<ref type="bibr" target="#b30">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Comprison with Tewari et al.<ref type="bibr" target="#b47">[48]</ref> (fine results). Top: results on different races. Bottom: results under occlusion. The images are from<ref type="bibr" target="#b47">[48]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Comparison with Sela et al. [45] under large pose and challenging appearance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Comparison with Richardson et al.<ref type="bibr" target="#b36">[37]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Results on in-the-wild image sets. The leftmost bar chart shows the sorted value of confidence vector summation of each image in the set. Five images sampled from a set are shown in the middle with their confidence vector summations shown in the top left corner. The last two columns are our final results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average reconstruction errors (mm) on MICC<ref type="bibr" target="#b0">[1]</ref> and FaceWarehouse<ref type="bibr" target="#b10">[11]</ref> datasets for R-Net trained with different losses. Our full hybrid-level loss function yields significantly higher accuracy than other baselines on both datasets.[51] 1.97±0.49 2.03±0.45 1.93±0.49 Genova et al. [16] 1.78±0.54 1.78±0.52 1.76±0.54 Ours 1.66±0.52 1.66±0.46 1.69±0.53</figDesc><table><row><cell cols="2">Losses L photo L lan Lper</cell><cell cols="2">MICC</cell><cell>Facewarehouse</cell></row><row><cell></cell><cell></cell><cell cols="2">1.87±0.43</cell><cell>2.70±0.73</cell></row><row><cell></cell><cell></cell><cell cols="2">1.80±0.52</cell><cell>2.17±0.65</cell></row><row><cell></cell><cell></cell><cell cols="2">1.71±0.43</cell><cell>2.11±0.48</cell></row><row><cell></cell><cell></cell><cell cols="2">1.67±0.50</cell><cell>1.81±0.50</cell></row><row><cell cols="5">Table 2. Mean Root Mean Squared Error (RMSE) across 53 sub-</cell></row><row><cell cols="5">jects on MICC dataset (in mm). We use ICP for alignment and</cell></row><row><cell cols="5">compute point-to-plane distance between results and ground truth.</cell></row><row><cell>Method</cell><cell cols="2">Cooperative</cell><cell cols="2">Indoor</cell><cell>Outdoor</cell></row><row><cell>Tran et al.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Multi-image reconstruction errors on MICC rendered images with different aggregation strategies (see text for details).</figDesc><table><row><cell>Shape error mean</cell><cell></cell><cell>1.97±0.70</cell></row><row><cell>Shape averaging</cell><cell></cell><cell>1.78±0.59</cell></row><row><cell>Our S1: Global Aggr. with c j</cell><cell></cell><cell>1.71±0.56</cell></row><row><cell>Our S2: Global Aggr. with i c j i Our S3: Max Conf. j = arg max j Our S4: Elementwise Aggr. with c j</cell><cell>c j i</cell><cell>1.70±0.55 1.71±0.50 1.67±0.54</cell></row><row><cell cols="3">6.2. Results on Multi-Image Reconstruction</cell></row><row><cell>6.2.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1.69±0.53 1.62±0.51 [34]-G 1.68±0.57 1.67±0.47 1.73±0.53 1.65±0.55 [34]-S 1.68±0.58 1.67±0.48 1.72±0.52 1.65±0.55 Ours (S4) 1.60±0.51 1.61±0.44 1.63±0.47 1.56±0.48</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Hybrid-level Weak-supervision for Single-Image ReconstructionGiven a training RGB image I, we use R-Net to regress a coefficient vector x, with which a reconstructed image I can be analytically generated with some simple, differentiable math derivations . Some examples of I can be found</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The florence 2d/3d hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Joint ACM Workshop on Human Gesture and Behavior Understanding</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="79" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d morphable models as spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshop on Geometry Meets Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="904" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fitting a 3d morphable model to edges: A comparison between hard and soft correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="377" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A statistical method for robust 3d surface reconstruction from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on 3D Data Processing, Visualization and Transmission</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A 3d morphable model learnt from 10,000 faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5543" to="5552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d shape regression for real-time facial animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised transformer network for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end 3d face reconstruction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reconstruction of personalized 3d face rigs from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<title level="m">Multi-PIE. Image and Vision Computing (IVC)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cnn-based real-time dense face reconstruction with inverserendered photo-realistic face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>TPAMI)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Example based 3d reconstruction from single 2d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Avatar digitization from a single image for real-time rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fursund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">195</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical color models with application to skin detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="96" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face reconstruction in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1746" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inversefacenet: Deep monocular inverse face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4625" to="4634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disentangling features in 3d face shapes for joint face reconstruction and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5216" to="5225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vetter. A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated 3d face reconstruction from multiple images using quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piotraschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A signal-processing framework for inverse rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computer Graphics and Interactive Techniques (SIG-GRAPH)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="460" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5553" to="5562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimating 3d shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unconstrained 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2606" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive 3d face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4197" to="4206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-time facial segmentation and performance capture from rgb input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1585" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SfSNet: learning shape, reflectance and illuminance of faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6296" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Total moving face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="796" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2549" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MoFa: model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1274" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1493" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Extreme 3d face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Nonlinear 3d face morphable model. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Face transfer with multilinear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="426" to="433" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning from millions of 3d scans for large-scale 3d face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zulqarnain</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1896" to="1905" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
