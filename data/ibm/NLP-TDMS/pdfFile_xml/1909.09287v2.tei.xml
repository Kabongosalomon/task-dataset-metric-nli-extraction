<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
						</author>
						<title level="a" type="main">Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. X, NO. X, SEPTEMBER 2019 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D point cloud</term>
					<term>spherical kernel</term>
					<term>graph neural network</term>
					<term>semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a spherical kernel for efficient graph convolution of 3D point clouds. Our metric-based kernels systematically quantize the local 3D space to identify distinctive geometric relationships in the data. Similar to the regular grid CNN kernels, the spherical kernel maintains translation-invariance and asymmetry properties, where the former guarantees weight sharing among similar local structures in the data and the latter facilitates fine geometric learning. The proposed kernel is applied to graph neural networks without edge-dependent filter generation, making it computationally attractive for large point clouds. In our graph networks, each vertex is associated with a single point location and edges connect the neighborhood points within a defined range. The graph gets coarsened in the network with farthest point sampling. Analogous to the standard CNNs, we define pooling and unpooling operations for our network. We demonstrate the effectiveness of the proposed spherical kernel with graph neural networks for point cloud classification and semantic segmentation using ModelNet, ShapeNet, RueMonge2014, ScanNet and S3DIS datasets. The source code and the trained models can be downloaded from https://github.com/hlei-ziyan/SPH3D-GCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C ONVOLUTIONAL neural networks (CNNs) <ref type="bibr" target="#b0">[1]</ref> are known for accurately solving a wide range of Computer Vision problems. Classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, image segmentation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, object detection <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, and face recognition <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> are just a few examples of the tasks for which CNNs have recently become the default modelling technique. The success of CNNs is mainly attributed to their impressive representational prowess. However, their representation is only amenable to the data defined over regular grids, e.g. pixel arrays of images and videos. This is problematic for applications where the data is inherently irregular <ref type="bibr" target="#b13">[14]</ref>, e.g. 3D Vision, Computer Graphics and Social Networks.</p><p>In particular, point clouds produced by 3D vision scanners (e.g. LiDAR, Matterport) are highly irregular. Recent years have seen a surge of interest in deep learning for 3D vision due to self-driving vehicles. This has also resulted in multiple public repositories of 3D point clouds <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Early attempts of exploiting CNNs for point clouds applied regular grid transformation (e.g. voxel grids <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, multi-view images <ref type="bibr" target="#b21">[22]</ref>) to point clouds for processing them with 3D-CNNs or enhanced 2D-CNNs <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>. However, this line of action does not fully exploit the sparse nature of point clouds, leading to unnecessarily large memory footprint and computational overhead of the methods. Riegler et al. <ref type="bibr" target="#b22">[23]</ref> addressed the memory issue in dense 3D-CNNs with an octree-based network, termed OctNet. However, the redundant computations over empty spaces still remains a discrepancy of OctNet.</p><p>Computational graphs are able to capitalize on the sparse nature of point clouds much better than volumetric or multi-view representations. However, designing effective modules such as convolution, pooling and unpooling layers, becomes a major challenge for the graph based convolutional networks. These modules are expected to perform point operations analogous to the pixel operations of CNNs, albeit for irregular data. Earlier instances of such modules exist in theoretical works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, which can be exploited to form Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b25">[26]</ref>. Nevertheless, these primitive GCNs are yet to be seen as a viable solution for point cloud processing due to their inability to effectively handle real-world point clouds. Based on the convolution operation, GCNs can be divided into two groups, namely; the spectral networks <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and the spatial networks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The former perform convolutions using the graph Laplacian and adjacency matrices, whereas the latter perform convolutions directly in the spatial domain. For the spectral networks, careful alignment of the graph Laplacians of different samples is necessary <ref type="bibr" target="#b26">[27]</ref>. This is not easily achieved for the real-world point clouds. Consequently, the spatial networks are generally considered more attractive than the spectral networks in practical applications.</p><p>The spatial GCNs are challenged by the unavailability of discrete convolutional kernels in the 3D metric space. To circumvent the problem, mini-networks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref> are often applied to dynamically generate edge-wise filters. This incurs significant computational overhead, which can be avoided in the case of discrete kernels. However, design and application of discrete kernels in this context is not straight forward. Beside effective discretization of the metric space, the kernel application must exhibit the properties of (a) translation-invariance that allows identification of similar local structures in the data, and (b) asymmetry for vertex pair processing to ensure that the overall representation remains compact.</p><p>Owing to the intricate requirements of discrete kernels arXiv:1909.09287v2 [cs.CV] 23 Mar 2020  <ref type="figure">Fig. 1</ref>. The proposed spherical convolutional kernel systematically splits the space around a point x i into multiple volumetric bins. For the j th neighboring point x j , it determines its relevant bin and uses the weight wκ for that bin to compute the activation. The kernel is employed with graph based networks that directly process raw point clouds using a pyramid of graph representations. This is a simplified U-Net-like <ref type="bibr" target="#b7">[8]</ref> architecture for semantic segmentation that coarsens the input graph G 0 into G 1 with pooling, and latter uses unpooling for expansion. In the network, the location of a point identifies a graph vertex and point neighbourhood decides the graph edges. Our network allows convolutional blocks with consecutive applications of the proposed kernels for more effective representation learning.</p><p>for irregular data, many existing networks altogether avoid the convolution operation for point cloud processing <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Although these techniques report decent performance on benchmark datasets, they do not contribute towards harnessing the power of convolutional networks for point clouds. PointCNN <ref type="bibr" target="#b36">[37]</ref> is a notable exception that uses a convolutional kernel for point cloud processing. However, its kernel is again defined using mini-networks, incurring high computational cost. Moreover, it is sensitive to the order of the neighborhood points, implicating that the underlying operation is not permutation-invariant, which is not a desired kernel property for point clouds.</p><p>In this work, we introduce a discrete metric-based spherical convolutional kernel that systematically partitions a 3D region into multiple volumetric bins as shown in <ref type="figure">Fig. 1</ref>. The kernel is directly applied to point clouds for convolution. Each bin of the kernel specifies learnable parameters to convolve the points falling in it. The convolution defined by our kernel preserves the properties of translationinvariance, asymmetry, as well as permutation-invariance. The proposed kernel is applied to point clouds using Graph Networks. To that end, we construct the networks with the help of range search <ref type="bibr" target="#b37">[38]</ref> and farthest point sampling <ref type="bibr" target="#b34">[35]</ref>. The former defines edges of the underlying graph, whereas the latter coarsens the graph as we go deeper into the network layers. We also define pooling and unpooling modules for our graph networks to downsample and upsample the vertex features. The novel convolutional kernel and its application to graph networks are thoroughly evaluated for the tasks of 3D point cloud classification and semantic segmentation. We achieve highly competitive performance on a wide range of benchmark datasets, including Model-Net <ref type="bibr" target="#b19">[20]</ref>, ShapeNet <ref type="bibr" target="#b15">[16]</ref>, RueMonge2014 <ref type="bibr" target="#b38">[39]</ref>, ScanNet <ref type="bibr" target="#b17">[18]</ref> and S3DIS <ref type="bibr" target="#b16">[17]</ref>. Owing to the proposed kernel, the resulting graph networks are found to be efficient in both memory and computation. This leads to fast training and inference on high resolution point clouds.</p><p>This work is a significant extension of our preliminary findings presented in IEEE CVPR 2019 <ref type="bibr" target="#b31">[32]</ref>. Below, we summarize the major directions along which the technique is extended beyond the preliminary work.</p><p>• Separable convolution. We perform the depth-wise and point-wise convolution operation separately in this work rather than simultaneously as in <ref type="bibr" target="#b31">[32]</ref>. The separable convolution strategy is inspired by Xception <ref type="bibr" target="#b39">[40]</ref>, and significantly reduces the number of network parameters and computational cost.</p><p>• Graph architecture. Instead of the octree-guided network of <ref type="bibr" target="#b31">[32]</ref>, we use a more flexible graph-based technique to design our network architectures. This allows us to exploit convolution blocks and define pooling/unpooling operations independent of convolution. In contrast to the convolution-based down/upsampling, specialized modules for these operations are highly desirable for processing large point clouds. Moreover, this strategy also brings our network architectures closer to the standard CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comprehensive evaluation on real-world data.</head><p>Compared to the preliminary work <ref type="bibr" target="#b31">[32]</ref>, we present a more thorough evaluation on real-world data. Highlights include 4.2% performance gain over <ref type="bibr" target="#b31">[32]</ref> for the RueMonge2014 dataset, and comprehensive evaluation on two additional datasets, ScanNet and S3DIS. The presented results ascertain the computational efficiency of our technique with highly competitive performance on the popular benchmarks.</p><p>• Tensorflow implementation. While <ref type="bibr" target="#b31">[32]</ref> was implemented in Matconvnet, with this article, we release cuda implementations of the spherical convolution and the pooling/unpooling operations for Tensorflow. The source code is available on Github (https://github.com/hlei-ziyan/SPH3D-GCN) for the broader research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>PointNet <ref type="bibr" target="#b32">[33]</ref> is one of the first techniques to directly process point clouds with deep networks. It uses the xyz coordinates of points as input features. The network learns point-wise features with shared MLPs, and extracts a global feature with max pooling. One limitation of this technique is that it does not explore the geometric context of points in representation learning. PointNet++ <ref type="bibr" target="#b34">[35]</ref> addresses that by applying max-pooling to the local regions hierarchically. However, both networks must rely on max-pooling to aggregate any context information without convolution. SO-Net <ref type="bibr" target="#b35">[36]</ref> builds an m × m rectangular map from the point cloud, and hierarchically learns node-wise features within the map using mini-PointNet. However, similar to the original PointNet, it also fails to exploit any convolution modules. KCNet <ref type="bibr" target="#b40">[41]</ref> learns features with kernel correlation between the local neighboring points and a template of learnable points. This can be optimized in a training session similar to convolutional kernels. In contrast to the imagelike map used by the SO-Net, KCNet is based on graph representation. Kd-network <ref type="bibr" target="#b33">[34]</ref> is a prominent contribution that processes point clouds with tree structure based networks. This technique also uses point coordinates as the input and computes the feature of a parent node by concatenating the features of its children in a balanced tree. Instead of using xyz coordinates, ShapeContextNet <ref type="bibr" target="#b41">[42]</ref> computes hand-crafted shape context descriptors <ref type="bibr" target="#b42">[43]</ref> for each point in the point cloud, and explores them as input features for PoinetNet-like architecture. Despite their varied network architecture construction, none of the above methods contribute towards developing convolutional networks for point clouds. Approaches that advance research in that direction can be divided into two broad categories, discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Convolutional Neural Networks</head><p>At the advent of 3D deep learning, researchers predominantly extracted features with 3D-CNN kernels using volumetric representations. The earlier attempts in this direction could only process voxel-grids of low resolution (e.g. 30×30×30 in ShapeNets <ref type="bibr" target="#b19">[20]</ref>, 32×32×32 in VoxNet <ref type="bibr" target="#b20">[21]</ref>), even with the modern GPUs. This issue also transcended to the subsequent works along this direction <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. The limitation of low input resolution was a natural consequence of the cubic growth of memory and computational requirements associated with the dense volumetric inputs. Different solutions later appeared to address these issues. For example, Engelcke et al. <ref type="bibr" target="#b47">[48]</ref> introduced sparsity in the input and hidden neural activations. Their solution is effective in reducing the number of convolutions, but not the amount of required memory. Li et al. <ref type="bibr" target="#b48">[49]</ref> proposed a field probing neural network, which transforms 3D data into intermediate representations with a small set of probing filters. Although this network is able to reduce the computational and memory costs of fully connected layers, the probing filters fail to support weight sharing. Later, Riegler et al. <ref type="bibr" target="#b22">[23]</ref> proposed the octree-based OctNet, which represents point clouds with a hybrid of shallow grid octrees (depth = 3). Compared to its dense peers, OctNet reduces the computational and memory costs to a large degree, and is applicable to high-resolution inputs up to 256×256×256. However, it still has to perform unnecessary computations in the empty spaces around the objects. SparseConvNet <ref type="bibr" target="#b49">[50]</ref> and MinkowskiNet <ref type="bibr" target="#b50">[51]</ref> exploit sparse tensors to represent the objects with their occupied voxels only. By applying the 3D-CNN kernels sparsely to those occupied voxels, they avoid unnecessary computations in the empty spaces. Pointwise CNN <ref type="bibr" target="#b51">[52]</ref> computes the feature of each voxel as the normalization of features of all points falling into the voxel, which is computationally expensive. Other recent techniques also transform the original point cloud into other regular representations like tangent image <ref type="bibr" target="#b52">[53]</ref>, "translating" tensor <ref type="bibr" target="#b53">[54]</ref> or high-dimensional lattice <ref type="bibr" target="#b54">[55]</ref> such that the standard CNNs can be applied to the transformed data. In particular, PCNN <ref type="bibr" target="#b53">[54]</ref> extends the features defined on sparse point clouds to functions defined over the entire Euclidean space (R 3 ). Its convolution strategy transforms the irregular points into a regular "translating" tensor, whose size is quadratic in the size of the point cloud. This leads to a significant computation and memory overhead, making PCNN not scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolutional Networks</head><p>The demand of irregular data processing with CNN-like architectures has resulted in a recent rise of graph convolutional networks <ref type="bibr" target="#b13">[14]</ref>. In general, the broader graph-based deep learning has also seen techniques besides convolutional networks that update vertex features recurrently to propagate the context information (e.g. <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>). However, here, our focus is on graph convolutional networks that relate to our work more closely.</p><p>Graph convolutional networks can be grouped into spectral networks (e.g. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>) and spatial networks (e.g. <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>). The spectral networks perform convolution on spectral vertex signals converted from Fourier transformation, while the spatial networks perform convolution directly on the spatial vertices. A major limitation of the spectral networks is that they require the graph structure to be fixed, which makes their application to the data with varying graph structures (e.g. point clouds) challenging. Yi et al. <ref type="bibr" target="#b26">[27]</ref> attempted to address this issue with Spectral Transformer Network (SpecTN), similar to STN <ref type="bibr" target="#b60">[61]</ref> in the spatial domain. However, the signal transformation from spatial to spectral domains and vice-versa has computational complexity O(n 2 ), resulting in prohibitive requirements for large point clouds.</p><p>ECC <ref type="bibr" target="#b27">[28]</ref> is among the pioneering works for point cloud analysis with graph convolution in the spatial domain. Inspired by the dynamic filter networks <ref type="bibr" target="#b61">[62]</ref>, it adapts MLPs to generate convolution filters between the connected vertices dynamically. The dynamic generation of filters naturally comes with a computational overhead. Monte Carlo (MC) convolution <ref type="bibr" target="#b62">[63]</ref> exploits MLPs as well to learn the edgewise filters. It otherwise introduces density parameters into the integration which results in a convolution for nonuniformly sampled point cloud processing. PCCN <ref type="bibr" target="#b63">[64]</ref> uses MLPs to parameterize its kernel to achieve a continuous convolution. Along similar lines, DGCNN <ref type="bibr" target="#b28">[29]</ref>, Flex-Conv <ref type="bibr" target="#b64">[65]</ref> and SpiderCNN <ref type="bibr" target="#b65">[66]</ref> explore different parameterizations to generate the edge-dependent filters. Instead of generating filters for the edges individually, few networks also generate a complete local convolution kernel at once using mini networks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Li et al. <ref type="bibr" target="#b36">[37]</ref> recently introduced PointCNN that uses a convolution module named X -Conv for point cloud processing. The network achieves good performance on the standard benchmarks (e.g. ShapeNet and S3DIS). However, the generated kernels are sensitive to the order of neighborhood points indicating that the underlying representation is not permutation-invariant. Moreover, the strategy of dynamic kernel generation makes the technique computationally inefficient. Besides the above MLP-based 'continuous' convolution kernels, SplineCNN <ref type="bibr" target="#b66">[67]</ref> also explores to generate the continuous convolution kernel using spline functions.</p><p>Discrete kernel is an attractive alternative to the 'continuous' kernels in avoiding the computational overhead. In our preliminary work <ref type="bibr" target="#b31">[32]</ref>, we divide the space in a deterministic and compact manner. In comparison, the recent KPConv kernel <ref type="bibr" target="#b67">[68]</ref> divides the spherical region into volumetric bins based on template points learned in data independent manner. Wang et al. <ref type="bibr" target="#b68">[69]</ref> inserted an attention mechanism in graph convolutional networks to develop GACNet. Such an extension of graph networks is particularly helpful for semantic segmentation as it enforces the neighborhood vertices to have consistent semantic labels similar to CRF <ref type="bibr" target="#b69">[70]</ref>. Besides the convolution operation, graph coarsening and edge construction are two essential parts for the graph network architectures. We briefly review the methods along these aspects below.</p><p>Graph coarsening: Point cloud sampling methods are useful for graph coarsening. PointNet++ <ref type="bibr" target="#b34">[35]</ref> utilizes farthest point sampling (FPS) to coarsen the point cloud, while Flex-Conv <ref type="bibr" target="#b64">[65]</ref> samples the point cloud based on inverse densities (IDS) of each point. Random sampling is the simplest alternative to FPS and IDS, but it does not perform as well for the challenging tasks like semantic segmentation. Recently, researchers also started to explore the possibility of learning sampling with deep neural networks <ref type="bibr" target="#b70">[71]</ref>. In this work, we exploit FPS as the sampling strategy for graph coarsening, as it does not need training and it reduces the point cloud resolution relatively uniformly.</p><p>Graph connections: Point neighborhood search can be used to build edge connections in a graph. KNN search generates fixed number of neighborhood points for a given point, which results in a regular graph. Range search generates flexible number of neighborhood points, which may results in irregular graphs. Tree structures can also be seen as special kinds of graphs <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref>, however, the default absence of intra-layer connections in trees drastically limits their potential as graph networks. In a recent example, Rao et al. <ref type="bibr" target="#b71">[72]</ref> proposed to employ spherical lattices for regular graph construction. Their technique relies on 1 × 1 convolution and max-pooling to aggregate the geometric context between neighbouring points.</p><p>In this paper, we use range search to establish the graph connections for its natural compatibility with the proposed kernel. Note that our spherical kernel does not restrict the graph vertex degrees to be fixed. Hence, unlike <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b36">[37]</ref>, our kernel is applicable to both regular and irregular graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DISCRETE CONVOLUTION KERNELS</head><p>Given an arbitrary point cloud of m points P = {x i ∈ R 3 } m i=1 , we represent the neighborhood of each point x i as N (x i ). To achieve graph convolution on the target point x i , the more common 'continuous' filter approaches <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref> parameterize convolution as a function of local point coordinates. For instance, suppose w is the filter that computes the output feature of channel c. These techniques may represent the filter as w = h(x j −x i ), where h(.) is a continuous function (e.g. MLP) and x j ∈ N (x i ). However, compared to the continuous filters, discrete kernel is predefined and it does not need the above mentioned (or similar) intermediate computations. This makes a discrete kernel computationally more attractive.</p><p>Following the standard CNN kernels, a primitive discrete kernel for point clouds can be defined on regular grids in the Euclidean space, similar to the 3D-CNN kernels for voxel-grids <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. For resolution h, this kernel comprises h 3 weight filters w κ∈{1,...,h 3 } . By incorporating the notion of separable convolution <ref type="bibr" target="#b39">[40]</ref> into this design, each weight filter is transformed from a vector w κ to a scalar w κ . It is noteworthy that the application of a discrete kernel to 'graph' representation is significantly different from its volumetric counterpart. Hence, to differentiate, we refer to a kernel for graphs as CNN3D kernel. A CNN3D kernel indexes the bins and for the κ th bin, it uses w κ to propagate features from all neighboring point x j , ∀j in that bin to the target point x i , see <ref type="figure">Fig. 2</ref>. It performs convolutions based on the existing points in the point cloud. Since no points are populated at empty spaces, the CNN3D kernel can avoid unnecessary computations at empty spaces. In contrast, the 3D-CNN kernel demands volumetric representations to assign an occupancy feature 0 or 1 respectively to each voxel in the empty or non-empty spaces around the objects. To fulfill the convolution, it requires computations at all voxels within the receptive field, whether it is occupied or unoccupied.</p><p>We make the following observation in relation to improving the CNN3D kernels. For images, the more primitive constituents, i.e. patches, have traditionally been used to extract hand-crafted features <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>. The same principle transcended to the receptive fields of automatic feature extraction with CNNs, which compute feature maps using the activations of well-defined rectangular regions of images. Whereas rectangular regions are intuitive choice for images, spherical regions are more suited to process unstructured 3D data such as point clouds. Spherical regions are inherently amenable to computing geometrically meaningful features for such data <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>. Inspired by this natural kinship, we introduce the concept of spherical convolution kernel 1 (termed SPH3D kernel) that considers a 3D sphere as the basic geometric shape to perform the convolution operation. We explain the proposed discrete spherical kernel in Section 3.1, and later contrast it to the existing CNN3D kernels in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spherical Convolutions</head><p>We define the convolution kernel with the help of a sphere of radius ρ ∈ R + , see <ref type="figure">Fig. 2</ref>. For a target point x i , we consider its neighborhood N (x i ) to be the set of points within the sphere centered at</p><formula xml:id="formula_0">x i , i.e. N (x i ) = {x : d(x, x i ) ≤ ρ}, where d(., .</formula><p>) is a distance metric -2 distance in this work. We divide the sphere into n × p × q 'bins' by partitioning the occupied space uniformly along the azimuth (θ) and elevation (φ) dimensions. We allow the partitions along the radial (r) dimension to be non-uniform because the cubic volume growth for large radius values can be undesirable. Our quantization of the spherical region is mainly inspired by 3DSC <ref type="bibr" target="#b74">[75]</ref>. We also define an additional bin corresponding to the origin of the sphere to allow the case of self-convolution of points on the graph. To produce an 1. The term spherical in Spherical CNN <ref type="bibr" target="#b77">[78]</ref> is used for surfaces (i.e. 360 • images) not the ambient 3D space. Our notion of spherical kernel is widely dissimilar, and it is used in a different context. Also, note that, different from the preliminary work <ref type="bibr" target="#b31">[32]</ref>, here the spherical kernel is only used to perform depth-wise spatial convolutions. output feature map, we define a learnable weight parameter w κ∈{0,1,...,n×p×q} ∈ R for each bin, where w 0 relates to selfconvolution. Combined, the n × p × q + 1 weight values specify a single spherical convolution kernel.</p><p>To compute the activation value for a target point x i , we first identify the relevant weight values of its neighboring points x j ∈ N (x i ). It is straightforward to associate w 0 to x i for self-convolution. For the non-trivial cases, we first represent the neighboring points in terms of their spherical coordinates that are referenced using x i as the origin. That is, for each x j we compute T (∆ ji ) → ψ ji , where T (.) defines the transformation from Cartesian to Spherical coordinates and ∆ ji = x j − x i . Supposing that the bins of the quantized sphere are respectively indexed by k θ , k φ and k r along the azimuth, elevation and radial dimensions, the weight values associated with each spherical kernel bin can then be indexed as</p><formula xml:id="formula_1">κ = k θ + (k φ − 1) × n + (k r − 1) × n × p, where k θ ∈ {1, . . . , n}, k φ ∈ {1, . . . , p}, k r ∈ {1, . . . , q}.</formula><p>Using this indexing, we relate the relevant weight value to each ψ ji , and hence x j . In the l th network layer, the activation for the i th point in channel c gets computed as:</p><formula xml:id="formula_2">z l ic = 1 |N (x i )| |N (xi)| j=1 w l κc a l−1 jc + b c ,<label>(1)</label></formula><formula xml:id="formula_3">a l ic = f (z l ic ),<label>(2)</label></formula><p>where a l−1 jc is the feature of a neighboring point from layer l − 1, w l κc is the weight value, and f (·) is the nonlinear activation function -ELU <ref type="bibr" target="#b78">[79]</ref> in our experiments. By applying the spherical convolution λ times for each input channel, we produce λC in output features for the target convolution point x i . We note that based on the geometry of points, different number of points may fall in different bins of our kernel. The distinctive influence of points resulting from this distribution encodes the local structure of data, which contributes towards learning discriminative features under the overall learning objective of our network.</p><p>To elaborate on the characteristics of the spherical convolution kernel, we denote the boundaries along θ, φ and r dimensions of the kernel bins as follows:</p><formula xml:id="formula_4">Θ = [Θ 1 , . . . , Θ n+1 ], Θ k &lt; Θ k+1 , Θ k ∈ [−π, π], Φ = [Φ 1 , . . . , Φ p+1 ] , Φ k &lt; Φ k+1 , Φ k ∈ − π 2 , π 2 ] R = [R 1 , . . . , R q+1 ], R k &lt; R k+1 , R k ∈ (0, ρ].</formula><p>The constraint of uniform splitting along the azimuth and elevation results in</p><formula xml:id="formula_5">Θ k+1 − Θ k = 2π n and Φ k+1 − Φ k = π p . Lemma 2.1: If Θ k · Θ k+1 ≥ 0, Φ k · Φ k+1 ≥ 0 and n &gt; 2,</formula><p>then for any two neighboring points x a = x b , where the spherical convolution kernel is centered at either x a or x b , the weight value w κ , ∀κ &gt; 0, are applied asymmetrically. Proof:</p><formula xml:id="formula_6">Let ∆ ab = x a − x b = [δ x , δ y , δ z ] , then ∆ ba = [−δ x , −δ y , −δ z ] .</formula><p>Under the Cartesian to Spherical coordinate transformation, we have T (∆ ab ) = ψ ab = [θ ab , φ ab , r] , and T (∆ ba ) = ψ ba = [θ ba , φ ba , r] . Assume that the resulting ψ ab and ψ ba fall in the same bin indexed by κ ← (k θ , k φ , k r ), i.e. w κ will have to be applied symmetrically to the original points. In that case, under the inverse transformation T −1 (.), we have δ z = r sin φ ab</p><formula xml:id="formula_7">and (−δ z ) = r sin φ ba . The condition Φ k φ · Φ k φ +1 ≥ 0 entails that −δ 2 z = δ z · (−δ z ) = (r sin φ ab ) · (r sin φ ba ) = r 2 (sin φ ab sin φ ba ) ≥ 0 =⇒ δ z = 0. Similarly, Θ k θ · Θ k θ +1 ≥ 0 =⇒ δ y = 0. Since x a = x b , for δ x = 0 we have cos θ ab = − cos θ ba =⇒ |θ ab − θ ba | = π.</formula><p>However, if θ ab , θ ba fall into the same bin, we have |θ ab − θ ba | = 2π n &lt; π, which entails δ x = 0. Thus, w κ can not be applied to any two points symmetrically unless both points are the same.</p><p>The kernel asymmetry forbids weight sharing between point pairs for the convolution operation, which leads to learning fine geometric details of the point clouds. In fact, asymmetry exists widely in the standard kernels, e.g. 2D-CNN, 3D-CNN for voxel-grids. However, for the discrete SPH3D kernel, not all partitionings guarantee asymmetry. Lemma 2.1 provides guidelines on how to divide the spherical space into kernel bins such that the asymmetry is always preserved. The importance of kernel asymmetry is also demonstrated quantitatively in Section 3.2. The resulting division also ensures translation-invariance of the kernel, similar to the standard CNN kernels. Additionally, unlike the convolution operation of PointCNN <ref type="bibr" target="#b36">[37]</ref>, the proposed kernel is invariant to point permutations because it explicitly incorporates the geometric relationships between the point pairs.</p><p>We can apply the spherical convolution kernel to learn depth-wise features in the spatial domain. The point-wise convolution can be readily achieved with shared MLP or 1 × 1 convolution using any modern deep learning library.</p><p>To be more precise, the two convolutions make our kernel perform separable convolution <ref type="bibr" target="#b39">[40]</ref>. However, we generally refer to it as spherical convolution, for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison to CNN3D kernel</head><p>CNN3D kernel rasterizes 3D data into uniform voxel grids, where the size of 3×3×3 = 27 is prevalently used. This size splits the space in 1 voxel for radius ρ = 0 (self-convolution); 6 voxels for radius ρ = 1; 12 voxels for radius ρ = √ 2; and 8 voxels for radius ρ = √ 3. An analogous spherical convolution kernel for the same region can be specified with a radius ρ = √ 3, using the following edges for the bins:</p><formula xml:id="formula_8">Θ = [−π, − π 2 , 0, π 2 , π]; Φ = [− π 2 , − π 4 , 0, π 4 , π 2 ]; R = [ , 1, √ 2, ρ], → 0 + .<label>(3)</label></formula><p>This division results in a kernel size (i.e. total number of bins) 4 × 4 × 3 + 1 = 49, which is one of the coarsest multi-scale quantization allowed by Lemma 2.1.</p><p>Notice that, if we move radially from the center to periphery of the spherical kernel, we encounter identical number of bins (16 in this case) after each edge defined by R, where fine-grained bins are located close to the origin that can encode detailed local geometric information of the points. This is in sharp contrast to CNN3D kernels that must keep the size of all cells constant and rely on increased resolution to capture the finer details. This makes their number of parameters grow cubicly, harming the scalability. The multi-scale granularity of spherical kernel (SPH3D) allows for more compact representation.  <ref type="figure">Fig. 2</ref>. Illustration of the primitive CNN3D and the proposed SPH3D discrete kernels: The point x i has seven neighboring points including itself (the self-loop). To perform convolution at x i , discrete kernels systematically partition the space around it into bins. With x i at the center, CNN3D divides a 3D 'cubic' space around the point into 'uniform voxel' bins. Our SPH3D kernel partitions a 'spherical' space around x i into 'non-uniform volumetric' bins. For both kernels, the bins and their corresponding weights are indexed. The points falling in the κ th bin are propagated to x i with the weight wκ. Multiple points falling in the same bin, e.g. xs and xt, use the same weight for computing the output feature at x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1</head><p>Segmentation results on Area 5 of S3DIS with different CNN3D kernels, their variants with self-convolution weight w 0 , and our SPH3D kernel.</p><p>Input point cloud size 2048 is used. To corroborate, we briefly touch upon semantic segmentation with CNN3D and SPH3D kernel, using a popular benchmark dataset S3DIS <ref type="bibr" target="#b16">[17]</ref> in <ref type="table">Table 1</ref>. We give further details on the dataset and experimental settings in Section 5. Here, we focus on the aspect of representation compactness resulting from the non-uniform granularity of the bins in SPH3D. In the table, the only difference in the networks is in the used kernels. All the other experimental details are 'exactly' the same for all networks. Network-1 and 3 use CNN3D kernels that partition the space into 3 × 3 × 3 = 27 and 5 × 5 × 5 = 125 bins, respectively. Network-4 and 5 use similar CNN3D kernels but with an additional self-convolution weight w 0 , resulting in kernels of 3×3×3+1 = 28 and 5×5×5+1 = 126 bins, respectively. Comparing the results of Network-1 to 4 and Network-3 to 5, we notice that self-convolution weight w 0 has minor influence on the final performance of CNN3D kernels. The SPH3D kernel partitions the space into 8 × 2 × 2 + 1 = 33 bins. Consequently, the kernel requires 1.22× and 1.18× parameters as compared to the Network-1 and 4 kernels, but only 0.26× parameters required by the Network-3 and 5 kernels. However, the performance of Network-6 is much better than Network-1 and 3, and also their variants with self-convolution weights. Such an advantage is a natural consequence of the non-uniform partitioning allowed by our kernel.</p><formula xml:id="formula_9">Kernel type CNN3D CNN3D + w 0 SPH3D Networks Network-1 Network-2 Network-3 Network-4 Network-5 Network-6 Kernel size 3 × 3 × 3 4 × 4 × 4 5 × 5 × 5 3 × 3 × 3 + 1 5 × 5 × 5 + 1 8 × 2 × 2 + 1 #</formula><p>We compare the representation compactness of SPH3D kernel to odd-size CNN3D kernels in the above experiments. Similar to the proposed SPH3D kernel, not all partitionings of the primitive CNN3D kernel guarantee asymmetry. In particular, the even-size CNN3D kernels preserve asymmetry while the odd-size kernels are unable to do so. We show the importance of asymmetry by exploring CNN3D kernels of sizes 4 × 4 × 4 and 5 × 5 × 5. Comparing the performance of Network-2 and 3 in <ref type="table">Table 1</ref>, we notice that although 4 × 4 × 4 kernel uses less parameters than the 5×5×5 kernel, it still produces better results. This is because the 4 × 4 × 4 kernel preserves asymmetry and hence learns more representative features. Lemma 2.1 ensures that our SPH3D kernel preserves asymmetry as well. The asymmetry, together with the non-uniform multi-scale granularity, results in Network-6 to easily match the performance of Network-2 with only 0.52× parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GRAPH NEURAL NETWORK</head><p>In this work, we employ graph neural network to process point clouds. Compared to the inter-layer connectivity of the octree-guided network of our preliminary work <ref type="bibr" target="#b31">[32]</ref>, graph representation additionally allows for intra-layer connections. This is beneficial in defining effective convolutional blocks as well as pooling/unpooling modules in the network. Let us consider a graph G = (V, E) constructed from a point cloud P = {x 1 , . . . , x m }, where V = {1, 2, . . . , m} and E ⊆ |V | × |V | respectively represent the sets of vertices and edges. It is straightforward to associate each vertex i ∈ V of the graph to a point location x i and its corresponding feature a i . However, the edge set E must be carefully established based on the neighborhood of the points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge construction:</head><p>We use range search with a specified radius ρ to get the spatial neighborhood of each point and construct the edge connections of each graph vertex. In the range search, neighborhood computations are independent of each other, which makes the search suitable for parallel processing and taking advantage of GPUs. The time complexity of constructing neighborhoods of a single point is linear in the number of vertices |V |, which makes O(|V | 2 ) to be the time complexity for a point cloud. However, the parallelism of modern GPU makes the neighbor search efficient enough for reasonably large input point clouds. We provide the neighbour search time for varying point cloud sizes as the 'graph construction' time in <ref type="table" target="#tab_9">Table 10</ref> (Section 7). One potential problem of using range search is that large number of neighborhood points in dense clouds can cause memory issues. We sidestep this problem by restricting the number of neighboring points to K ∈ Z + by randomly sub-sampling the neighborhood, if required. The edges are finally built on the sampled points. As a result, the neighborhood indices of the i th vertex can be denoted as N (i) = {j : (j, i) ∈ E}, in which |N (i)| ≤ K. With these sets identified, we can later compute features for vertices with spherical convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph coarsening:</head><p>We use Farthest Point Sampling (FPS) to coarsen the point graph in our network layer-by-layer.</p><p>The FPS algorithm selects one random seed vertex, and iteratively searches for the point that is farthest apart from the previously selected points for the sampling purpose. The algorithm terminates when the desired number of sampled points are acquired, which form the coarsened graph. By alternately constructing the edges and coarsening the graph for l max times, we construct a graph pyramid composed of</p><formula xml:id="formula_10">l max + 1 graphs, i.e. G 0 → G 1 → · · · → G lmax−1 → G lmax .</formula><p>As compared to the octree structure based graph coarsening adopted in the preliminary work <ref type="bibr" target="#b31">[32]</ref>, FPS coarsening has the advantage of keeping the number of vertices of each layer fixed across different samples, which is conducive for more systematic application of convolutional kernels.</p><p>Pooling: Once a graph is coarsened, we still need to compute the features associated with its vertices. To that end, we define max pooling and average pooling operations to sample features for the coarsened graph vertices. Inter-layer graph connections facilitate these operations. To be consistent, we denote the graphs before and after pooling layer</p><formula xml:id="formula_11">l ∈ {1, . . . , l max } as G l−1 = (V l−1 , E l−1 ) and G l = (V l , E l ) respectively, where V l−1 ⊃ V l . Let i l−1 ∈ V l−1 and i l ∈ V l</formula><p>be the two vertices associated with the same point location.</p><p>The inter-layer neighborhood of i l can be readily constructed from graph G l−1 as N (i l ) = {j : (j, i l−1 ) ∈ E l−1 }. We denote the features of i l and its neighborhood point j ∈ N (i l ) as a l i = [. . . , a l ic , . . . ] and a l−1 j = [. . . , a l−1 jc , . . . ] respectively. The max pooling operation then computes the feature of the vertex i l as</p><formula xml:id="formula_12">a l ic = max{a l−1 jc : j ∈ N (i l )},<label>(4)</label></formula><p>while the average pooling computes it as</p><formula xml:id="formula_13">a l ic = 1 |N (i l )| j∈N (i l ) a l−1 jc .<label>(5)</label></formula><p>We introduce both pooling operations in our source code release, but use max pooling in our experiments as it is commonly known to have superior performance in point cloud processing <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b40">[41]</ref>.</p><p>Unpooling: Decoder architectures with increasing neuron resolution are important for element-wise predictions in semantic segmentation <ref type="bibr" target="#b7">[8]</ref>, dense optical flow <ref type="bibr" target="#b79">[80]</ref>, etc. We build graph decoder by inverting the graph pyramid as G lmax−1 → · · · → G 1 → G 0 . The coarsest graph G lmax is ommited in the reversed pyramid because it is shared between encoder and decoder. We denote the graphs before and after an unpooling layer l ∈ {1, . . . , l max } as G lmax−l+1 and G lmax−l respectively. To upsample the features from G lmax−l+1 to G lmax−l , we define two types of feature interpolation operations, namely; uniform interpolation and weighted interpolation. Notice that the neighborhood set N (.) in Eqs. (4), (5) is readily available because of the relation V l−1 ⊃ V l . However, the vertices of graphs G lmax−l+1 and G lmax−l satisfy V lmax−l+1 ⊂ V lmax−l on the contrary. Therefore, we have to additionally construct the neighborhood of i lmax−l ∈ V lmax−l from V lmax−l+1 . For that, we again use the range search to compute N (i lmax−l ). The features of i lmax−l and its neighborhood points j ∈ N (i lmax−l ) can be consistently denoted as a lmax+l ic and a lmax+l−1 jc . The uniform interpolation computes the feature of vertex i lmax−l as the average features of its inter-layer neighborhood points, i.e.</p><formula xml:id="formula_14">a lmax+l ic = 1 |N (i lmax−l )| j∈N (i lmax−l ) a lmax+l−1 jc .<label>(6)</label></formula><p>The weighted interpolation computes the features of vertex i lmax−l by weighing its neighborhood features based on their distance to i lmax−l . Mathematically,</p><formula xml:id="formula_15">a lmax+l ic = j∈N (i lmax−l ) w ji a lmax+l−1 jc j∈N (i lmax−l ) w ji ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_16">w ji = d(x lmax−l+1 j , x lmax−l i ).</formula><p>Here, d(., .) is the 2 distance function and the points x lmax−l+1 j and x lmax−l i are associated to vertices j and i lmax−l , respectively. In our source code, we provide both types of interpolation functionalities for upsampling. However, the experiments in Section 5 are performed with uniform interpolation for its computational efficiency.</p><p>In <ref type="figure" target="#fig_2">Fig. 3</ref>, we illustrate an encoder-decoder graph neural network constructed by our technique for a toy example. In the shown network, a graph G l of 12 vertices gets coarsened to 8 (G l+1 ) and 4 (G l+2 ) vertices in the encoder network, and later gets expanded in the decoder network. The pooling/unpooling operations are applied to learn features of the structure altered graphs. The graph structure remains unchanged during convolution operation. Notice, we apply consecutive spherical convolutions to form convolution blocks in our networks. In the figure, variation in width of the feature maps depicts different number of channels (e.g. 128, 256 and 384) for the features. The shown Ushape architecture for the task of semantic segmentation also exploits skipping connections similar to U-Net <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. These connections copy features from the encoder and concatenate them to the decoder features. For the classification task, these connections and the decoder part are removed, and a global feature representation is fed to a classifier comprising fully connected layers. The simple architecture in <ref type="figure" target="#fig_2">Fig. 3</ref> graphically illustrates the application of the abovementioned concepts to our networks in Section 5, where we provide details of the architectures used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software for Tensorflow:</head><p>With this article, we also release a cuda enabled implementation for the above presented concepts. The package is Tensorflow compatible <ref type="bibr" target="#b80">[81]</ref>. As compared to the Matconvnet <ref type="bibr" target="#b81">[82]</ref> source code of the preliminary work <ref type="bibr" target="#b31">[32]</ref>, Tensorflow compatibility is chosen due to the popularity of the programming framework. In the package, we provide cuda implementations of the spherical convolution, range search, max pooling, average pooling, uniform interpolation and weighted interpolation. The provided spherical kernel implementation can be used for convolutions on both regular and irregular graphs. Unlike the existing methods (e.g. <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b36">[37]</ref>), we do not impose any constraint on the vertex degree of the graph except the upper pragmatic limit K, allowing the graphs to be more flexible, similar to ECC <ref type="bibr" target="#b27">[28]</ref>. In our implementation, the spherical convolutions are all followed by batch normalization <ref type="bibr" target="#b82">[83]</ref>. In the preliminary work <ref type="bibr" target="#b31">[32]</ref>, the implemented spherical convolution does not separate the depth-wise convolution from the point-wise convolution <ref type="bibr" target="#b39">[40]</ref>, thereby performing the two convolutions simultaneously similar to a typical convolution operation. Additionally, the previous implementation is specialized to octree structures, and hence not applicable to general graph architectures. The newly released implementation for Tensorflow improves on all of these aspects. The source code and further details of the released package can be found at https://github.com/hleiziyan/SPH3D-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate our technique for classification and semantic segmentation tasks using clean CAD point clouds and large-scale noisy point clouds of real-world scenes. The dataset used in our experiments include ModelNet <ref type="bibr" target="#b19">[20]</ref>, ShapeNet <ref type="bibr" target="#b15">[16]</ref>, RueMonge2014 <ref type="bibr" target="#b38">[39]</ref>, ScanNet <ref type="bibr" target="#b17">[18]</ref> and S3DIS <ref type="bibr" target="#b16">[17]</ref>, for which representative samples are illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. We only use the (x, y, z) coordinates of points to train our networks, except when the (r, g, b) values are also available. In that case, we additionally use those values by rescaling them into the range [−1, 1]. We note that, a few existing methods also take advantage of normals as input features <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b71">[72]</ref>. However, normals are not directly sensed by the 3D sensors and must be computed separately, entailing additional computational burden. Hence, we avoid using normals as input features except for RueMonge2014, which already provides the normals.</p><p>Throughout the experiments, we apply the spherical convolution with a kernel size 8 × 2 × 2 + 1, where the radial dimension is split uniformly. Our network training is conducted on a single Titan Xp GPU with 12 GB memory. We use Adam Optimizer <ref type="bibr" target="#b83">[84]</ref> with an initial learning rate of 0.001 and momentum 0.9 to train the network. The batch size is kept fixed to 32 in ModelNet and ShapeNet, and 16 the remaining datasets. The maximum neighborhood connections for each vertex is set to K = 64. These hyperparameters are empirically optimized with cross-validation. We also employ data augmentation in our experiments. For that, we use random sub-sampling to drop points, and random rotation, which include azimuth rotation (up to 2π rad) and small arbitrary perturbations (up to 10 • degrees) to change the view of point clouds. We also apply random scaling, shifting and noisy translation of points with std. dev = 0.01. These operations are commonly found in the related literature. We apply them on-the-fly in each training epoch of the network. <ref type="table" target="#tab_2">Table 2</ref> provides the summary of network configurations used in our experiments for the classification and segmentation tasks. We use identical configurations for semantic segmentation on the realistic datasets RueMonge2014, ScanNet and S3DIS, but a different one for the part segmentation of the synthetic ShapeNet. Our network for the realistic datasets takes input point clouds of size <ref type="bibr" target="#b7">8,</ref><ref type="bibr">192</ref>. To put this size into perspective, it is four times of 2, 048 points accepted by PointCNN <ref type="bibr" target="#b36">[37]</ref>. We denote the radius of range search by ρ. It is set to increase by a factor of 2 between hierarchical graphs, while the initial ρ is based on intuition. Since 0.1 is a reasonable context scale to start with, we hence apply it to all datasets except for the ShapeNet dataset. Our choice of a smaller initial ρ (0.08) for  ShapeNet is to benefit the learning of small parts. Further discussion on network configuration is also provided the related sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Configuration:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ModelNet40</head><p>The benchmark ModelNet40 dataset <ref type="bibr" target="#b19">[20]</ref> is used to demonstrate the promise of our technique for object classification. The dataset comprises object meshes for 40 categories with 9,843/2,468 training/testing split. To train our network, we create the point clouds by uniformly sampling on mesh surfaces. Compared to the existing methods (e.g. <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b40">[41]</ref>), the convolutions performed in our network enable processing large input point clouds. Hence, our network is trained employing 10K input points. The channel settings of the first MLP and the six SPH3D layers is 32 and 64-64-64-128-128-128. We use the same classifier 512-256-40 as the previous works <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b40">[41]</ref>. The Encoder4 in <ref type="table" target="#tab_2">Table 2</ref> indicates that the network learns a global representation of the point cloud using G-SPH3D. For that, we create a virtual vertex whose associated coordinates are computed as the average coordinates of the real vertices in the graph. We connect all the real vertices to the virtual vertex, and use a spherical kernel of size 8 × 2 × 1 + 1 for feature computation. G-SPH3D computes the feature only at the virtual vertex,  We use weight decay of 10 −5 in the end-to-end network training, where 0.5 dropout <ref type="bibr" target="#b84">[85]</ref> is also applied to the fully connected layers of the classifier to alleviate overfitting. <ref type="table" target="#tab_3">Table 3</ref> benchmarks the performance of our technique that is abbreviated as SPH3D-GCN. All the tabulated techniques uses xyz coordinates as the raw input features. We also report the training and inference time of PointNet 2 , PointNet++ 3 , DGCNN 4 , Ψ-CNN and SPH3D-GCN on our local Titan Xp GPU. The timings for PointCNN are taken from <ref type="bibr" target="#b36">[37]</ref>, which are based on a more powerful Tesla P100 GPU. Titan Xp and Tesla P100 performance can be compared using <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>. These timings are computed by dividing per batch inference time for each network with the used batch size, where optimized batch sizes reported in the original works are employed. As shown in the <ref type="table">Table,</ref> SPH3D-GCN and Ψ-CNN -our preliminary work -achieve very competitive results. Comparing the computational and memory advantage of SPH3D-GCN over Ψ-CNN, for 10K input points, SPH3D-GCN requires much less parameters (0.78M vs. 3.0M) and performs much faster (18.1/8.4ms vs. 84.3/34.1ms). We also report the performance of SPH3D-GCN for 2, 048 points, where the training/inference time becomes comparable to PointNet++, but performance does not deteriorates much. It is worth mentioning that, relative to PointCNN, the slightly higher number of parameters for our technique results from the classifier. In fact, our parameter size for learning the global feature representation is 0.2M, which is much less than the 0.5M for the PointCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ShapeNet</head><p>The ShapeNet part segmentation dataset <ref type="bibr" target="#b15">[16]</ref> contains 16,881 synthetic models from 16 categories. The models in each category have two to five annotated parts, amounting to 50 parts in total. The point clouds are created with uniform sampling from well-aligned 3D meshes. This dataset provides xyz coordinates of the points as raw features, and has 14,007/2,874 training/testing split defined. Following the existing works <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b87">[88]</ref>, we train independent 2. https://github.com/charlesq34/pointnet. 3. https://github.com/charlesq34/pointnet2. 4. https://github.com/WangYueFt/dgcnn/tree/master/tensorflow. networks to segment the parts of each category. The configuration of our U-shape graph network is shown in <ref type="table" target="#tab_2">Table 2</ref>. The output class number C of the classifier is determined by the number of parts in each category. We standardize the input models of ShapeNet by normalizing the input point clouds to unit sphere with zero mean. Among other ground truth labelling issues pointed out by the existing works <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b54">[55]</ref>, there are some samples in the dataset that contain parts represented with only one point. Differentiating these point with only geometric information is misleading for deep models, both from training and testing perspective. Hence, after normalizing each model, we also remove such points from the point cloud <ref type="bibr" target="#b4">5</ref> .</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we compare our results with the popular techniques that also take irregular point clouds as input, using the part-averaged IoU (mIoU) metric proposed in <ref type="bibr" target="#b32">[33]</ref>. In the table, techniques like PointNet, PointNet++, SO-Net also exploit normals besides point coordinates as the input features, which is not the case for the proposed SPH3D-GCN. In our experiments, SPH3D-GCN not only achieves the same instance mIoU as Ψ-CNN <ref type="bibr" target="#b31">[32]</ref>, but also outperforms the other approaches on 9 out of 16 categories, resulting in the highest class mIoU 84.9%. We also trained a single network with the configuration shown in <ref type="table" target="#tab_2">Table 2</ref> to segment the 50 parts of all categories together. In that case, the obtained instance and class mIoUs are 85.4% and 82.7%, respectively. These results are very close to highly competitive method SFCNN <ref type="bibr" target="#b71">[72]</ref>. In all segmentation experiments, we apply the random sampling operation multiple times to ensure that every point in the test set is evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RueMonge2014</head><p>We test our technique for semantic segmentation of the realworld outdoor scenes using RueMonge2014 dataset <ref type="bibr" target="#b38">[39]</ref>. This dataset contains 700 meters Haussmanian style facades along a European street annotated with point-wise labelling. There are 7 classes in total, which include window, wall, balcony, door, roof, sky and shop. The point clouds are provided with normals and color features. We use xyz coordinates as well as normals and color values to form 9-dim input features for a point. The detailed network configuration used in this experiment is shown in <ref type="table" target="#tab_2">Table 2</ref>, for which C = 7 5. We remove parts represented with a single point within a sphere of radius 0.3. for RueMonge2014. The original point clouds are split into smaller point cloud blocks following the pcl split.mat indexing file provided with the dataset. We randomly sample 8, 192 points from each block and use the sampled point clouds for training and testing. To standardize the points, we force their x and y dimensions to have zero mean values, and the z dimension is kept non-negative. In the realworld applications (here and the following sections), we use data augmentation but no weight decay or dropout. As compared to the preliminary work <ref type="bibr" target="#b31">[32]</ref>, we do not perform pre-processing in terms of alignment of the facade plane and gravitational axis correction. Besides, the processed blocks are also mostly much larger. Under the evaluation protocol of <ref type="bibr" target="#b89">[90]</ref>, <ref type="table" target="#tab_5">Table 5</ref> compares our current approach SPH3D-GCN with the recent methods, including Ψ-CNN <ref type="bibr" target="#b31">[32]</ref>. It can be seen that SPH3D-GCN achieves very competitive performance, using only 0.4M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">ScanNet</head><p>ScanNet <ref type="bibr" target="#b17">[18]</ref> is an RGB-D video dataset of indoor environments that contains reconstructed indoor scenes with rich annotations for 3D semantic labelling. It provides 1, 513 scenes for training and 100 scenes for testing. Researchers are required to submit their test results to an online server for performance evaluation. The dataset provides 40 class labels, while only 20 of them are used for performance evaluation. For this dataset, we keep the network configuration identical to that used for RueMonge2014, as shown in <ref type="table" target="#tab_2">Table 2</ref>, where C = 21. To process each scene, we first downsample the point cloud with the VoxelGrid algorithm <ref type="bibr" target="#b94">[95]</ref> using a 3cm grid. Then, we split each scene into 1.5m×1.5m blocks, padding along each side with 0.3m context points. The context points themselves are neither used in the loss computation nor the final prediction. Following <ref type="bibr" target="#b36">[37]</ref>, the split is only applied to the x and y dimensions, whereas both spatial coordinates (x, y, z) and color values (r, g, b) are used as the input features. Here, (x, y, z) refer to the coordinates after aligning x and y of each block to its center, while z is aligned to the bottom point of the unsplit scene such that z ∈ [0, +∞). We use these aligned xyz coordinates as spatial features of the network, which indicates that the split blocks are processed as independent point cloud samples in both the training and test stages. We compare our approach with PointConv <ref type="bibr" target="#b30">[31]</ref>, PointCNN <ref type="bibr" target="#b36">[37]</ref>, Tangent-Conv <ref type="bibr" target="#b52">[53]</ref>, SPLATNet <ref type="bibr" target="#b54">[55]</ref>, PointNet++ <ref type="bibr" target="#b34">[35]</ref> and ScanNet <ref type="bibr" target="#b17">[18]</ref> in <ref type="table" target="#tab_6">Table 6</ref>. These algorithms report their performance using the xyz coordinates and rgb values as input features similar to our method. A common evaluation protocol is followed by all the techniques in <ref type="table" target="#tab_6">Table 6</ref>. As can be noticed, SPH3D-GCN outperforms other approaches on 16 out of 20 categories, resulting in significant overall improvement in mIoU. The low performance of our method on picture can be attributed to the lack of rich 3D structures. We observed that the network often confuses pictures with walls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">S3DIS</head><p>The Stanford large-scale 3D Indoor Spaces (S3DIS) dataset <ref type="bibr" target="#b16">[17]</ref> comprises colored 3D point clouds collected for 6 large-scale indoor areas of three different buildings using the Matterport scanner. The segmentation task defined on this dataset aims at labelling 13 semantic elements, namely; ceiling, floor, wall, beam, column, window, door, table, chair, sofa, bookcase, board, and clutter. The elements that are not among the first 12, are considered clutter. We use the same network configuration for this dataset as used for the RueMonge2014 and ScanNet, except that C = 13 now. Following the convention <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref>, we perform 6-fold experiment using the six areas, and explicitly experiment with the Area 5. It is a common practice to separately analyze performance on Area 5 because it relates to a building not covered by the other areas <ref type="bibr" target="#b91">[92]</ref>. The used evaluation metrics include the Overall Accuracy (OA), the mean Accuracy of all 13 categories (mAcc), the Intersection Over Union (IoU) for each category, and their mean (i.e. mIoU).</p><p>Most of the scenes in S3DIS has millions of points. We use the same downsampling and block splitting strategy as in ScanNet. The input features also comprise 3D coordinates and color values that are standardized similar to those in ScanNet. The results of our experiments are summarized in <ref type="table" target="#tab_7">Table 7</ref>. Since our work focuses on deep convolutional networks for point cloud processing, we make comparison to the original SPG method <ref type="bibr" target="#b92">[93]</ref> in the table. The SSP+SPG enhancement <ref type="bibr" target="#b95">[96]</ref> of <ref type="bibr" target="#b92">[93]</ref> mainly contributes towards oversegmentation of point clouds, which is not the topic of this paper. With 0.4M parameters, the proposed SPH3D-GCN achieves much better performance than the other convolutional networks (e.g. <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b92">[93]</ref>). For the experiments on Area 5, we also report results of an additional experiment with SPH3D-GCN(9-dim) that follows PointNet <ref type="bibr" target="#b32">[33]</ref> in creating the input feature. The 9-dim input feature comprises xyz+rgb values and the relative location of the point in the scene. Comparing the performance of the proposed network that uses 6-dim input feature, we notice that removing the relative locations actually benefits the performance, which can be attributed to sensitivity of the relative locations to the scene scale. Finally, we visualize two representative prediction examples generated by our technique for the segmentation of Area 5 in <ref type="figure">Fig. 5</ref>. As can be noticed, despite the complexity of the scenes, SPH3D-GCN is able to segment the points effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regular vs. separable:</head><p>In contrast to regular convolution, the use of separable convolution is a major difference between the proposed technique and its preliminary work <ref type="bibr" target="#b31">[32]</ref>. Hence, here we analyze the two choices to clarify the motivation of preferring the latter in this paper. In the following, 'm' denotes the input size of the point cloud, C in and C out are respectively the number of input and  <ref type="figure" target="#fig_2">5 77.3 79.2 70.5 54.9 50.7 53.2 77.2 57.0 85.9 60.2 53.4 4.6 48.9 64.3 70.2 40.4 51.0 85.8 41.</ref>4 <ref type="bibr">†</ref> We include <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b90">[91]</ref> as representative 3D-CNN methods for reference only. These methods do not employ GCN. output feature maps of a kernel, K is the maximum edge connections allowed by each vertex in the graph, and λ is the multiplier of the separable convolution. Let the number of kernel bins be F , which equals to n×p×q+1 for the spherical kernel. To this end, the number of learnable parameters in the regular convolutional kernel is F × C in × C out , while the number of learnable parameters in the separable convolutional kernel is</p><formula xml:id="formula_17">(F × C in × λ) + (C in × λ × C out ).</formula><p>The ratio of the former to the latter is F ×Cout λ×(F +Cout) . Generally, F C out , which allows us to write F ×Cout λ×(F +Cout) ≈ F λ 1. Hence, the number of parameters of separable convolutional kernel is significantly smaller than that of the regular kernel. Regarding the computational complexity, the regular convolution contains 'at most' m × K × C in × C out multiplications, while the separable convolution contains at most m × λ × C in × (K + C out ) multiplications. The former is K×Cout λ×(K+Cout) times of the latter. For the common setup of K C out , we have K×Cout λ×(K+Cout) ≈ K λ 1. This indicates that the number of computations of separable convolution is also significantly less. Reductions in both memory and computation make separable convolution a natural choice for processing large-scale point clouds using densely connected graph representations.</p><p>From the implementation view-point, there are two ways to achieve the regular convolution. One is to use the 'tf.matmul' operation of Tensorflow which exploits the optimized CUBLAS library, referred as regular-V1 in the text to follow. The other is to wrap all the convolutional computations as bottom-level CUDA kernels, referred as regular-V2. The regular-V1 is efficient because of CUBLAS, but memory expensive because context aggregation demands padding of the input feature maps along an additional <ref type="bibr">TABLE 8</ref> Runtime and memory consumption comparison between regular and separable kernel. 'Random' indicates random point clouds and random input features of the mentioned sizes with C in = 64, Cout = 128, λ = 2 and batch size 16. Due to memory requirements, regular-V1 is unable to process point clouds of size 16384, whereas regular-V2 is unable to perform classification and segmentation on the standard datasets in reasonable time. Separable convolution provides considerable memory and runtime advantage to allow better scalability. We use 2048 input points for both classification and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolution</head><p>Random ModelNet40 (Classification) ShapeNet For quantitative comparison, we first compare the forward pass time of input samples of different sizes for separable convolution, regular-V1 and regular-V2. Fixing the batch size to 16, we generate the point clouds and their input features randomly, as we are not concerned with the accuracy at this stage. These results are summarized in the left column of <ref type="table">Table 8</ref>. Due to the memory requirements, regular-V1 is unable to process samples of size 16384 under batch size 16. We also compute the performance of our technique for classification and segmentation with both types of convolutions. These results are also included in <ref type="table">Table 8</ref>. We use ModelNet40 for classification and the <ref type="table">Table category</ref> of ShapeNet for segmentation. Due to its unreasonably slow computation, we exclude the results of regular-V2 which is expected to achieve similar accuracy as regular-V1 because both variants are actually implementations of the exact same mathematical operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling &amp; unpooling &amp; augmentation:</head><p>To analyze the contribution of different constituents of our technique, we also perform ablation study by varying our pooling strategy, interpolation method and data augmentation. In <ref type="table">Table 9</ref>, we summarize the results of these experiments using the Area 5 of S3DIS. By default, our technique uses max-pooling, uniform interpolation and data augmentation, which constitutes the 'baseline' in the table. Variations from the baseline are noted as the labels of the columns. We provide results for the interpolations for two different point cloud sizes, i.e. 8192 and 4096. From the table, we can conclude that 1) the adopted max pooling strategy is more beneficial than average pooling for our network. 2) Data augmentation provides a reasonable performance boost to our technique. 3) Weighted interpolation can provide better results for denser point clouds. However, its performance against uniform interpolation is inferior for sparser clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Scalability: The combination of discrete kernel, separable convolution and graph-based architecture adds to the scalibility of the proposed SPH3D-GCN. In <ref type="table" target="#tab_9">Table 10</ref>, we compare our network on computational and memory grounds with a highly competitive convolutional network PointCNN that is able to take 2, 048 points as input. The reported values are for S3DIS, using the configuration in <ref type="table" target="#tab_2">Table 2</ref> for our network, where we vary the input point size. The performance is also provided for reference. We note that smaller input sizes tend to produce slightly better results because they benefit more from data augmentation of random sampling. We use split blocks of at most 30,000 points. Hence, inputs with more than 30000 points (e.g. 32768, 65536) are affected adversely as their size increases. We provide the results for 32,768 points as a reference for such cases. We show the memory consumption and training/testing time of our network, as well as the graph construction time. From the table, the convolution time can be computed as the difference between the inference time and graph construction time. With a batch size 16 on 12GB GPU, our network can take point cloud of size up to 65, 536, which is identical to the number of pixels in a 256 × 256 image. It is worth mentioning that the memory consumption of our 'segmentation' network for 32, 678 input points is slightly lower than that of PointNet++ 'classification' network for 1, 024 points (8.45GB vs. 8.57GB), using the same batch size, i.e. 16. Our 0.4M parameters are 10+ times less than the 4.4M of PointCNN. Considering that we use a larger batch size than the PointCNN, we include both the per-batch and per-sample training/testing time for a fair comparison. It can be seen that our per-sample running time for 2, 048, 4, 096, and 8, 192 points is less than or comparable to that of PointCNN for 2, 048 points. We refer to the websites <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref> for a speed comparison between Tesla P100 and Tian Xp. Although our SPH3D-GCN can take larger input size, we use point cloud of size 8, 192 for S3DIS in <ref type="table" target="#tab_7">Table 7</ref> in the interest of time. Graph coarsening visualization: We coarsen point cloud along our network with the Farthest Point Sampling (FPS) that reduces graph resolution layer-by-layer, similar to the image resolution reduction in the standard CNNs. We visualize the coarsening effects of the FPS in <ref type="figure" target="#fig_4">Fig. 6(top)</ref>, using a chair from ModelNet40 as an example. The point clouds from left to right associate to the vertices of graphs G 0 , G 1 , G 2 , G 3 in the network of ModelNet40. The resolution of the point cloud systematically reduces from left to right. Specifically, according to <ref type="table" target="#tab_2">Table 2</ref>, these point clouds contain 10K, 2500, 625, 156 points. Kernel visualization: We also visualize few learned spherical kernels in <ref type="figure" target="#fig_4">Fig. 6(bottom)</ref>. The two rows correspond to the spherical kernels of two SPH3D layers in Encoder2 of the network for S3DIS dataset. The size of these kernels are 8 × 2 × 2 + 1. As can be noticed, the weights of different kernels distribute differently in the range [−0.5, 0.4]. For example, the third kernel in the first row contains positive weights dominantly in its upper hemisphere, but negative weights in the lower hemisphere, while the kernel exactly below it is mainly composed of negative weights. These differences indicate that different kernels can identify different features for the same neighborhoods. For better visualization, we color each bin only on the sphere surface, not the 3D volume. Moreover, we also do not show the weight of self-loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We introduced separable spherical convolutional kernel for point clouds and demonstrated its utility with graph pyra- mid architectures. We built the graph pyramids with range search and farthest point sampling techniques. By applying the spherical convolution block to each graph resolution, the resulting graph convolutional networks are able to learn more effective features in larger contexts, similar to the standard CNNs. To perform the convolutions, the spherical kernel partitions its occupied space into multiple bins and associates a learnable parameter with each bin. The parameters are learned with network training. We down/upsample the vertex features of different graphs with pooling/unpooling operations. The proposed convolutional network is shown to be efficient in processing high resolution point clouds, achieving highly competitive performance on the tasks of classification and semantic segmentation on synthetic and large-scale real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of Encoder-decoder graph neural network for a toy example. A graph G l of 12 vertices gets coarsened to G l+1 (8 vertices) and further to G l+2 (4 vertices), and expanded back to 12 vertices. The width variation of feature maps depicts different number of feature channels, whereas the number of cells indicates the total vertices in the corresponding graph. The pooling/unpooling operations compute features of the coarsened/expanded graphs. Consecutive convolutions are applied to form convolution blocks. The shown architecture for semantic segmentation uses skip connections for feature concatenation, similar to U-Net. For classification, the decoder and skip connections are removed and a global representation is fed to a classifier. We omit self loops in the shown graphs for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Representative samples from datasets: ModelNet40 and ShapeNet provide point clouds of synthetic models. We also illustrate ground truth segmentation for ShapeNet. RueMonge2014 comprises point clouds for outdoor scenes, while ScanNet and S3DIS contain indoor scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>(Top) Graph coarsening with FPS: A chair is coarsened from left to right into point clouds of smaller resolutions. (Bottom) Kernel visualization: Each row shows five spherical kernels learned in an SPH3D layer of the network for S3DIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Network configuration details: NN(ρ) denotes a range search with radius ρ. SPH3D(C in , Cout, λ) represents a separable spherical convolution that takes C in input features, performs a depth-wise convolution with a multiplier λ followed by a point-wise convolution to generate Cout features. When λ is omitted in the table, we use λ = 2. MLP(C in , Cout) and FC(C in , Cout) indicate multilayer perceptron and fully connected layer taking C in input features, and output Cout features. G-SPH3D denotes global spherical convolution that applies SPH3D once to a single point for global feature learning. The brackets [ ] are used to show feature concatenation. The pool(A, B) and unpool(A, B) operations transform vertices A into B, and C indicates the number of classes in a dataset.</figDesc><table><row><cell cols="2">Layer Name MLP1</cell><cell>Encoder1</cell><cell>Encoder2</cell><cell>Encoder3</cell><cell>Encoder4</cell><cell>Decoder4</cell><cell>Decoder3</cell><cell>Decoder2</cell><cell>Decoder1</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell>NN(ρ = 0.1)</cell><cell>NN(ρ = 0.2)</cell><cell>NN(ρ = 0.4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC(832,512)</cell></row><row><cell>ModelNet40</cell><cell cols="4">MLP SPH3D(64,64) SPH3D(64,64,1) SPH3D(128,128,1) (3,32) SPH3D(64,64,1) SPH3D(64,128) SPH3D(128,128,1)</cell><cell>G-SPH3D (128,512)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>FC(512,256) FC(256,40)</cell></row><row><cell></cell><cell></cell><cell cols="2">pool(10K,2500) pool(2500,625)</cell><cell>pool(625,156)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>NN(ρ = 0.08)</cell><cell>NN(ρ = 0.16)</cell><cell>NN(ρ = 0.36)</cell><cell>NN(ρ = 0.64)</cell><cell></cell><cell>Enc4,Dec4</cell><cell>Enc3,Dec3</cell><cell>Enc2,Dec2</cell><cell>Enc1,Dec1</cell></row><row><cell>ShapeNet</cell><cell cols="10">MLP SPH3D(64,128) SPH3D(128,256) SPH3D(256,256) SPH3D(256,512) SPH3D(512,512) SPH3D(1024,256) SPH3D(512,256) SPH3D(512,128) [MLP1,MLP (3,64) SPH3D(128,128) SPH3D(256,256) SPH3D(256,256) SPH3D(512,512) SPH3D(512,512) SPH3D(256,256) SPH3D(256,256) SPH3D(128,128) (256,64)]</cell></row><row><cell></cell><cell></cell><cell cols="2">pool(2048,1024) pool(1024,768)</cell><cell>pool(768,384)</cell><cell cols="6">pool(384,128) unpool(128,384) unpool(384,768) unpool(768,1024) unpool(1024,2048) FC(128,C)</cell></row><row><cell cols="2">RueMonge-MLP</cell><cell>NN(ρ = 0.1)</cell><cell>NN(ρ = 0.2)</cell><cell>NN(ρ = 0.4)</cell><cell>NN(ρ = 0.8)</cell><cell></cell><cell>Enc4,Dec4</cell><cell>Enc3,Dec3</cell><cell>Enc2,Dec2</cell><cell>Enc1,Dec1</cell></row><row><cell>2014</cell><cell cols="10">(9,64) SPH3D(64,128) SPH3D(128,256) SPH3D(256,256) SPH3D(256,512) SPH3D(512,512) SPH3D(1024,256) SPH3D(512,256) SPH3D(512,128) FC(256,C)</cell></row><row><cell>ScanNet,</cell><cell cols="9">MLP SPH3D(128,128) SPH3D(256,256) SPH3D(256,256) SPH3D(512,512) SPH3D(512,512) SPH3D(256,256) SPH3D(256,256) SPH3D(128,128)</cell></row><row><cell>S3DIS</cell><cell cols="3">(6,64) pool(8192,2048) pool(2048,768)</cell><cell>pool(768,384)</cell><cell cols="5">pool(384,128) unpool(128,384) unpool(384,768) unpool(768,2048) unpool(2048,8192)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>ModelNet40 classification: Average class and instance accuracies are reported along with the number of input points per sample (#point), the number of network parameters (#params), and the train/test time.</figDesc><table><row><cell>Method</cell><cell cols="4">#point #params class instance</cell><cell cols="2">time(ms) training testing</cell></row><row><cell>ECC [28]</cell><cell>1000</cell><cell>0.2M</cell><cell>83.2</cell><cell>87.4</cell><cell>-</cell><cell>-</cell></row><row><cell>PointNet [33]</cell><cell>1024</cell><cell>3.5M</cell><cell>86.2</cell><cell>89.2</cell><cell>7.9</cell><cell>2.5</cell></row><row><cell cols="2">PointNet++ [35] 1024</cell><cell>1.5M</cell><cell>88.0</cell><cell>90.7</cell><cell>4.9</cell><cell>1.3</cell></row><row><cell>Kd-net(10) [34]</cell><cell>1024</cell><cell>3.5M</cell><cell>86.3</cell><cell>90.6</cell><cell>-</cell><cell>-</cell></row><row><cell>SO-Net [36]</cell><cell>2048</cell><cell>2.4M</cell><cell>87.3</cell><cell>90.9</cell><cell>-</cell><cell></cell></row><row><cell>KCNet [41]</cell><cell>2048</cell><cell>0.9M</cell><cell>-</cell><cell>91.0</cell><cell>-</cell><cell>-</cell></row><row><cell>DGCNN [29]</cell><cell>1024</cell><cell>1.8M</cell><cell>89.3</cell><cell>91.5</cell><cell>15.3</cell><cell>5.6</cell></row><row><cell>PointCNN [37]</cell><cell>1024</cell><cell>0.6M</cell><cell>88.0</cell><cell>91.7</cell><cell>19.4</cell><cell>7.5</cell></row><row><cell>SFCNN [72]</cell><cell>1024</cell><cell>8.6M</cell><cell>-</cell><cell>91.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Ψ-CNN [32]</cell><cell>10000</cell><cell>3.0M</cell><cell>88.7</cell><cell>92.0</cell><cell>84.3</cell><cell>34.1</cell></row><row><cell>SPH3D-GCN</cell><cell>2048</cell><cell>0.7M</cell><cell>88.5</cell><cell>91.4</cell><cell>4.0</cell><cell>1.4</cell></row><row><cell>(Proposed)</cell><cell>10000</cell><cell>0.8M</cell><cell>89.3</cell><cell>92.1</cell><cell>18.1</cell><cell>8.4</cell></row><row><cell cols="7">that becomes the global representation of point cloud for</cell></row><row><cell>the classifier.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Following our preliminary work for Ψ-CNN [32], we</cell></row><row><cell cols="7">boost performance of the classification network by apply-</cell></row><row><cell cols="7">ing max pooling to the intermediate layers, i.e. Encoder1,</cell></row><row><cell cols="7">Encoder2, Encoder3. We concatenate these max-pooled fea-</cell></row><row><cell cols="7">tures to the global feature representation in Encoder4 to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 Part</head><label>4</label><figDesc>Segmentation Results on ShapeNet dataset.</figDesc><table><row><cell></cell><cell cols="4">instance class Air-Bag Cap Car Chair mIoU mIoU plane</cell><cell cols="4">Ear-Guitar Knife Lamp Laptop phone</cell><cell cols="4">Motor-Mug Pistol Rocket bike</cell><cell>Skate-Table board</cell></row><row><cell># shapes</cell><cell></cell><cell></cell><cell>2690 76</cell><cell>55 898 3758</cell><cell>69</cell><cell>787</cell><cell>392 1547</cell><cell>451</cell><cell>202</cell><cell>184</cell><cell>283</cell><cell>66</cell><cell>152</cell><cell>5271</cell></row><row><cell>3D-CNN [33]</cell><cell>79.4</cell><cell cols="3">74.9 75.1 72.8 73.3 70.0 87.2</cell><cell>63.5</cell><cell>88.4</cell><cell>79.6 74.4</cell><cell>93.9</cell><cell>58.7</cell><cell cols="2">91.8 76.4</cell><cell>51.2</cell><cell>65.3</cell><cell>77.1</cell></row><row><cell>Kd-net [34]</cell><cell>82.3</cell><cell cols="3">77.4 80.1 74.6 74.3 70.3 88.6</cell><cell>73.5</cell><cell>90.2</cell><cell>87.2 81.0</cell><cell>94.9</cell><cell>57.4</cell><cell cols="2">86.7 78.1</cell><cell>51.8</cell><cell>69.9</cell><cell>80.3</cell></row><row><cell>PointNet [33]</cell><cell>83.7</cell><cell cols="3">80.4 83.4 78.7 82.5 74.9 89.6</cell><cell>73.0</cell><cell>91.5</cell><cell>85.9 80.8</cell><cell>95.3</cell><cell>65.2</cell><cell cols="2">93.0 81.2</cell><cell>57.9</cell><cell>72.8</cell><cell>80.6</cell></row><row><cell>Spec-CNN [27]</cell><cell>84.7</cell><cell cols="3">82.0 81.6 81.7 81.9 75.2 90.2</cell><cell>74.9</cell><cell>93.0</cell><cell>86.1 84.7</cell><cell>95.6</cell><cell>66.7</cell><cell cols="2">92.7 81.6</cell><cell>60.6</cell><cell>82.9</cell><cell>82.1</cell></row><row><cell>SPLATNet 3D [55]</cell><cell>84.6</cell><cell cols="3">82.0 81.9 83.9 88.6 79.5 90.1</cell><cell>73.5</cell><cell>91.3</cell><cell>84.7 84.5</cell><cell>96.3</cell><cell>69.7</cell><cell cols="2">95.0 81.7</cell><cell>59.2</cell><cell>70.4</cell><cell>81.3</cell></row><row><cell>KCNet [41]</cell><cell>84.7</cell><cell cols="3">82.2 82.8 81.5 86.4 77.6 90.3</cell><cell>76.8</cell><cell>91.0</cell><cell>87.2 84.5</cell><cell>95.5</cell><cell>69.2</cell><cell cols="2">94.4 81.6</cell><cell>60.1</cell><cell>75.2</cell><cell>81.3</cell></row><row><cell>SO-Net [36]</cell><cell>84.9</cell><cell cols="3">81.0 82.8 77.8 88.0 77.3 90.6</cell><cell>73.5</cell><cell>90.7</cell><cell>83.9 82.8</cell><cell>94.8</cell><cell>69.1</cell><cell cols="2">94.2 80.9</cell><cell>53.1</cell><cell>72.9</cell><cell>83.0</cell></row><row><cell>PointNet++ [35]</cell><cell>85.1</cell><cell cols="3">81.9 82.4 79.0 87.7 77.3 90.8</cell><cell>71.8</cell><cell>91.0</cell><cell>85.9 83.7</cell><cell>95.3</cell><cell>71.6</cell><cell cols="2">94.1 81.3</cell><cell>58.7</cell><cell>76.4</cell><cell>82.6</cell></row><row><cell>DGCNN [29]</cell><cell>85.2</cell><cell cols="3">82.3 84.0 83.4 86.7 77.8 90.6</cell><cell>74.7</cell><cell>91.2</cell><cell>87.5 82.8</cell><cell>95.7</cell><cell>66.3</cell><cell cols="2">94.9 81.1</cell><cell>63.5</cell><cell>74.5</cell><cell>82.6</cell></row><row><cell>SpiderCNN [66]</cell><cell>85.3</cell><cell cols="3">81.7 83.5 81.0 87.2 77.5 90.7</cell><cell>76.8</cell><cell>91.1</cell><cell>87.3 83.3</cell><cell>95.8</cell><cell>70.2</cell><cell cols="2">93.5 82.7</cell><cell>59.7</cell><cell>75.8</cell><cell>82.8</cell></row><row><cell>SFCNN [72]</cell><cell>85.4</cell><cell cols="3">82.7 83.0 83.4 87.0 80.2 90.1</cell><cell>75.9</cell><cell>91.1</cell><cell>86.2 84.2</cell><cell>96.7</cell><cell>69.5</cell><cell cols="2">94.8 82.5</cell><cell>59.9</cell><cell>75.1</cell><cell>82.9</cell></row><row><cell>PointCNN [37]</cell><cell>86.1</cell><cell cols="3">84.6 84.1 86.5 86.0 80.8 90.6</cell><cell>79.7</cell><cell>92.3</cell><cell>88.4 85.3</cell><cell>96.1</cell><cell>77.2</cell><cell cols="2">95.3 84.2</cell><cell>64.2</cell><cell>80.0</cell><cell>82.3</cell></row><row><cell>Ψ-CNN [32]</cell><cell>86.8</cell><cell cols="3">83.4 84.2 82.1 83.8 80.5 91.0</cell><cell>78.3</cell><cell>91.6</cell><cell>86.7 84.7</cell><cell>95.6</cell><cell>74.8</cell><cell cols="2">94.5 83.4</cell><cell>61.3</cell><cell>75.9</cell><cell>85.9</cell></row><row><cell>SPH3D-GCN(Prop.)</cell><cell>86.8</cell><cell>84.9</cell><cell cols="2">84.4 86.2 89.2 81.2 91.5</cell><cell>77.4</cell><cell>92.5</cell><cell>88.2 85.7</cell><cell>96.7</cell><cell>78.6</cell><cell cols="2">95.6 84.7</cell><cell>63.9</cell><cell>78.5</cell><cell>84.0</cell></row><row><cell cols="6">form a more effective representation. This results in features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">with 832 = 64 + 128 + 128 + 512 channels for the classifier.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 Semantic</head><label>5</label><figDesc>Segmentation on RueMonge2014 dataset.</figDesc><table><row><cell>Method</cell><cell>mAcc</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>Riemenschneider et al. [39]</cell><cell>-</cell><cell>-</cell><cell>42.3</cell></row><row><cell>Martinovic et al. [89]</cell><cell>-</cell><cell>-</cell><cell>52.2</cell></row><row><cell>Gadde et al. [90]</cell><cell>68.5</cell><cell>78.6</cell><cell>54.4</cell></row><row><cell>OctNet 256 3 [23]</cell><cell>73.6</cell><cell>81.5</cell><cell>59.2</cell></row><row><cell>SPLATNet 3D [55]</cell><cell>-</cell><cell>-</cell><cell>65.4</cell></row><row><cell>Ψ-CNN [32]</cell><cell>74.7</cell><cell>83.5</cell><cell>63.6</cell></row><row><cell>SPH3D-GCN (Proposed)</cell><cell>80.0</cell><cell>84.4</cell><cell>66.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>3D semantic labelling on Scannet: All the techniques use 3D coordinates and color values as input features for network training. Method mIoU floor wall chair sofa table door cab bed desk toil sink wind pic bkshf curt show cntr fridg bath other SSC-UNet [91] † 30.8 90.8 68.5 55.3 36.3 34.5 14.8 16.6 29.0 14.7 54.6 35.4 27.8 6.4 27.8 28.6 1.8 16.9 2.3 35.3 18.2 SparseConvNet [50] † 72.5 95.5 86.5 86.9 82.3 62.8 61.4 72.1 82.1 60.3 93.4 72.4 68.3 32.5 84.6 75.4 87.0 53.3 71.0 64.7 57.2 ScanNet [18] 30.6 78.6 43.7 52.4 34.8 30.0 18.9 31.1 36.6 34.2 46.0 31.8 18.2 10.2 50.1 0.2 15.2 21.1 24.5 20.3 14.5 PointNet++ [35] 33.9 67.7 52.3 36.0 34.6 23.2 26.1 25.6 47.8 27.8 54.8 36.4 25.2 11.7 45.8 24.7 14.5 25.0 21.2 58.4 18.3 SPLATNET 3D [55] 39.3 92.7 69.9 65.6 51.0 38.3 19.7 31.1 51.1 32.8 59.3 27.1 26.7 0.0 60.6 40.5 24.9 24.5 0.1 47.2 22.7</figDesc><table><row><cell>Tangent-Conv [53]</cell><cell>43.8 91.8 63.3 64.5 56.2 42.7 27.9 36.9 64.6 28.2 61.9 48.7 35.2 14.7 47.4 25.8 29.4 35.3 28.3 43.7 29.8</cell></row><row><cell>PointCNN [37]</cell><cell>45.8 94.4 70.9 71.5 54.5 45.6 31.9 32.1 61.1 32.8 75.5 48.4 47.5 16.4 35.6 37.6 22.9 29.9 21.6 57.7 28.5</cell></row><row><cell>PointConv [31]</cell><cell>55.6 94.4 76.2 73.9 63.9 50.5 44.5 47.2 64.0 41.8 82.7 54.0 51.5 18.5 57.4 43.3 57.5 43.0 46.4 63.6 37.2</cell></row><row><cell cols="2">SPH3D-GCN (Prop.) 61.0 93.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 Performance</head><label>7</label><figDesc>Prediction visualization for two representative scenes of Area 5 in S3DIS dataset. We have removed the ceiling so that the details inside the offices are clearly visible. Despite the scene complexity, the proposed SPH3D-GCN generally segments the points accurately.</figDesc><table><row><cell></cell><cell cols="10">on S3DIS dataset: Area 5 (top), all 6 folds (bottom). For the Area 5, SPH3D-GCN (9-dim) follows PointNet [33] to construct 9-dim</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">input feature instead of 6-dim feature used by the proposed network.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Methods</cell><cell cols="9">OA mAcc mIoU ceiling floor wall beam column window door table chair sofa bookcase board clutter</cell></row><row><cell></cell><cell>PointNet [33]</cell><cell>-</cell><cell>49.0 41.1</cell><cell>88.8 97.3 69.8 0.1</cell><cell>3.9</cell><cell>46.3</cell><cell>10.8 58.9 52.6 5.9</cell><cell>40.3</cell><cell>26.4</cell><cell>33.2</cell></row><row><cell></cell><cell>SEGCloud [92]</cell><cell>-</cell><cell>57.4 48.9</cell><cell>90.1 96.1 69.9 0.0</cell><cell>18.4</cell><cell>38.4</cell><cell>23.1 70.4 75.9 40.9</cell><cell>58.4</cell><cell>13.0</cell><cell>41.6</cell></row><row><cell>Area 5</cell><cell>Tangent-Conv [53] SPG [93] PointCNN [37]</cell><cell cols="2">82.5 62.2 52.8 86.4 66.5 58.0 85.9 63.9 57.3</cell><cell>-89.4 96.9 78.1 0.0 ---92.3 98.2 79.4 0.0</cell><cell>-42.8 17.6</cell><cell>-48.9 22.8</cell><cell>-61.6 75.4 84.7 52.6 ---62.1 74.4 80.6 31.7</cell><cell>-69.8 66.7</cell><cell>-2.1 62.1</cell><cell>-52.2 56.7</cell></row><row><cell></cell><cell cols="3">SPH3D-GCN (9-dim) 86.6 65.9 58.6</cell><cell>92.2 97.2 79.9 0.0</cell><cell>32.0</cell><cell>52.2</cell><cell>41.6 76.9 85.3 36.5</cell><cell>67.2</cell><cell>50.7</cell><cell>50.0</cell></row><row><cell></cell><cell cols="3">SPH3D-GCN (Prop.) 87.7 65.9 59.5</cell><cell>93.3 97.1 81.1 0.0</cell><cell>33.2</cell><cell>45.8</cell><cell>43.8 79.7 86.9 33.2</cell><cell>71.5</cell><cell>54.1</cell><cell>53.7</cell></row><row><cell>All 6 Folds</cell><cell cols="3">PointNet [33] Engelmann et al. [94] 81.1 66.4 49.7 78.5 66.2 47.6 DGCNN [29] 84.1 -56.1 SPG [93] 85.5 73.0 62.1 PointCNN [37] 88.1 75.6 65.4</cell><cell>88.0 88.7 69.3 42.4 90.3 92.1 67.9 44.7 ----89.9 95.1 76.4 62.8 94.8 97.3 75.8 63.3</cell><cell>23.1 24.2 -47.1 51.7</cell><cell>47.5 52.3 -55.3 58.4</cell><cell>51.6 42.0 54.1 38.2 51.2 47.4 58.1 39.0 ----68.4 73.5 69.2 63.2 57.2 71.6 69.1 39.1</cell><cell>9.6 6.9 -45.9 61.2</cell><cell>29.4 30.0 -8.7 52.2</cell><cell>35.2 41.9 -52.9 58.6</cell></row><row><cell></cell><cell cols="3">SPH3D-GCN (Prop.) 88.6 77.9 68.9</cell><cell>93.3 96.2 81.9 58.6</cell><cell>55.9</cell><cell>55.9</cell><cell>71.7 72.1 82.4 48.5</cell><cell>64.5</cell><cell>54.8</cell><cell>60.4</cell></row><row><cell></cell><cell>Office</cell><cell></cell><cell></cell><cell>mIoU= 94.4%</cell><cell></cell><cell></cell><cell>Office</cell><cell cols="2">mIoU= 91.2%</cell></row><row><cell></cell><cell cols="2">Ground truth</cell><cell></cell><cell>Proposed</cell><cell></cell><cell cols="2">Ground truth</cell><cell cols="2">Proposed</cell></row><row><cell cols="2">Fig. 5.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table ( TABLE 9</head><label>(9</label><figDesc>Ablation study (using Area 5 of S3DIS) when pooling is changed to average pooling, data augmentation is excluded and when uniform or weighted interpolation is performed. The 'baseline' column has results for our proposed method that uses max pooling+with augmentation+uniform interpolation. Average pooling is not as good as max pooling and data augmentation does improve the results. Weighted interpolation works better when the number of points is high (i.e. dense point cloud) but degrades the performance in case the points are less (i.e. sparse point cloud). KPConv<ref type="bibr" target="#b67">[68]</ref> exploits 'tf.matmul' for computational efficiency. However, this inevitably restrains its scalability. The regular-V2 is memory friendly, but inefficient because it fails to exploit CUBLAS. We note that separable convolution does not suffer from the implementation dilemma of the regular convolution. On one hand, it performs context aggregation with the spatial convolution on CUDA kernels in the bottom level, which saves a large amount of memory. One the other hand, for the point-wise convolution, it can readily exploit the 'tf.matmul' operator without padding requirement. The low theoretical complexity along with efficient implementation solution make our technique scalable with separable spherical convolutions.</figDesc><table><row><cell></cell><cell cols="3">2048 4096 8192 16384</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Segmentation)</cell></row><row><cell></cell><cell cols="3">single forward pass</cell><cell cols="2">batch instance</cell><cell>GPU</cell><cell cols="4">train/test batch instance</cell><cell>GPU</cell><cell>train/test</cell></row><row><cell></cell><cell cols="3">per-batch time(ms)</cell><cell cols="6">size accuracy memory time(ms) size</cell><cell>mIoU memory time(ms)</cell></row><row><cell>separable</cell><cell cols="3">14.0 38.7 98.5 280.3</cell><cell>32</cell><cell>91.4</cell><cell>0.8GB</cell><cell cols="2">4.0/1.4</cell><cell>32</cell><cell>84.0</cell><cell>4.4GB</cell><cell>33.4/9.4</cell></row><row><cell cols="4">regular-V1 (fast) 36.1 94.3 214.8 N/A</cell><cell>32</cell><cell>91.3</cell><cell>8.7GB</cell><cell cols="2">12.5/5.3</cell><cell>12</cell><cell>84.1</cell><cell>10.7GB 180.8/25.2</cell></row><row><cell cols="4">regular-V2 (slow) 84.5 453.0 1185.1 2746.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">8192 points</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4096 points</cell></row><row><cell cols="2">Metrics</cell><cell>baseline</cell><cell>average pooling</cell><cell cols="2">without augmentation</cell><cell cols="2">weighted interpolation</cell><cell cols="2">uniform interpolation</cell><cell>weighted interpolation</cell></row><row><cell>OA</cell><cell></cell><cell>87.7</cell><cell>87.3</cell><cell>86.4</cell><cell></cell><cell>88.1</cell><cell></cell><cell cols="2">88.2</cell><cell>87.8</cell></row><row><cell cols="2">mAcc</cell><cell>65.9</cell><cell>65.5</cell><cell>64.4</cell><cell></cell><cell>67.2</cell><cell></cell><cell cols="2">68.8</cell><cell>67.8</cell></row><row><cell cols="2">mIoU</cell><cell>59.5</cell><cell>58.8</cell><cell>57.0</cell><cell></cell><cell>61.2</cell><cell></cell><cell cols="2">62.2</cell><cell>61.1</cell></row><row><cell cols="2">neighboring dimension.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc>Computational and memory requirements of the proposed technique and comparison to PointCNN. The performance is also included for reference. The GPU memory usage is computed with NVIDIA's standard 'nvidia-smi' command.</figDesc><table><row><cell>Method</cell><cell>GPU</cell><cell>batch size</cell><cell cols="2">#params #point</cell><cell>GPU  † memory</cell><cell cols="6">time(ms) training per-batch per-sample per-batch per-sample per-batch per-sample graph construction inference</cell><cell cols="2">performance OA mAcc mIoU</cell></row><row><cell>PointCNN</cell><cell cols="2">Tesla P100 12</cell><cell>4.4M</cell><cell>2048</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>610</cell><cell>50.8</cell><cell>250</cell><cell>20.8</cell><cell cols="2">85.9 63.9</cell><cell>57.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2048 2.31GB</cell><cell>26</cell><cell>1.6</cell><cell>450</cell><cell>28.1</cell><cell>150</cell><cell>9.4</cell><cell cols="2">87.9 67.8</cell><cell>61.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4096 2.31GB</cell><cell>35</cell><cell>2.2</cell><cell>566</cell><cell>35.4</cell><cell>201</cell><cell>12.6</cell><cell cols="2">88.2 68.8</cell><cell>62.2</cell></row><row><cell cols="2">SPH3D-GCN Titan Xp</cell><cell>16</cell><cell>0.4M</cell><cell cols="2">8192 4.36GB 16384 4.36GB</cell><cell>65 162</cell><cell>4.1 10.1</cell><cell>869 1509</cell><cell>54.3 94.3</cell><cell>337 650</cell><cell>21.1 40.6</cell><cell cols="2">87.7 65.9 87.3 66.2</cell><cell>59.5 59.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32768 8.45GB</cell><cell>511</cell><cell>31.9</cell><cell>2803</cell><cell>175.2</cell><cell>1354</cell><cell>84.6</cell><cell cols="2">85.8 63.9</cell><cell>56.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">65536 11.10GB</cell><cell>1816</cell><cell>113.5</cell><cell>5880</cell><cell>183.8</cell><cell>3311</cell><cell>103.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported by the Australian Research Council (ARC) grant DP190102443. The Titan Xp GPU used for this research is donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense 3d face correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1584" to="1598" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic3D.net: A new large-scale point cloud classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="98" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2282" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">305</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Octree guided cnn with spherical kernels for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9631" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Computational geometry: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Preparata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Shamos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning where to classify in multi-view semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bódis-Szomorú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weissenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="516" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attentional ShapeContextNet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3D convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Orientationboosted voxel nets for 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03351</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3DMatch: Learning local geometric descriptors from RGB-D reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepcontext: Context-encoding neural pathways for 3D holistic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1192" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Hay</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">FPNN: Field probing neural networks for 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGB-D semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">3D recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="403" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">235</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Flex-convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Lensch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">282289</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dovrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Spherical fractal convolutional neural networks for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="452" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Recognizing objects in range data using regional point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kolluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bülow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="224" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unique shape context for 3D data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM workshop on 3D object retrieval</title>
		<meeting>the ACM workshop on 3D object retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">GPU</title>
		<ptr target="https://www.nvidia.com" />
		<imprint>
			<date type="published" when="2019-08-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">FP32 Throughput Comparison</title>
		<ptr target="http://www.cs.toronto.edu/∼pekhimenko/tbd/throughput.html" />
		<imprint>
			<date type="published" when="2019-08-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Interactive shape co-segmentation via label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="248" to="254" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">3D all the way: Semantic segmentation of urban scenes from start to end in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4456" to="4465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Efficient 2D and 3D facade segmentation using auto-context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1280" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Submanifold sparse convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="716" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">3D is here: Point Cloud Library (PCL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Point cloud oversegmentation with graph-structured deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boussaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
