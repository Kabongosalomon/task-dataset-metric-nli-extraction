<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ocean: Object-aware Anchor-free Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
							<email>zhipeng.zhang.cv@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">CASIA &amp; AI School</orgName>
								<address>
									<country>UCAS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<email>houwen.peng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CASIA &amp; AI School</orgName>
								<address>
									<country>UCAS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CASIA &amp; AI School</orgName>
								<address>
									<country>UCAS</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ocean: Object-aware Anchor-free Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anchor-based Siamese trackers have achieved remarkable advancements in accuracy, yet the further improvement is restricted by the lagged tracking robustness. We find the underlying reason is that the regression network in anchor-based methods is only trained on the positive anchor boxes (i.e., IoU ≥ 0.6). This mechanism makes it difficult to refine the anchors whose overlap with the target objects are small. In this paper, we propose a novel object-aware anchor-free network to address this issue. First, instead of refining the reference anchor boxes, we directly predict the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference. Second, we introduce a feature alignment module to learn an object-aware feature from predicted bounding boxes. The object-aware feature can further contribute to the classification of target objects and background. Moreover, we present a novel tracking framework based on the anchor-free model. The experiments show that our anchor-free tracker achieves state-of-the-art performance on five benchmarks, including VOT-2018, VOT-2019, OTB-100, GOT-10k and LaSOT. The source code is available at https://github.com/researchmm/TracKit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object tracking is a fundamental vision task. It aims to infer the location of an arbitrary target in a video sequence, given only its location in the first frame. The main challenge of tracking lies in that the target objects may undergo heavy occlusions, large deformation and illumination variations <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49]</ref>. Tracking at real-time speeds has a variety of applications, such as surveillance, robotics, autonomous driving and human-computer interaction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>In recent years, Siamese tracker has drawn great attention because of its balanced speed and accuracy. The seminal works, i.e., SINT <ref type="bibr" target="#b34">[35]</ref> and SiamFC <ref type="bibr" target="#b0">[1]</ref>, employ Siamese networks to learn a similarity metric between the object target and candidate image patches, thus modeling the tracking as a search problem of the target over the entire image. A large amount of follow-up Siamese trackers have been proposed and achieved promising performances <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50]</ref>. Among them, the Siamese region proposal networks, dubbed SiamRPN <ref type="bibr" target="#b21">[22]</ref>, is representative. It introduces region proposal networks <ref type="bibr" target="#b30">[31]</ref>, which consist of a classification network for foreground-background estimation and a regression network for anchor-box refinement, i.e., learning 2D offsets to the predefined anchor boxes. This anchor-based trackers have shown tremendous potential in tracking accuracy. However, since the regression network is only trained on the positive anchor boxes (i.e., IoU ≥ 0.6), it is difficult to refine the anchors whose overlap with the target objects are small. This will cause tracking failures especially when the classification results are not reliable. For instance, due to the error accumulation in tracking, the predictions of target positions may become unreliable, e.g., IoU &lt; 0.3. The regression network is incapable of rectifying this weak prediction because it is previously unseen in the training set. As a consequence, the tracker gradually drifts in subsequent frames.</p><p>It is natural to throw a question: can we design a bounding-box regressor with the capability of rectifying inaccurate predictions? In this work, we show the answer is affirmative by proposing a novel object-aware anchor-free tracker. Instead of predicting the small offsets of anchor boxes, our object-aware anchorfree tracker directly regresses the positions of target objects in a video frame. More specifically, the proposed tracker consists of two components: an objectaware classification network and a bounding-box regression network. The classification is in charge of determining whether a region belongs to foreground or background, while the regression aims to predict the distances from each pixel within the target objects to the four sides of the groundtruth bounding boxes. Since each pixel in the groundtruth box is well trained, the regression network is able to localize the target object even when only a small region is identified as the foreground. Eventually, during inference, the tracker is capable of rectifying the weak predictions whose overlap with the target objects are small.</p><p>When the regression network predicts a more accurate bounding box (e.g., rectifying weak predictions), the corresponding features can in turn help the classification of foreground and background. We use the predicted bounding box as a reference to learn an object-aware feature for classification. More concretely, we introduce a feature alignment module, which contains a 2D spatial transformation to align the feature sampling locations with predicted bounding boxes (i.e., regions of candidate objects). This module guarantees the sampling is specified within the predicted regions, accommodating to the changes of object scale and position. Consequently, the learned features are more discriminative and reliable for classification.</p><p>The effectiveness of the proposed framework is verified on five benchmarks: VOT-2018 <ref type="bibr" target="#b16">[17]</ref>, VOT-2019 <ref type="bibr" target="#b17">[18]</ref>, OTB-100 <ref type="bibr" target="#b43">[44]</ref>, GOT-10k <ref type="bibr" target="#b13">[14]</ref> and LaSOT <ref type="bibr" target="#b7">[8]</ref>. Our approach achieves state-of-the-art performance (an EAO of 0.467) on VOT-2018 <ref type="bibr" target="#b16">[17]</ref>, while running at 58 fps, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. It obtains up to 92.2% and 12.8% relative improvements over the anchor-based methods, i.e., SiamRPN <ref type="bibr" target="#b21">[22]</ref> and SiamRPN++ <ref type="bibr" target="#b20">[21]</ref>, respectively. On other datasets, the performance of our tracker is also competitive, compared with recent state-of-the-arts. In addition, we further equip our anchor-free tracker with a plug-in online update module, and enable it to capture the appearance changes of objects during inference. The online module further enhances the tracking performance, which shows the scalability of the proposed anchor-free tracking approach.</p><p>The main contributions of this work are two-fold. 1) We propose an objectaware anchor-free network based on the observation that the anchor-based method is difficult to refine the anchors whose overlap with the target object is small. The proposed algorithm can not only rectify the imprecise bounding-box predictons, but also learn an object-aware feature to enhance the matching accuracy.</p><p>2) We design a novel tracking framework by combining the proposed anchor-free network with an efficient feature combination module. The proposed tracking model achieves state-of-the-art performance on five benchmarks while running in real-time speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the related work on anchor-free mechanism and feature alignment in both tracking and detection, as well as briefly review recent Siamese trackers.</p><p>Siamese trackers. The pioneering works, i.e., SINT <ref type="bibr" target="#b34">[35]</ref> and SiamFC <ref type="bibr" target="#b0">[1]</ref>, employ Siamese networks to offline train a similarity metric between the object target and candidate image patches. SiamRPN <ref type="bibr" target="#b21">[22]</ref> improves it with a region proposal network, which amounts to a target-specific anchor-based detector. With the predefined anchor boxes, SiamRPN <ref type="bibr" target="#b21">[22]</ref> can capture the scale changes of objects effectively. The follow-up studies mainly fall into two camps: designing more powerful backbone networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50]</ref> or proposing more effective proposal networks <ref type="bibr" target="#b8">[9]</ref>. Although these offline Siamese trackers have achieved very promising results, their tracking robustness is still inferior to the recent state-of-the-art online trackers, such as ATOM <ref type="bibr" target="#b3">[4]</ref> and DiMP <ref type="bibr" target="#b1">[2]</ref>.</p><p>Anchor-free mechanism. Anchor-free approaches recently became popular in object detection tasks, because of their simplicity in architectures and superiority in performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref>. Different from anchor-based methods which estimate the offsets of anchor boxes, anchor-free mechanisms predict the location of objects in a direct way. The early anchor-free work <ref type="bibr" target="#b46">[47]</ref> predicts the intersection over union with objects, while recent works focus on estimating the keypoints of objects, e.g., the object center <ref type="bibr" target="#b6">[7]</ref> and corners <ref type="bibr" target="#b18">[19]</ref>. Another branch of anchor-free detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref> predicts the object bounding box at each pixel, without using any references, e.g., anchors or keypoints. The anchor-free mechanism in our method is inspired by, but different from that in the recent detection algorithm <ref type="bibr" target="#b35">[36]</ref>. We will discuss the key differences in Sec. 3.4.</p><p>Feature alignment. The alignment between visual features and reference ROIs (Regions of Interests) is vital for localization tasks, such as detection and tracking <ref type="bibr" target="#b39">[40]</ref>. For example, ROIAlign <ref type="bibr" target="#b11">[12]</ref> are commonly recruited in object detection to align the features with the reference anchor boxes, leading to remarkable improvements on localization precision. In visual tracking, there are also several approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref> considering the correspondence between visual features and candidate bounding boxes. However, these approaches only take account of the bounding boxes with high classification scores. If the high scores indicate the background regions, then the corresponding features will mislead the detection of target objects. To address this, we propose a novel feature alignment method, in which the alignment is independent of the classification results. We sample the visual features from the predicted bounding boxes directly, without considering the classification score, generating object-aware features. This object-aware features, in turn, help the classification of foreground and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Object-aware Anchor-Free Networks</head><p>This section proposes the Object-aware anchor-free networks (Ocean) for visual tracking. The network architecture consists of two components: an objectaware classification network for foreground-background probability prediction and a regression network for target scale estimation. The input features to these two networks are generated by a shared backbone network (elaborated in Sec. 4.1). We introduce the regression network first, followed by the classification branch, because the regression branch provides object scale information to enhance the classification of the target object and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Anchor-free Regression Network</head><p>Revisiting recent anchor-based trackers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, we observed that the trackers drift speedily when the predicted bounding box becomes unreliable. The underlying reason is that, during training, these approaches only consider the anchor boxes whose IoU with groundtruth are larger than a high threshold, i.e., IoU ≥ 0.6. Hence, these approaches lack the competence to amend the weak predictions, e.g., the boxes whose overlap with the target are small.</p><p>To remedy this issue, we introduce a novel anchor-free regression for visual tracking. It considers all the pixels in the groundtruth bounding box as the training samples. The core idea is to estimate the distances from each pixel within the target object to the four sides of the groundtruth bounding box. Specifically, let B = (x 0 , y 0 , x 1 , y 1 ) ∈ R 4 denote the top-left and bottom-right corners of the groundtruth bounding box of a target object. A pixel is considered as the regression sample if its coordinates (x, y) fall into the groundtruth box B. Hence, the labels T * = (l * , t * , r * , b * ) of training samples are calculated as</p><formula xml:id="formula_0">l * = x − x 0 , t * = y − y 0 , r * = x 1 − x, b * = y 1 − y,<label>(1)</label></formula><p>which represent the distances from the location (x, y) to the four sides of the bounding box B, as shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. The learning of the regression network is through four 3 × 3 convolution layers with channel number of 256, followed by one 3 × 3 layer with channel number of 4 for predicting the distances. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the upper "Conv" block indicates the regression network. This anchor-free regression allows for all the pixels in the groundtruth box during training, thus it can predict the scale of target objects even when only a small region is identified as foreground. Consequently, the tracker is capable of rectifying weak predictions during inference to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object-aware Classification Network</head><p>In prior Siamese tracking approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, the classification confidence is estimated by the feature sampled from a fixed regular region in the feature map, e.g., the purple points in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. This sampled feature depicts a fixed local region of the image, and it is not scalable to the change of object scale. As a result, the classification confidence is not reliable in distinguishing the target object from complex background.</p><p>To address this issue, we propose a feature alignment module to learn an object-aware feature for classification. The alignment module transforms the fixed sampling positions of a convolution kernel to align with the predicted bounding box. Specifically, for each location (d x , d y ) in the classification map, it has a corresponding object bounding box M = (m x , m y , m w , m h ) predicted by the regression network, where m x and m y denote the box center while m w and m h represent its width and height. Our goal is to estimate the classification confidence for each location (d x , d y ) by sampling features from the corresponding candidate region M . The standard 2D convolution with kernel size of k × k samples features using a fixed regular grid G = {(− k/2 , − k/2 ), ..., ( k/2 , k/2 )}, where · denotes the floor function. The regular grid G cannot guarantee the sampled features cover the whole content of region M .</p><p>Therefore, we propose to equip the regular sampling grid G with a spatial transformation T to convert the sampling positions from the fixed region to the predicted region M . As shown in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>, the transformation T (the dashed yellow arrows) is obtained by measuring the relative direction and distance from the sampling positions in G (the purple points) to the positions aligned with the predicted bounding box (the cyan points). With the new sampling positions, the object-aware feature is extracted by the feature alignment module, which is formulated as</p><formula xml:id="formula_1">f [u] = g∈G,∆t∈T w[g] · x[u + g + ∆t],<label>(2)</label></formula><p>where x represents the input feature map, w denotes the learned convolution weight, u indicates a location on the feature map, and f represents the output object-aware feature map. The spatial transformation ∆t ∈ T represents the distance vector from the original regular sampling points to the new points aligned with the predicted bounding box. The transformation is defined as</p><formula xml:id="formula_2">T = {(m x , m y ) + B} − {(d x , d y ) + G},<label>(3)</label></formula><p>where {(m x , m y ) + B} represents the sampling positions aligned with M , e.g., the cyan points in <ref type="figure">Fig</ref>  <ref type="figure" target="#fig_1">Fig. 2(c)</ref>) relative to the box center (e.g., (m x , m y )). It is worth noting that when the transformation ∆t ∈ T is set to 0 in Eq. (2), the feature sampling mechanism is degenerated to the fixed sampling on regular points, generating the regular-region feature. The transformations of the sampling positions are adaptive to the variations of the predicted bounding boxes in video frames. Thus, the extracted object-aware feature is robust to the changes of object scale, which is beneficial for feature matching during tracking. Moreover, the object-aware feature provides a global description of the candidate targets, which enables the distinguish of the object and background to be more reliable.</p><p>We exploit both the object-aware feature and the regular-region feature to predict whether a region belongs to target object or image background. For the classification based upon the object-aware feature, we apply a standard convolution with kernel size of 3 × 3 over f to predict the confidence p o (visualized as the "OA.Conv" block of the classification network in <ref type="figure" target="#fig_3">Fig. 3</ref>). For the classification based on the regular-region feature, four 3 × 3 standard convolution layers with channel number of 256, followed by one standard 3 × 3 layer with channel number of one are performed over the regular-region feature f to predict the confidence p r (visualized as the "Conv" block of the classification network in <ref type="figure" target="#fig_3">Fig. 3</ref>). Calculating the summation of the confidence p o and p r obtains the final classification score. The object-aware feature provides a global description of the target, thus enhancing the matching accuracy of candiate regions. Meanwhile, the regular-region feature concentrates on local parts of images, which is robust to localize the center of target objects. The combination of the two features improves the reliability of the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>To optimize the proposed anchor-free networks, we employ IoU loss <ref type="bibr" target="#b46">[47]</ref> and binary cross-entropy (BCE) loss <ref type="bibr" target="#b5">[6]</ref> to train the regression and classification networks jointly. In regression, the loss is defined as</p><formula xml:id="formula_3">L reg = − i ln(IoU (p reg , T * )),<label>(4)</label></formula><p>where p reg denotes the prediction, and i indexes the training samples. In classification, the loss L o based upon the object-aware feature f is formulated as</p><formula xml:id="formula_4">L o = − j p * o log(p o ) + (1 − p * o )log(1 − p o ),<label>(5)</label></formula><p>while the loss L r based upon the regular-region feature f is formulated as</p><formula xml:id="formula_5">L r = − j p * r log(p r ) + (1 − p * r )log(1 − p r ),<label>(6)</label></formula><p>where p o and p r are the classification score maps computed over the object-aware feature and regular-region feature respectively, j indexes the training samples for classification, and p * o and p * r denote the groundtruth labels. More concretely, p * o is a probabilistic label, in which each value indicates the IoU between the predicted bounding box and groundtruth, i.e., the region with red slash lines in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. p * r is a binary label, where the pixels closing to the center of the target are labeled as 1, i.e., the red region in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, which is formulated as</p><formula xml:id="formula_6">p * r [v] = 1, if ||v − c|| ≤ R, 0, otherwise.<label>(7)</label></formula><p>The joint training of the entire object-aware anchor-free networks is to optimize the following objective function:</p><formula xml:id="formula_7">L = L reg + λ 1 L o + λ 2 L r ,<label>(8)</label></formula><p>where λ 1 and λ 2 are the tradeoff hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relation to Prior Anchor-Free Work</head><p>Our anchor-free mechanism shares similar spirit with recent detection methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref> (discussed in Sec. 2). In this section, we further discuss the differences to the most related work, i.e., FCOS <ref type="bibr" target="#b35">[36]</ref>. Both FCOS and our method predict the object locations directly on the image plane at pixel level. However, our work differs from FCOS <ref type="bibr" target="#b35">[36]</ref> in two fundamental ways. 1) In FCOS <ref type="bibr" target="#b35">[36]</ref>, the training samples for the classification and regression networks are identical. Both are sampled from the positions within the groundtruth boxes. Differently, in our method, the data sampling strategies for classification and regression are asymmetric which is tailored for tracking tasks. More specifically, the classification network only considers the pixels closing to the target as positive samples (i.e., R ≤ 16 pixels), while the regression network considers all the pixels in the ground-truth box as training samples. This fine-grained sampling strategy guarantees the classification network can learn a robust similarity metric for region matching, which is important for tracking. 2) In FCOS <ref type="bibr" target="#b35">[36]</ref>, the objectness score is calculated with the feature extracted from a fixed regular-region, similar to the purple points in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. By contrast, our method additionally introduce an object-aware feature, which captures the global appearance of target objects. The object-aware feature aligns the sampling regions with the predicted bounding box (e.g., cyan points in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>), thus it is adaptive to the scale change of objects. The combination of the regular-region feature and the object-aware feature allows the classification to be more reliable, as verified in Sec. 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Object-aware Anchor-Free Tracking</head><p>This section depicts the tracking algorithm building upon the proposed objectaware anchor-free networks (Ocean). It contains two parts: an offline anchor-free model and an online update model, as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework</head><p>The offline tracking is built on the object-aware anchor-free networks, consisting of three steps: feature extraction, combination and target localization.</p><p>Feature extraction. Following the architecture of Siamese tracker <ref type="bibr" target="#b0">[1]</ref>, our approach takes an image pair as input, i.e., an exemplar image and a candidate search image. The exemplar image represents the object of interest, i.e., an image patch centered on the target object in the first frame, while the search image is typically larger and represents the search area in subsequent video frames. Both inputs are processed by a modified ResNet-50 <ref type="bibr" target="#b12">[13]</ref> backbone and then yield two feature maps. More specifically, we cut off the last stage of the standard ResNet-50 <ref type="bibr" target="#b12">[13]</ref>, and only retain the first fourth stages as the backbone. The first three stages share the same structure as the original ResNet-50. In the fourth stage, the convolution stride of down-sampling unit <ref type="bibr" target="#b12">[13]</ref> is modified from 2 to 1 to increase the spatial size of feature maps, meanwhile, all the 3 × 3 convolutions are augmented with a dilation with stride of 2 to increase the receptive fields. These modifications increase the resolution of output features, thus improving the feature capability on object localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Feature combination. This step exploits a depth-wise cross-correlation operation <ref type="bibr" target="#b20">[21]</ref> to combine the extracted features of the exemplar and search images, and generates the corresponding similarity features for the subsequent target localization. Different from the previous works performing the cross-correlation on multi-scale features <ref type="bibr" target="#b20">[21]</ref>, our method only performs over a single scale, i.e., the last stage of the backbone. We pass the single-scale features through three parallel dilated convolution layers <ref type="bibr" target="#b47">[48]</ref>, and then fuse the correlation features through point-wise summation, as presented in <ref type="figure" target="#fig_3">Fig. 3 (feature combination)</ref>.</p><p>For concreteness, the feature combination process can be formulated as</p><formula xml:id="formula_8">S = ab Φ ab (f e ) * Φ ab (f s )<label>(9)</label></formula><p>where f e and f s represent the features of the exemplar and search images respectively, Φ ab indicates a single dilated convolution layer, and * denotes the cross-correlation operation <ref type="bibr" target="#b0">[1]</ref>. The kernel size of the dilated convolution Φ ab is set to 3 × 3, while the dilation strides are set to a along the X-axis and b along the Y -axis. Φ ab also reduces the feature channels from 1024 to 256 to save computation cost. In experiments, we found that increasing the diversity of dilations </p><p>Similar to prior works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>, we impose a penalty on scale change to suppress the large variation of object size and aspect ratio as follows</p><formula xml:id="formula_10">α = exp(k · max( r r , r r ) · max( s s , s s )),<label>(11)</label></formula><p>where k is a hyper-parameter, r and r represent the aspect ratio of the predicted bounding boxes in the previous and current frames respectively, while s and s denote the size (i.e., height and width) of the predicted boxes in the previous and current frames. The final target classification probabilityp cls is calculated asp cls = α · p cls . The maximum value in the classification mapp cls indicates the position of the foreground target. To keep the shape of predicted bounding boxes changing smoothly, a linear weight function is used to calculate the final scale asŝ reg = β · s + (1 − β) · s, where β is a weight parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Integrating Online Update</head><p>We further equip the offline algorithm with an online update model. Inspired by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, we introduce an online branch to capture the appearance changes of target object during tracking. As shown in <ref type="figure" target="#fig_3">Fig. 3 (bottom part)</ref>, the online branch inherits the structure and parameters from the first three stages of the backbone network, i.e., modified ResNet-50 <ref type="bibr" target="#b12">[13]</ref>. The fourth stage keep the same structure as the backbone, but its initial parameters are obtained through the pretraining strategy proposed in <ref type="bibr" target="#b1">[2]</ref>. For model update, we employ the fast conjugate gadient algorithm <ref type="bibr" target="#b1">[2]</ref> to train online branch during inference. The foreground score maps estimated by the online branch and the classification branch are weighted as</p><formula xml:id="formula_11">p = ω p onl + (1 − ω )p cls ,<label>(12)</label></formula><p>where ω represents the weights between the classification scorep cls and the online estimation score p onl . Note that the IoUNet in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> is not used in our model. We refer readers to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section presents the results of our Ocean tracker on five tracking benchmark datasets, with comparisons to the state-of-the-art algorithms. Experimental analysis is provided to evaluate the effects of each component in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Training. The backbone network is initialized with the parameters pretrained on ImageNet <ref type="bibr" target="#b31">[32]</ref>. The proposed trackers are trained on the datasets of Youtube-BB <ref type="bibr" target="#b28">[29]</ref>, ImageNet VID <ref type="bibr" target="#b31">[32]</ref>, ImageNet DET <ref type="bibr" target="#b31">[32]</ref>, GOT-10k <ref type="bibr" target="#b13">[14]</ref> and COCO <ref type="bibr" target="#b25">[26]</ref>. The size of input exemplar image is 127×127 pixels, while the search image is 255 × 255 pixels. We use synchronized SGD [20] on 8 GPUs, with each GPU hosting 32 images, hence the mini-batch size is 256 images per iteration. There are 50 epochs in total. Each epoch uses 6 × 10 5 training pairs. For the first 5 epochs, we start with a warmup learning rate of 10 −3 to train the objectaware anchor-free networks, while freezing the parameters of the backbone. For the remaining epochs, the backbone network is unfrozen, and the whole network is trained end-to-end with a learning rate exponentially decayed from 5 × 10 −3 to 10 −5 . The weight decay and momentum are set to 10 −3 and 0.9, respectively. The threshold R of the classification label in Eq. <ref type="formula" target="#formula_6">(7)</ref> is set to 16 pixels. The weight parameters λ 1 and λ 2 in Eq. (8) are set to 1 and 1.2, respectively.</p><p>We noticed that the training settings (data selection, iterations, etc.) are often different in recent trackers, e.g., SiamRPN <ref type="bibr" target="#b21">[22]</ref>, SiamRPN++ <ref type="bibr" target="#b20">[21]</ref>, ATOM <ref type="bibr" target="#b3">[4]</ref> and DiMP <ref type="bibr" target="#b3">[4]</ref>. It is difficult to compare different models under a unified training schedule. But for a fair comparison, we additionally evaluate our method and SiamRPN++ <ref type="bibr" target="#b20">[21]</ref> under the same training setting, as discussed in Sec. 5.3.</p><p>Testing. For the offline model, tracking follows the same protocols as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>. The feature of the target object is computed once at the first frame, and then is continuously matched to subsequent search images. The fusion weight ω of the object-aware classification score in Eq. (10) is set to 0.07, while the weight ω in Eq. (12) is set to 0.5. The hyperparameter k in Eq. (11) for the penalty of large scale change is set to 0.021, while the scale weight β is set to 0.7. These hyper-parameters in testing are selected with the tracking toolkit <ref type="bibr" target="#b49">[50]</ref>, which contains an automated parameter tuning algorithm. Our trackers are implemented using Python 3.6 and PyTorch 1.   on a server with 8 Tesla V100 GPUs and a Xeon E5-2690 2.60GHz CPU. Note that we run the proposed tracker three times, the standard deviation of the performance is ±0.5%, demonstrating the stability of our model. We report the average performance of the three-time runs in the following comparisons. Evaluation datasets and metrics. We use five benchmark datasets including VOT-2018 <ref type="bibr" target="#b16">[17]</ref>, VOT-2019 <ref type="bibr" target="#b17">[18]</ref>, OTB-100 <ref type="bibr" target="#b43">[44]</ref>, GOT-10k <ref type="bibr" target="#b13">[14]</ref> and LaSOT <ref type="bibr" target="#b7">[8]</ref> for tracking performance evaluation. In particular, VOT-2018 <ref type="bibr" target="#b16">[17]</ref> contains 60 sequences. VOT-2019 <ref type="bibr" target="#b17">[18]</ref> is developed by replacing the 20% least challenging videos in VOT-2018 <ref type="bibr" target="#b16">[17]</ref>. We adopt the Expected Average Overlap (EAO) <ref type="bibr" target="#b17">[18]</ref> which takes both accuracy (A) and robustness (R) into account to evaluate overall performance. The standardized OTB-100 <ref type="bibr" target="#b43">[44]</ref> benchmark consists of 100 videos. Two metrics, i.e., precision (Prec.) and area under curve (AUC) are used to rank the trackers. GOT-10k <ref type="bibr" target="#b13">[14]</ref> is a large-scale dataset containing over 10 thousand videos. The trackers are evaluated using an online server on a test set of 180 videos. It employs the widely used average overlap (AO) and success rate (SR) as performance indicators. Compared to these benchmark datasets, LaSOT <ref type="bibr" target="#b7">[8]</ref> has longer sequences, with an average of 2,500 frames per sequence. Success (SUC) and precision (Prec.) are used to evaluate tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">State-of-the-art Comparison</head><p>To extensively evaluate the proposed method, we compare it with 22 stateof-the-art trackers, which cover most of current representative methods. There are 9 anchor-based Siamese framework based methods (SiamFC <ref type="bibr" target="#b0">[1]</ref>, GradNet <ref type="bibr" target="#b23">[24]</ref>, DSiam <ref type="bibr" target="#b10">[11]</ref>, MemDTC <ref type="bibr" target="#b45">[46]</ref>, SiamRPN <ref type="bibr" target="#b21">[22]</ref>, C-RPN <ref type="bibr" target="#b8">[9]</ref>, SiamMASK <ref type="bibr" target="#b42">[43]</ref>, SiamRPN++ <ref type="bibr" target="#b20">[21]</ref> and SiamRCNN <ref type="bibr" target="#b38">[39]</ref>), 8 discriminative correlation filter based methods (CFNet <ref type="bibr" target="#b37">[38]</ref>, ECO <ref type="bibr" target="#b4">[5]</ref>, STRCF <ref type="bibr" target="#b22">[23]</ref>, LADCF <ref type="bibr" target="#b44">[45]</ref>, UDT <ref type="bibr" target="#b41">[42]</ref>, STN <ref type="bibr" target="#b36">[37]</ref>, ATOM <ref type="bibr" target="#b3">[4]</ref> and DiMP <ref type="bibr" target="#b1">[2]</ref>), 3 multi-domain learning based methods (MDNet <ref type="bibr" target="#b26">[27]</ref>, RT-MDNet <ref type="bibr" target="#b14">[15]</ref> and VITAL <ref type="bibr" target="#b33">[34]</ref>), 1 graph network based method (GCT <ref type="bibr" target="#b9">[10]</ref>) and 1 meta-learning based tracker (MetaCREST <ref type="bibr" target="#b27">[28]</ref>). The results are summarized in Tab. 1 -3 and <ref type="figure" target="#fig_4">Fig. 4</ref>. VOT-2018. The evaluation on VOT-2018 is performed by the official toolkit <ref type="bibr" target="#b16">[17]</ref>. As shown in Tab. 1, Our offline Ocean tracker outperforms the champion method of VOT-2018, i.e., (LADCF <ref type="bibr" target="#b44">[45]</ref>), by 7.8 points. Compared to the state-of-the-art offline tracker SiamRPN++ <ref type="bibr" target="#b20">[21]</ref>, our offline model achieves EAO improvements of 5.3 points, while running faster, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. It is worth noting that the improvements mainly come from the robustness score, which obtains 27.8% relative increases over SiamRPN++. Moreover, our offline model is superior to the recent online trackers ATOM <ref type="bibr" target="#b3">[4]</ref> and DiMP <ref type="bibr" target="#b1">[2]</ref>. The online augmented model further improves our tracker by 2.2 points in terms of EAO.</p><p>VOT-2019. Tab. 2 reports the evaluation results with the comparisons to recent prevailing trackers on VOT-2019. We can see that the recent proposed DiMP <ref type="bibr" target="#b1">[2]</ref> achieves the best performance, while our method ranks second. However, in realtime testing scenarios, our offline Ocean tracker achieves the best performance, surpassing DiMP r by 0.6 points in terms of EAO. Moreover, the EAO of our offline model surpasses SiamRPN++ <ref type="bibr" target="#b20">[21]</ref> by 3.5 points.</p><p>GOT-10k. The evaluation on GOT-10k follows the protocols in <ref type="bibr" target="#b13">[14]</ref>. The proposed offline Ocean tracker model achieves the state-of-the-art AO score of 0.592, outperforming SiamRPN++ <ref type="bibr" target="#b20">[21]</ref>, as shown in Tab. 3. Our online model improves the AO by 4.5 points over ATOM <ref type="bibr" target="#b3">[4]</ref>, while outperforming DiMP <ref type="bibr" target="#b1">[2]</ref> by 0.9 points in terms of success rate.</p><p>OTB-100. The last evaluation in short-term tracking is performed on the classical OTB-100 benchmark. As reported in <ref type="figure" target="#fig_4">Fig. 4</ref>, among the compared methods, our online tracker achieves the best precision score of 0.920, while DiMP <ref type="bibr" target="#b20">[21]</ref> achieves best AUC score of 0.686.</p><p>LaSOT. To further evaluate the proposed models, we report the results on LaSOT, which is larger and more challenging than previous benchmarks. The results on the 280 videos test set is presented in <ref type="figure" target="#fig_4">Fig. 4</ref>. Our offline Ocean tracker achieves SUC score of 0.527, outperforming SiamRPN++ with score of 0.496. Compared to ATOM <ref type="bibr" target="#b3">[4]</ref>, our online tracker improves the SUC score by 4.6 points,  <ref type="table">Table 5</ref>. Analysis of the impact of different strides over dilated convolution in the feature combination module.</p><formula xml:id="formula_12">Φ 11 0.425 2 Φ 11 Φ 11 0.433 3 Φ 11 Φ 12 0.446 4 Φ 11 Φ 21 0.443 5 Φ 11 Φ 12 Φ 21 0.467</formula><p>giving comparable results to top-ranked tracker DiMP-50 <ref type="bibr" target="#b1">[2]</ref>. Moreover, the proposed online tracker achieves the best precision score of 0.566.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of the Proposed Method</head><p>Component-wise analysis. To verify the efficacy of the proposed method, we perform a component-wise analysis on the VOT-2018 benchmark, as presented in Tab. 4. The baseline model consists of a backbone network (detailed in Sec. 4.1), an anchor-free regression network (detailed in Sec. 3.1) and a classification network using regular-region feature (detailed in Sec. 3.2). In the training of baseline model, all pixels in the groundtruth box are considered as positive samples. The baseline model obtains an EAO of 0.358. In 2 , the "centralized sampling" indicates that we only consider the pixels closing to the target's center as positive samples in the training of classification (formulated as Eq. <ref type="formula" target="#formula_6">(7)</ref>). It brings significant gains, i.e., 3.8 points on EAO ( 2 vs. 1 ). This verifies that the sampling helps to learn a robust similarity metric for region matching. Adding the feature combination module (detailed in Sec. 4.1) can bring a large improvement of 4.2 points in terms of EAO ( 3 vs. 2 ). This demonstrates the effectiveness of the proposed irregular dilated convolution module. It introduces a multi-scale modeling of target objects, without increasing much computation overhead. Furthermore, the object-aware classification (detailed in Sec. 3.2) can also bring an improvement of 2.9 points in terms of EAO ( 4 vs. 3 ). This shows that the object-aware features generated by the proposed feature alignment module contribute significantly to the tracker. Finally, the tracker equipped with the plug-in online update module (detailed in Sec. 4.2) yields another improvement of 2.2 points ( 5 vs. 4 ), showing the scalability of the proposed framework. Feature combination. We further evaluate the impact of dilated convolutions in the feature combination module and report the results on VOT-2018 in Tab. 5. The baseline setting is a normal convolution with dilation stride of 1 along both the X-axis and Y -axis, i.e., Φ 11 . We observe that adding a standard convolution Φ 11 brings an improvement of 0.8 points in terms of EAO ( 2 vs. 1 ). This indicates that the proposed parallel convolutions in the feature combination module is effective. It is very interesting to see that if we modify the dilation strides along X and Y directions to be different, the performance can be further improved, e.g., 1.3 points gains for 3 vs. 2 while 1.0 points gains for 4 vs.</p><p>2 . This verifies that the irregular dilations is effective to enhance feature representability. A combination of the three dilation kernels with different strides obtains the best results in our experiment. It is worth noting that the feature   3 ). One possible reason is that the object categories in LaSOT <ref type="bibr" target="#b7">[8]</ref> have been covered by other datasets, thus it cannot further elevate model capacities.</p><p>Feature visualization. We visualize the features extracted before and after the alignment, i.e., the regular-region feature and object-aware feature, in <ref type="figure" target="#fig_5">Fig.  5</ref>. We observe that the object-aware feature focuses on the entire object, while the regular-region feature concentrates on the center part of the target. The former improves the reliability of the classification since it provides a global view of the target. The latter contributes more to localize the object centerness since the features are more sensitive to local changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a novel object-aware anchor-free tracking framework (Ocean) based upon the observation that the anchor-based method is difficult to refine the anchors whose overlap with the target objects are small. Our model directly regresses the positions of target objects in a video frame instead of predicting offsets for predefined anchors. Moreover, the learned object-aware feature by the alignment module provides a global description of the target, contributing to the reliable matching of objects. The experiments demonstrate that the proposed tracker achieves state-of-the-art performance on five benchmark datasets. In the future work, we will study the update of the parameters in the objectaware classification network without integrating an additional online branch. Besides, applying our framework to other online video tasks, e.g., video object detection and segmentation, is also a potential direction.</p><formula xml:id="formula_13">-------Supplementary Material --------</formula><p>The supplementary material presents the additional experiments of Section 5: 1) We provide additional ablation experiments. 2) We provide qualitative comparisons of our tracker with state-of-the-arts.</p><p>Impact of the convolution layers in the anchor-free networks. For both of the regression network and regular-region classification network (i.e., "Conv" of the anchor-free networks in the <ref type="figure" target="#fig_3">Fig. 3</ref>, we use four convolution layers to predict the fine-grained regression and classification score maps. To study the impact of the number of convolution layers in these two networks, we perform an ablation experiment on OTB-100 <ref type="bibr" target="#b43">[44]</ref>, as shown in Tab. 7. We can see that as the number of convolution layers increases, the performance (i.e., AUC) becomes saturated. The balanced choice between performance and speed is 3 or 4 in our model.  <ref type="table">Table 7</ref>. Impact of the number of layers in the anchor-free networks.</p><p>Analysis of rectification capacity. The rectification capacity indicates the prediction accuracy (mIoU ) of the model when the tracker drifts from the target. To evaluate the rectification capacity, we first sample exemplar and search image pairs from adjacent frames in the VOT-2018 dataset <ref type="bibr" target="#b16">[17]</ref>. We sample locations on the score maps away from the target to simulate weak predictions, and the shifting magnitude is illustrated in the first row of Tab. 8. Then we compute the overlap between the predicted bounding box and the groundtruth, i.e., mIoU in Tab. 8. Larger mIoU means that the regression network can better rectify the inaccurate prediction. We can see that the performance (mIoU ) of the proposed model outperforms SiamRPN++ <ref type="bibr" target="#b20">[21]</ref> when the tracker drifts away from the target's center. This demonstrates the superior robustness of our tracker compared to the anchor-based method.  of rectifying inaccurate bounding boxes and robust to noisy predictions. This verifies the advancement of our anchor-free regression mechanism compared to anchor-based methods. When the target undergoes large deformation or rotation, e.g., dinasour and motocross1, the predicted locations of ATOM and DiMP are not accurate enough. One of the reasons is that the regular-region sampling strategy in these approaches may lack global information to generate discriminative appearance features. Benefiting from the object-aware feature, our model can predict better results in this case. The visualization results demonstrate the robustness and effectiveness of the proposed tracker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A comparison of the performance and speed of state-of-the-art tracking methods on VOT-2018. We visualize the Expected Average Overlap (EAO) with respect to the Frames-Per-Seconds (FPS). Offline-1 and Offline-2 indicate the proposed offline trackers with and without feature alignment module, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Regression: the pixels in groundtruth box, i.e. the red region, are labeled as the positive samples in training. (b) Regular-region classification: the pixels closing to the target's center, i.e. the red region, are labeled as the positive samples. The purple points indicate the sampled positions of a location in the score map. (c) Object-aware classification: the IoU of predicted box and groundtruth box, i.e., the region with red slash lines, is used as the label during training. The cyan points represent the sampling positions for extracting object-aware features. The yellow arrows indicate the offsets induced by spatial transformation. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. 2(c), {(d x , d y ) + G} indicates the regular sampling positions used in standard convolution, e.g., the purple points in Fig. 2(c), and B = {(−m w /2, −m h /2), ..., (m w /2, m h /2)} denotes the coordinates of the new sampling positions (e.g., the cyan points in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of the proposed tracking framework, consisting of an offline anchorfree part (top) and an online model update part (bottom). The offline tracking includes feature extraction, feature combination and target localization with object-aware anchor-free networks, as elaborated in Sec. 4.1. The plug-in online update network models the appearance changes of target objects, as detailed in Sec. 4.2. Φ ab indicates a 3×3 convolution layer with dilation stride of a along the X-axis and b along the Y -axis. can improve the representability of features, thereby we empirically choose three different dilations, whose strides are set to (a, b) ∈ {(1, 1), (1, 2), (2, 1)}. The convolutions with different dilations can capture the features of regions with different scales, improving the scale invariance of the final combined features. Target localization. This step employs the proposed object-aware anchorfree networks to localize the target from search images. The probabilities p o and p r predicted by the classification network are averaged with a weight ω as p cls = ωp o + (1 − ω)p r .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Success and precision plots on OTB-100 [44] (top) and LaSOT [8] (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of (a) the regularregion feature and (b) the object-aware feature over the video "ants1" in VOT-2018<ref type="bibr" target="#b16">[17]</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Visual comparisons of our tracker with statr-of-the-art trackers on 6 video sequences: basketball, dinasour, fernando, girl, motocross1 and soccer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>1.0. The experiments are conducted CRPN ECO STRCF LADCF ATOM SRCNN SiamRPN++ DiMP Ocean Ocean Performance comparisons on VOT-2018 benchmark. Red, green and blue fonts indicate the top-3 trackers. "Ocean" denotes our propose model.</figDesc><table><row><cell></cell><cell>[9]</cell><cell>[5]</cell><cell>[23]</cell><cell>[45]</cell><cell>[4]</cell><cell>[39]</cell><cell></cell><cell>[21]</cell><cell>[2]</cell><cell>offline online</cell></row><row><cell cols="4">EAO ↑ 0.273 0.280 0.345</cell><cell>0.389</cell><cell>0.401</cell><cell>0.408</cell><cell></cell><cell>0.414</cell><cell>0.440 0.467 0.489</cell></row><row><cell>A ↑</cell><cell cols="3">0.550 0.484 0.523</cell><cell>0.503</cell><cell cols="2">0.590 0.609</cell><cell></cell><cell>0.600</cell><cell>0.597 0.598 0.592</cell></row><row><cell>R ↓</cell><cell cols="3">0.320 0.276 0.215</cell><cell>0.159</cell><cell>0.204</cell><cell>0.220</cell><cell></cell><cell>0.234</cell><cell>0.153 0.169 0.117</cell></row><row><cell></cell><cell cols="9">MemDTC SiamMASK SiamRPN++ ATOM STN DiMP r DiMP b Ocean Ocean</cell></row><row><cell></cell><cell>[46]</cell><cell></cell><cell>[43]</cell><cell>[21]</cell><cell></cell><cell>[4]</cell><cell>[37]</cell><cell>[2]</cell><cell>[2]</cell><cell>offline online</cell></row><row><cell>EAO ↑</cell><cell>0.228</cell><cell></cell><cell>0.287</cell><cell cols="2">0.292</cell><cell cols="4">0.301 0.314 0.321 0.379 0.327 0.350</cell></row><row><cell>A ↑</cell><cell>0.485</cell><cell></cell><cell>0.594</cell><cell cols="2">0.580</cell><cell cols="4">0.603 0.589 0.582 0.594 0.590 0.594</cell></row><row><cell>R ↓</cell><cell>0.587</cell><cell></cell><cell>0.461</cell><cell cols="2">0.446</cell><cell cols="4">0.411 0.349 0.371 0.278 0.376 0.316</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparisons on VOT-2019. The "DiMP r " and "DiMP b " indicate realtime and baseline performances of DiMP, as reported in<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table><row><cell></cell><cell cols="9">CFNet MDNet SiamFC ECO DSiam SiamRPN++ ATOM DiMP Ocean Ocean</cell></row><row><cell></cell><cell>[38]</cell><cell>[27]</cell><cell>[1]</cell><cell>[5]</cell><cell>[11]</cell><cell>[21]</cell><cell>[4]</cell><cell>[2]</cell><cell>offline online</cell></row><row><cell>AO ↑</cell><cell>0.261</cell><cell>0.299</cell><cell cols="3">0.392 0.395 0.417</cell><cell>0.518</cell><cell cols="3">0.556 0.611 0.592 0.611</cell></row><row><cell cols="2">SR0.5 ↑ 0.243</cell><cell>0.303</cell><cell cols="3">0.406 0.407 0.461</cell><cell>0.618</cell><cell cols="3">0.634 0.712 0.695 0.721</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance comparisons on GOT-10k test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>combination module with irregular dilations is extremely lightweight, and it has large potentials to be plugged into other models in both tracking and detection. Training setting. We conduct another ablation study to evaluate the impact of training settings. For a fair comparison, we follow the same setting as the well-performing SiamRPN++<ref type="bibr" target="#b20">[21]</ref>, i.e. training on YTB<ref type="bibr" target="#b28">[29]</ref>, VID<ref type="bibr" target="#b31">[32]</ref>, DET<ref type="bibr" target="#b31">[32]</ref> and COCO<ref type="bibr" target="#b25">[26]</ref> datasets for 20 epochs and using 6 × 10 5 image pairs in each epoch. As the results presented in Tab. 6 ( 2 v.s. 1 ), our model surpasses SiamRPN++ by 4.1 points in terms of EAO under the same training settings. Moreover, we further add GOT-10k<ref type="bibr" target="#b13">[14]</ref> images into training, and observe that it brings an improvement of 1.2 points ( 3 v.s. 2 ). This demonstrates that the main performance gains are induced by the proposed model. If we continue to add LaSOT<ref type="bibr" target="#b7">[8]</ref> into training, the performance cannot improve further ( 4 v.s.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Comparisons of rectification capacity between SiamRPN++ and our model. mIoU indicates mean IoU. Distance indicates shifting magnitude ( 1 distance) for generating search images.Qualitative comparisons.Fig. 6qualitatively compares the results of recent top-performing trackers: DiMP<ref type="bibr" target="#b1">[2]</ref>, ATOM<ref type="bibr" target="#b3">[4]</ref>, SiamRPN++<ref type="bibr" target="#b20">[21]</ref> and our Ocean tracker on 6 challenging sequences. SiamRPN++ drifts from the target when fast motion occurs (soccer ). The underlying reason is that fast motion results in imprecise prediction, which is difficult to be rectified by the anchor-based method. By contrast, our model performs better in this case, since it is capable</figDesc><table><row><cell>Distance (pixels)</cell><cell>8</cell><cell>16</cell><cell>24</cell><cell>32</cell><cell>40</cell><cell>48</cell><cell>56</cell></row><row><cell cols="2">mIoU of SiamRPN++ [21] 0.65</cell><cell>0.48</cell><cell>0.36</cell><cell>0.30</cell><cell>0.28</cell><cell>0.25</cell><cell>0.21</cell></row><row><cell>mIoU of our tracker</cell><cell>0.73</cell><cell>0.73</cell><cell>0.72</cell><cell>0.71</cell><cell>0.71</cell><cell>0.72</cell><cell>0.54</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4660" to="4669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6931" to="6939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tutorial on the crossentropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>De Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LaSOT: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5374" to="5383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7952" to="7961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4649" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1763" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">117</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<title level="m">Real-Time MDNet</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCVW</publisher>
			<biblScope unit="page" from="1" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The seventh visual object tracking vot2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning spatial-temporal regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4904" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GradNet: Gradientguided network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6162" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of appearance models in visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="569" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large high-precision humanannotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">VITAL: Visual tracking via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8990" to="8999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Tracking the known and the unknown by leveraging semantic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>BMVC</publisher>
			<biblScope unit="page" from="1192" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2805" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12836</idno>
		<title level="m">Visual tracking by re-detection</title>
		<meeting><address><addrLine>Siam R-CNN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cascade RPN: Delving into high-quality region proposal network with adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1430" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SPM-tracker: Series-parallel matching for real-time visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3643" to="3652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning adaptive discriminative correlation filters via temporal consistency preserving spatial feature selection for robust visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5596" to="5609" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning dynamic memory networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="152" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fast compressive tracking</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2002" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4591" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
