<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AXM-Net: Cross-Modal Alignment and Contextual Attention for Person Re-ID</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammarah</forename><surname>Farooq</surname></persName>
							<email>ammarah.farooq@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><forename type="middle">Kittler</forename><surname>Syed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safwan</forename><surname>Khalid</surname></persName>
							<email>s.khalid@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVSSP</orgName>
								<orgName type="institution" key="instit2">University of Surrey</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AXM-Net: Cross-Modal Alignment and Contextual Attention for Person Re-ID</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-modal person re-identification (Re-ID) is critical for modern video surveillance systems. The key challenge is to align inter-modality representations conforming to semantic information present for a person and ignore background information. In this work, we present a novel convolutional neural network (CNN) based architecture designed to learn semantically Aligned cross-Modal (AXM-Net) visual and textual representations. The underlying building block, AXM-Block, is a unified multi-layer network that dynamically exploits the multi-scale knowledge from both modalities and re-calibrates each modality according to shared semantics. To complement the convolutional design, a contextual attention is applied in text branch to manipulate long-term dependencies. Moreover, we propose contextual attention on local image parts to capture fine-grained details of the person. Our design is unique in its ability to implicitly learn aligned semantics between modalities during feature learning stage. The unified feature learning effectively utilises textual data as a super-annotation signal for visual representation learning and automatically rejects irrelevant information. The entire AXM-Net is trained endto-end on CUHK-PEDES data. We report results on two tasks, person search and cross-modal Re-ID. The AXM-Net outperforms the current state-of-the-art (SOTA) method by 5.22% in Rank@1 on the CUHK-PEDES and by &gt;10% for cross-viewpoint text-to-image Re-ID scenario on CrossRe-ID and CUHK-SYSU datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (Re-ID) has become a principal component of intelligent video surveillance systems that aims to retrieve a queried person from a large database of pedestrian images. The database typically contains nonoverlapping camera viewpoints with respect to the query images. Depending on the type of information provided as a query, the task is referred to as person Re-ID or per- son search in the literature. Person search <ref type="bibr" target="#b20">[21]</ref> aims to find a person based on the natural language description of the person while images are provided as a query in person Re-ID.In person search, there is no constraint on the camera viewpoints of the person, i.e., in the visual gallery person may have the same pose for which the description is given. Nevertheless, in both tasks, it is critical to learn discriminative feature representations which are unique to an individual and well-aligned within the class for finer matching.</p><p>The literature is packed with numerous deep learning based person re-identification approaches. These methods aim to learn robust person representations <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10]</ref>, apply various attention mechanisms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref>, look for crossdomain knowledge transfer <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9]</ref> and so on. Crossmodal person Re-ID is another important aspect of Re-ID task <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref>. The dependency on available image queries limits the practical application of a vision-based system. For example, in the case of criminal search, often the CCTV footage (or image) of a criminal is not available. Therefore, police rely on the unique cues of the criminal from the witnesses' descriptions often given in terms of natural language description. In such cases, with no images available, this descriptive information serves as a query for person Re-ID. Hence, employing a multi-modal Re-ID system can overcome the limitations of image-based systems.</p><p>This work focuses on cross-modal person Re-ID using visual and textual information of the person. The aim of the work is to design a system that semantically aligns crossmodal and cross-pose representations implicitly by focusing on the information present in the two modalities. By implicit alignment we mean alignment of two modalities without using external cues, like segmentation, human body landmarks, attributes prediction from image etc. Note, that existing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b15">16]</ref> rely on complex external cues and explicit alignment of the feature embeddings. There are several challenges while dealing with two distinct modalities. First, the structure of information in both modalities is quite different for persons. Presumably, images have persons always standing upright while the person description can have any sequence. Second, it is critical to learn a network that can extract the semantics in data instead of memorising corresponding image-text pairs for the identities seen during training. Third, the attributes information present in the features, for example, colour and type of clothes person is wearing, the activity of the person, accessories, etc, should be aligned across the modalities to learn the associations among image parts and textual phrases and disregard background noise ( <ref type="figure" target="#fig_0">Figure 1</ref>). Henceforth, the terms 'semantic concepts' and 'attributes' of the person are used interchangeably.</p><p>The main idea of this work is to align the visual and textual features of a person to enable cross-modal or multimodal search seamlessly. To achieve this goal, we present AXM-Net, a novel convolutional neural network (CNN) based architecture designed to deal with the challenges mentioned above and capable of learning semantically aligned cross-modal feature representations. The underlying building block consists of multiple streams of feature maps capturing a variable amount of intra-modal information and an alignment network that is learnt based on the semantics present in both modalities. The output representations are, hence, attended by the fused cross-modal details. The alignment network leverages multi-context intramodality information and cross-modal attributes to boost informative concepts and suppress the noisy or background information.</p><p>Apart from exploiting inter-modal semantic knowledge, we also propose modality-specific contextual attention mechanisms to effectively extract complementary intramodality representations. To be specific, we introduce an image part-based contextual attention block to locally attend different spatial parts for finer details. We also note that contextual information from other parts can provide useful information while attending a given spatial part, therefore, we propose to use part-level context sharing. Since the unified feature learning is performed using a CNN based design, the sequential nature of the language modality demands learning long-term associations among the person attributes. For example, a description may contain information about the head (hairs, style) at the beginning follows by a description for the lower body, and again carries information about the head (wearing hat). Hence, we also propose to employ contextual attention to learn these useful associations among the textual attributes. Our contributions can be summarised as follows: • Unified cross-modal semantic alignment block (AXM-Block) is proposed to mutually learn representations based on person attributes. To our knowledge, this is the first work to employ implicit semantic alignment across modalities at feature learning stage in the Re-ID setting. • We put forward effective intra-modality contextual attention mechanisms to extract local spatial region based details in vision while exploiting part-level context sharing and to manage inter-dependencies among textual phrases. • Extensive experiments demonstrate the superiority of the proposed AXM-Net over the current SOTA by 5.22% on the CUHK-PEDES <ref type="bibr" target="#b20">[21]</ref> benchmark and by a wide margin on CrossRe-ID and CUHK-SYSU <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> in all retrieval scenarios. We also propose a cross-modal protocol for the famous Market-1501 data <ref type="bibr" target="#b40">[41]</ref>. Note that performance is evaluated based on the stricter but more realistic Re-ID constraints of non-overlapping camera viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Person Re-identification: Recent vision based Re-ID approaches are mainly based on deep learning techniques <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29]</ref>. These approaches can be divided into the following categories. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2]</ref> devise attention based mechanisms on the basis of higher order information present in the features, person's body mask or pixel/region level features. <ref type="bibr" target="#b5">[6]</ref> advocates a diverse attention mechanism along with orthogonality constraint to preserve diversity between the layer activations and weights. Pose based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> use pose detectors to learn aligned features across poses. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> use GANs for learning poseinvariant features. Another group of works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b17">18]</ref> focus on local part based feature learning. The work <ref type="bibr" target="#b42">[43]</ref> proposes an omni-scale network to learn fused multi-scale features. Our approach also uses multi-scale features, however, the design aims to attend the most useful features across modalities based on the context present in these features. Person Search: The task of person search by natural language descriptions was introduced by <ref type="bibr" target="#b20">[21]</ref>. The proposed method was a CNN-RNN network to learn global level cross-modal features. The following works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7]</ref> also focused on similar network architectures and a little im- provement was observed in performance. Major improvements were shown by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> where researchers start exploiting global-local associations and improving the feature embedding space. More recent works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1]</ref> started employing auxiliary learning branches to explicitly make use of pose key-points, person attributes, segmentation masks, body parts and textual phrases. These approaches brought improvements as compared to using only global features. Our method also takes advantage of local level features from both modalities in a unique and effective way. Moreover, it is the first design to learn visual and textual features using an integrated convolutional block.</p><p>Cross-modal Person Re-identification: As mentioned earlier, cross-modal searches help to overcome limitations of vision only systems. A number of approaches have been proposed based on using infra-red images <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> along with RGB images. For text based person Re-ID, <ref type="bibr" target="#b37">[38]</ref> published the pioneering work and reported results on the CUHK03 and Viper datasets under multiple retrieval scenarios. Recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> proposed to jointly optimise the two modalities and applied canonical correlation analysis to enhance similarity between the corresponding features. These works also reported results for larger crossmodal test splits including CrossRe-ID and CUHK-SYSU. <ref type="figure" target="#fig_1">Figure 2</ref> shows the block diagram of the proposed AXM-Net, which includes a unified feature learning network, Visual Contextual Attention (VCA) branch and a Textual Contextual Attention (TCA) branch. The details of each part are presented in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross-modal AXM-Net Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Unified Feature Learning using AXM-Block</head><p>Intrinsically, the semantic information present in both modalities is the same as both are describing the same person. However, the corresponding semantic concepts and their relationships may be available at different scales and locations within each modality. We propose a novel idea to align these representations across modalities based on the semantic information during feature extraction phase. For this purpose, we introduce a cross-modal semantic alignment block called as AXM-block ( <ref type="figure" target="#fig_2">Figure 3</ref>) which is the basic building block of unified feature learning across modalities. The unified feature learning module consists of stacked AXM-blocks. The design is based on interaction of multi-scale features within and across modalities. Intramodal multi-scale learning captures the locally critical context with respect to a spatial location. While inter-modal multi-scale alignment dynamically conform each modality with respect to the other. Each stream of channels con- tains a coarser-to-finer level of semantics present in the input image or text. Likewise, cross-modal multi-scale learning captures and semantically align an unaligned coarserto-finer level of semantics present in multi-modal input. It is important to understand the cross-modal unified feature learning for semantic alignment to differentiate from typical multi-scale feature learning methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>.To show this difference, we show baseline results in the experiments section for a model that leverages intra-modal multi-scale information but fails to outperform AXM-Net. The proposed cross-modal cross-scale alignment is the key element of the AXM-Net giving it an advantage over baseline and SOTA methods. The alignment network analyses the semantic concepts in each stream of channels and reinforces the mutually agreed concepts across the different modalities. The details of the layer-wise architecture of the feature learning backbone in the AXM-Net are provided in supplementary materials. Architecture of the AXM-Block: The design of the proposed AXM-block is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. To model the shared semantics across modalities, we first define X r = (V r , T r ) to be the visual and textual features maps at receptive fields r = 3, 5, 7, 9. We apply a global average pooling operation to collect the channelwise global concepts present in each feature map generating x r = (v r , t r ) vectors of length spanning the entire channel dimension. These vectors are then passed to the semantic alignment network together with the feature maps to get recalibrated by the mutual semantic information.</p><formula xml:id="formula_0">x r = SA(x r )<label>(1)</label></formula><p>The alignment function SA(x) indicates the non-linear mapping to generate a set of scale vectorsx r = (ṽ r ,t r ) corresponding to each input feature map. These scaling vectors carry the importance of each channel with respect to the shared context between modalities. Finally, feature alignment is performed by taking element-wise product be-tween scaling vectors and the corresponding input feature map.</p><p>(Ṽ ,T ) = r=3,5,7,9</p><formula xml:id="formula_1">(V r ṽ r , T r t r )<label>(2)</label></formula><p>To our knowledge, the proposed system is the first approach to align cross-modal features during feature learning stage in Re-ID. The implicit learning approach of AXM-Net utilises the textual input as a super-annotation signal and does not require auxiliary learning tasks as in the current SOTA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>. This property is beneficial as the textual input does not have background clutter information. Therefore, the effect of background can be reduced by the unified feature learning as shown in Subsection 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual Contextual Attention (VCA)</head><p>The person Re-ID task depends on the discriminative local cues characterising each individual. The global visual branch focuses on the person as a whole while preserving shared semantics across modalities. However, it is important to pay attention to local cues within the modality. Recent method <ref type="bibr" target="#b33">[34]</ref> used sum of global channel-wise and spatial attention to attend intra-modal information. However, this way is sub-optimal for attending finer person details related to each body part. To overcome this limitation of typical channel and spatial attention methods, we propose part based visual contextual attention. The key idea is to enhance information in each local image part while taking into account the context from all other parts. To do this, we divide the feature maps into multiple horizontal strips. Each strip is locally attended channel-wise by a learnable multilayer network. The network looks into each local channel strip and preserves the most informative channels for a given spatial region. The attended region strips are concatenated back into the input-sized features. We use single fused part level feature V p for ID classification instead of applying multi-label classification setting, which is computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Textual Contextual Attention (TCA)</head><p>The design of unified feature learning is based on simultaneous alignment of modalities from the beginning of the network. This leads to the choice of convolution based feature extraction in both modalities. The output of the convolution operation is characterised by the local receptive field. However, the structure of the free-form natural language descriptions demands to incorporate long-term relationships among the attributes present in the textual representations. To model these long-term dependencies we take inspiration from <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15]</ref> and propose a non-local textual contextual attention by directly computing interactions between pairs of textual features. This step is important to complement our convolutional design for unified feature learning. We take feature maps from the last convolution block of the feature learning network as input to TCA. Intuitively, these features represent the important semantic attributes present in the person's description. Each spatial index represents a response from a local spatial region (phrases). The TCA block takes the feature from each spatial position, computes its affinity with all other features(context) and attends input feature maps accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>The loss function for training is the sum of cross-entropy (CE) loss of the three feature branches, triplet-loss <ref type="bibr" target="#b2">[3]</ref> and a simple affinity loss defined below. Specifically, the weights of the classifier layer are shared among all the branches to enforce intra-identity feature alignment and the applied CE loss is denoted as L IDjoint . The other losses are optimised in a pairwise manner for each visual feature with textual feature.</p><formula xml:id="formula_2">L T otal = λ 1 L IDjoint + λ 2 [L trip (V g , T ) + L trip (V p , T )] + λ 3 [L af f (V g , T ) + L af f (V p , T )] (3)</formula><p>where, λ i , i=1,2,3 determines the proportion of each loss in the total. Given tuples (V a , T + , T − ), (T a , V + , V − ), L trip is a margin (α) based ranking loss, defined over similarity(S) of cross-modal positive and negative feature pairs as follows:</p><formula xml:id="formula_3">L trip = max[0, α − (S(V a , T + ) − (S(V a , T − )] + max[0, α − (S(V + , T a ) − (S(V − , T a )] (4)</formula><p>Cross-modal Affinity Loss: In order to enhance retrieval performance, we propose to use a simple yet effective affinity loss. We experimented with more complex losses but they bring little advantage. It is based on the affinity between image-text feature pairs. The image features and the corresponding textual features should be aligned and have high similarity, ideally +1 (in case of cosine similarity) and non corresponding features should be unaligned (uncorrelated) and have as low similarity as possible, ideally 0. Given the representations for an image-text pair, affinity is measured in terms of cosine similarity score between image feature V and text feature T . The affinity loss is implemented as a binary cross entropy criterion, defined over {V i , T j , y ij } where y ij = 1 for matching pairs and y ij = 0 for non-matching ones.</p><formula xml:id="formula_4">L af f = −[ y ij . log(σ (S(V i , T j ))) + (1 − y ij ) . log(σ (1 − S(V i , T j ))) ] (5)</formula><p>where σ is sigmoid function applied to features similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We follow a two stage training strategy to learn the AXM-Net. In the first stage, we focus on learning the textual branch, the VCA branch and all fully connected layers from scratch, while keeping the vision backbone fixed to pretrained ImageNet weights. For the first stage the training follows the standard classification paradigm considering each person as an individual class and only using L IDjoint . We also apply label smoothing to our cross entropy loss. We use batch size 64, weight decay 5e-4 and initial learning rate 0.01 with stochastic gradient descent optimisation. Images are resized to 224 × 224. Each textual description is mapped to its 300 dimensional word2vec <ref type="bibr" target="#b25">[26]</ref> embedding and resized as 1 × 56 × 300 where 56 is the maximum sentence length. We adopted random flipping, random erasing for images and random circular shift of sentences as data augmentation. To achieve computational efficiency, we employ depth-wise separable convolutions at each layer. The retrieval performance is measured based on the cosine similarity between the features and reported in terms of Rank@1 and mean average precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUHK-PEDES:</head><p>The CUHK person description data <ref type="bibr" target="#b20">[21]</ref> is the only large-scale benchmark available for cross-modal person search. It has 13003 person IDs with <ref type="bibr" target="#b39">40</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Results on Person Search</head><p>We summarise the performance of the proposed AXM-Net with the state-of-the-art methods on person search in <ref type="table">Table  1</ref>. The methods have been grouped according to the type of representations used for learning. We implement the baseline network with multi-scale features for both modalities and a joint classifier layer. The baseline method performs best among all global level techniques which signifies the benefit of having multi-scale features but still it falls behind the other complex methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b33">34]</ref> due to lack of cross-modal alignment. It is worth noting that our method is simple but powerful and enhances the semantic alignment between modalities without any explicit supervision from segmented body parts <ref type="bibr" target="#b32">[33]</ref> or attributes <ref type="bibr" target="#b0">[1]</ref>. The proposed AXM-Net with simple L IDjoint loss outperforms current SOTA by a large margin, achieving over 59% Rank@1 performance. By using affinity and triplet loss together, we set the new SOTA Rank@1 of 61.9%. Note that parameter free affinity loss enhances the retrieval and is competitive to the triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results on Cross-modal Re-ID</head><p>We follow the evaluation protocol of <ref type="bibr" target="#b11">[12]</ref> for cross-modal re-identification. In <ref type="table" target="#tab_3">Tables 2, 3</ref>, V − → V indicates image based search, T − → V indicates description to image search and VT − → V indicates using both modalities for query and vision as gallery.We report the detailed results including Rank@5 and Rank@10 for all the datasets in the supplementary materials. For CrossRe-ID and CUHK-SYSU, we compare the results with the recently reported joint training technique followed by applying canonical correlation analysis to embed cross-modal features <ref type="bibr" target="#b11">[12]</ref>. It is mentioned as JT+CCA in Tables 2. For both datasets, the proposed AXM-Net outperformed the previous method by a significant margin in all retrieval scenarios. Note that, now the T − → V indicates a description of a person from a different pose, and the gallery images have different poses. We can witness the potential of semantic alignment across-modalities in this challenging case. The improvement in Rank@1 performance shows the capability of the AXM-Net in matching viewpoint-invariant semantic details.</p><p>Retrieval results on the proposed test split of Market-1501 are reported in <ref type="table" target="#tab_3">Table 3</ref>. We trained the OSNet <ref type="bibr" target="#b42">[43]</ref> on the proposed training set and report the results for different scenarios. Focusing on V − → V , we see large performance gap between single and multi-modality learning. Note that the OSNet results are different from the ones report in <ref type="bibr" target="#b42">[43]</ref> due to change in split and stricter protocol. Here, the OS-Net is trained with much larger data as compared to training set of only Market data which was the case in <ref type="bibr" target="#b42">[43]</ref>. Nevertheless, due to stricter Re-ID protocol the results of OSNet are worse here. As can be seen from the rest of the rows in V − → V under proposed AXM-Net base cross-modal learning the results improved significantly. This shows that the matching capability of the single modality is limited by number of samples per class seen during training. It gives an important insight that having textual descriptions also decrease the dependency on number of samples of training set. More importantly, these results established that the textual description of images can act as a super-annotation  <ref type="table">Table 2</ref>. Performance comparison on cross-modal Re-ID. Query − → Gallery   <ref type="table">Table 5</ref>. Design parameters for the visual contextual attention part branch signal. Experimental results for the AXM-Net proves that the proposed system is able to focus on important cues of the person instead of holistically memorising the imagedescription pairs.</p><formula xml:id="formula_5">Model CrossRe-ID CUHK-SYSU V − → V T − → V VT − → V V − → V T − → V VT − → V Rank@1</formula><formula xml:id="formula_6">Model V − → V T − → V VT − → V Rank@1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Component Analysis</head><p>In order to assess the contribution of each component in the complete AXM-Net, we perform ablation study on the proposed framework by adding the components stepby-step. The study is performed on the CUHK-PEDES test set with joint cross-entropy loss L IDjoint and all hyperparameters are kept the same for training in all settings. <ref type="table">Table 4</ref> presents the corresponding results. The baseline model has the same architecture as the global visual and textual branch, including multi-scale features for both modalities but without alignment. We abbreviate unified semantic alignment using AXM-blocks as SA in the Working with cross-modal networks is also challenging in terms of the learning policy being adopted. As mentioned earlier, we used two stage training for network learning.</p><p>We also test a single stage policy in which we tune all parameters together with the same initialisation setting. Single stage model clearly shows the difference between the two policies. We notice that aligning the textual branch with vision in the first stage and then training them jointly is the best strategy. • Design Parameters for VCA Part Branch. We consider several parameters for designing the part-based visual feature branch as presented in <ref type="table">Table 5</ref>. First of all, we test with separate attention networks for each strip of feature maps. We find that having a shared attention network helps in learning as well as reducing the number of parameters. It supports our idea of context sharing between image parts and signifies the connectedness of various semantic concepts across the strips, for example, a long coat covers both the upper and lower parts of the body. Next, we note that the global max pool helps in capturing local cues by focusing on the highest responses in each region. For each branch of AXM-Net, we also optimise the number of FC layers as it is critical to avoid any performance degradation and network overfitting. We observe from the table that having identical linear layer structure not always implies the best performing solution. Being a richer modality and focusing on information from the whole image, two FC layers help in the global branch to learn better representations. We also consider feature dropping technique <ref type="bibr" target="#b9">[10]</ref> to obtain robust features. However, we observed a decrease in Rank@1 using both batchwise random location and horizontal strip (part) drops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualisation</head><p>We analyse the attention maps for the proposed AXM-Net to deduce its capability to learn person specific semantic information from the input data. We show the visual attention maps for the baseline network and our proposed AXM-Net in <ref type="figure" target="#fig_3">Figure 4</ref>. The visualisations are arranged in three columns (a,b,c), highlighting different aspects of a retrieval system. In all three columns, we observe that AXM-Net ignores person's background with high confidence. Specifically, in the column (b), top example, we note that the attention is focused on the lady whose description is provided, while the baseline model generates a spanned attention. In the column (c), we observe that with the help of the textual description, the visual attention is refined to minute details in the image like 'backpack with a white stripe' and 'flip flops'. Overall, the semantic information present in the textual description is highly emphasised across vision, which verifies the feature alignment induced by the inter-modal semantic alignment. Note that the baseline network also has access to multi-scale information, but the proposed method has intelligently aligned this information. We provide query and output retrieval images in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a novel AXM-Net model to address the challenges of cross-modal person re-identification and search. Our innovation involves a unified feature learning block, called AXM-Block, which implicitly aligns the features coming from the visual and textual modalities and effective intra-modal contextual attentions. In contrast to existing methods, the proposed AXM-Net is the first framework that is based on an integrated cross-modal feature alignment and learning stage in Re-ID context. The experimental results show that our network defines new SOTA performance on the CUHK-PEDES benchmark and also demonstrate the potential of the proposed network for more challenging cross-modal person Re-ID applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of semantic alignment for visual and textual features. The semantic information present in the features should be aligned to learn the cross-modality associations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustrative diagram of our cross-modal AXM-Net, which generates global visual feature VG, part based visual feature VP and textual feature T . Softmax loss LID joint is shared for all the features. Matching losses are trained pairwise with the textual feature for each visual feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>AXM-Block design. Unified cross-modal feature learning block. R = 3 indicates features at receptive field size 3×3 and 1×3 for text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualisation of attention maps generated by baseline network and our proposed AXM-Net (warmer colours show higher attention). High-lighted text phrases correspond to the semantic concepts of the person which are precisely attended by the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>,206 images and 80,440 descriptions. There are 11003,1000,1000 pre-defined IDs for training, validation and test sets. The training and test set include 34054/68126 and 3074/6156 images/descriptions respectively. CrossRe-ID Dataset: For cross-modal Re-ID, we evaluate the models on the protocol introduced by [12] on the test split of CUHK-PEDES data. The gallery and query splits have been carefully separated across viewpoints. The descriptions are also varying across viewpoints. The dataset includes 824 unique IDs. There are 1511/3022 and 1096/2200 images/descriptions in gallery and query sets respectively. CUHK-SYSU: We evaluate our model on the test protocol provided by [11]. There are 5532 IDs for training and 2099 IDs for testing. The corresponding descriptions have been extracted from CUHK-PEDES data. The final gallery and query splits contain 5070/10140 and 3271/6550 images/descriptions respectively. Market-1501: We also propose a test split for the Market Re-ID dataset. It is a part of the CUHK-PEDES data. We separate the train and test IDs of Market-1501 according to the original protocol. Then we merge the train IDs with the rest of the CUHK-PEDES data to form the training set and evaluate on the test set. The test set is again split into nonoverlapping gallery and query sets to define Re-ID specific protocol. Hence, the training set includes 11253 IDs with 34191/68396 images/descriptions. The test set has 750 IDs. The gallery and query sets have 1816/3632 and 1173/2346 images/descriptions respectively. The results are evaluated under more strict single-shot and single-query scenarios for this split. Please note that this split respect Re-ID protocol</figDesc><table><row><cell>Model</cell><cell>Feature Type</cell><cell cols="4">Rank@1 Rank@5 Rank@10 mAP</cell></row><row><cell>GNA-RNN [21]</cell><cell></cell><cell>19.05</cell><cell>-</cell><cell>53.64</cell><cell>-</cell></row><row><cell>IATV [20]</cell><cell></cell><cell>25.94</cell><cell>-</cell><cell>60.48</cell><cell>-</cell></row><row><cell>PWM [7]</cell><cell></cell><cell>27.14</cell><cell>49.45</cell><cell>61.02</cell><cell>-</cell></row><row><cell>DPCE [42]</cell><cell>global</cell><cell>44.40</cell><cell>66.26</cell><cell>75.07</cell><cell>-</cell></row><row><cell>GLA [5]</cell><cell></cell><cell>43.58</cell><cell>66.93</cell><cell>76.26</cell><cell>-</cell></row><row><cell>CMPC + CMPM [40]</cell><cell></cell><cell>49.37</cell><cell>-</cell><cell>79.27</cell><cell>-</cell></row><row><cell>Baseline: Multi-scale features + joint ID</cell><cell></cell><cell>52.78</cell><cell>72.33</cell><cell>80.29</cell><cell>49.04</cell></row><row><cell>PMA [16]</cell><cell>global + keypoints</cell><cell>53.81</cell><cell>73.54</cell><cell>81.23</cell><cell>-</cell></row><row><cell>IMG-Net [34]</cell><cell>global + parts</cell><cell>56.48</cell><cell>76.89</cell><cell>85.01</cell><cell>-</cell></row><row><cell>ViTAA [33] CMAAM [1]</cell><cell>global + attribute</cell><cell>55.97 56.68</cell><cell>75.84 77.18</cell><cell>83.52 84.86</cell><cell>--</cell></row><row><cell>AXM-Net + joint ID</cell><cell></cell><cell>59.11</cell><cell>77.46</cell><cell>83.80</cell><cell>54.24</cell></row><row><cell>AXM-Net + joint ID + affinity</cell><cell></cell><cell>59.81</cell><cell>77.43</cell><cell>84.27</cell><cell>54.89</cell></row><row><cell>AXM-Net + joint ID + triplet</cell><cell>global+ part</cell><cell>60.12</cell><cell>77.84</cell><cell>84.95</cell><cell>55.35</cell></row><row><cell>AXM-Net + joint ID + triplet + affinity</cell><cell></cell><cell>61.9</cell><cell>79.41</cell><cell>85.75</cell><cell>57.38</cell></row></table><note>Table 1. Comparison with SOTA models on the CUHK-PEDES dataset and hence, it is different from test split commonly used for Market-1501.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison on the proposed cross-modal split for Market-1501 . Query − → Gallery</figDesc><table><row><cell>Model</cell><cell cols="3">SA VCA TCA</cell><cell>Rank@1</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.78</cell></row><row><cell>Model: 1</cell><cell></cell><cell>-</cell><cell>-</cell><cell>55.90</cell></row><row><cell>Model: 2</cell><cell>-</cell><cell></cell><cell>-</cell><cell>56.99</cell></row><row><cell>Model: 3</cell><cell>-</cell><cell>-</cell><cell></cell><cell>53.25</cell></row><row><cell>Model: 4</cell><cell></cell><cell>-</cell><cell></cell><cell>56.27</cell></row><row><cell>Model: 5</cell><cell></cell><cell></cell><cell>-</cell><cell>58.59</cell></row><row><cell>Unified Visual</cell><cell></cell><cell></cell><cell></cell><cell>54.53</cell></row><row><cell>Single Stage</cell><cell></cell><cell></cell><cell></cell><cell>55.05</cell></row><row><cell cols="2">Proposed AXM-Net</cell><cell></cell><cell></cell><cell>59.11</cell></row><row><cell cols="5">Table 4. Ablation study on the AXM-Net on CUHK-PEDES test set</cell></row><row><cell cols="2">Attention Weights Rank@1</cell><cell cols="2">Pooling Type</cell><cell>Rank@1</cell></row><row><cell>Separate</cell><cell>57.62</cell><cell cols="2">Average (GAP)</cell><cell>57.36</cell></row><row><cell>Shared</cell><cell>57.93</cell><cell cols="2">Max (GMP)</cell><cell>58.82</cell></row><row><cell>Feature Drop</cell><cell cols="4">Rank@1 No. of FC Layers Rank@1</cell></row><row><cell>Random</cell><cell>57.62</cell><cell>1</cell><cell></cell><cell>58.45</cell></row><row><cell>Part</cell><cell>58.33</cell><cell>2</cell><cell></cell><cell>58.06</cell></row><row><cell>No drop</cell><cell>59.11</cell><cell cols="2">Proposed</cell><cell>59.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table .</head><label>.</label><figDesc>Each component is essential to the design of cross-modal Re-ID. The semantic alignment induced by the unified feature learning enhances the useful information across modalities. The auxiliary VCA branch brings the locally informative cues, while the TCA keeps track of textual dependencies. Models: 4, 5 also emphasise the complementary effect of various components together.• Effect of Unified Visual Feature Branch. We experiment with unified visual branch by removing the global branch and keeping only the VCA based part branch. It is indicated as Unified Visual inTable 4. We observe a performance drop of 4.58% as compared to the proposed design. Separate learning branches for global and part level features complement each other in our design. • Effect of Single Stage versus Two Stages of Learning.</figDesc><table><row><cell>our observations as follows:</cell></row><row><cell>• Effect of Individual Components. Models: 1, 2, 3 cor-</cell></row><row><cell>respond to individual component's contribution. We note</cell></row><row><cell>that each component has boosted the retrieval capability</cell></row><row><cell>compared to the baseline.</cell></row><row><cell>We list</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Text-based person search via attribute-aided matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surbhi</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishnan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2617" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale body-part mask guided attention for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxing</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person re-identification by global and local image-language association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving text-based person search by spatial matching and adaptive threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person reidentification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2590" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instanceguided context rendering for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person reidentification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A convolutional baseline for person re-identification using vision and language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammarah</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Syed Safwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00808</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross modal person re-identification with visual-textual queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ammarah</forename><surname>Farooq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Syed Safwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khalid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8295" to="8302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fd-gan: Pose-guided feature distilling gan for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1222" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pose-guided multi-granularity attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.08440</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crossmodal cross-domain moment alignment network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global-local temporal representations for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3958" to="3967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose transferrable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4099" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification with shared-specific feature transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatial-temporal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peigen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8933" to="8940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rgb-infrared cross-modality person re-identification via joint pixel and feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Guan&amp;apos;an Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengguang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Vitaa: Visual-textual attributes alignment in person search by natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07327</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Img-net: inner-cross-modal attentional multigranular network for description-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aichun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxin</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glad: Global-local-alignment descriptor for scalable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="986" to="999" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan ; Ning) Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person reidentification with vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification via modality-aware collaborative ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="9387" to="9399" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
