<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attribute-Aware Attention Model for Fine-grained Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-22">2018. October 22-26. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<email>hankai@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
							<email>jyguo@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<email>c.zhang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjian</forename><surname>Zhu</surname></persName>
							<email>zhumingjian@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attribute-Aware Attention Model for Fine-grained Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">2018 ACM Multimedia Conference (MM &apos;18)</title>
						<meeting> <address><addrLine>Seoul</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2018-10-22">2018. October 22-26. 2018</date>
						</imprint>
					</monogr>
					<note>ACM Reference Format: Republic of Korea. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Machine learning approaches</term>
					<term>Machine learning</term>
					<term>Neural networks</term>
					<term>KEYWORDS Attribute-Aware Attention, Fine-grained recognition, Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to learn a discriminative fine-grained representation is a key point in many computer vision applications, such as person reidentification, fine-grained classification, fine-grained image retrieval, etc. Most of the previous methods focus on learning metrics or ensemble to derive better global representation, which are usually lack of local information. Based on the considerations above, we propose a novel Attribute-Aware Attention Model (A 3 M), which can learn local attribute representation and global category representation simultaneously in an end-to-end manner. The proposed model contains two attention models: attribute-guided attention module uses attribute information to help select category features in different regions, at the same time, category-guided attention module selects local features of different attributes with the help of category cues. Through this attribute-category reciprocal process, local and global features benefit from each other. Finally, the resulting feature contains more intrinsic information for image recognition instead of the noisy and irrelevant features. Extensive experiments conducted on Market-1501, CompCars, CUB-200-2011 and CARS196 demonstrate the effectiveness of our A 3 M. Code is available at https://github.com/iamhankai/attribute-aware-attention.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: The overall architecture of our Attribute-Aware Attention Model (A 3 M). Attributes like hair, handbag, shoes, etc. provide rich information for fine-grained feature learning. A 3 M learns local attribute feature (green part) and global category feature (orange part) simultaneously in an end-toend manner. Details of the attention models will be presented in section 3.</p><p>surveillance system (see examples in <ref type="figure" target="#fig_0">Figure 2</ref>), are quite challenging since the visual differences among categories or instances are so small that they are easily overwhelmed by other factors, such as, object location, pose, lighting or viewpoint changes. In the wave of deep learning, fine-grained visual visual recognition is more and more close to practical application in recent years. A common approach to mining the visual cues in fine-grained scenarios is localizing the manually defined parts and modeling based on these parts <ref type="bibr" target="#b12">[13]</ref>. Recent studies show that this method has improved significantly over simple convolutional neural networks (CNNs). However, most of these works are trained by fully annotated parts, which is time-consuming and laborious. Moreover, the hand-annotated parts may be suboptimal for image recognition. Another approach is to localize the hidden semantic parts only under the supervision of object labels <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>. Nevertheless, this manner is not reliable enough to mark accurate parts due to weak labels and lack of other helpful information.</p><p>Attributes which provide rich information to learn the correlation among categories are important auxiliary signals in image recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref>. Attributes usually describe the high-level properties, which are discriminative for the objects. Taking images in <ref type="figure" target="#fig_0">Figure 2</ref> as an example, there are 3 persons, namely, A, B, C taken from Market-1501 dataset <ref type="bibr" target="#b51">[52]</ref>. A is similiar to B on the whole, but A carries a bag in hand while B does not, so the attribute handbag is one of the keys to distinguish them. Certainly, other attributes are useful too, such as gender or upper color can help recognizing A from C. Hence, it is crucial to make good use of these local details for fine-grained recognition.</p><p>In this paper, we propose an Attribute-Aware Attention Model (A 3 M) to learn a discriminative representation for fine-grained recognition, object re-identification, etc. As shown in <ref type="figure">Figure 1</ref>, our model includes two branches: attribute feature learning and category feature learning. Our intuition is that small attribute cues are usually crucial to distinguish different categories, such as birds from two species with small difference. Through attention models, category features are used for learning basic representation and the attribute features help to refine category features simultaneously. In fact, the importance of different attributes should vary in different categories, these two attention models can also learn to choose the important attribute features for corresponding input images. The addition of the category features and attribute features helps to gain better representation, and yields better performance.</p><p>Main contributions of this work are as follows:</p><p>• We propose Attribute-Aware Attention Model (A 3 M) to train an attribute-category reciprocal CNN, where attention models can use attribute information to help select key features for person re-identification and fine-grained recognition. Moreover, we use A 3 M in fine-grained image retrieval tasks, in which no attribute labels but category information is provided, to combine local and global feature, leading to a better representation without time-consuming ensemble or complicated losses. • Empirical results show the superiority of the proposed A 3 M for person re-identification, fine-grained classification and fine-grained retrieval on four large benchmarks, Market-1501 <ref type="bibr" target="#b51">[52]</ref>, CompCars <ref type="bibr" target="#b43">[44]</ref>, CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref> and CARS196 <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Person Re-identification. Person re-identification is a challenging task, due to the appearance variations caused by lighting, pose, viewpoint, or occlusion changes. The previous works to tackle this task can be divided into two categories, namely, non-deep learning methods and deep learning methods. Non-deep learning methods extract hand-crafted features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b50">51]</ref>, especially color and texture descriptors, then use distance metric learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> to further capture discriminative factors. Deep learning methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref> learn image representations and distance metrics simultaneously in an end-to-end manner. However, the performance of most of the previous works were limited due to insufficient utilization of attribute and regional features.</p><p>Fine-grained Recognition. Deep learning methods have shown significant improvement on the fine-grained recognition. A bilinear model is used to learn the interacted feature of two independent CNNs, which performs superbly in bird classification <ref type="bibr" target="#b20">[21]</ref>. Key parts of the object are utilized by some methods to extract subtle features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref>. But those methods heavily rely on manually predefined parts, so some methods proposed to automatically discover critical parts in weak supervised way <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref>. A recent proposed attention model <ref type="bibr" target="#b23">[24]</ref> used attributes to help crop discriminative parts, but the hard-cropped parts may not be the optimal and the attribute features were not sufficiently utilized. OPAM <ref type="bibr" target="#b28">[29]</ref> proposes object-part attention model to localize object and discriminative parts. FDL <ref type="bibr" target="#b30">[31]</ref> uses tailored fine-grained dictionaries to help image classification. Cascaded Part-Based System <ref type="bibr" target="#b3">[4]</ref> learns a part-based model for each category and a cascading scheme for fine-grained classification.</p><p>Attributes for Image Recognition. In image recognition, attribute is a type of important auxiliary information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> and have been investigated in many works. Yutian et al. <ref type="bibr" target="#b21">[22]</ref>, Tetsu et al. <ref type="bibr" target="#b25">[26]</ref> train CNNs using attribute loss, while Chi et al. <ref type="bibr" target="#b33">[34]</ref> use attributes triplet loss to fine-tune CNNs. They simply use the last layer features without further exploring the importance of different features and the relationship between attributes and category. The person dataset Market-1501 <ref type="bibr" target="#b51">[52]</ref>, bird dataset CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref> and car dataset CompCars <ref type="bibr" target="#b43">[44]</ref> are annotated with attribute labels, which facilitate the future research on attributes for fine-grained recognition. In this paper, we explore attention mechanism to select better attribute features.</p><p>Metric Learning for Fine-grained Image Retrieval. Metric learning projects images to a high dimensional embedding space, the main goal is to narrow the distances between similar images while pushing away dissimilar images as far as possible in embedding space. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref> take image triplets as inputs and learn a similarity metric by adding triplet comparison constraints. Unfortunately, because of the massive training data (Θ(N 3 )), they need a carefully designed hard example mining method to choose triplets for accelerating converge speed in training phase. <ref type="bibr" target="#b32">[33]</ref> defines constraints on all images in a batch. Some other works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref> ensemble multiple independent models, the sub-embeddings generated by previous models are primarily learned over easy samples and the latter sub-embeddings are mainly tuned on hard samples, leading to the state-of-the-art performance on various benchmarks with the drawback that training multiple models are time-consuming and take on too much memory.</p><p>Attention Mechanism. The attention mechanism has recently been used in computer vision and natural language processing, such as image caption <ref type="bibr" target="#b42">[43]</ref>, machine translation <ref type="bibr" target="#b1">[2]</ref> and visual question answering <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref>. Here we focus on soft attention mechanism which computes a weighted combination of the features to focus on important parts when performing a particular task. Within an image, the importance of different parts is different when comparing whether two images are from the same category. As mentioned above, attention models have been used in fine-grained recognition to crop the key parts. In <ref type="bibr" target="#b22">[23]</ref>, attention mechanism was firstly used in person re-identification by integrating a recurrent attention with the siamese model. Liming et al. <ref type="bibr" target="#b49">[50]</ref> performed human body partition to learn more robust representations, which is a kind of hard attention model. In this paper, we propose a novel Attribute-Aware Attention Model to generate attentions for global category features fusion and local attribute features fusion simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ATTRIBUTE-AWARE ATTENTION MODEL</head><p>In this paper, we propose an end-to-end Attribute-Aware Attention Model for fine-grained representation learning. We firstly introduce two branches in the network extracting attribute feature and category feature separately, then explain the attribute-guided attention module (the green one in <ref type="figure" target="#fig_2">figure 3</ref>) and category-guided attention model (the orange one in <ref type="figure" target="#fig_2">figure 3</ref>), which can help each other to select better attribute feature and category feature simultaneously, and finally present A 3 M architectures for person re-identification and image recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initial Category and Attribute Recognition</head><p>3.1.1 Shared CNN. The shared CNN can be various networks, such as AlexNet <ref type="bibr" target="#b15">[16]</ref>, VGG <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b11">[12]</ref>, etc. And we take off the final fully connected layer. The shared CNN is pre-trained on ImageNet dataset <ref type="bibr" target="#b29">[30]</ref>. Given an input image, it is used to extract high-level visual features of local regions. Taking ResNet-50 for example, when the size of the input image is set as 224 × 224 × 3, and the output feature map after the last pooling layer is in the dimension of 2048 × 7 × 7. This feature map is shared by all the subsequent branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Category Branch.</head><p>In the category recognition branch (the orange feature extraction branch in <ref type="figure">Figure 1</ref>), after a convolution layer with d 2048 × 1 × 1 kernels, the shared feature map is transformed to category-related feature map in the dimension of d ×h×w.</p><p>Each d-dimensional vector in the feature map corresponds to a local region of the input image, for instance, the last feature map's size from ResNet-50 is 2048 × 7 × 7, thus we obtain L = 49 local feature vectors, simply represented as</p><formula xml:id="formula_0">V = [v 1 , v 2 , · · · , v L ] where v l ∈ R d , l = 1, 2, · · · , L.</formula><p>After a global average pooling layer, we get the category embedding v (cat eдory) ∈ R d . The prediction of  category classification is given by the fully connected layer with softmax activation, and cross-entropy loss is used for training.</p><formula xml:id="formula_1">p (cat eдory) = so f tmax(W (c) v (cat eдory) + b (cat eдory) ), (1) L (cat eдory) = − C (c at eдor y) j=1 p (cat eдory) j logp (cat eдory) j ,<label>(2)</label></formula><p>p (cat eдory) is the predicted probability, W (c) ∈ R C (c at eдor y) ×d , b (cat eдory) ∈ R C (c at eдor y) are the weight matrix and bias vector of the fully connected layer, and C (cat eдor y) is the number of categories. Let t be the ground-truth category label, so that p  3.1.3 Attribute Branch. As shown in the green branch in <ref type="figure">Figure  1</ref>, for the k-th attribute of the object (1 ≤ k ≤ K, K is the number of attributes), we feed the shared feature map into convolution layer with d 2048 × 1 × 1 kernels and a pooling layer to obtain attribute embedding vector a (k) ∈ R d for every attribute. Similar to category recognition, we use softmax loss for attribute classification.</p><formula xml:id="formula_2">p (k ) = so f tmax(W (k ) a (k ) + b (k ) ),<label>(3)</label></formula><formula xml:id="formula_3">L (k ) = − C (k ) j=1 p (k ) j logp (k ) j ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">W (k ) ∈ R C (k ) ×d , b (k ) ∈ R C (k )</formula><p>are the weight matrix and bias vector, and C (k ) is the number of the k-th attribute categories.p (k ) is the predicted probability, and p (k ) j is the target probability. Let t be the target attribute label, so that p (k ) t = 1 and p (k ) j = 0 for all j t. During training, the loss function will force the k-th attribute embedding a (k ) paying more attention to the regions related to the k-th attribute. All K attribute embeddings are learned by the way mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attribute-Category Reciprocal Attention</head><p>Based on the category feature map and attribute embeddings, we design an attribute-category reciprocal attention module to use them effectively for fine-grained representation learning. The intuition is to select regions in category feature map which are most relevant to intrinsic attributes and to select attributes most related to category. This is a reciprocal process with the help of each other. The proposed A 3 M consists of two components: attribute-guided attention module and category-guided attention module, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Attribute-guided Attention for Regional Features Selection. We give an overview of the attribute-guided attention module in <ref type="figure" target="#fig_2">Figure  3</ref>(a). For each attribute, the attribute-guided attention module takes the category feature map V and K attribute embeddings (a <ref type="bibr" target="#b0">(1)</ref> , · · · , a (K ) ) as input, and produces an attention map for regional features. The intuition here is to select the local regions involved with intrinsic attributes rather than the background relevant parts. The k-th attribute-guided attention weights are given as follow:</p><formula xml:id="formula_5">m (k) = σ (V T a (k ) ),<label>(5)</label></formula><p>where σ (x) = 1/(1 + e −x ) is sigmoid function. The generated attention weight mask m (k ) ∈ R L reflects the correlations between the L local regions and the k-th attribute. There are K attributes, so we get K attention maps. After merging them via maxpooling, we get m (r eдion) = max(m <ref type="bibr" target="#b0">(1)</ref> , m <ref type="bibr" target="#b1">(2)</ref> , · · · , m (K ) ) as the final attribute-guided attention map . The values in the resulting attention map m (r eдion) ∈ R L are high in selected regions and low in other regions. The regional features are multiplied by the attention weights and summed to produce the category representation</p><formula xml:id="formula_6">f (r eдion) ∈ R d , f (r eдion) = 1 L Vm (r eдion) .<label>(6)</label></formula><p>Category-guided Attention for Attribute Features Selection. The category-guided attention module is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>(b). With the K attribute embeddings A = [a <ref type="bibr" target="#b0">(1)</ref> , · · · , a (K ) ] and the category embedding v (cat eдory) , the category-guided attention weights are computed similarly to attribute-guided attention, s (at t r ) = σ (A T v (cat eдory) ).</p><p>The K attributes features are fused via category-guided attention weighting:</p><formula xml:id="formula_8">f (at t r ) = 1 K As (at t r ) .<label>(8)</label></formula><p>The resulting attribute representation f (at t r ) ∈ R d selects attributes which are most relevant to category recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Final Category Recognition</head><p>The final feature is the addition of the weighted category feature and attribute feature, f (f inal ) = f (r eдion) + f (at t r ) , and it contains information of both category and intrinsic attributes after reciprocal selection. As shown in <ref type="figure" target="#fig_5">figure 4</ref>, we can learn better feature embedding through attention models and local attribute features help to refine the global category feature, enabling the similar images to have smaller distances and dissimilar images have larger distances (right part of <ref type="figure" target="#fig_5">figure 4</ref>). Our goal in this paper is finegrained visual recognition, thus the main category classification with cross-entropy loss is used for final representation learning,</p><formula xml:id="formula_9">p (add ) = so f tmax(W f (f inal ) + b),<label>(9)</label></formula><formula xml:id="formula_10">L (add) = − C (c at eдor y) j=1 p (cat eдory) j logp (add) j ,<label>(10)</label></formula><p>p (add ) is the predicted category probability, W ∈ R C (c at eдor y) ×d , b ∈ R C (c at eдor y) are the weight matrix and bias vector. By combining all the loss (2), (4) and <ref type="formula" target="#formula_10">(10)</ref>, the final loss function for end-to-end training in our method is</p><formula xml:id="formula_11">L (f inal ) = L (addit ive) + α L (cat eдory) + β 1 K K k =1 L (k ) ,<label>(11)</label></formula><p>where α and β are the trade-off parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conducted experiments on a person re-identification dataset, two fine-grained classification datasets and two image retrieval datasets. Methods rank-1 rank-5 rank-10 rank-20 mAP CAN <ref type="bibr" target="#b22">[23]</ref> 60.3 ---35.9 Gated S-CNN <ref type="bibr" target="#b34">[35]</ref> 65.88 ---39.55 CRAFT-MFA+LOMO <ref type="bibr" target="#b5">[6]</ref> 71.8 ---45.5 Re-ranking <ref type="bibr" target="#b53">[54]</ref> 77.11 ---63.63 ResNet-50 (I+V) <ref type="bibr" target="#b52">[53]</ref> 79  CompCars. CompCars <ref type="bibr" target="#b43">[44]</ref> is a large-scale car dataset for finegrained categorization and verification. We use the categorization subset which contains 431 car models with a total of 36,456 training images and 15,627 test images. Each car model is annotated with five attributes, namely, maximum speed, displacement, number of doors, number of seats, and type of car.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Fine-grained Image Retrieval.</head><p>CUB-200-2011. For image retrieval, we don't use the attributes information mentioned above, the first 100 classes are for training <ref type="bibr" target="#b4">(5,</ref><ref type="bibr">864</ref> images) and the rest of classes are for testing <ref type="bibr">(5,924 images)</ref>.</p><p>CARS196. CARS196 dataset <ref type="bibr" target="#b14">[15]</ref> has 196 classes of cars with 16,185 images, where the first 98 classes are for training <ref type="bibr" target="#b7">(8,</ref><ref type="bibr">054</ref> images) and the other 98 classes are for testing <ref type="bibr" target="#b7">(8,</ref><ref type="bibr">131</ref> images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implemented our model using Keras <ref type="bibr" target="#b6">[7]</ref> deep learning framework. The embedding dimension was set as d = 512 for person re-id and image classification experiments, for image retrieval, we set the last FC layers to be 128 and 384 for fair comparison with other methods. We empirically set the value of the trade-off parameters as α = 0.5, β = 0.5. As mentioned above, the shared CNN was pre-trained on ImageNet dataset <ref type="bibr" target="#b29">[30]</ref>, and then we fine-tuned the pre-trained model on the target datasets. During fine-tuning, SGD optimizer with momentum was used to update the weights. We trained the model for 60 epochs with batch size of 32. The initial learning rate was set as 0.001 and changed to 0.0001 in the last 10 epochs. We used a momentum of µ = 0.9 and weight decay of 5e −4 .</p><p>All the experiments were conducted on a NVIDIA Pascal TITAN X GPU. The size of model input in different dataset was a little different. In Market-1501 dataset, input images was resized to 448 × 224, the input size was 448 × 448 in CompCars dataset and 224 × 224 for other datasets. Random crop and random flip were used as data augmentation.</p><p>We adopt the mean average precision (mAP) <ref type="bibr" target="#b2">[3]</ref> as evaluation metrics for person re-identification, and the normalized mutual information or NMI metric <ref type="bibr" target="#b24">[25]</ref> to measure clustering quality for image retrieval tasks. For image classification, accuracy is used for metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluations on Person Re-identification</head><p>Comparison with the state-of-the-art methods. We compare our method with other state-of-the-art methods, and the person reidentification results are listed in <ref type="table" target="#tab_0">Table 1</ref>. With ability to learn representation automatically, deep learning methods achieve impressive results. <ref type="bibr" target="#b21">[22]</ref> has explored the usage of attributes by a multi-task CNN, which is much similar to our Baseline-2. We come up with two attention models which are identity-attribute reciprocal to further improve the usage of attribute features and regional features. JLML <ref type="bibr" target="#b40">[41]</ref> is designed to share identity label constraints by allocating each attribute branch with loss function separately, however, in real world applications, pedestrians may take off their coats at any time due to the weather, or one will help his or her partner to hold the handbag, separately train the local attribute feature to distinguish each person is not enough. The attention model in A 3 M and pair-wise addition in final representation can help to combine attributes and identity in a more flexible manner, making CNN more robust to pick crucial local attribute feature. A 3 M outperforms these state-of-the-art methods, achieves 2.25% and 4.30% improvements on rank-1 and mAP compared with <ref type="bibr" target="#b21">[22]</ref>. On the one hand, the previous works are designed with treating all the features equally; on the other hand, A 3 M pays more attention to the important parts rather than the background and integrates attribute features with attention weights.</p><p>Ablation Study. We construct 2 baseline models to demonstrate the effectiveness of proposed A 3 M on Market-1501. Baseline-1 is a CNN pre-trained on ImageNet <ref type="bibr" target="#b29">[30]</ref> and fine-tuned to predict the person category on the target dataset. Baseline-2 is a simplification of proposed A 3 M by replacing the attention weights with equal weights. The results with ResNet-50 as the base network are shown in <ref type="table" target="#tab_0">Table 1</ref>. It can be seen that Baseline-1 obtains a decent result, and Baseline-2 achieves better performance with the auxiliary information of attributes. Our A 3 M exceeds both baselines by a significant margin. This illustrates that the attention mechanism can capture the important parts of spatial features and attribute features. For A 3 M w/o attr-label, we don't use attributes' label information in loss to supervise the CNN, that is to say, our A 3 M can learn local attribute feature in an unsupervised manner, which outperforms Baseline-2 by a large margin.</p><p>In order to see whether both attention modules are effective, we conduct two more experiments to evaluate each attention module. Att-1 set the category-guided attention weights to equal values, which means that attribute features are fused via averaging. Att-2 uses equal values as attribute-guided attention weights, indicating the regional features are fused by average pooling. From <ref type="figure" target="#fig_7">Figure 5</ref>, the model only uses attribute-guided attention or category-guided attention outperforms Baseline-2 which doesn't use any attention module. This shows that either attention module is beneficial to person re-identification. In addition, A 3 M defeats either Att-1 or Att-2 by a large margin, indicating that the usage of both attention modules is helpful to each other and obtained the best results.</p><p>Visualization of Attention Maps. We also visualize the attributeguided attention maps for two example images in test set. <ref type="figure" target="#fig_8">Figure  6</ref> shows two query images (the first column) with the selected 5 attribute-guided attention maps (the middle 5 columns) of regional features. It can be seen that in different attribute-guided attention maps, high values distribute in different regions. The corresponding relations are: gender -body region, bag -bag region, upper-clothing -upper body region, sleeve length -sleeve region and lower-clothing -lower body region. Through the attention model, A 3 M can select crucial local attribute cues such as bag and lower-clothing to help person re-identification. Nevertheless, attribute sleeve is not easy to learn well. The visualized results verify our idea that the attention mechanism can select regions most related to the intrinsic attributes for re-identification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluations on Image Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Attribute Parts Acc(%) MG-CNN <ref type="bibr" target="#b37">[38]</ref> 83.0 DeepLAC <ref type="bibr" target="#b48">[49]</ref> 84.5 Bilinear-CNN <ref type="bibr" target="#b20">[21]</ref> 85.1 RA-CNN <ref type="bibr" target="#b9">[10]</ref> 85.3 Part RCNN <ref type="bibr" target="#b47">[48]</ref> w/ 76.4 PS-CNN <ref type="bibr" target="#b12">[13]</ref> w/ 76.6 PN-CNN <ref type="bibr" target="#b4">[5]</ref> w/ 85.4 Image+Parts+Attribute <ref type="bibr" target="#b23">[24]</ref> w/ 85.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Attribute Acc(%) AlexNet <ref type="bibr" target="#b43">[44]</ref> 81.9 Overfeat <ref type="bibr" target="#b43">[44]</ref> 87.9 GoogLeNet <ref type="bibr" target="#b43">[44]</ref> 91. Comparison with State-of-the-Art Methods. We conduct experiments on two benchmark fine-grained classification dataset, namely, CUB-200-2011 <ref type="bibr" target="#b36">[37]</ref> and CompCars <ref type="bibr" target="#b43">[44]</ref>. The results of our model and previous state-of-the-art methods are summarized in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>. From the results shown, our model shows competitive performance with the best accuracy. This verifies the effectiveness of our model.  It can also be seen that the proposed method exceeds both baselines significantly, with a gain of at least 1.3% in accuracy over baselines. Our A 3 M which integrates both attention modules further improves result, indicating that the attribute-guided attention module and category-guided attention module help to each other to obtain a better performance.</p><p>Analysis of hyper-parameters. To further evaluate the impact of the trade-off parameters α and β, we show in <ref type="figure" target="#fig_9">Figure 7</ref> the result variation on CUB-200-2011 dataset. It can be seen that the performance is gradually improved when α or β increase from 0.1 to 0.5, however, further increasing α makes the performance decrease. The performance is not sensible to the hyper-parameters, so we can easily choose a proper trade-off parameter to obtain a good performance in practice.</p><p>Evaluation of Attribute Classification. We test attribute classification on the testing set of CUB-200-2011 dataset, and the results are listed in <ref type="figure" target="#fig_10">Figure 8</ref>. The proposed A 3 M and Baseline-2 are compared. The overall accuracy of A 3 M is higher than that of Baseline-2, with an significant increase, indicating the attribute-category reciprocal attention module not only benefits to category classification, but also helps to learn a more discriminative attribute classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluations on Image Retrieval</head><p>To ensure the fairness of the comparison, we reproduce the HDC method based on the InceptionBN. HDC <ref type="bibr" target="#b46">[47]</ref> trains an ensemble of diverse models of different hardness levels, achieving Recall@1 at 60.0% and 79.2% on CUB-200-2011 and CARS196, <ref type="table" target="#tab_6">Table 4</ref> quantifies the advantages of A 3 M on CUB-200-2011. A 3 M avhieves 61.2% for   Recall@1, which is significantly better than all the previous state-ofthe-art methods including the reproduced HDC. <ref type="table" target="#tab_7">Table 5</ref> reports the results on CARS196, where A 3 M achieves improvements around 1% over the baseline-1 measured by Recall@{1,2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a novel Attribute-Aware Attention Model to learn fine-grained representation for the tasks of person reidentification, fine-grained classification and retrieval. In our model, both global category features and local attribute features are learned, and an attribute-category reciprocal attention module is used to select the most important category regional features and attribute features with the help of each other. Experimental results have shown the effectiveness of our method in fine-grained image recognition problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Person image examples with some attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Attribute-Category Reciprocal Attention module, including two components: (a) attribute-guided attention module, (b) category-guided attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>cat eдory) j t = 0 for all j and p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>cat eдor y) t = 1. The learned category embedding v (cat eдory) contains global information for image recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>2D example for the addition of category feature and attribute feature. The global category feature embedding of the three images corresponds the left one, with the help of adding attribute feature, final feature can learn a better representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 1 . 2</head><label>12</label><figDesc>Fine-grained Image Classification. CUB-200-2011. CUB-200-2011 [37] is a well-known fine-grained benchmark dataset for birds classification. It contains 11,788 images of 200 classes. Each image is annotated with 15 part locations, 312 binary attributes, and 1 bounding box. Here we classify the attributes to 28 multi-class attributes for experiments. The object bounding box is used in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Systematic evaluation of the attention modules on Market-1501 dataset. The shared CNN architecture is ResNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of some attention maps. The first column is the query image, and the rest columns show the attributeguided attention maps (from left to right: gender, carrying bag, upper-clothing color, sleeve length, lower-clothing color), red means high value while blue means low. (a) and (b) are from different query persons. The last column shows the Rank-1 result of A 3 M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Classification accuracy w.r.t α/β on CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Attribute classification accuracy on CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art on Market-1501 dataset. We also provided the results of baseline models. -means that no reported results are available.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Market-1501<ref type="bibr" target="#b51">[52]</ref> is a large person re-identification dataset, containing 32,668 bounding box images of 1,501 persons captured by 6 cameras. The dataset is split into 751 identities for training and 750 identities for testing.<ref type="bibr" target="#b21">[22]</ref> annotated 27 attributes for each category, we follow<ref type="bibr" target="#b21">[22]</ref> and use the attributes annotated Market-1501 dataset for experiments.</figDesc><table><row><cell></cell><cell>.51</cell><cell>90.91</cell><cell>94.09</cell><cell>96.23</cell><cell>59.87</cell></row><row><cell>Body-Parts-Fusion [19]</cell><cell>80.31</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.53</cell></row><row><cell>Part-Aligned [50]</cell><cell>81.0</cell><cell>92.0</cell><cell>94.7</cell><cell>96.4</cell><cell>63.4</cell></row><row><cell>SSM [3]</cell><cell>82.21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.80</cell></row><row><cell>JLML [41]</cell><cell>83.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.4</cell></row><row><cell>Attr-Id [22]</cell><cell>84.29</cell><cell>93.20</cell><cell>95.19</cell><cell>97.00</cell><cell>64.67</cell></row><row><cell>Baseline-1</cell><cell>73.90</cell><cell>87.68</cell><cell>91.54</cell><cell>94.80</cell><cell>47.78</cell></row><row><cell>Baseline-2</cell><cell>80.07</cell><cell>91.06</cell><cell>94.12</cell><cell>96.08</cell><cell>59.13</cell></row><row><cell>A 3 M w/o attr-label</cell><cell>85.21</cell><cell>94.57</cell><cell>96.53</cell><cell>97.92</cell><cell>66.42</cell></row><row><cell>A 3 M</cell><cell>86.54</cell><cell>95.16</cell><cell>97.03</cell><cell>98.30</cell><cell>68.97</cell></row><row><cell>4.1 Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4.1.1 Person Re-Identification. Market-1501.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on CUB-200-2011 dataset. w/ means using attributes or parts annotation during training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on CompCars dataset. w/ means using attributes annotation during training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Recall@K(%) and NMI(%) on CUB-200-2011 for image retrieval</figDesc><table><row><cell>Methods</cell><cell>Network</cell><cell cols="3">Dim R@1 R@2 R@4 R@8 NMI</cell></row><row><cell cols="2">Npairs [33] GoogLeNet</cell><cell>128</cell><cell cols="2">47.2 58.9 70.2 80.2 56.2</cell></row><row><cell>HDC [47]</cell><cell>GoogLeNet</cell><cell>384</cell><cell>53.6 65.7 77.0 85.6</cell><cell>-</cell></row><row><cell>BIER [27]</cell><cell>GoogLeNet</cell><cell>512</cell><cell>55.3 67.2 76.9 85.1</cell><cell>-</cell></row><row><cell>HDC</cell><cell cols="2">InceptionBN 384</cell><cell cols="2">60.0 71.8 81.5 88.6 65.6</cell></row><row><cell>Baseline-1</cell><cell cols="2">InceptionBN 384</cell><cell cols="2">59.1 70.7 81.5 88.8 65.6</cell></row><row><cell>A 3 M</cell><cell cols="4">InceptionBN 384 61.2 72.4 81.8 89.2 66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Recall@K(%) and NMI(%) on CARS196</figDesc><table><row><cell>Methods</cell><cell>Network</cell><cell cols="3">Dim R@1 R@2 R@4 R@8 NMI</cell></row><row><cell cols="2">Npairs [33] GoogLeNet</cell><cell>128</cell><cell cols="2">71.1 79.7 86.5 91.6 62.7</cell></row><row><cell>HDC [47]</cell><cell>GoogLeNet</cell><cell>384</cell><cell>73.7 83.2 89.5 93.8</cell><cell>-</cell></row><row><cell>BIER [27]</cell><cell>GoogLeNet</cell><cell>512</cell><cell>78.0 85.8 91.1 95.1</cell><cell>-</cell></row><row><cell>HDC</cell><cell cols="2">InceptionBN 384</cell><cell cols="2">79.2 87.3 92.2 95.5 66.8</cell></row><row><cell>Baseline-1</cell><cell cols="2">InceptionBN 384</cell><cell cols="2">78.5 86.8 92.3 95.4 66.8</cell></row><row><cell>A 3 M</cell><cell cols="4">InceptionBN 384 80.0 87.5 92.3 95.5 67.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ejaz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Cascaded Part-Based System for Fine-Grained Vehicle Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Biglari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Soleimani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Hassanpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="273" to="283" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person re-identification by camera correlation aware feature augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras." />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-Grained Categorization and Dataset Bootstrapping Using Deep Metric Learning with Humans in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1153" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weldon: Weakly supervised learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autoencoder inspired unsupervised feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2941" to="2945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Part-stacked CNN for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person Re-identification by Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person reidentification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving Person Re-identification by Attribute and Identity Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-toend comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Localizing by Describing: Attribute-Guided Attention Localization for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4190" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghavan Prabhakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schãĳtze</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511809071</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511809071" />
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Person re-identification using cnn features learned from combination of attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einoshin</forename><surname>Es Tetsu Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suzuki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BIER-Boosting Independent Embeddings Robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5189" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object-Part Attention Model for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image classification with tailored fine-grained dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Siamese Long Short-Term Memory Architecture for Human Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2399" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint learning of visual attributes, object classes and visual saliency. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mining Discriminative Triplets of Patches for Fine-Grained Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01130</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person Re-Identification by Deep Joint Learning of Multi-Loss Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shaogang Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/305</idno>
		<idno>IJCAI-17. 2194-2200</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/305" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring questionguided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1611.05720</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deeply-Learned Part-Aligned Representations for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scalable Person Re-identification: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A Discriminatively Learned CNN Embedding for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Re-ranking Person Re-identification with k-reciprocal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
