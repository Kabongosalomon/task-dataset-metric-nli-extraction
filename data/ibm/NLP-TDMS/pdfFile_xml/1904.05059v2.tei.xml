<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">C3AE: Exploring the Limits of Compact Model for Age Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
							<email>liushuaicheng@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">C3AE: Exploring the Limits of Compact Model for Age Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Age estimation is a classic learning problem in computer vision. Many larger and deeper CNNs have been proposed with promising performance, such as AlexNet, Vg-gNet, GoogLeNet and ResNet. However, these models are not practical for the embedded/mobile devices. Recently, MobileNets and ShuffleNets have been proposed to reduce the number of parameters, yielding lightweight models. However, their representation has been weakened because of the adoption of depth-wise separable convolution. In this work, we investigate the limits of compact model for smallscale image and propose an extremely Compact yet efficient Cascade Context-based Age Estimation model(C3AE). This model possesses only 1/9 and 1/2000 parameters compared with MobileNets/ShuffleNets and VggNet, while achieves competitive performance. In particular, we redefine age estimation problem by two-points representation, which is implemented by a cascade model. Moreover, to fully utilize the facial context information, multibranch CNN network is proposed to aggregate multi-scale context. Experiments are carried out on three age estimation datasets. The state-of-the-art performance on compact model has been achieved with a relatively large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) are being developed deeper and larger for more precise accuracy in recent years. This trend has brought in unprecedented computation cost to either training or deploying. In particular, deploying existing classic large models, e.g., AlexNet <ref type="bibr" target="#b16">[17]</ref>, VggNet <ref type="bibr" target="#b32">[33]</ref> and ResNet <ref type="bibr" target="#b10">[11]</ref>, on mobile phones, cars and robots is next to impossible due to the model size and computational cost.</p><p>To deal with above problem, recently MobileNets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> and ShuffleNets <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23]</ref> have been proposed to greatly reduce the parameters by exploiting the depth-wise separa- * Corresponding author Initial image with high resolution/size 200*240*3</p><p>Cropped image with low resolution/size 64*64*3</p><p>Cropped image with medium resolution/size 64*64*3</p><p>Cropped image with high resolution/size 64*64*3 <ref type="figure">Figure 1</ref>: Human can recognize the age of person in one of the four images, regardless of different resolutions or scales. Is it necessary to use the first image that is with large size?</p><p>In this work, we use small-scale image (64 × 64 × 3) for age estimation, which can achieve very competitive performance.</p><p>ble convolution. In these models, the traditional convolution is replaced by two step convolutions, namely the filtering layer and combining layer. For example, in MobileNets, the filtering layer first convolves each corresponding channel separately, thus breaking the interactions among various output channels, which can reduce the number of parameters dramatically. A 1×1 convolution then stitches different channels to combine the information acquired from different input channels. For large-scale images, such operation is reasonable because images need to be represented by large number of channels, e.g., 512 and 384 in VggNet <ref type="bibr" target="#b32">[33]</ref> and ResNet <ref type="bibr" target="#b10">[11]</ref>. Whereas, for small-scale images, e.g., images with low resolution and small dimension, such predicate remains questionable.</p><p>In contrast to large-scale images, small-scale images can be often represented by fewer number of channels in the network, and so does the number of parameters and memory. Therefore, standard convolution layer with small size kernel does not require much more parameters and memory compared with depth-wise separable convolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>. From the perspective of image representation, the output channels of depth-wise convolution are many times larger than that of standard convolution. To compensate the representation ability, the depth-wise convolution has to pay for the cost of increased parameters. Therefore, we believe the conventional convolution layer with small kernel size is more suitable for processing small-scale images than depthwise counterpart.</p><p>Images must often be stored and processed with low resolution and scale, aka small-scale images, on low-cost mobile devices. One of the eminent problems which falls into the category is age estimation. For example, human can easily recognize the age of the man in <ref type="figure">Fig. 1</ref> in either full or low resolution and partial or full view of the face. We, therefore, conjecture such ability is applicable to contemporary CNNs and design a compact with standard convolution layers with small-scale face images as input for age estimation.</p><p>Recent advances in age estimation are usually summarized into two mainstream directions: jointly category classification and value regression, and distribution matching. For the former, the psychological evidence <ref type="bibr" target="#b14">[15]</ref> reveals that humans are inclined to give categorical ratings on image rather than continuous scores, i.e., preferring to different levels. Some works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref> utilize the category information and ordinal information to implement classification and regression simultaneously. For the latter one, distribution matching can achieve promising results under the assumption that distribution label of each image is provided. Nevertheless, acquiring distributional labels for thousands of face images itself is a non-trivial task. In this work, we propose to exploit the information on classification, regression and label distribution simultaneously. This is achieved by representing discrete age as a distribution over two discrete age levels and the training objective is to minimize the match between distributions. In deep regression model, a fully connected layer with semantic distribution is inserted in between the feature layer and age value prediction layer.</p><p>To summarize, we design a compact model that takes small-scale image as input. Specifically, we utilize standard convolution instead of depth-wise convolution, with suitable kernel and number of channels. To the best of our knowledge, this is the smallest model that has been obtained so far on the facial recognition, i.e., 0.19MB for plain model and 0.25MB for full model. We then represent the discrete age value as a distribution and design a cascade model. Moreover, we introduce a context based regression model which takes as input multiple scales of facial image. With the Compact basic model, Casaced training and multi-scale Context, we aim to tackle small-scale image Age Estimation. Thus we name the network C3AE.</p><p>Our main contributions are as follows. First, we study the relationship between the channel number and the representation on depth-wise convolution, especially on the small scale image. Our discussion and results advocate a rethinking of MobileNets and ShuffleNets for the smallscale/medium-scale images. Second, we present a novel age representation that exploits the information on classification, regression and label distribution simultaneously and design a cascade model. Finally, we propose a context based age inference method collecting different granularity of input images. The proposed model, named C3AE, achieves the state-of-the-art performance compared with alternative compact models and even outperforms many bulky models. With the extremely compact model (0.19MB and 0.25 MB for plain and full model, respectively), C3AE is suitable to be deployed on low-end mobiles and embedded platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Age Estimation The age progression displayed on faces is uncontrollable and personalized <ref type="bibr" target="#b4">[5]</ref>, and the traditional methods often have the problem of generalization. With the success of deep learning, many recent works applied deep CNN to achieve the state-of-the-art performance on various applications such as image classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b13">14]</ref>, semantic segmentation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>. As for age estimation, CNNs are also being used for its strong generalization. Yi et al. <ref type="bibr" target="#b38">[39]</ref> firstly utilized CNN models to extract features from several facial regions, and used a square loss for age estimation. AgeNet <ref type="bibr" target="#b17">[18]</ref> used one-dimensional real-value as an age group for age classification. Rothe et al. <ref type="bibr" target="#b28">[29]</ref> proposed to use expected value on the softmax probabilities and discrete age values for age estimation. It is a weighted softmax classifier only in the testing phase. Niu et al. <ref type="bibr" target="#b23">[24]</ref> formulated age estimation as an ordinal regression by employing multiple output CNNs. Following <ref type="bibr" target="#b23">[24]</ref>, Chen et al. <ref type="bibr" target="#b2">[3]</ref> utilized ranking-CNN for age estimation, in which there were a series of basic binary CNNs, aggregating to the final estimation. Han et al. <ref type="bibr" target="#b8">[9]</ref> used multiple attributes for multi-task learning. Gao et al. <ref type="bibr" target="#b5">[6]</ref> used KL divergence to measure the similarity between the estimated and groundtruth distributions for age. Pan et al. <ref type="bibr" target="#b24">[25]</ref> designed a new mean-variance loss for distribution learning.</p><p>However, in real applications, the distribution is usually not available for a face image. In this work, we consider two objectives simultaneously. The first one minimizes the Kullback-Leibler loss between distributions, and the second one optimizes the squared loss between discrete ages. Compact Model As the increasing requirement of mobile/embedded devices running deep learning, various efficient models such as GoogLeNet <ref type="bibr" target="#b34">[35]</ref>, SqueezeNet <ref type="bibr" target="#b15">[16]</ref>, ResNet <ref type="bibr" target="#b10">[11]</ref> and SENet <ref type="bibr" target="#b12">[13]</ref>, are designed to cater this wave. Recently, depth-wise convolution was adopted by MobileNets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> and ShuffleNets <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23]</ref> to reduce computation costs and model sizes. They were built primarily from depth-wise separable convolutions initially introduced in <ref type="bibr" target="#b31">[32]</ref> and subsequently used in Inception models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34]</ref> to reduce the computation in the first few layers. In particular, the separation of filtering -applying convolution at each channel separately and combination -recombine the output of individual channels achieved fewer computations. MobileNet-V1 <ref type="bibr" target="#b11">[12]</ref> based on the depth-wise separable convolution explored some important design guidelines for an efficient model. ShuffleNet-V1 <ref type="bibr" target="#b39">[40]</ref> utilized novel point-wise group convolution and channel shuffle to reduce computation cost while maintaining accuracy. MobileNet-V2 <ref type="bibr" target="#b30">[31]</ref> proposed a novel inverted residual with linear bottleneck. ShuffleNet-V2 <ref type="bibr" target="#b22">[23]</ref> mainly analyzed the runtime performance of the model and give four guidelines for efficient network design.</p><p>For age estimation, we argue that for small-scale images, the channel size is often small and the depth-wise separation does not benefit. Instead, a standard convolution is adequate for the trade-off between accuracy and compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Model</head><p>In this section, we firstly present the compact model and its architecture as well as some important discussions on practical guidelines. Then we describe a novel two-points representation of age, and utilize the cascade style to insert it in deep regression model. Next a context based module is embedded into a single regression model by exploiting facial information at three granularity levels. Finally some discussions are given for rethinking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Compact Model for Small-scale Image: Revisiting Standard Convolution</head><p>Our plain model is composed of five standard convolution and two fully connected layers as shown in Tab. 1 1 . For standard convolution layer followed by batch normalization, Relu and average pooling, its kernel, number of channels and parameters are 3, 32 and 9248, respectively. As a basic module, we will show why we use standard convolution block instead of the separable convolution block that used in MobileNets and ShuffleNets. We shall demonstrate later in the experiment, our basic model produces competitive performance compared with fashionable models though its simplicity.</p><p>In MobileNets, the status regarding the saving of parameters and computation were analyzed, especially comparing between standard convolution and depth-wise separable convolution. That analysis is suitable for large-scale image while for the small-scale/medium image it may not work well.</p><p>Given an input and output as D F × D F × M feature map F and D F ×D F ×N feature map G, D F denotes the size of feature map, M and N are the number of input channels and output channels for a convolution layer, respectively. The number of computation cost is given by</p><formula xml:id="formula_0">D 2 K · M · D 2 F + M · N · D 2 F [12]</formula><p>. In comparison, the standard convolution layer is parameterized by convolution kernel K of size D 2 K ×M × N . The reduction between standard convolution and depthwise separable convolution in computation cost <ref type="bibr" target="#b11">[12]</ref> is:</p><formula xml:id="formula_1">D 2 K · M · D 2 F + M · N · D 2 F D 2 K ·M ·N · D 2 F = M MN + M N MN D 2 K<label>(1)</label></formula><p>Only with the assumption that both the depth-wise convolution and standard convolution need the same channel size, i.e. M =M and N =N , Eq. 1 can be reduced to</p><formula xml:id="formula_2">1 N + 1 D 2 K &lt; 1.</formula><p>However, the depth-wise convolution often requires much more channel numbers in order to perform comparable to standard convolution on small-scale images. Therefore, in reality,M is much less than M and so doeŝ N . For instance, images can be represented by 32 channels in standard convolution rather than 144 or even larger in MobileNet-V2. In this situation, the reduction ratio is</p><formula xml:id="formula_3">M M ·N + M N D 2 K ·M ·N = 144 32·32 + 144·144 3 2 ·32·32 = 2.39 &gt; 1.</formula><p>It indicates a standard convolution can even save more than half of computation cost compared with MobileNet-V2. Hence, it is reasonable to select the standard convolution layer for small size image and model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two-Points Representation of Age</head><p>In this section, we present a novel age representation as a distribution over two discrete adjacent bins. Given a set of images {(I n , y n )} n=1,2,··· ,N , deep regression model can be written as a mapping F : I → Y, where I n and y n represent image and regression label, respectively. For any regression label y n , it can be represented as a convex combination of two other numbers z 1 n and z 2 n (z 1 n = z 2 n ),</p><formula xml:id="formula_4">y n = λ 1 z 1 n + λ 2 z 2 n ,<label>(2)</label></formula><p>where λ 1 and λ 2 are the weights, λ 1 , λ 2 ∈ R + , λ 1 +λ 2 = 1.</p><p>Given the age interval [a, b], a label y n ∈ [a, b] and bins {z m } with uniform interval K , y n can be represented by z 1 n = yn K · K and z 2 n = yn K · K, where · and · are the floor and ceiling function. Accordingly, the coefficients λ 1 and λ 2 are computed as</p><formula xml:id="formula_5">λ 1 =1 − y n − z 1 n K = 1 − y n − yn K · K K λ 2 =1 − z 2 n − y n K = 1 − yn K · K − y n K<label>(3)</label></formula><p>For example, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the corresponding representation of 68 or 74 with K = 10 (second row in  In fact, λ 1 and λ 2 represent the probability belonging to two bins, which include rich distribution information. The main trend on age estimation includes two aspects: simultaneously classification and regression, and distribution learning. For the former, according to the above <ref type="figure" target="#fig_1">Fig. 3</ref>, 68 more likely belongs to bin 70 instead of bin 60. Two-points representation can disambiguate this problem naturally. For the latter, some methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref> use distribution matching for better results. However, that requires extensive labeling to obtain the distribution that is very costly.</p><p>What is more, two-points representation gets two adjacent bins instead of any other two or more points, and the two adjacent bins are assigned with nonzero elements. In fact, each point/age in the linesegment can be represented by multiple points in which the number of combinations is very diversified. Each point can also be represented by two points or any other more points. However, those combinations is probably not what we want, e.g., 50 = 0.5×0+0.5×100 = 0.2×10+0.2×40+0.2×60+0.2×90. For age estimation, these representation is useless. While for deep regression model, these combinations need to be eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cascade Training</head><p>From the above section, age value y n can be represented as distribution vector y n . However, the combination of y n is diversified. Two-points representation is suitable to control it. The next question is how to embed the vector information into an end-to-end network. We implement this step by the cascade model shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. In specific, a fully connected layer with semantic distribution is inserted in between feature layer y n and the regression layer y n . The mapping f from feature X to age value y can be decomposed into two steps f 1 and f 2 , i.e., f = f 2 • f 1 . In fact, the whole process can be denoted as f :</p><formula xml:id="formula_6">I n Conv −−−→ X W1 − − → y n W2 − − → y n .</formula><p>Here we define two losses for two cascade task. The first one measures discrepancy between ground-truth label and predicted age distribution. We adopt KL-Divergence as the measurement, </p><p>where W 1 is the weight of the mapping f 1 from concatenated feature X to the distributionŷ n , λ is used to control the sparsity of theŷ n . The second loss controls the prediction of the final age and is implemented as L1 distance (MAE loss), L reg (y n ,ŷ n ) = n ||y n −ŷ n ||.</p><p>In the training process, two loss functions are trained in cascade style as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. However they are still trained jointly, and the total loss is given as</p><formula xml:id="formula_9">L total = αL kl + L reg<label>(6)</label></formula><p>where α is the hyperparameter to balance two losses. The cascade training can properly control the distributionŷ n in case of diversified combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Context-based Regression Model</head><p>The resolution and the size of small-scale image is limited. Exploiting facial information at different granularity levels is necessary. As shown in <ref type="figure">Fig. 1</ref>, each cropped image has a special view on the face. The image with high resolution contains rich local information, in return one with low resolution may contain global and scene information. Other than selecting one aligned facial center in SSR <ref type="bibr" target="#b37">[38]</ref>, we crop face centers with three granularity levels, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, then fed them into the shared CNN network. Finally the bottlenecks of three-scale facial images are aggregated by concatenation that followed by cascade module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussions</head><p>In this section, we summarize two non-trivial empirical guidelines for small-scale images and models. We will support our claims by experiments in the next section.</p><p>Residual module For small-scale image and model, is the residual module necessary? At least for age estimation dataset, it is not. Residual module with shortcut strategy is first designed by <ref type="bibr" target="#b10">[11]</ref> to solve the problem of gradient vanishing, especially on very deep network. Its shortcut power can only be disclosed when enough layers were involved. The small-size model usually includes only shallow layers. According to our experiment, common connection on plain convolution is enough for small image and model. This discussion reminds us to rethink the apparent ideas in deep learning, especially on the small size image and model. SE module The squeeze-and-excitation (SE) module has been validated by many works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref> for large scale image. While for small size image and model it also works well. So we integrate the SE module into our network and it costs very few parameters. For example, when the squeeze factor is 2, each SE module's parameters is only 32*16 *2 = 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The experiments consist of three parts. The first part is ablation study I on the comparison among SSR, MobileNet-V2, ShuffleNet-V2 and C3AE using plain model. The second one gives ablation study II on necessity of cascade module and context based module. The last one mainly provides the comparison with some state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We study age estimation on three datasets: IMDB-WIKI <ref type="bibr" target="#b28">[29]</ref>, Morph II <ref type="bibr" target="#b27">[28]</ref> and FG-NET <ref type="bibr" target="#b4">[5]</ref>. We follow the conventions in the literature SSR <ref type="bibr" target="#b37">[38]</ref>, DEX <ref type="bibr" target="#b28">[29]</ref> and Hot <ref type="bibr" target="#b28">[29]</ref>, WIKI-IMDB are used for pre-training and the ablation study. Because Morph II is the most popular and large benchmark for age estimation, we choose it for ablation studies. Morph II and FG-NET are used to compare with the state-of-the-arts.</p><p>IMDB-WIKI is the largest facial dataset with age labels, which is introduced in <ref type="bibr" target="#b28">[29]</ref> and consists of 523, 051 images in total. The range is from 0 to 100. It is separated as two parts: IMDB(460, 723 images) and WIKI (62, 328 images). However, it is not suitable for the performance evaluation on the age estimation because it contains much more noise. Thus, following previous works, e.g., SSR <ref type="bibr" target="#b37">[38]</ref> and DEX <ref type="bibr" target="#b28">[29]</ref>, we utilize IMDB-WIKI only for pre-training.</p><p>Morph II is the most popular benchmark for age estimation, which has around 55, 000 face images of 13, 000 subjects with age label. The age ranges from 16 to 77(on average, 4 images per subject). Similar to some previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>, we randomly partition the dataset into two independent parts: training (80%) and testing (20%).</p><p>FG-NET contains 1, 002 face images from 82 noncelebrity subjects with large variation of lighting, pose, and expression. The age ranges from 0 to 69 (on average, 12 images per subject) <ref type="bibr" target="#b4">[5]</ref>. Since the size of FG-NET is small, some previous methods usually use leave-one-out setting which needs to train 82 deep models. Under this setting, there are about 12 samples for the testing. Here we randomly choose 30 samples as the testing set and the remaining ones are for the training. We repeat this split 10 times and compute their average performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Following SSR <ref type="bibr" target="#b37">[38]</ref> and DEX <ref type="bibr" target="#b28">[29]</ref>, the model is firstly pre-trained on the IMDB and WIKI dataset, and is with size of 64 × 64 × 3. In all the experiments, Adam optimizer is employed. In the first ablation study, because the plain model of C3AE is compared with other plain models, each model is trained 160 epochs with batch size of 50. Similar to SSR, the initial learning rate, dropout rate, the momentum and the weight decay are set to 0.002, 0.2, 0.9 and 0.0001, respectively. The learning rate is decreased by a factor of the regression value with patience epochs 10 on the change value of 0.0001. In the second ablation study, for comparing with the state-of-the-art methods, each model is trained 600 epochs in total with the batch size of 50. We use the strategy in <ref type="bibr" target="#b41">[42]</ref> with randomly dropping out blocks on the input image. In this phase, the initial learning rate, dropout rate, the momentum and the weight decay are set to 0.005, 0.3, 0.9 and 0.0001, respectively. The learning rate is decreased by a factor of the regression value with patience epochs 20 on the change value of 0.0005. Following SSR <ref type="bibr" target="#b37">[38]</ref>, the evaluation criteria is mean absolute value (MAE). The factor α in Eq. 6 is set to 10 in all the experiments. For all the cascade model, K in Eq. 3 is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>The ablation study is conducted as two parts. For the first one, our plain model is compared with SSR, MobileNet-V2 and ShuffleNet-V2 to demonstrate that standard convolution yields competitive performance, even better than fashionable models such as MobileNet-V2 and ShuffleNet-V2. We further study whether the residual module and SE module can benefit small network. For the second part, we conduct ablation study on the necessity of two-points representation and context module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Study I: the Plain Model of C3AE</head><p>This part includes three groups of experiments: comparison among our plain model, SSR, MobileNet-V2 and ShuffleNet-V2; comparing with/without residual module; and comparing with/without SE module.</p><p>The results of three methods (SSR, MobileNet-V2 and ShuffleNet-V2) on Morph II(M-MAE), IMDB (I-MAE) and WIKI (W-MAE) are given in Tab. 2. For fair comparison, we implement extensive factor combinations(Comb.). In Tab. 2, for MobileNet-V2 (M-V2) 2 , (α pw , α exp ) means the number of the pointwise filters and the expansion factor for each expansion layer, respectively. For ShuffleNet-V2 (S-V2) <ref type="bibr" target="#b2">3</ref> , (α ra , α f a ) means ratio of bottleneck module's output channels for each stage and the scale factor for each stage's output channels, respectively. To conclude from the comparison, our plain model even with minimal parameters(Param.) and memory achieved best result regardless of parameter tuning in the alternative three methods.</p><p>We also give a speed analysis from two points: MACC and runtime speed. The former is the theoretical number of multi-add operations. The latter is the measured speed all under the same condition (forward single image 2000 times and then average), on CPU(Intel Xeon 2.1GHZ) and GPU(Titan X). The comparison is shown in Tab. 3.</p><p>As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the plain model of C3AE is consistently better than SSR, ShuffleNet-V2 and MobileNet-V2 with lower validation loss (val loss in orange, training loss in blue). More examples can be found in the supplementary material. For MobileNet-V2 and ShuffleNet-V2, with <ref type="bibr" target="#b1">2</ref> The code is from keras application <ref type="bibr" target="#b2">3</ref> The code is from https://github.com/opconty/keras-shufflenetV2 the depth-wise convolution, is by no means inferior than our plain model with standard convolution. In addition, there is a strange observation that the result of α exp = 4 is superior to α exp = 6. We believe that too large inverted bottleneck may be not suitable for small size model. For SSR, the standard convolution is also used. However, its full model is still inferior to our plain model. In addition, the gap between train and validate loss in our plain is the least. It shows our plain model has better generalization. All these observations suggest the effectiveness of our plain model. Although our plain model is plain enough without any bells and whistles, it still can get very competitive performance.</p><p>We further investigate the effectiveness of residual connection and SE module. According to the results in Tab. 4 and the comparison in supplementary material, we observe that residual module does not benefit in the small size model, in particular for three datasets on age estimation. While SE module work well for small size model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Ablation Study II: Cascade and Context Module</head><p>In this section, we analyze how the choice of cascade module (two-points representation) and context module affect the performance of age estimation. The result of two-points representation is implemented by cascaded training, i.e., with/without cascade module. As shown in <ref type="figure">Fig. 5</ref>, regardless of the regularizer λ in Eq. 4 we choose, the result with casacde module is consistently better than that without cascade. If the context module is further applied (Cascade + Context) it outperforms the other two.The validations demonstrate the importance of two-points representation and context module.</p><p>In specific, we give some examples in <ref type="figure">Fig. 6</ref>. GT means the groundtruth value, and the legend gives the predicted age. The X-axis is the learned weights W 2 , and the Y-axis is the predicted vectorŷ n . Their dot/inner product is the predicted age. We can see that the learned weights are almost equivalent to groundtruth bins W 2 = <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr">50,</ref><ref type="bibr">60,</ref><ref type="bibr">70,</ref><ref type="bibr">80]</ref>. That is to say, W 2 controls two-points representation so that the diversified combinations are eliminated. The last element of the predicted bins is very strange, i.e., 92.73, 55.49. After the analysis of the data distribution, we found that there are only 9 samples in the range <ref type="bibr">[70,</ref><ref type="bibr">80]</ref>, and it is easy to explain why the last element is abnormal. The predicted distribution is sparse with only two or three adjacent nonzero elements because of two-points representation. Fully connected layer will lead to the phenomenon that each age can be represented by many different combinations.</p><p>In addition, as shown in <ref type="figure">Fig. 6</ref>, we also observe that the predicted distribution and age on the top is better than that on the bottom. The colors of the bar, legend and the distribution correspond to the colored bounding box on the top image. Context based model (top) achieves better performance than that of single scale input (bottom).</p><p>Finally, in order to show generality of our model, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-arts on Morph-II</head><p>In this section, we further compare our model with stateof-the-art models on Morph II. As shown in Tab. 5, our full model achieves 2.78 and 2.75 MAE under the condition: trained from scratch and pretrained on IMDB-WIKI, which is the state-of-the-art performance among compact models. The previous best performance achieved in the compact model is 3.16 in SSR <ref type="bibr" target="#b37">[38]</ref>. Some results in the Tab. 5 are from SSR <ref type="bibr" target="#b37">[38]</ref>. In fact, our plain model achieves 3.13 MAE even without any bells and whistles. The results of all other compact models are pretrained on IMDB-WIKI. Our results on with/without pretrained process are very similar. We believe Morph II is large enough to train our tiny model. On the other hand, our result is much competitive compared with the bulky models, and it even surpasses sev- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT=16</head><p>GT=36 GT=45 GT=45 GT=57 <ref type="figure">Figure 6</ref>: Some examples on the C3AE. Top: the result of the context based regression model. The yellow bars denote the predicted distributionŷ n , and the X-axis is the learned weight W 2 from the distribution to age value. Bottom: Three different colors RGB correspond to each facial context and predicted distributionŷ n .(Best viewed in color and magnifier.) eral bulky models despite it consumes only 1/2000 of their model sizes. All the bulky models are pretrained on Ima-geNet or IMDB-WIKI using VggNet. Our result without pretrained process even surpasses some pretrained bulky models. In general, C3AE gets very competitive performance on Morph II with extremely lightweight model.  <ref type="bibr" target="#b29">[30]</ref> 3.45 530MB 138M ODFL <ref type="bibr" target="#b18">[19]</ref> 3.12 530MB 138M DEX <ref type="bibr" target="#b28">[29]</ref> 3.25 530MB 138M DEX (IMDB-WIKI) <ref type="bibr" target="#b28">[29]</ref> 2.68 530MB 138M ARN <ref type="bibr" target="#b0">[1]</ref> 3.00 530MB 138M AP <ref type="bibr" target="#b40">[41]</ref> 2.52 530MB 138M MV <ref type="bibr" target="#b24">[25]</ref> 2.41 530MB 138M MV (IMDB-WIKI) <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-arts on FG-NET</head><p>As shown in Tab. 6, we compare our model with stateof-the-art models on FG-Net. Without training 82 models, we randomly repeat the experiment ten times. This is also challenging because we use less train dataset. In fact, Han <ref type="bibr" target="#b9">[10]</ref>, Luu <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> in Tab. 6 are also use the different splits. Using mean-variance loss, MV <ref type="bibr" target="#b24">[25]</ref> with pre-trained process gets the best result of 2.68. While our result with pretrained process is 2.95 MAE and 0.17 std, i.e., the second best performance compared with Bulky models. In addition, without any pre-trained process, our result of 4.09 is slightly better than MV [25] of 4.10. In general, the validation on FG-NET demonstrate the effectiveness of C3AE.  <ref type="bibr" target="#b42">[43]</ref> 3.62 530MB 138M Liu et al. <ref type="bibr" target="#b18">[19]</ref> 3.89 530MB 138M DEX <ref type="bibr" target="#b28">[29]</ref> 4.63 530MB 138M DEX (WIKI-IMDB) <ref type="bibr" target="#b28">[29]</ref> 3.09 530MB 138M MV <ref type="bibr" target="#b24">[25]</ref> 4.10 530MB 138M MV (WIKI-IMDB) <ref type="bibr" target="#b24">[25]</ref> 2.68 530MB 138M C3AE (Scratch)</p><p>4.09 ± 0.19 0.25MB 39.7K C3AE (WIKI-IMDB)</p><p>2.95 ± 0.17 0.25MB 39.7K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a compact model, C3AE, that has achieved state-of-the-art performance among compact models and competitive performance among bulky models. From various ablation study, we have demonstrated the effectiveness of C3AE. For the small/medium-size image and model, some analysis and rethinking are given. In the future work, we will evaluate the effectiveness of our observation on general datasets and applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our compact model on age estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 )</head><label>3</label><figDesc>or K = 20 (third row inFig. 3) is given. If K = 10, the set of bins is {10, 20, 30, 40, 50, 60, 70, 80} and y n is 68, the corresponding vector representation is y n = [0, 0, 0, 0, 0, 0.2, 0.8, 0]. This operation assigns a distribution to the label, and will not incur any additional cost on distribution labeling. Moreover, the distribution of twopoints representation is sparse. 0, 0, 0, 0, 0, 0.2, 0.8, 0, 0, 0) 74 (0, 0, 0, 0, 0, 0, 0.6, 0.4, 0, 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A new definition on the age estimation by twopoints representation. Any point is represented by two adjacent bins instead of any other two or more bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>L+</head><label></label><figDesc>kl (y n ,ŷ n ) = n D KL (y n |ŷ n ) + λ||W 1 || 1 λ||W 1 || 1 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparison on the training process of M-V2, S-V2, SSR and our plain model.(Best viewed in color and magnifier.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall architecture of the compact plain model</figDesc><table><row><cell>Layer</cell><cell cols="5">Kernel Stride Output size Parameters MACC</cell></row><row><cell>Image</cell><cell>-</cell><cell>1</cell><cell>64*64*3</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Conv1 3*3*32 1</cell><cell>62*62*32</cell><cell>896</cell><cell>3321216</cell></row><row><cell>BRA</cell><cell>-</cell><cell>1</cell><cell>31*31*32</cell><cell>128</cell><cell>-</cell></row><row><cell cols="3">Conv2 3*3*32 1</cell><cell>29*29*32</cell><cell>9248</cell><cell>7750656</cell></row><row><cell>BRA</cell><cell>-</cell><cell>1</cell><cell>14*14*32</cell><cell>128</cell><cell>-</cell></row><row><cell cols="3">Conv3 3*3*32 1</cell><cell>12*12*32</cell><cell>9248</cell><cell>1327104</cell></row><row><cell>BRA</cell><cell>-</cell><cell>1</cell><cell>6*6*32</cell><cell>128</cell><cell>-</cell></row><row><cell cols="3">Conv4 3*3*32 1</cell><cell>4*4*32</cell><cell>9248</cell><cell>147456</cell></row><row><cell>BN+ReLu</cell><cell>-</cell><cell>1</cell><cell>4*4*32</cell><cell>128</cell><cell>-</cell></row><row><cell cols="3">Conv5 1*1*32 1</cell><cell>4*4*32</cell><cell>1056</cell><cell>16384</cell></row><row><cell>Feat</cell><cell cols="2">1*1*12 1</cell><cell>12</cell><cell>6156</cell><cell>-</cell></row><row><cell>Pred</cell><cell>1*1*1</cell><cell>1</cell><cell>1</cell><cell>13</cell><cell>-</cell></row><row><cell>Total</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36377</cell><cell>-</cell></row><row><cell cols="6">(BRA) indicates batch normalization(BN), Relu and average pooling.</cell></row></table><note>(MACC) Here we only count MACC of the conv layer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparsion among SSR, M-V2, S-V2 and C3AE.</figDesc><table><row><cell cols="6">Methods Comb. M-MAE I-MAE W-MAE Param. Memory MACC</cell></row><row><cell></cell><cell>(0.25, 4)</cell><cell>3.72</cell><cell>7.23</cell><cell cols="2">7.29 107129 808.7KB 2.2M</cell></row><row><cell></cell><cell>(0.25, 6)</cell><cell>4.26</cell><cell>7.01</cell><cell cols="2">7.30 153561 994.7KB 3.0M</cell></row><row><cell></cell><cell>(0.5, 4)</cell><cell>3.71</cell><cell>6.76</cell><cell cols="2">6.76 354713 1.8MB 5.7M</cell></row><row><cell>M-V2</cell><cell>(0.5, 6)</cell><cell>4.05</cell><cell>6.75</cell><cell cols="2">6.83 518857 2.5MB 8.1M</cell></row><row><cell></cell><cell>(0.75, 4)</cell><cell>3.24</cell><cell>6.57</cell><cell cols="2">6.49 747961 3.4MB 12.3M</cell></row><row><cell></cell><cell>(0.75, 6)</cell><cell>4.10</cell><cell>6.69</cell><cell cols="2">6.72 1102537 4.8MB 17.7M</cell></row><row><cell></cell><cell cols="2">(0.25, 0.5) 4.85</cell><cell>8.22</cell><cell>8.78</cell><cell>76589 1.0MB 0.6M</cell></row><row><cell></cell><cell>(0.25, 1)</cell><cell>4.11</cell><cell>7.67</cell><cell cols="2">8.02 464185 2.6MB 4.0M</cell></row><row><cell></cell><cell>(0.5, 0.5)</cell><cell>4.11</cell><cell>7.66</cell><cell cols="2">8.04 155753 1.3MB 1.4M</cell></row><row><cell>S-V2</cell><cell>(0.5, 1)</cell><cell>3.83</cell><cell>7.40</cell><cell cols="2">7.63 1284087 5.9MB 12.7M</cell></row><row><cell></cell><cell cols="2">(0.75, 0.5) 3.98</cell><cell>7.55</cell><cell cols="2">7.91 250829 1.7MB 2.5M</cell></row><row><cell></cell><cell>(0.75, 1)</cell><cell>3.63</cell><cell>7.07</cell><cell cols="2">7.19 2473043 10.7MB 26.1M</cell></row><row><cell cols="3">SSR Full model 3.16</cell><cell>6.94</cell><cell>6.76</cell><cell>40915 326.4KB 17.6M</cell></row><row><cell cols="3">C3AE Plain model 3.13</cell><cell>6.57</cell><cell>6.44</cell><cell>36345 197.8KB 12.8M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The Speed analysis</figDesc><table><row><cell cols="7">evaluation our-plain SSR M-v2(.5,6) M-v2(.75,6) S-v2(.5,1) S-v2(.75,1)</cell></row><row><cell>MACC (M)</cell><cell>12.8</cell><cell>17.6</cell><cell>8.1</cell><cell>17.7</cell><cell>12.7</cell><cell>26.1</cell></row><row><cell cols="4">runtime-cpu(s) 0.0126 0.0233 0.0245</cell><cell>0.0394</cell><cell>0.0228</cell><cell>0.0295</cell></row><row><cell cols="4">runtime-gpu(s) 0.0029 0.0050 0.0070</cell><cell>0.0080</cell><cell>0.0080</cell><cell>0.0082</cell></row><row><cell>MAE</cell><cell>3.13</cell><cell>3.16</cell><cell>4.05</cell><cell>4.10</cell><cell>3.83</cell><cell>3.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The role of residual module and SE</figDesc><table><row><cell>Datasets</cell><cell cols="3">w/o Res+w/o SE w. Res w. SE</cell></row><row><cell>Morph II</cell><cell>3.13</cell><cell>3.21</cell><cell>3.11</cell></row><row><cell>IMDB</cell><cell>6.57</cell><cell>6.66</cell><cell>6.50</cell></row><row><cell>WIKI</cell><cell>6.44</cell><cell>6.57</cell><cell>6.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Evaluation of cascade and context module. finetune the hyperparameters α as 5, 8, 10, 12 and 15 on our full model, and the corresponding results are 2.79, 2.79, 2.75, 2.79 and 2.80, respectively. These results does not change too much. It shows the robustness of our model.</figDesc><table><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAE</cell><cell>2.8</cell><cell></cell><cell></cell><cell></cell><cell>w/o Cascade</cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell>Cascade</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cascade + Context</cell></row><row><cell></cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5e-5</cell><cell>5e-4</cell><cell>5e-3</cell><cell>5e-2</cell><cell>5e-1</cell><cell>5e0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Lambda</cell><cell></cell></row><row><cell></cell><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparsion with state-of-the-arts that use compact and bulky basic models on Morph II.</figDesc><table><row><cell>Type</cell><cell>Methods</cell><cell cols="2">MAE Memory Parameters</cell></row><row><cell></cell><cell>ORCNN [24]</cell><cell cols="2">3.27 1.7MB 479.7K</cell></row><row><cell></cell><cell>MRCNN [24]</cell><cell cols="2">3.42 1.7MB 479.7K</cell></row><row><cell>Compact</cell><cell>DenseNet [14] MobileNet-V1 [12]</cell><cell cols="2">5.05 1.1MB 242.0K 6.50 1.0MB 226.3K</cell></row><row><cell></cell><cell>SSR [38]</cell><cell cols="2">3.16 0.32MB 40.9K</cell></row><row><cell></cell><cell>Ranking CNN [3]</cell><cell>2.96 2.2GB</cell><cell>500M</cell></row><row><cell></cell><cell>Hot</cell><cell></cell></row><row><cell>Bulky</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>25] 2.16 530MB 138M C3AE Full model (Scratch) 2.78 0.25MB 39.7K Full model (IMDB-WIKI) 2.75 0.25MB 39.7K</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state-of-the-arts on FG-Net.</figDesc><table><row><cell>Methods</cell><cell>MAE</cell><cell cols="2">Memory Parameters</cell></row><row><cell>Geng et al. [7]</cell><cell>5.77</cell><cell>-</cell><cell>-</cell></row><row><cell>Han et al. [10]</cell><cell>4.80</cell><cell>-</cell><cell>-</cell></row><row><cell>Luu et al. [21]</cell><cell>4.37</cell><cell>-</cell><cell>-</cell></row><row><cell>Luu et al. [22]</cell><cell>4.12</cell><cell>-</cell><cell>-</cell></row><row><cell>Wang et al. [37]</cell><cell>4.26</cell><cell>-</cell><cell>-</cell></row><row><cell>Feng et al. (1) [4]</cell><cell>4.35</cell><cell cols="2">530MB 138M</cell></row><row><cell>Feng et al. (2) [4]</cell><cell>4.09</cell><cell cols="2">530MB 138M</cell></row><row><cell>Zhu et al. (Actual) [43]</cell><cell>4.58</cell><cell cols="2">530MB 138M</cell></row><row><cell>Zhu et al. (Synthesized)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">(-) in the whole manuscript indicates value not available, or also useless for comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anchored regression networks applied to age estimation and super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using ranking-cnn for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caojin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human facial age estimation by cost-sensitive label ranking and trace norm regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congyan</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="136" to="148" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Age synthesis and estimation via faces: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongzi</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDMW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Heterogeneous face attribute estimation: A deep multi-task learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Demographic estimation from face images: Human vs. machine performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1148" to="1161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Squeeze-and-excitation networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cognition in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Hutchins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ordinal deep feature learning for facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Age estimation using active appearance models and support vector machine regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching Y</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics: Theory, Applications, and Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contourlet appearance model for facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching Y</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output cnn for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mean-variance loss for deep age estimation from a face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamirat</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some like it hot-visual guidance for preference prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeply-learned feature for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ssr-net: A compact soft stagewise regression network for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Cheng</forename><surname>Hsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Age estimation by multiscale convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083.1</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Quantifying facial age by posterior of age comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09687</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Facial aging and rejuvenation by conditional multiadversarial autoencoder with ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02740</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
