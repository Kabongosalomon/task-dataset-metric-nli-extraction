<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
							<email>valeo.aimaxime.bucher@valeo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne Université</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
							<email>valeo.aituan-hung.vu@valeo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne Université</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<email>valeo.aimatthieu.cord@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne Université</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne Université</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called "generalized" zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps. Code and models are available at: https: // github. com/ valeoai/ ZS3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation has achieved great progress using convolutional neural networks (CNNs). Early CNN-based approaches classify region proposals to generate segmentation predictions <ref type="bibr" target="#b16">[17]</ref>. FCN [28]  was the first framework adopting fully convolutional networks to address the task in an endto-end manner. Most recent state-of-the-art models like UNet [37], SegNet [3], DeepLabs [10, 11], PSPNet [50] are FCN-based. An effective strategy for semantic segmentation is to augment CNN features with contextual information, e.g. using atrous/dilated convolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">46]</ref>, pyramid context pooling <ref type="bibr" target="#b49">[50]</ref> or a context encoding module <ref type="bibr" target="#b46">[47]</ref>.</p><p>Segmentation approaches are mainly supervised, but there is an increasing interest in weaklysupervised segmentation models using annotations at the image-level <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> or box-level <ref type="bibr" target="#b12">[13]</ref>. We propose in this paper to investigate a complementary learning problem where part of the classes are missing altogether during the training. Our goal is to re-engineer existing recognition architectures to effortlessly accommodate these never-seen, a.k.a. unseen, categories of scenes and objects. No manual annotations or real samples, only unseen labels are needed during training. This line of works is usually coined zero-shot learning (ZSL).</p><p>ZSL for image classification has been actively studied in recent years. Early approaches address it as an embedding problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>. They learn how to map image data and class descriptions into a common space where semantic similarity translates into spacial proximity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are different variants in the literature on how the projections or the similarity measure are computed: simple linear projection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>, non-linear multi-modal embeddings <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43]</ref> or even hybrid methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Recently, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref> proposed to generate synthetic instances of unseen classes by training a conditional generator from the seen classes. There are very few extensions of zero-shot learning to other tasks than classification. Very recently <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref> attacked object detection: unseen objects are detected while box annotation is only available for seen classes. As far as we know, there is no approach considering zero-shot setting for segmentation.</p><p>In this paper, we introduce the new task of zero-shot semantic segmentation (ZS3) and propose an architecture, called ZS3Net, to address it: Inspired by most recent zero shot classification apporaches, we combine a backbone deep net for image embedding with a generative model of class-dependent features. This allows the generation of visual samples from unseen classes, which are then used to train our final classifier with real visual samples from seen classes and synthetic ones from unseen classes. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the potential of our approach for visual segmentation. We also propose a novel self-training step in a relaxed zero-shot setup where unlabelled pixels from unseen classes are already available at training time. The whole zero-shot pipeline including self-training is coined as ZS5Net (ZS3Net with Self-Supervision) in this work.</p><p>Lastly, we further extend our model by exploiting contextual cues from spatial region relationship. This strategy is motivated by the fact that similar objects not only share similar properties but also similar contexts. For example, 'cow' and 'horse' are often seen in fields while most 'motorbike' and 'bicycle' show in urban scenes.</p><p>We report evaluations of ZS3Net on two datasets (Pascal-VOC and Pascal-Context) and in zero-shot setups with varying numbers of unseen classes. Compared to a ZSL baseline, our method delivers excellent performances, which are further boosted using self-training and semantic contextual cues. A common idea is to transfer semantic similarities between linguistic identities from some suitable text embedding space to a visual representation space. Effectively, classes like 'zebra' and 'donkey' that share a lot of semantic attributes are likely to stay closer in the representation space than very different classes, 'bird' and 'television' for instance. Such a joint visual-text perspective enables statistical training of zero-shot recognition models.</p><p>We address in this work the problem of learning a semantic segmentation network capable of discriminating between a given set of classes where training images are only available for a subset of it. To this end, we start from an existing semantic segmentation model trained with a supervised loss on seen data (DeepLabv3+ in <ref type="figure" target="#fig_1">Fig. 2</ref>). This model is limited to trained categories and, hence,  <ref type="formula">(2)</ref> fine-tuning classification layer. In (1), the generator, conditioned on the word2vec (w2c) embedding of seen classes' labels, learns to generate synthetic features that match real DeepLab's ones on seen classes. Later in <ref type="formula">(2)</ref>, the classifier is trained to classify real features from seen classes and synthetic ones from unseen classes. At run-time, the classifier operates on real DeepLab features stemming from both types of classes.</p><p>unable to recognize new unseen classes. In <ref type="figure" target="#fig_0">Figure 1</ref>-(middle) for instance, the person (a seen class) is correctly segmented unlike the motorbike (an unseen class) whose pixels are wrongly predicted as a mix of 'person', 'bicycle' and 'background'.</p><p>To allow the semantic segmentation model to recognize both seen and unseen categories, we propose to generate synthetic training data for unseen classes. This is obtained with a generative model conditioned on the semantic representation of target classes. This generator outputs pixel-level multi-dimensional features that the segmentation model relies on (blue zone 1 in <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>Once the generator is trained, many synthetic features can be produced for unseen classes, and combined with real samples from seen classes. This new set of training data is used to retrain the classifier of the segmentation network (orange zone 2 in <ref type="figure" target="#fig_1">Fig. 2</ref>) so that it can now handle both seen and unseen classes. At test time, an image is passed through the semantic segmentation model equipped with the retrained classification layer, allowing prediction for both seen and unseen classes. <ref type="figure" target="#fig_0">Figure 1</ref>-(right) shows a result after this procedure, with the model now able to delineate correctly the motorbike category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture and learning of ZS3Net</head><p>We denote the set of all classes as C = S ∪ U, with S the set of seen classes and U the set of unseen ones, where S ∩ U = ∅. Each category c ∈ C can be mapped through word embedding to a vector representation a[c] ∈ R da of dimension d a . In the experiments, we will use to this end the 'word2vec' model <ref type="bibr" target="#b29">[30]</ref> learned on a dump of the Wikipedia corpus (approx. 3 billion words). This popular embedding is based on the skip-gram language model which is learned through predicting the context (nearby words) of words in the target dictionary. As a result of this training strategy, words that frequently share common contexts in the corpus are located in close proximity in the embedded vector space. In other words, this semantic representation is expected to capture geometrically the semantic relationship between the classes of interest.</p><p>In FCN-based segmentation frameworks, the input images are forwarded through an encoding stack consisting of fully convolutional layers, which results in smaller feature maps compared to the original resolution. Effectively, logit prediction maps have small resolutions and often require an additional up-sampling step to match the input size <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>. In the current context, with a slight abuse of notation, we attach each spatial location on the encoded feature map to a pixel of input image down-sampled to a comparable size. We can therefore assign class labels to encoded features and construct training data in the feature space. From now on, 'pixel' will refer to pixel locations in this down-sampled image. Definition and collection of pixel-wise data (step 0). We start from DeepLabv3+ semantic segmentation model <ref type="bibr" target="#b10">[11]</ref>, pre-trained with a supervised loss on annotated data from seen classes. Based on this architecture, we need to choose suitable features, out of several feature maps that can be used independently for classification. Conversely, the classifier to be later fine-tuned must be able to operate on individual pixel-wise features. As a result, we choose the last 1 × 1 convolutional classification layer of DeepLabv3+ and the features it ingests as our classifier f and targeted features x respectively.</p><formula xml:id="formula_0">Let the training set D s = {(x s i , y s i , a s i )} be composed of triplets where x s i ∈ R M ×N ×dx is a d x -dimensional feature map, y s i ∈ S M ×N</formula><p>is the associated ground-truth segmentation map and a s i ∈ R M ×N ×da is the class embedding map that associates to each pixel the semantic embedding of its class.</p><p>We note that M × N is the resolution of the encoded feature maps, as well as of the down-sampled image and segmentation ground-truth. <ref type="bibr" target="#b0">1</ref> For the K = |U| unseen classes, no training data is available, only the category embeddings a[c], c ∈ U.</p><p>On seen data, the DeepLabv3+ model is trained with full-supervision using the standard cross-entropy loss. After this training phase, we remove the last classification layer and only use the remaining network for extracting seen features, as illustrated in blue part (1) of <ref type="figure" target="#fig_0">Figure 1</ref>. To avoid supervision leakage from unseen classes <ref type="bibr" target="#b43">[44]</ref>, we retrain the backbone network on ImageNet <ref type="bibr" target="#b37">[38]</ref> on seen classes solely. Details are given later in Section 3.1. Generative model (step 1). Key to our approach is the ability to generate image features conditioned on a class embedding vector, without access to any images of this class. Given a random sample z from a fixed multivariate Gaussian distribution, and the semantic description a, new pixel features will be generated as x = G(a, z; w) ∈ R dx , where G is a trainable generator with parameters w. Toward this goal, we can leverage any generative approach like GAN <ref type="bibr" target="#b17">[18]</ref>, GMMN <ref type="bibr" target="#b26">[27]</ref> or VAE <ref type="bibr" target="#b21">[22]</ref>. This feature generator is trained under supervision of features from seen classes.</p><p>We follow <ref type="bibr" target="#b6">[7]</ref> and adopt the "Generative Moment Matching Network" (GMMN) <ref type="bibr" target="#b26">[27]</ref> for the feature generator. GMNN is a parametric random generative process G using a differential criterion to compare the target data distribution and the generated one. The generative process will be considered as good if, for each semantic description a, two random populations X (a) ⊂ R dx from D s and X (a; w) sampled with the generator have low maximum mean discrepancy (a classic divergence measure between two probability distributions):</p><formula xml:id="formula_1">L GMMN (a) = x,x ∈X (a) k(x, x ) + x, x ∈ X (a;w) k( x, x ) − 2 x∈X (a) x∈ X (a,w) k(x, x)</formula><p>, where k is a kernel that we choose as Gaussian, k(x, x ) = exp(− 1 2σ 2 x − x 2 ) with bandwidth parameter σ. The parameters w of the generative network are optimized by Stochastic Gradient Descent <ref type="bibr" target="#b4">[5]</ref>. Classification model (step 2). Similar to DeepLabv3+, the classification layer f consists of a 1 × 1 convolutional layer. Once G is trained in step 1, arbitrarily many pixel-level features can be sampled for any classes, unseen ones in particular. We build this way a synthetic unseen training set</p><formula xml:id="formula_2">D u = {( x u j , y u j , a u j )} of triplets in R dx × U × R da .</formula><p>Combined with the real features from seen classes in D s , this set of synthetic features for unseen categories allows the fine-tuning of the classification layer f . The new pixel-level classifier for categories in C becomes y = f (x; D u , D s ). It can be used to conduct the semantic segmentation of images that exhibit objects from both types of classes. Zero-shot learning and self-training. Self-training is a useful strategy in semi-supervised learning that leverages a model's own predictions on unlabelled data to heuristically obtain additional pseudoannotated training data <ref type="bibr" target="#b51">[52]</ref>. Assuming that unlabelled images with objects from unseen classes are now available (this is thus a relaxed setting compared to pure ZSL), such a self-supervision can be mobilized to improve our zero-shot model. The trained ZS3Net (one gotten after step 2) can indeed be used to "annotate" these additional images automatically, and for each one, the top p% of the most confident among these pseudo-labels provide new training features for unseen classes. The semantic segmentation network is then retrained accordingly. We coin this new model ZS5Net for ZS3Net with Self-Supervision.</p><p>Note that there exists a connection between ZS5Net and transductive zero-shot learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>, but that our model is not transductive. Indeed, under purely transductive settings, no data even unlabelled, is available at train time for unseen classes. Differently, our ZS5Net learns from a mix of labelled and unlabelled training data, and is evaluated on a different test set (effectively, a form of semi-supervised learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3:</head><p>Graph-context encoding in the generative pipeline. The segmentation mask is encoded as an adjacency graph of semantic connected components (represented as nodes with different colors in the graph). Each semantic node is attached to its corresponding word2vec embedding vector. The generative process is conditioned on this graph. The generated output is also a graph with the same structure as the input's, except that attached to each output node is a generated visual feature. Best viewed in color.</p><p>Graph-context encoding. Understanding and utilizing contextual information is very important for semantic segmentation, especially in complex scenes. Indeed, by design, FCN-based architectures already encode context with convolutional layers. For more context encoding, DeepLabv3+ applies several parallel dilated convolutions at different rates. In order to reflect this mechanism we propose to leverage contextual cues for our feature generation. To this end, we introduce a graph-based method to encode the semantic context for complex scenes with lots of objects as ones in the Pascal-Context dataset.</p><p>In general, the structural object arrangement contains informative cues for recognition. For example, it is common to see 'dog' sitting on 'chairs' but very rare to have 'horses' doing the same thing. Such spatial priors are naturally captured by a form of relational graph, that can be constructed in different ways, e.g., using manual sketches or semantic segmentation masks of synthetic scenes. What matters most is the relative spatial arrangement, not the precise shapes of the objects. As a proof of concept, we simply exploit true segmentation masks during the training of the generative model. It it important to note that the images associated to these masks are not used during training if they contain unseen classes.</p><p>A segmentation mask is represented by the adjacency graph G = (V, E) of its semantic connected components: each node corresponds to one connected group of single class labels, and two such groups are neighbors if they share a boundary (hence being of two different classes). We re-design the generator to accept G as additional input using graph convolutional layers <ref type="bibr" target="#b22">[23]</ref>. As shown in <ref type="figure">Figure 3</ref>, each input node ν ∈ V is represented by concatenating its corresponding semantic embedding a ν with a random Gaussian sample z ν . This modified generator outputs features attached to the nodes of the input graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental details</head><p>Datasets. Experimental evaluation is done on the two datasets: Pascal-VOC 2012 <ref type="bibr" target="#b14">[15]</ref> and Pascal-Context <ref type="bibr" target="#b30">[31]</ref>. Pascal-VOC contains 1, 464 training images with segmentation annotations of 20 object classes. Similar to <ref type="bibr" target="#b10">[11]</ref>, we adopt additional supervision from semantic boundary annotations <ref type="bibr" target="#b18">[19]</ref> during training. Pascal-Context provides dense semantic segmentation annotations for Pascal-VOC 2010, which comprises 4, 998 training and 5, 105 validation images of 59 object/stuff classes.</p><p>Zero-shot setups. We consider different zero-shot setups varying in number of unseen classes, we randomly construct the 2-, 4-, 6-, 8and 10-class unseen sets. We extend the unseen set in an incremental manner, meaning that for instance the 4unseen set contains the 2-. Details of the splits are as follows:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics.</head><p>In our experiments we adopt standard semantic segmentation metrics <ref type="bibr" target="#b27">[28]</ref>, i.e. pixel accuracy (PA), mean accuracy (MA) and mean intersection-over-union (mIoU). Similar to <ref type="bibr" target="#b43">[44]</ref>, we also report harmonic mean (hIoU) of seen and unseen mIoUs. The reason behind choosing harmonic rather than arithmetic mean is that seen classes often have much higher mIoUs, which will significantly dominate the overall result. As we expect ZS3 models to produce good performance for both seen and unseen classes, the harmonic mean is an interesting indicator.</p><p>A zero-shot semantic segmentation baseline. As a baseline, we adapt the ZSL classification approach in <ref type="bibr" target="#b15">[16]</ref> to our task. To this end, we first modify the vanilla segmentation network, i.e. DeepLabv3+, to not produce class-wise probabilistic predictions for all the pixels, but instead to regress corresponding semantic embedding vectors. Effectively, the last classification layer of DeepLabv3+ is replaced by a projection layer which transforms 256−channel features maps into 300−channel word2vec embedding maps. The model is trained to maximize the cosine similarity between the output and target embeddings.</p><p>At run-time, the label predicted at a pixel is the one with the text embedding which the most similar (in cosine sense) to the regressed embedding for this pixel. Implementation details. We adopt the DeepLabv3+ framework <ref type="bibr" target="#b10">[11]</ref> built upon the ResNet-101 backbone <ref type="bibr" target="#b19">[20]</ref>. Segmentation models are trained by SGD <ref type="bibr" target="#b4">[5]</ref> optimizer using polynomial learning rate decay with the base learning rate of 7e −3 , weight decay 5e −4 and momentum 0.9.</p><p>The GMMN is a multi-layer perceptron with one hidden layer, leaky-RELU non-linearity <ref type="bibr" target="#b28">[29]</ref> and dropout <ref type="bibr" target="#b40">[41]</ref>. In our experiments, we fix the number of hidden neurons to 256 and set the kernel bandwidths as {2, 5, 10, 20, 40, 60}. These hyper-parameters are chosen with the "zero-shot cross-validation procedure" in <ref type="bibr" target="#b6">[7]</ref>. The input Gaussian noise has the same dimension as used w2c embeddings, namely 300. The generative model is trained using Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with the learning rate of 2e −4 . To encode the graph context as described in Section 2.2, we replace the linear layers in GMMN by graph convolutional layers <ref type="bibr" target="#b22">[23]</ref>, with no change in other hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Zero-shot semantic segmentation</head><p>We report in <ref type="table" target="#tab_1">Tables 1 and 3</ref> results on Pascal-VOC and Pascal-Context datasets, according to the three metrics. Instead of only evaluating on the unseen set (which does not show the strong prediction bias toward seen classes), we jointly evaluate on all classes and report results for seen, unseen, and all classes. Such an evaluation protocol is more challenging for zero-shot settings, known as "generalized ZSL evaluation" <ref type="bibr" target="#b8">[9]</ref>. In both <ref type="table" target="#tab_1">Tables 1 and 3</ref>, we report in the first line the 'oracle' performance of the model trained with full-supervision on the complete dataset (including both seen and unseen).</p><p>Pascal-VOC. <ref type="table" target="#tab_1">Table 1</ref> reports segmentation performance on 5 different splits comparing the ZSL baseline and our approach ZS3Net. We observe that the embedding-based ZSL baseline (DeViSe <ref type="bibr" target="#b15">[16]</ref>), while nicely performing on seen classes, produces much worse results for the unseen. We conducted additional experiments, adapting ALE <ref type="bibr" target="#b0">[1]</ref> with K = 2. They yielded 68.1% and 4.6% mIoU for seen and unseen classes (harmonic mean of 8.6%), on a par with the DeViSe-based baseline.</p><p>Not strongly harmed by the bias toward seen classes, the proposed ZS3Net provides significant gains (in PA, MA and most importantly mIoU) on the unseen classes, e.g. +32.2% mIoU in the 2-split, similarly large gaps in other splits. As for seen classes, the ZS3Net performs comparably to the ZSL baseline, with slight improvement in some splits. Overall, we have favourable results on all the classes using our generative approach. The last column of <ref type="table" target="#tab_1">Table 1</ref> shows harmonic mean of the seen and unseen mIoUs, denoted as hIoU (%). As discussed in 3.1, the harmonic mean is a good indicator of effective zero-shot performance. Again according to this metric, ZS3Net outperforms the baseline by significant margins.</p><p>As mentioned in 2.2, our framework is agnostic to the choice of the generative model. We experimented a variant of ZS3Net based on GAN <ref type="bibr" target="#b44">[45]</ref>, which turned out to be on a par with the reported GMMN-based one. In our experiments, GMMN was chosen due to its better stability.</p><p>In all experiments, we only report results with the generalized ZSL evaluation. <ref type="table" target="#tab_2">Table 2</ref> shows the difference between this evaluation protocol and the common vanilla one. In the vanilla case, prediction scores of seen classes are completely ignored, only unseen scores are used to classify the unseen objects. As a result, this evaluation protocol does not show how well the models discriminate the seen and unseen pixels. In zero-shot settings, predictions are mostly biased toward seen classes given the strong supervision during training. To clearly reflect such a bias, the generalized ZSL evaluation is a better choice. We see that the ZSL baseline achieves reasonably good result using the vanilla ZSL evaluation while showing much worse figures with the generalized one. With ZS3Net, leveraging both real features from seen classes and synthetic ones from unseen classes to train the classifier helps reduce the performance bias toward the seen set.</p><p>The two first examples in <ref type="figure" target="#fig_2">Fig. 4</ref> illustrate the merit of our approach. Model trained only on seen classes ('w/o ZSL') interprets unseen objects as background or as one of the seen classes. For example, 'cat' and 'plane' (unseen) are detected as 'dog' and 'boat' (seen); a large part of the 'plane' is considered as 'background'. The proposed ZS3Net correctly recognizes these unseen objects.</p><p>Pascal-Context. In <ref type="table" target="#tab_3">Table 3</ref>, we provide results on the Pascal-Context dataset, a more challenging benchmark compared to Pascal-VOC. Indeed Pascal-Context scene pixels are densely annotated with 59 object/stuff classes, compared to only a few annotated objects (most of the time 1-2 objects) per scene in Pascal-VOC. As a result, segmentation models universally report lower performance on Pascal-Context. Regardless of such difference, we observe similar behaviors from the ZSL baseline and our method. The ZS3Net outperforms the baseline by significant margins on all evaluation metrics. We emphasize the important improvements on the seen classes, as well as the overall harmonic mean of the seen and unseen mIoUs. We visualize in the two last rows of <ref type="figure" target="#fig_2">Figure 4</ref> qualitative ZS3 results on Pascal-Context. Unseen objects, i.e. 'cow' and 'boat', while being wrongly classified as some seen classes without ZSL, can be fully recognized by our zero-shot framework.</p><p>As mentioned above, Pascal-Context scenes are more complex with much denser object annotations. We argue that, in this case, context priors on object arrangement convey beneficial cues to improve recognition performance. We have introduced a novel graph-context mechanism to encode such prior in Section 2.2. Proposed models enriched with this graph context, denoted as 'ZS3Net + GC' in <ref type="table" target="#tab_3">Table 3</ref>, show consistent improvements over the ZS3Net models. We note that for semantic segmentation, the pixel accuracy metric (PA) is biased toward the more dominant classes and might suggest misleading conclusions <ref type="bibr" target="#b11">[12]</ref>, as opposed to IoU metrics. We report performance of ZS5Net (ZS3Net with self-training) on Pascal-VOC and Pascal-Context in <ref type="table" target="#tab_4">Table 4</ref> (with different splits according to datasets). For performance comparison, the reader is referred to ZS3Net results in <ref type="table" target="#tab_1">Tables 1 and 3</ref>. Through zero-shot cross-validation we fixed the  percentage of high-scoring unseen pixels as p = 25% for Pascal-VOC and p = 75% for Pascal-Context. We show in <ref type="figure" target="#fig_3">Figure 5</ref> the influence of this percentage on the final performance. In general, the additional self-training step strongly boosts the performance in seen, unseen and all classes. Remarkably, on the 2-unseen split in both datasets, the overall performance in all metrics is very close to the supervised performance (reported in the first lines of <ref type="table" target="#tab_1">Tables 1 and 3</ref>). <ref type="figure" target="#fig_4">Figure 6</ref> shows semantic segmentation results on Pascal-VOC and Pascal-Context datasets. On both cases, self-training helps to disambiguate pixels wrongly classified as seen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Zero-shot segmentation with self-training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we introduced a deep model to deal with the task of zero-shot semantic segmentation. Based on zero-shot classification, our ZS3Net model combines rich text and image embeddings, generative modeling and classic classifiers to learn how to segment objects from already seen classes as well as from new, never-seen ones at test time. First of its kind, proposed ZS3Net shows good behavior on the task of zero shot semantic segmentation, setting competitive baselines on various benchmarks. We also introduced a self-training extension of the approach for scenarios where unlabelled pixels from unseen classes are available at training time. Finally, a graph-context encoding has been used to improve the semantic class representation of ZS3Net when facing complex scenes.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Introducing and addressing zero shot semantic segmentation. In this example, there are no 'motorbike' examples in the training set. As a consequence, a supervised model (middle) fails on this object, seeing it as a mix of the seen classes person , bicycle and background . With proposed ZS3Net method (right), pixels of the never-seen motorbike class are recognized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our deep ZS3Net for zero-shot semantic segmentation. The figure is separated into two parts by colors corresponding to: (1) training generative model and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on Pascal-VOC and Pascal-Context. (a) Input image, (b) semantic segmentation ground-truth, (c) segmentation without zero-shot learning, (d) results with proposed ZS3Net. Unseen classes: plane cat cow boat ; Some seen classes: dog bird horse dining-table . Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Influence of parameter p on ZS5Net. Evolution of the mIoU performance as a function of percentage of high-scoring unseen pixels, on the 2-unseen classes split from Pascal-VOC and Pascal-Context datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Zero shot segmentation with self-training. (a) Input image, (b) semantic segmentation ground-truth, (c) segmentation with ZS3Net, (d) result with additional self-training (ZS5Net). Unseen classes: motorbike sofa ; Some seen classes car chair . Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Additional qualitative results on Pascal-VOC. From top to bottom, results of the 2-, 4-, 6-, 8and 10-unseen set-ups. (a) Input image, (b) semantic segmentation ground-truth, (c) segmentation without zero-shot learning, (d) results with proposed ZS3Net. Unseen classes: motorbike plane cat sofa train chair ; Some seen classes: bicycle boat dog person background . Best seen in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Additional qualitative results on Pascal-Context. From top to bottom, results of the 2-, 4-, 6-, 8and 10-unseen set-ups. (a) Input image, (b) semantic segmentation ground-truth, (c) segmentation without zero-shot learning, (d) results with proposed ZS3Net. Unseen classes: cow cat boat bird plane ; Some seen classes: horse dog background sky . Best viewed in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Additional qualitative results with self-training on Pascal-VOC. From top to bottom, results of the 2-, 4-, 6-, 8and 10-unseen set-ups. (a) Input image, (b) semantic segmentation groundtruth, (c) segmentation with ZS3Net, (d) result with additional self-training (ZS5Net). Unseen classes: motorbike plane cat train chair; Some seen classes: bicycle boat person background . Best seen in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Additional qualitative results with self-training on Pascal-Context. From top to bottom, results of the 2-, 4-, 6-, 8and 10-unseen set-ups. (a) Input image, (b) semantic segmentation ground-truth, (c) segmentation with ZS3Net, (d) result with additional self-training (ZS5Net). Unseen classes: cow cat sofa boat bird plane; Some seen classes: horse dog background sky . Best viewed in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Zero-shot semantic segmentation on Pascal-VOC.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell>Unseen</cell><cell></cell><cell cols="2">Overall</cell></row><row><cell cols="2">K Model</cell><cell></cell><cell cols="3">PA MA mIoU</cell><cell cols="3">PA MA mIoU</cell><cell cols="2">PA MA mIoU hIoU</cell></row><row><cell></cell><cell cols="2">Supervised</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.7 87.2</cell><cell>76.9</cell><cell>-</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">92.1 79.8</cell><cell>68.1</cell><cell cols="2">11.3 10.5</cell><cell>3.2</cell><cell>89.7 73.4</cell><cell>44.1</cell><cell>6.1</cell></row><row><cell>2</cell><cell>ZS3Net</cell><cell></cell><cell cols="2">93.6 84.9</cell><cell>72.0</cell><cell cols="2">52.8 53.7</cell><cell>35.4</cell><cell>92.7 81.9</cell><cell>68.5</cell><cell>47.5</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">89.9 72.6</cell><cell>64.3</cell><cell cols="2">10.3 10.1</cell><cell>2.9</cell><cell>86.3 62.1</cell><cell>38.9</cell><cell>5.5</cell></row><row><cell>4</cell><cell>ZS3Net</cell><cell></cell><cell cols="2">92.0 78.3</cell><cell>66.4</cell><cell cols="2">43.1 45.7</cell><cell>23.2</cell><cell>89.8 72.1</cell><cell>58.2</cell><cell>34.4</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">79.5 45.1</cell><cell>39.8</cell><cell>8.3</cell><cell>8.4</cell><cell>2.7</cell><cell>71.1 38.4</cell><cell>33.4</cell><cell>5.1</cell></row><row><cell>6</cell><cell>ZS3Net</cell><cell></cell><cell cols="2">85.5 52.1</cell><cell>47.3</cell><cell cols="2">67.3 60.7</cell><cell>24.2</cell><cell>84.2 54.6</cell><cell>40.7</cell><cell>32.0</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">75.8 41.3</cell><cell>35.7</cell><cell>6.9</cell><cell>5.7</cell><cell>2.0</cell><cell>68.3 34.7</cell><cell>24.3</cell><cell>3.8</cell></row><row><cell>8</cell><cell>ZS3Net</cell><cell></cell><cell cols="2">81.6 31.6</cell><cell>29.2</cell><cell cols="2">68.7 62.3</cell><cell>22.9</cell><cell>80.3 43.3</cell><cell>26.8</cell><cell>25.7</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell cols="2">68.7 33.9</cell><cell>31.7</cell><cell>6.7</cell><cell>5.8</cell><cell>1.9</cell><cell>60.1 26.9</cell><cell>16.9</cell><cell>3.6</cell></row><row><cell>10</cell><cell>ZS3Net</cell><cell></cell><cell cols="2">82.7 37.4</cell><cell>33.9</cell><cell cols="2">55.2 45.7</cell><cell>18.1</cell><cell>79.6 41.4</cell><cell>26.3</cell><cell>23.6</cell></row><row><cell></cell><cell>Models</cell><cell cols="5">Generalized eval. Vanilla eval.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell>1.9</cell><cell></cell><cell>41.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ZS3Net</cell><cell></cell><cell>18.1</cell><cell></cell><cell>46.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Generalized-vs. vanilla ZSL eval-</figDesc><table /><note>uation. Results are reported with mIoU met- ric on the 10-unseen split from Pascal-VOC dataset.Pascal-VOC: 2-cow/motorbike, 4-airplane/sofa, 6-cat/tv, 8-train/bottle, 10-chair/potted-plant; Pascal-Context: 2-cow/motorbike, 4-sofa/cat, 6-boat/fence, 8-bird/tvmonitor, 10-keyboard/aeroplane.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Zero-shot semantic segmentation on Pascal-Context.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell>Unseen</cell><cell></cell><cell cols="2">Overall</cell></row><row><cell cols="2">K Model</cell><cell cols="3">PA MA mIoU</cell><cell cols="3">PA MA mIoU</cell><cell cols="3">PA MA mIoU hIoU</cell></row><row><cell></cell><cell>Supervised</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.9 52.4</cell><cell>42.2</cell><cell>-</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">70.2 47.7</cell><cell>35.8</cell><cell cols="2">9.5 10.2</cell><cell>2.7</cell><cell>66.2 43.9</cell><cell>33.1</cell><cell>5.0</cell></row><row><cell>2</cell><cell cols="3">ZS3Net ZS3Net + GC 73.0 52.9 71.6 52.4</cell><cell>41.6 41.5</cell><cell cols="2">49.3 46.2 65.8 62.2</cell><cell>21.6 30.0</cell><cell>71.2 52.2 72.6 53.1</cell><cell cols="2">41.0 28.4 41.3 34.8</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">66.2 37.9</cell><cell>33.4</cell><cell>9.0</cell><cell>8.4</cell><cell>2.5</cell><cell>62.8 34.6</cell><cell>30.7</cell><cell>4.7</cell></row><row><cell>4</cell><cell cols="3">ZS3Net ZS3Net + GC 70.3 49.1 68.4 46.1</cell><cell>37.2 39.5</cell><cell cols="2">58.4 53.3 61.0 56.3</cell><cell>24.9 29.1</cell><cell>67.8 46.6 69.0 49.7</cell><cell cols="2">36.4 29.8 38.6 33.5</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">60.8 36.7</cell><cell>31.9</cell><cell>8.8</cell><cell>8.0</cell><cell>2.1</cell><cell>55.9 33.5</cell><cell>28.8</cell><cell>3.9</cell></row><row><cell>6</cell><cell cols="3">ZS3Net ZS3Net + GC 64.5 42.7 63.3 38.0</cell><cell>32.1 34.8</cell><cell cols="2">63.6 55.8 57.2 53.3</cell><cell>20.7 21.6</cell><cell>63.3 39.8 64.2 43.7</cell><cell cols="2">30.9 25.2 33.5 26.7</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">54.1 24.7</cell><cell>22.0</cell><cell>7.3</cell><cell>6.8</cell><cell>1.7</cell><cell>49.1 20.9</cell><cell>19.2</cell><cell>3.2</cell></row><row><cell>8</cell><cell cols="3">ZS3Net ZS3Net + GC 53.0 27.1 51.4 23.9</cell><cell>20.9 22.8</cell><cell cols="2">68.2 59.9 68.5 61.1</cell><cell>16.0 16.8</cell><cell>53.1 28.7 54.6 31.4</cell><cell cols="2">20.3 18.1 22.0 19.3</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">50.0 20.8</cell><cell>17.5</cell><cell>5.7</cell><cell>5.0</cell><cell>1.3</cell><cell>45.1 16.8</cell><cell>14.3</cell><cell>2.4</cell></row><row><cell>10</cell><cell cols="3">ZS3Net ZS3Net + GC 50.3 27.9 53.5 23.8</cell><cell>20.8 24.0</cell><cell cols="2">58.6 43.2 62.6 47.8</cell><cell>12.7 14.1</cell><cell>52.8 27.0 51.2 31.0</cell><cell cols="2">19.4 15.8 22.3 17.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot with self-training. ZS5Net results on Pascal-VOC and Pascal-Context datasets.</figDesc><table><row><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell>Unseen</cell><cell></cell><cell cols="2">Overall</cell></row><row><cell cols="2">K Dataset</cell><cell cols="2">PA MA mIoU</cell><cell cols="2">PA MA mIoU</cell><cell cols="3">PA MA mIoU hIoU</cell></row><row><cell>2</cell><cell cols="2">VOC Context 72.9 53.6 94.3 85.2</cell><cell>75.7 41.8</cell><cell>89.5 89.9 81.0 78.1</cell><cell>75.8 55.5</cell><cell>94.2 85.9 71.8 50.6</cell><cell>75.8 42.0</cell><cell>75.7 47.7</cell></row><row><cell>4</cell><cell cols="2">VOC Context 66.0 46.3 93.9 84.8</cell><cell>74.0 37.5</cell><cell>57.5 62.9 86.4 82.8</cell><cell>53.0 45.1</cell><cell>92.4 80.9 68.0 48.5</cell><cell>69.8 38.0</cell><cell>61.8 41.0</cell></row><row><cell>6</cell><cell cols="2">VOC Context 59.7 42.2 93.8 82.0</cell><cell>71.2 34.6</cell><cell>68.3 61.2 80.9 76.8</cell><cell>53.1 36.0</cell><cell>92.1 75.8 62.1 45.8</cell><cell>66.1 35.2</cell><cell>60.8 35.2</cell></row><row><cell>8</cell><cell cols="2">VOC Context 51.8 34.1 92.6 77.8</cell><cell>68.3 28.5</cell><cell>68.2 62.0 76.2 71.3</cell><cell>50.0 24.1</cell><cell>90.2 71.9 54.3 39.5</cell><cell>61.3 27.8</cell><cell>57.7 26.1</cell></row><row><cell>10</cell><cell cols="2">VOC Context 46.8 32.3 90.1 83.9</cell><cell>72.3 27.0</cell><cell>57.8 48.0 70.2 57.1</cell><cell>34.5 20.7</cell><cell>86.8 66.9 49.5 36.4</cell><cell>54.4 26.0</cell><cell>46.7 23.4</cell></row><row><cell></cell><cell cols="2">(a) Input image</cell><cell>(b) GT</cell><cell></cell><cell>(c) ZS3Net</cell><cell></cell><cell cols="2">(d) ZS5Net</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Final softmax and decision layers operate in effect after logits are up-sampled back to full resolution.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMPSTAT</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating visual representations for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study and analysis of generalized zero-shot learning for object recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis. Zero-shot object detection by hybrid region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkan</forename><surname>Demirel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gundeep</forename><surname>Vinay Kumar Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot recognition using dual visual-semantic mapping paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuetan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transductive unbiased embedding for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A simple exponential family framework for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Zero shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
