<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AuGPT: Dialogue with Pre-trained Language Models and Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonáš</forename><surname>Kulhánek</surname></persName>
							<email>jonas.kulhanek@live.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Robotics and Cybernetics</orgName>
								<orgName type="institution">Czech Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojtěch</forename><surname>Hudeček</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Nekvinda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
							<email>odusek@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AuGPT: Dialogue with Pre-trained Language Models and Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention-based pre-trained language models such as GPT-2 brought considerable progress to end-to-end dialogue modelling. However, they also present considerable risks for taskoriented dialogue, such as lack of knowledge grounding or diversity. To address these issues, we introduce modified training objectives for language model finetuning, and we employ massive data augmentation via backtranslation to increase the diversity of the training data. We further examine the possibilities of combining data from multiples sources to improve performance on the target dataset. We carefully evaluate our contributions with both human and automatic methods. Our model achieves state-of-the-art performance on the MultiWOZ data and shows competitive performance in human evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike traditional task-oriented systems based on modularized pipelines <ref type="bibr" target="#b39">(Young et al., 2013;</ref><ref type="bibr" target="#b9">Gao et al., 2019)</ref>, end-to-end dialogue systems integrate nearly all functionality required to hold a dialogue into a single neural network <ref type="bibr" target="#b36">(Wen et al., 2017;</ref><ref type="bibr" target="#b7">Eric et al., 2017;</ref><ref type="bibr" target="#b15">Lei et al., 2018)</ref>, reducing error-propagation and data annotation requirements. While these systems are not yet ready for production use, they made considerable progress in recent years, especially with the advent of pretrained neural language models (LMs) <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b26">Radford et al., 2019;</ref><ref type="bibr" target="#b42">Zhang et al., 2020c)</ref>. Systems such as GPT-2 finetuned by <ref type="bibr" target="#b0">Budzianowski and Vulić (2019)</ref> show that with an LM pre-trained on a large number of generaldomain dialogues without annotation, only small amounts of data are required to perform in a given task-oriented domain.</p><p>The pre-trained LMs still run enormous risks. First, solely training for response generation may result in a lack of grounding for the responses, where the LM hallucinates words without any relation to the database. This has been addressed by multi-task training and auxiliary training objectives <ref type="bibr" target="#b43">(Peng et al., 2020)</ref> to an extent. Second, finetuning on small datasets may reduce response diversity and fluency due to neural networks' known propensity for catastrophic forgetting <ref type="bibr" target="#b10">(Greco et al., 2019)</ref> -the model overfits the finetuning set too tightly, "forgetting" the language modeling capabilities learned during pre-training.</p><p>This paper presents an end-to-end model for multi-domain task-oriented response generation on the MultiWOZ data <ref type="bibr" target="#b1">(Budzianowski et al., 2018)</ref>, 1 where we address the above problems with pretrained LMs. AuGPT is based on the DialoGPT pre-trained LM. To improve response consistency, we build upon <ref type="bibr" target="#b43">Peng et al. (2020)</ref>'s auxiliary training objectives and introduce improvements. For higher output diversity and fluency, we apply massive training data augmentation through additional task-oriented datasets and back-translation <ref type="bibr" target="#b28">(Sennrich et al., 2016)</ref>. In addition to standard automatic evaluation, we evaluate our system in a shared task human evaluation campaign, where it reaches high performance. We also include a detailed error analysis on a sample of dialogues collected in-house by experts.</p><p>Our contributions can be summarized as follows:</p><p>• We show that augmenting data using backtranslation markedly improves performance in task-oriented settings.</p><p>• We introduce an improved dialogue consistency classification task used as an auxiliary training objective, based on subtle changes to the dialogue state (instead of replacing the state with a random one). This brings additional performance improvement.</p><p>• To further improve output diversity, we experiment with unlikelihood loss .</p><p>• We provide results for our model and comparisons to multiple baselines on MultiWOZ versions 2.0 and 2.1. To our knowledge, our system obtains state-of-the-art results for an end-to-end model on this dataset.</p><p>To advocate research transparency and reproducibility, we publish our augmented training data, source code, and pre-trained models on GitHub. 2 2 Related Work While the first attempts to build generative endto-end task-oriented systems mimicked the traditional dialogue system components <ref type="bibr" target="#b36">(Wen et al., 2017)</ref>, the task was soon recast as a sequence prediction problem in a two-stage setup. A sequenceto-sequence (seq2seq) model first generates the belief state based on dialogue context, then generates the system response based on the context and the belief state (Sequicity; <ref type="bibr" target="#b15">Lei et al., 2018)</ref>. The basic recurrent seq2seq architecture <ref type="bibr" target="#b30">(Sutskever et al., 2014)</ref> was enhanced with various attention mechanisms to support copying tokens from the context into the belief state or from the belief state onto the output <ref type="bibr" target="#b29">Shu et al., 2019)</ref>. Database operations typically stay outside the model, but some approaches even include the database in the model via attention over possible entries <ref type="bibr" target="#b7">(Eric et al., 2017;</ref><ref type="bibr" target="#b21">Madotto et al., 2018)</ref>.</p><p>Recently, large-scale task-oriented datasets were proposed <ref type="bibr" target="#b1">(Budzianowski et al., 2018;</ref><ref type="bibr" target="#b2">Byrne et al., 2019;</ref><ref type="bibr" target="#b27">Rastogi et al., 2020)</ref> to encourage research in the field of multi-domain dialogue modeling. Even though the aforementioned approaches achieved promising results on smaller domains such as Cam-Rest676 <ref type="bibr" target="#b35">(Wen et al., 2016)</ref>, their ability to generalize over multiple domains remains limited <ref type="bibr" target="#b41">(Zhang et al., 2020b)</ref>. To address this, <ref type="bibr" target="#b40">Zhang et al. (2020a)</ref> introduce the LABES-S2S model that -in addition to a two-stage seq2seq approach -models belief states as discrete latent variables. <ref type="bibr" target="#b41">Zhang et al. (2020b)</ref> present DAMD, a three-stage seq2seq architecture which explicitly decodes the system action. They optimize for multiple good actions given a single belief state.</p><p>The line of research that is closest to our work makes use of large pre-trained LMs based on the transformer architecture <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> such as GPT-2 <ref type="bibr" target="#b26">(Radford et al., 2019)</ref> or BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. Finetuned variants of these models achieved state-of-the-art results on many NLP tasks, e.g., question answering or natural language inference , and they seem to be useful even in the context of dialogue systems. For example, <ref type="bibr" target="#b37">Wu et al. (2020)</ref> propose finetuning BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> for task-oriented dialogue on multiple datasets; <ref type="bibr" target="#b42">Zhang et al. (2020c)</ref> extended the GPT-2 LM to model open-domain chit-chat.</p><p>We follow research initiated by <ref type="bibr" target="#b0">Budzianowski and Vulić (2019)</ref>, who use GPT-2 to model multidomain task-oriented dialogues. Recently, three similar modifications to their model were proposed, namely SOLOIST <ref type="bibr" target="#b43">(Peng et al., 2020)</ref>, SimpleTOD <ref type="bibr">(Hosseini-Asl et al., 2020)</ref>, and the approach by <ref type="bibr" target="#b11">Ham et al. (2020)</ref>. Our work extends these models and proposes a novel training approach and data augmentation strategies based on back-translation <ref type="bibr" target="#b4">(Edunov et al., 2018;</ref><ref type="bibr" target="#b8">Federmann et al., 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The task-oriented setting requires the dialogue system to respond adequately to the user's input and fulfill its goal. The goal could be, e.g., booking a train or requesting restaurant details. To achieve that, the system has to process the user's input, keep track of the belief state with respect to user preferences regarding individual in-domain attributes (slots) and generate a relevant response in natural language. The system also must be able to interact with an external database to incorporate the necessary information into the generated response (see <ref type="figure">Figure 1</ref> for an example).</p><p>Due to its excellent language modeling and language generation capabilities, we have chosen the pre-trained GPT-2 LM as our system's backbone architecture. Similarly to <ref type="bibr" target="#b0">Budzianowski and Vulić (2019)</ref>, we use the LM to model both the belief state and the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Representation</head><p>The training instances for an LM-based taskoriented dialogue system can be considered as tuples <ref type="bibr">(c, b, d, r)</ref>, where r is the system's response, c is the context (i.e., a concatenation of all previous utterances in the dialogue -both system's and user's), b is the system's belief state which is  <ref type="figure">Figure 1</ref>: The architecture of AuGPT. The pipeline runs in two stages. First, a finetuned GPT-2 LM is used to predict a belief. Then the database results are obtained and everything is passed to the GPT-2 again to predict a final delexicalized response, along with possible auxiliary tasks (belief consistency, intent classification, system action classification). Unlikelihood loss is used for response prediction training. also used for querying the database, and d are the database results.</p><p>In our case, the dialogue system handles multiple domains and the belief state is a set of pairs (domain name, domain belief ), where the domain belief is an assignment of values into slots, i.e., a set of pairs (slot name, value) (see Example 1). Similarly, the database results d are a set of pairs (domain name, domain database results), where the domain database results are an ordered list of entities returned by the database. We further define the database result counts d c denoting the number of results in d for each domain.</p><p>Ideally, we would like our system to model the probability distribution over possible responses conditioned on the context p(r|c). To simplify computation and model the interaction with an external database, this distribution can be factorized as follows:</p><formula xml:id="formula_0">p(r|c) = d p(r|d, c)p(d|c) = d b p(r|d, b, c)p(d|b)p(b|c) = b p(r|Query(b), b, c)p(b|c) ,<label>(1)</label></formula><p>where p(d|b) is a deterministic distribution over the database results, and Query is a function returning database results. By using this formulation and by modeling p(r|d, b, c) and p(b|c), our model would be able to process the context, query the database, and generate the response based on the database results. However, we would face a problem with data sparsity when estimating parameters of p(r|d, b, c). The reason for the data sparsity is the relatively small size of datasets for task-oriented dialogues and the responses containing underrepresented, sometimes unique words, such as reference numbers, hotel names, etc. To maximally reuse the training samples, we choose to train our model on delexicalized responses <ref type="bibr" target="#b34">(Wen et al., 2015)</ref> denoted r, where slot values are replaced with placeholders. During inference, the responses are lexicalized back deterministically using both the belief state and the database results. We assume perfect lexicalization, i.e., always being able to lexicalize the responser back based on d and b. <ref type="bibr">3</ref> Both the database lookup and the lexicalization are deterministic, and the delexicalized responsē r does not depend on the database results d, but only on their counts d c . Therefore, the distribution p(r|d, b, c) is equal to the distribution p(r|d c , b, c), and by maximizing its likelihood we are achieving the goal of maximizing the likelihood of p(r|c).</p><p>We use the same language modelp to model the belief state and to generate the delexicalized prediction. That is,</p><formula xml:id="formula_1">p(r|d c , b, c) ≈p(r|d c , b, c, θ) (2) p(b|c) ≈p(b|∅, ∅, c, θ) ,<label>(3)</label></formula><p>where we denote the model's parameters as θ.</p><p>In the MultiWOZ dataset <ref type="bibr" target="#b1">(Budzianowski et al., 2018;</ref><ref type="bibr">Eric et al., 2020, see Section 4)</ref>, responses are delexicalized by replacing concrete values with placeholder tokens of the form domain_slot. For better generalization across domains, we chose to use only slot instead. We had noticed it was never the case that a response would involve more than one domain. Therefore, we decided to train our model to detect the active domain and used the predicted active domain during the final lexicalization. The model predicts the active domain by outputting it as the first domain in the belief state. The other domains then follow in lexicographical order. The disadvantage of this approach is that we cannot determine the active domain if the belief state is empty. However, in such a case the lexicalization would fail anyway, so the system's performance is not affected by this decision. To generate the belief state and to input the database result counts to our model, we need a string representation. To fully exploit pre-training on natural language texts, we have chosen a compact representation containing as few special tokens as possible (see Example 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training</head><p>Although the parameters are shared for the belief state predictor and the delexicalized response predictor, the training objectives slightly differ. We use the cross-entropy loss for both predictions. For the response prediction, the unlikelihood loss  is used as an additional objective. The unlikelihood loss gives a penalty for each repeated token, which helps the model avoid repetitions and makes frequent words less likely, increasing the answers' diversity.</p><p>To help the model learn a better internal representation from the data, we employ additional auxiliary tasks. Similarly to <ref type="bibr" target="#b3">Devlin et al. (2019)</ref> and <ref type="bibr" target="#b43">Peng et al. (2020)</ref>, we train a binary classifier to detect dialogue inconsistencies. In each training batch, we corrupt half of the samples by randomly applying one or more of the following changes with the same probability:</p><p>1. We replace the belief state b with another belief state, sampled uniformly randomly from the training data.</p><p>2. We replace the delexicalized responser with a different randomly chosen one. If this change is applied in combination with the first one, the delexicalized response and the belief state are taken from the same random sample.</p><p>3. A different valid value is uniformly sampled for each slot in the belief state. In this case, the domain names and domain order are unchanged (i.e., the active domain is the same).</p><p>The first two changes are the same as those applied by <ref type="bibr" target="#b43">Peng et al. (2020)</ref>, whereas the third one is a new one which we find very useful in the context of multiple domains, where it is much more challenging to detect if the belief state was changed when the domain names are kept the same. The consistency detection binary classifier is trained to recognize negative samples from the positive ones based on logits of the last response token. It is represented by an affine classifier trained using binary cross-entropy (BCE). We also experiment with additional two classifiers predicting the user intent and the system action. These are implemented as two fullyconnected layers attached to the feature representations of the last context token and the last database result token, respectively. However, based on our experimental results, we decided not to use these tasks in the final model.</p><p>We train the whole pipeline by optimizing the non-weighted sum of individual component losses, i.e., cross-entropy for the belief state and the response prediction, unlikelihood loss for the response, and BCE for the consistency detection are summed in our final system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Response Generation</head><p>For each user input, the system transitions through several stages before the final response is generated. First, only the previous dialogue context is passed to the LM, which greedily generates the string representation of the belief state. The belief state is then parsed and passed to the database handler. The database handler then constructs a query and returns a set of results for each domain.</p><p>We take the number of results for each domain and generate the string representation of database result counts (see Example 1). All strings are concatenated and again passed to the language model. This time, we utilize the nucleus sampling <ref type="bibr" target="#b12">(Holtzman et al., 2020)</ref> to generate the delexicalized response. We found nucleus sampling useful for generating the response since it increases diversity, but we prefer greedy decoding for the belief state with a fixed structure. Finally, the tokens in the delexicalized response are substituted by values from the database results and the belief state. The process is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Augmentation</head><p>Following its successful usage in other NLP tasks, <ref type="bibr" target="#b14">(Konstas et al., 2017;</ref><ref type="bibr" target="#b5">Elder et al., 2020)</ref>, we experiment with data augmentation using paraphrases, i.e., variants of training utterances with different surface forms. In our setup, we generate multiple paraphrases for each training utterance and use them to augment the training data. This way, we effectively increase the variability of the data.</p><p>Generating paraphrases is not a trivial process. Various data-driven approaches were proposed, the majority of them corpora-based <ref type="bibr" target="#b20">(Madnani and Dorr, 2010)</ref>. Recently, machine translation systems proved strong performance in generating paraphrases using the back-translation procedure <ref type="bibr" target="#b28">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b4">Edunov et al., 2018;</ref><ref type="bibr" target="#b8">Federmann et al., 2019)</ref>. We take advantage of these findings and use a trained multilingual machine translation model <ref type="bibr" target="#b19">(Macháček et al., 2020;</ref><ref type="bibr" target="#b4">Edunov et al., 2018)</ref> to paraphrase our data. We employ ten intermediate languages and thus obtain a set of different paraphrases for each input utterance. When training, we choose the input user utterance uniformly at random from the set of all variants of the utterance including the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We consider a series of experiments to compare our model to current state-of-the-art methods, and include human evaluation and detailed error analysis. We also carefully evaluate all proposed contributions through a series of ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We have used several datasets for training our system and for the final evaluation and comparison.</p><p>We use MultiWOZ 2.1, an enhanced version of MultiWOZ 2.0 <ref type="bibr" target="#b1">(Budzianowski et al., 2018)</ref> that reduces the amount of noise in the data; we also use the 2.0 version in additional experiments so that we can compare to previous works. The dataset contains 7 distinct domains (all related to tourist information, e.g., hotels, restaurants) and 10,438 dialogues, 7,032 of which are multi-domain. We experiment with pre-training our model on additional datasets. For the pre-training phase, we use Taskmaster-1 <ref type="bibr" target="#b2">(Byrne et al., 2019)</ref> and Schema-Guided Dialogue <ref type="bibr" target="#b27">(Rastogi et al., 2020)</ref>. Both Taskmaster-1 and Schema-Guided Dialogue are multi-domain, task-oriented, large dialogue corpora consisting of 12,215 and 22,825 dialogues, respectively. Taskmaster-1 was obtained using the Wizard-of-Oz and self-dialogue methods, while the collection of Schema-Guided Dialogue is somewhat artificial -humans are only employed to paraphrase machine-generated utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Preprocessing</head><p>Although the MultiWOZ 2.1 dataset was collected by humans, it contains a lot of inconsistencies. We hypothesize that when using only clean samples which are consistent with the database, the benefit of using higher quality training data outweighs the decrease in the number of training samples. This claim is further supported by experiments (see the Ablation section). To filter the training data, we choose only those dialogues where the annotated dialogue goal corresponds with the turn-level annotated data. When using the clean samples, we omit about 30% of the training data.</p><p>To effectively combine all our datasets, we unified the domain-slot pairs in the belief states and the delexicalization. However, the datasets use different naming conventions (e.g., leaveAt vs. leave_at) and different domain and slot names even though the corresponding domain-slot pairs describe the same concepts (e.g., restaurant-food vs. restaurant-type). Therefore, we created a new unified ontology and manually designed a mapping between slot names. Notably, we decided to rename some slots so they use natural language tokens, as we base our model on the GPT-2 LM which is pre-trained on natural language texts (e.g. "leaveAt" → "leave at"). Our final ontology that unifies all three datasets contains 22 domains and 135 slots.</p><p>We use our own implementation of delexicalization, which directly produces our belief state string  representation (see Section 3.1 and Example 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details</head><p>We implement our model in the PyTorch framework <ref type="bibr" target="#b24">(Paszke et al., 2019)</ref>. The model extends the small variant of the GPT-2 model. It consists of 12 transformer blocks with a model layer size equal to 768, having 124 million parameters in total. For all auxiliary tasks, we use a dropout of 0.1 with label smoothing 0.1. We use the AdamW optimizer <ref type="bibr" target="#b18">(Loshchilov and Hutter, 2019)</ref>. For greater training effectiveness, we employ mixed-precision training <ref type="bibr" target="#b22">(Micikevicius et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Corpus-based Evaluation</head><p>To compare with previous results on MultiWOZ, we evaluate the model performance with a set of corpus-based intrinsic metrics on both versions of the data. In the case of MultiWOZ 2.0, we use the original delexicalization used also by other compared methods <ref type="bibr" target="#b43">(Peng et al., 2020;</ref><ref type="bibr">Hosseini-Asl et al., 2020;</ref><ref type="bibr" target="#b41">Zhang et al., 2020b)</ref>. For MultiWOZ 2.1, we use our own delexicalization. We employ the original evaluation scheme by <ref type="bibr" target="#b1">Budzianowski et al. (2018)</ref>, which provides two metrics -the inform rate and the success rate. The inform rate is the percentage of dialogues in which the system provided an appropriate entity, whereas the success rate is the percentage of dialogues in which the system outputted all the requested information. Additionally, we compute the BLEU score <ref type="bibr" target="#b23">(Papineni et al., 2002</ref>) between the generated system utterances and the ground truth to get an approximation of the output fluency. Note that both the inform rate and the success rate are unaffected by using a different delexicalization and these metrics can be directly compared to other methods. A different delexicalization could, however, render a slightly different BLEU, but based on preliminary results, we believe this change has almost no effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ConvLab 2 Evaluation</head><p>We use the ConvLab 2 platform <ref type="bibr" target="#b43">(Zhu et al., 2020)</ref> for automatic evaluation. The platform includes an agent-based evaluation component, therefore we obtain results gathered from interaction between our system and the simulated user agent. We run the evaluation component 1,000 times, i.e. on 1,000 simulated conversations. The agent mimics user behavior, interacts with the system under evaluation, and computes multiple metrics, among which the most relevant are complete, success and book rates.  The complete rate reflects the ratio of dialogues that are completed, i.e. all the user requests have been met. The success rate computes the percentage of dialogues which are successful, meaning the system captures correct informed entities and provides a valid booking if requested. Finally, the book rate is the proportion of dialogues where the system was able to book the correct entity (hotel, restaurant, train) if it was asked to. We also compute precision, recall and F1 score for the informed entities and the average number of turns in the dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Human Evaluation and Error Analysis</head><p>Thanks to our participation in a task-oriented dialogue shared task, the best one of our submissions was evaluated by human judges on the Amazon Mechanical Turk platform. The judges communicated with the agent in natural language and rated the system afterward with respect to the success/failure of the dialogue, language understanding score, and response appropriateness. Information provided by the system was additionally checked for consistency with the database, and the average of success rates given by the judges and by database grounding is used as the main metric.</p><p>In addition to the crowdsourced evaluation, we perform a detailed in-house error analysis of the model behavior based on human interactions with our final system. Expert annotators followed randomly chosen dialogue goals accompanying the MultiWOZ test set and recorded any incorrect system behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, we first describe and discuss the quantitative results for both corpus-based and crowdsourced human evaluation. In the second part, we also perform a qualitative analysis of the model behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to State-of-the-Art on</head><p>MultiWOZ <ref type="table">Table 1</ref> shows a comparison between our methods and current state-of-the-art systems, which are described in the Related Work section. Since Mul-tiWOZ 2.1 has been released quite recently and some of the compared methods do not provide results with this version, we report results on both MultiWOZ 2.0 and MultiWOZ 2.1. As we can see, AuGPT outperforms all other approaches in terms of the inform and success metrics. However, DAMD and LABES-S2S produce higher BLEU scores. This would indicate better fluency of these models, however, one would need human evaluation to confidently claim that. One possible reason for this behavior would be our removal of some training samples (see Data Preprocessing), which may have decreased the BLEU score. Importantly, thanks to the higher success metric, we can say that our model is better at providing all the necessary information in the responses. <ref type="table" target="#tab_2">Table 2</ref> shows a comparison with two other models in the ConvLab evaluation scheme with a simulated user. The compared systems were chosen because they both implement fully trainable end-toend methods. Our system outperforms both compared systems by a wide margin. Our model is able to perform well not just in a single-turn response generation scenario, but over the course of the whole dialogue. As the example of DAMD shows, this is not always guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluation</head><p>We show the human evaluation results in <ref type="table" target="#tab_5">Table 3</ref>. Our model outperformed the baseline on the main average success metric (along with only half of other submissions). We did not match the winning submission in terms of average success, but we achieved the best overall success rate based on human judgments (without checking consistency with the database). This suggests that the biggest drawback of our model was the integration with database grounding. This can be caused by our   modified delexicalization, which makes it harder in some cases to match the database properly. In addition, our model needed the smallest number of turns overall to complete the evaluation dialogues. We note that out of 5 submitted model variants in the competition (cf. <ref type="table">Table 6</ref>), only the model variant scoring best in the automatic evaluation was considered for human evaluation. In our case, the chosen model variant did not include additional data pre-training, which could possibly improve performance in the human evaluation scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detailed Error Analysis</head><p>Our expert annotators evaluated 130 dialogues in total, 50 of which contained at least one error. However, in most cases, the system was able to recover from the errors, resulting in an overall success rate of 86.9% (i.e., 17 unsuccessful dialogues).</p><p>The purpose of this analysis was to find out what kind of errors occur during real interaction. The annotators were familiar with the model architecture and were instructed to categorize the errors according to the cause of the problem. Specifically, they identified which component caused the respective error based on the internal system state. In addition, the annotators categorized the errors into more specific types.</p><p>The overall results are given in <ref type="table" target="#tab_7">Table 4</ref>. We <ref type="bibr">[...]</ref> user: I want to make a booking at sitar tandoori belief: restaurant {name = sitar tandoori} train {arrive by = 18:00, day = sunday, departure = cambridge, destination = london liverpool street, people = 1} system: What day would you like the booking for? user: the same day, 13:30 belief: restaurant {day = sunday, name = sitar tandoori, people = 1, time = 13:30} train {arrive by = 18:00, day = sunday, departure = cambridge, destination = london liverpool street, people = 1} system: Booking was successful. The table will be reserved for 15 minutes. Reference number is: 00000200.</p><p>Example 2: A dialogue where the system did not ask for necessary information. The user previously booked a train trip for one person (cf. the belief state in the first turn shown). When they went on to book a restaurant, the system took over the number of persons from the train ride and did not ask again.</p><p>observe that the most common reason for a failed dialogue is an error related to the belief state. Also, although policy errors happen relatively often, they rarely cause the whole dialogue to fail. We observe that we have a slightly higher number of successful dialogues compared to the 82.6% success rate (without checking database consistency) found in human evaluation (cf. <ref type="table" target="#tab_5">Table 3</ref>). The most likely user: I'm looking for a theatre in Cambridge belief: attraction {type = theatre} system: There are 5 theatres in Cambridge. Is there an area of town you prefer? user: no I don't care belief: attraction {type = theatre} system: I recommend Adc Theatre in the centre. Would you like more information? user: no, I'm also looking for some place to stay belief: attraction {type = theatre} system: I have 5 theatres in the centre. Do you have a price range? user: I'm looking for a place to stay belief: attraction {type = theatre} system: There are 5 theatres in the centre. Do you have a preference for area? user: I'm looking for a hotel belief: hotel {type = hotel} attraction {type = theatre} system: There are 9 hotels in the north. Do you have a price range?</p><p>Example 3: Dialogue sample with a bad domain focus and a hallucination. First, the system ignores that the user switched from searching for a theater to searching for a hotel. After accepting the new domain, the system replies with hotels "in the north" even though the user did not specify.</p><p>cause of this behavior is that our expert annotators were more motivated to recover from erroneous system behavior and finish the dialogue. Fine-grained error types identified by annotators are given in <ref type="table" target="#tab_8">Table 5</ref>. We now provide examples for some of the frequent types which deserve more detail. By far the most common error was hallucinated values -lack of grounding for some of the information provided (see the end of Example 2). One of the causes of this error, as in the case of this example, were hallucinations coming from the decoder. This probably stems from an imperfect delexicalization of some of the training samples. Another error type is missing information, i.e., not asking for information that is required (Example 3). In this case, the system uses information from a different domain without the user explicitly confirming this. A most probable cause of this is that most instances of the training data carry over the information. A third common error type is bad domain (see the beginning of Example 2). Here, the system does not react to the user's request for a different search (hotels instead of attractions in the example). This might be caused by a less frequent wording of the request in the given context, and usually gets resolved by rephrasing the request.</p><p>Apart from the errors, many examples of nontrivial positive behavior were found during the anal- ysis. As illustrated in Example 4, the model is able to deal with paraphrases and is robust to a certain amount of noise in the data. Specifically, it handles typos, various time formats, etc. Interaction between domains is also successful in most casesthe system is able to resolve references to another domain's belief state (e.g., make a booking for the same group of people as done before in a different venue).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Ablation Study</head><p>We tested many variants of our method with different combinations of our proposed system's components to evaluate their contributions. The results are presented in <ref type="table">Table 6</ref>. Namely, we are interested in the following components: (1) the unlikelihood loss, (2) the auxiliary tasks, (3) the data augmentation, (4) the modified consistency task and (5) unclean data filtering.</p><p>We can see that all proposed contributions which are a part of our final system have a positive effect on the system performance with respect to the primary metrics. We can see that removing either the pre-training or the back-translations decreases the BLEU score and, more importantly, the success rates. Furthermore, we notice the positive effect of using our improved consistency detection task over the one used in SOLOIST <ref type="bibr" target="#b43">(Peng et al., 2020)</ref>, which in turn scores better than no consistency detection.</p><p>Removing either the unlikelihood loss or training on all data as opposed to only "clean" samples clearly reduces performance. However, we did not notice any increase in performance when the user intent prediction and system action prediction auxiliary tasks were used (cf. Section 3.2). The reason for this behavior could be that the model learns to represent the actions well enough implicitly, without the need for these additional objectives. How-  <ref type="table">Table 6</ref>: Ablation study (inf = inform, suc = success, book = book rate; see Section 4.4 for a description of metrics). The model version with the best ConvLab 2 success rate is chosen as our best model. Variants are denoted with their respective modifications compared to the default: "w/o. unlikelihood" = unlikelihood loss was not used for training; "w/o. clean" uses all training samples as opposed to using only the ones consistent with the database; "w/o. pre-training" = the additional Taskmaster-1 and Schema-Guided datasets were not used for training; "all auxiliary" = using two additional auxiliary tasks (see the Method section for details); "w/o. consistency" = dialogue consistency task is not used; "old consistency" refers to the consistency task as defined by <ref type="bibr" target="#b43">Peng et al. (2020)</ref> (see the Section 3.2 for details).</p><p>ever, these tasks are not a part of our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions &amp; Future Work</head><p>We present a dialogue modeling pipeline based on the pre-trained GPT-2 language model. AuGPT uses modified training objectives and employs data augmentation to increase the diversity of generated utterances. Our experiments show that the proposed approach performs better than state-ofthe-art baselines in a multi-domain scenario on the MultiWOZ dataset. We also run a series of ablation experiments to assess the individual contributions of the modifications. According to our detailed ablation study, training data augmentation using back-translation via multiple languages and a modified auxiliary training objective for dialogue consistency detection are the features that contribute most to our system's performance. Additionally, we perform a qualitative analysis of the outputs to give a better insight into our model behavior.</p><p>In the future, we plan to construct a latent representation of the belief state and optimize it jointly with the language model. We will replace the deterministic lexicalization with a trainable alternative, and possibly even integrate the database module into the model. To improve the transfer to new domains, we will learn a domain embedding and optimize it jointly with the model, unifying all datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>ConvLab evaluation comparison with other works (see Section 4.5 for a description of the metrics).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation results obtained during the shared task using Amazon Mechanical Turk. Note that only 4 submissions outperformed the Baseline according to the average success metric.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Interactive analysis performed by human evaluators using 130 prepared dialogue goals. 17 of these dialogues contained an error that caused the dialogue to fail. We show summary statistics regarding the number of respective error sources (BS = belief state, DB = database). Note that some of the dialogues contain more than one error.</figDesc><table><row><cell>Type</cell><cell>Count</cell><cell>Source</cell><cell>Description</cell></row><row><cell>Hallucinated values</cell><cell>21</cell><cell>BS/Policy</cell><cell>Used a slot value in the reply that is not grounded in the DB nor in the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>context</cell></row><row><cell>Wrong lexicalization</cell><cell>6</cell><cell>Policy</cell><cell>Repeats the same value in a list of choices during lexicalization</cell></row><row><cell>Missing information</cell><cell>5</cell><cell>Policy</cell><cell>Makes booking while not all information is specified</cell></row><row><cell>Ignored input</cell><cell>5</cell><cell>BS</cell><cell>Keeps asking for information that was provided</cell></row><row><cell>Bad domain</cell><cell>4</cell><cell>BS</cell><cell>Fails to focus on the correct domain</cell></row><row><cell>False response</cell><cell>4</cell><cell>Policy</cell><cell>States a different value of a slot than the value stored in DB</cell></row><row><cell>Repeated output</cell><cell>3</cell><cell>Policy</cell><cell>Repeats the same slot twice on the output</cell></row><row><cell>Failed booking</cell><cell>3</cell><cell>DB/Policy</cell><cell>Booking was unsuccessful due to DB mismatch</cell></row><row><cell>Other</cell><cell cols="3">10 BS/DB/P/Oth (Various rare errors that could not be categorized)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Distribution of the most common error types encountered during the human evaluation of 130 dialogues. Absolute counts of errors in the 50 erroneous dialogues are shown. The total error count is 61 as some dialogues contained multiple errors. The most likely source of the error (cf. Table 4) and a short description are given for each type.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>user : I am looking for a cheap chinse restaurant. belief: restaurant {food = chinese, price range = cheap} system: I have 4 cheap chinese restaurants. What area of town would you like to dine in? user: downtown belief: restaurant {area = centre, food = chinese, price range = cheap} system: I have 3 cheap chinese restaurants in the centre.</figDesc><table><row><cell>Would you like to book at Charlie Chan?</cell></row><row><cell>...</cell></row><row><cell>Example 4: A snippet from challenging yet successful</cell></row><row><cell>dialogue. The system deals with input noise (chinse) as</cell></row><row><cell>well as with paraphrases (downtown).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://convlab.github.io/ arXiv:2102.05126v1 [cs.CL] 9 Feb 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ufal/augpt/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We found in our experiments on the MultiWOZ data (see Section 4) that this assumption was almost always fulfilled.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Charles University GAUK grant No. 302120, the SVV project No. 260575, and the Charles University project PRIMUS/19/SCI/10. Jonáš Kulhánek was supported by the European Regional Development </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hello, it&apos;s GPT-2 -how can I help you? towards the use of pretrained language models for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5602</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT)</title>
		<meeting>the 3rd Workshop on Neural Generation and Translation (WNGT)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MultiWOZa large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gašić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5016" to="5026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Taskmaster-1: Toward a realistic and diverse dialog dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyu-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Cedilnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4516" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP</title>
		<meeting>the 2018 EMNLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th ACL</title>
		<meeting>the 58th ACL<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7465" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MultiWOZ 2.1: A consolidated multidomain dialogue dataset with state corrections and state tracking baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachi</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kumar Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanchit</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference (LREC)</title>
		<meeting>the 12th Language Resources and Evaluation Conference (LREC)<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="422" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilingual whispers: Generating paraphrases with translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oussama</forename><surname>Elachqar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5503</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</title>
		<meeting>the 5th Workshop on Noisy User-generated Text (W-NUT 2019)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural Approaches to Conversational AI: Question Answering, Task-oriented Dialogues and Social Chatbots. volume 13 of Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>now publishers</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Psycholinguistics Meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1350</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3601" to="3605" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end neural pipeline for goal-oriented dialogue systems using GPT-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghoon</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Gwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngsoo</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kee-Eung</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.54</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00796</idno>
		<title level="m">Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th ACL</title>
		<meeting>the 55th ACL<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Don&apos;t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4715" to="4728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Elitr non-native speech translation at iwslt 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Macháček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonáš</forename><surname>Kratochvíl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeet</forename><surname>Sagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matúš</forename><surname>Žilinec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai-Son</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuekun</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT)</title>
		<meeting>the 17th International Conference on Spoken Language Translation (IWSLT)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="200" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generating phrasal and sentential paraphrases: A survey of data-driven methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="387" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mem2Seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1468" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICLR</title>
		<meeting>the ICLR<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Philadelphia</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NeurIPS)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05298</idno>
		<title level="m">Shahin Shayandeh, Lars Liden, and Jianfeng Gao. 2020. Soloist: Few-shot task-oriented dialog with a single pretrained auto-regressive model</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Khaitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8689" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flexibly-structured model for task-oriented dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Namazifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaixiu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5922</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="178" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NeurIPS)</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural text generation with unlikelihood training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning for task-oriented dialogue with dialogue state representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 27th International Conference on Computational Linguistics (COLING)<address><addrLine>Santa Fe, NM, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3781" to="3792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGdial)</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGdial)<address><addrLine>Prague, Czechia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional generation and snapshot learning in neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1233</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao U</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tod-bert: Pre-trained natural language understanding for task-oriented dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="917" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Global-to-local memory pointer networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2012.2225812</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A probabilistic end-to-end taskoriented dialog model with latent belief states towards semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 EMNLP</title>
		<meeting>the 2020 EMNLP<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9207" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Taskoriented dialog systems that consider multiple appropriate responses under the same context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9604" to="9611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DIALOGPT : Largescale generative pre-training for conversational response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.30</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convlab-2: An open-source toolkit for building, evaluating, and diagnosing dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th ACL: System Demonstrations</title>
		<meeting>the 58th ACL: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
