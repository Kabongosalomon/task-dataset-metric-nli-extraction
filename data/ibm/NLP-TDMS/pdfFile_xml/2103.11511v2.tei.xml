<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MoViNets: Mobile Video Networks for Efficient Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
							<email>dankondratyuk@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
							<email>lzyuan@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
							<email>yandongli@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
							<email>tanmingxing@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>bgong@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">MoViNets: Mobile Video Networks for Efficient Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Mobile Video Networks (MoViNets), a family of computation and memory efficient video networks that can operate on streaming video for online inference. 3D convolutional neural networks (CNNs) are accurate at video recognition but require large computation and memory budgets and do not support online inference, making them difficult to work on mobile devices. We propose a three-step approach to improve computational efficiency while substantially reducing the peak memory usage of 3D CNNs. First, we design a video network search space and employ neural architecture search to generate efficient and diverse 3D CNN architectures. Second, we introduce the Stream Buffer technique that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrarylength streaming video sequences for both training and inference with a small constant memory footprint. Third, we propose a simple ensembling technique to improve accuracy further without sacrificing efficiency. These three progressive techniques allow MoViNets to achieve state-ofthe-art accuracy and efficiency on the Kinetics, Moments in Time, and Charades video action recognition datasets. For instance, MoViNet-A5-Stream achieves the same accuracy as X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less memory. Code will be made available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Efficient video recognition models are opening up new opportunities for mobile camera, IoT, and self-driving applications where efficient and accurate on-device processing is paramount. Despite recent advances in deep video modeling, it remains difficult to find models that run on mobile devices and achieve high video recognition accuracy. On the one hand, 3D convolutional neural networks (CNNs) <ref type="bibr" target="#b49">[76,</ref><ref type="bibr" target="#b53">80,</ref><ref type="bibr">25,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b36">63]</ref> offer state-of-the-art accuracy, but consume copious amounts of memory and computation. * Work done as a part of the Google AI Residency. MoViNets are more accurate than 2D networks and more efficient than 3D networks. Top (log scale): MoViNet-A2 achieves 6% higher accuracy than MobileNetV3 [33] at the same FLOPs while MoViNet-A6 achieves state-of-the-art 83.5% accuracy being 5.1x faster than X3D-XL <ref type="bibr">[24]</ref>. Bottom: Streaming MoViNets require 10x less memory at the cost of 1% accuracy. Note that we only train on the 93% of Kinetics 600 examples that are available at the time of writing. Best viewed in color.</p><p>On the other hand, 2D CNNs <ref type="bibr" target="#b21">[48,</ref><ref type="bibr" target="#b61">88]</ref> require far fewer resources suitable for mobile and can run online using frameby-frame prediction, but fall short in accuracy.</p><p>Many operations that make 3D video networks accurate (e.g., temporal convolution, non-local blocks <ref type="bibr" target="#b53">[80]</ref>, etc.) require all input frames to be processed at once, limiting the opportunity for accurate models to be deployed on mobile devices. The recently proposed X3D networks <ref type="bibr">[24]</ref> provide a significant effort to increase the efficiency of 3D CNNs. However, they require large memory resources on large temporal windows which incur high costs, or small temporal windows which reduce accuracy. Other works aim to improve 2D CNNs' accuracy using temporal aggregation <ref type="bibr" target="#b21">[48,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b55">82,</ref><ref type="bibr" target="#b24">51,</ref><ref type="bibr">22]</ref>, however their limited inter-frame interactions reduce these models' abilities to adequately model long-range temporal dependencies like 3D CNNs. This paper introduces three progressive steps to design efficient video models which we use to produce Mobile Video Networks (MoViNets), a family of memory and computation efficient 3D CNNs.</p><p>1. We first define a MoViNet search space to allow Neural Architecture Search (NAS) to efficiently trade-off spatiotemporal feature representations.</p><p>2. We then introduce Stream Buffers for MoViNets, which process videos in small consecutive subclips, requiring constant memory without sacrificing long temporal dependencies, and which enable online inference.</p><p>3. Finally, we create Temporal Ensembles of streaming MoViNets, regaining the slightly lost accuracy from the stream buffers.</p><p>First, we design the MoViNet search space to explore how to mix spatial, temporal, and spatiotemporal operations such that NAS can find optimal feature combinations to trade-off efficiency and accuracy. <ref type="figure" target="#fig_0">Figure 1</ref> visualizes the efficiency of the generated MoViNets. MoViNet-A0 achieves similar accuracy to MobileNetV3-large+TSM <ref type="bibr">[33,</ref><ref type="bibr" target="#b21">48]</ref> on Kinetics 600 <ref type="bibr" target="#b12">[39]</ref> with 75% fewer FLOPs. MoViNet-A6 achieves state-of-the-art 83.5% accuracy, 1.6% higher than X3D-XL [24], requiring 60% fewer FLOPs.</p><p>Second, we create streaming MoViNets by introducing the stream buffer to reduce memory usage from linear to constant in the number of input frames for both training and inference, allowing MoViNets to run with substantially fewer memory bottlenecks. E.g., the stream buffer reduces MoViNet-A5's memory usage by 90%. In contrast to traditional multi-clip evaluation (test-time data augmentation) approaches <ref type="bibr" target="#b38">[65,</ref><ref type="bibr" target="#b51">78]</ref> which also reduce memory, a stream buffer carries over temporal dependencies between consecutive non-overlapping subclips by caching feature maps at subclip boundaries. The stream buffer allows for a larger class of operations to enhance online temporal modeling than the recently proposed temporal shift <ref type="bibr" target="#b21">[48]</ref>. We equip the stream buffer with temporally unidirectional causal operations like causal convolution <ref type="bibr" target="#b28">[55]</ref>, cumulative pooling, and causal squeeze-and-excitation <ref type="bibr">[34]</ref> with positional encoding to force temporal receptive fields to look only into past frames, enabling MoViNets to operate incrementally on streaming video for online inference. However, the causal operations come at a small cost, reducing accuracy on Kinetics 600 by 1% on average. Third, we temporally ensemble MoViNets, showing that they are more accurate than single large networks while achieving the same efficiency. We train two streaming MoViNets independently with the same total FLOPs as a single model and average their logits. This simple technique gains back the loss in accuracy when using stream buffers.</p><p>Taken together, these three techniques create MoViNets that are high in accuracy, low in memory usage, efficient in computation, and support online inference. We search for MoViNets using the Kinetics 600 dataset <ref type="bibr" target="#b8">[9]</ref> and test them extensively on Kinetics 400 <ref type="bibr" target="#b12">[39]</ref>, Kinetics 700 <ref type="bibr" target="#b9">[10]</ref>, Moments in Time <ref type="bibr" target="#b27">[54]</ref>, Charades <ref type="bibr" target="#b37">[64]</ref>, and Something-Something V2 [28].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Efficient Video Modeling. Deep neural networks have made remarkable progress for video understanding [35, <ref type="bibr" target="#b38">65,</ref><ref type="bibr" target="#b47">74,</ref><ref type="bibr" target="#b52">79,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b53">80,</ref><ref type="bibr" target="#b33">60,</ref><ref type="bibr">24,</ref><ref type="bibr">25]</ref>. They extend 2D image models with a temporal dimension, most notably incorporating 3D convolution <ref type="bibr">[35,</ref><ref type="bibr" target="#b46">73,</ref><ref type="bibr" target="#b47">74,</ref><ref type="bibr" target="#b57">84,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b32">59,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b36">63]</ref>.</p><p>Improving the efficiency of video models has gained increased attention <ref type="bibr">[25,</ref><ref type="bibr" target="#b48">75,</ref><ref type="bibr">26,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b21">48,</ref><ref type="bibr">23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">14,</ref><ref type="bibr" target="#b18">45,</ref><ref type="bibr" target="#b30">57]</ref>. Some works explore the use of 2D networks for video recognition by processing videos in smaller segments followed by late fusion <ref type="bibr" target="#b11">[38,</ref><ref type="bibr">20,</ref><ref type="bibr" target="#b59">86,</ref><ref type="bibr" target="#b52">79,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b42">69,</ref><ref type="bibr" target="#b17">44,</ref><ref type="bibr" target="#b20">47,</ref><ref type="bibr" target="#b53">80,</ref><ref type="bibr" target="#b60">87,</ref><ref type="bibr" target="#b61">88]</ref>. The Temporal Shift Module <ref type="bibr" target="#b21">[48]</ref> uses early fusion to shift a portion of channels along the temporal axis, boosting accuracy while supporting online inference. Causal Modeling. WaveNet <ref type="bibr" target="#b28">[55]</ref> introduces causal convolution, where the receptive field of a stack of 1D convolutions only extends to features up to the current time step. We take inspiration from other works using causal convolutions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">13,</ref><ref type="bibr">17,</ref><ref type="bibr">15</ref>, 18] to design stream buffers for online video model inference, allowing frame-by-frame predictions with 3D kernels. Multi-Objective NAS. The use of NAS <ref type="bibr" target="#b62">[89,</ref><ref type="bibr" target="#b22">49,</ref><ref type="bibr" target="#b29">56,</ref><ref type="bibr" target="#b44">71,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">37]</ref> with multi-objective architecture search has also grown in interest, producing more efficient models in the process for image recognition <ref type="bibr" target="#b44">[71,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4]</ref> and video recognition <ref type="bibr" target="#b30">[57,</ref><ref type="bibr" target="#b36">63]</ref>. We make use of TuNAS <ref type="bibr" target="#b3">[4]</ref>, a one-shot NAS framework which uses aggressive weight sharing that is well-suited for computation intensive video models. Efficient Ensembles. Deep ensembles are widely used in classification challenges to boost the performance of CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">66,</ref><ref type="bibr" target="#b43">70,</ref><ref type="bibr">30]</ref>. More recent results indicate that deep ensembles of small models can be more efficient than single large models on image classification <ref type="bibr" target="#b13">[40,</ref><ref type="bibr" target="#b25">52,</ref><ref type="bibr" target="#b40">67,</ref><ref type="bibr" target="#b15">42,</ref><ref type="bibr">27]</ref>, and we extend these findings to video classification.</p><formula xml:id="formula_0">STAGE NETWORK OPERATIONS OUTPUT SIZE data stride τ , RGB T × S 2 conv 1 1 × k 2 1 , c 1 T × S 2</formula><p>MoViNet Search Space. We build our base search space on MobileNetV3 <ref type="bibr">[33]</ref>, which provides a strong baseline for mobile CPUs. It consists of several blocks of inverted bottleneck layers with varying filter widths, bottleneck widths, block depths, and kernel sizes per layer. Similar to X3D [24], we expand the 2D blocks in MobileNetV3 to deal with 3D video input. <ref type="table">Table 1</ref> provides a basic overview of the search space, detailed as follows.</p><p>We denote by T × S 2 = 50 × 224 2 and τ = 5 (5fps) the dimensions and frame stride, respectively, of the input to the target MoViNets. For each block in the network, we search over the base filter width c base and the number of layers L ≤ 10 to repeat within the block. We apply multipliers {0.75, 1, 1.25} over the feature map channels within every block, rounded to a multiple of 8. We set n = 5 blocks, with strided spatial downsampling for the first layer in each block except the 4th block (to ensure the last block has spatial resolution 7 2 ). The blocks progressively increase their feature map channels: {16, 24, 48, 96, 96, 192}. The final convolution layer's base filter width is 512, followed by a 2048D dense layer before the classification layer.</p><p>With the new time dimension, we define the 3D kernel size within each layer, k time × (k space ) 2 , to be chosen as one of the following: {1x3x3, 1x5x5, 1x7x7, 5x1x1, 7x1x1, 3x3x3, 5x3x3} (we remove larger kernels from consideration). These choices enable a layer to focus on and aggregate different dimensional representations, expanding the network's receptive field in the most pertinent directions while reducing FLOPs along other dimensions. Some kernel sizes may benefit from having different numbers of input filters, so we search over a range of bottleneck widths c expand defined as multipliers in {1.5, 2.0, 2.5, 3.0, 3.5, 4.0} relative to c base . Each layer surrounds the 3D convolution with two 1x1x1 convolutions to expand and project between c base and c expand . We do not apply any temporal downsampling to enable frame-wise prediction.</p><p>Instead of applying spatial squeeze-and-excitation (SE) [34], we use SE blocks to aggregate spatiotemporal features via 3D average pooling, applying it to every bottleneck block as in <ref type="bibr">[33,</ref><ref type="bibr" target="#b45">72]</ref>. We allow SE to be searchable, optionally disabling it to conserve FLOPs.</p><p>Scaling the Search Space. Our base search space forms the basis for MoViNet-A2. For the other MoViNets, we apply a compound scaling heuristic similar to the one used in EfficientNet <ref type="bibr" target="#b45">[72]</ref>. The major difference in our approach is that we scale the search space itself rather than a single model (i.e., search spaces for models A0-A5). Instead of finding a good architecture and then scaling it, we search over all scalings of all architectures, broadening the range of possible models.</p><p>We use a small random search to find the scaling coefficients (with an initial target of 300 MFLOPs per frame), which roughly double or halve the expected size of a sampled model in the search space. For the choice of coefficients, we resize the base resolution S 2 , frame stride τ , block filter width c base , and block depths L. We perform the search on different FLOPs targets to produce a family of models ranging from MobileNetV3-like sizes up to the sizes of ResNet3D-152 <ref type="bibr">[30,</ref><ref type="bibr">29]</ref>. Appendix Aprovides more details of the search space, the scaling technique, and a description of the search algorithm.</p><p>The MoViNet search space gives rise to a family of versatile networks, which outperform state-of-the-art efficient video recognition CNNs on popular benchmark datasets. However, their memory footprints grow proportionally to the number of input frames, making them difficult to handle long videos on mobile devices. The next subsection introduces a stream buffer to reduce the networks' memory consumption from linear to constant in video length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Stream Buffer with Causal Operations</head><p>Suppose we have an input video x with T frames that may cause a model to exceed a set memory budget. A common solution to reduce memory is multi-clip evalua-  <ref type="figure">Figure 2</ref>. Streaming Evaluation vs. Multi-Clip Evaluation. In multi-clip evaluation (a type of test-time data augmentation), we embed overlapping subclips of an input video with 3D convolutions and average the logits. In streaming evaluation, we use the stream buffer to carry forward input features between non-overlapping subclips and apply causal operations, thereby allowing the temporal receptive field to cover the whole video. This stream buffer increases accuracy while retaining the benefits of reduced memory from multi-clip evaluation.</p><p>tion <ref type="bibr" target="#b38">[65,</ref><ref type="bibr" target="#b51">78]</ref>, where the model averages predictions across n overlapping subclips with T clip &lt; T frames each, as seen in <ref type="figure">Figure 2</ref> (left). It reduces memory consumption to O(T clip ). However, it poses two major disadvantages: 1) It limits the temporal receptive fields to each subclip and ignores longrange dependencies, potentially harming accuracy. 2) It recomputes frame activations which overlap, reducing efficiency.</p><p>Stream Buffer. To overcome the above mentioned limitations, we propose stream buffer as a mechanism to cache feature activations on the boundaries of subclips, allowing us to expand the temporal receptive field across subclips and requiring no recomputation, as shown in <ref type="figure">Figure 2</ref> (right). Formally, let x clip i be the current subclip (raw input or activation) at step i &lt; n, where we split the video into n adjacent non-overlapping subclips of length T clip each. We start with a zero-initialized tensor representing our buffer B with length b along the time dimension and whose other dimensions match x clip i . We compute the feature map F i of the buffer concatenated (⊕) with the subclip along the time dimension as:</p><formula xml:id="formula_1">F i = f (B i ⊕ x clip i )<label>(1)</label></formula><p>where f represents a spatiotemporal operation (e.g., 3D convolution). When processing the next clip, we update the contents of the buffer to:</p><formula xml:id="formula_2">B i+1 = (B i ⊕ x clip i ) [−b:]<label>(2)</label></formula><p>where we denote [−b :] as a selection of the last b frames of the concatenated input. As a result, our memory consumption is dependent on O(b + T clip ), which is constant as the total video frames T or number of subclips n increase.</p><p>Relationship to TSM. The Temporal Shift Module (TSM) <ref type="bibr" target="#b21">[48]</ref> can be seen as a special case of the stream buffer, where b = 1 and f is an operation that shifts a proportion of channels in the buffer B t = x t−1 to the input x t before computing a spatial convolution at frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Causal Operations</head><p>A reasonable approach to fitting 3D CNNs' operations to the stream buffer is to enforce causality, i.e., any features must not be computed from future frames. This has a number of advantages, including the ability to reduce a subclip x clip i down to a single frame without affecting activations or predictions, and enables 3D CNNs to work on streaming video for online inference. While it is possible to use non-causal operations, e.g., buffering in both temporal directions, we would lose online modeling capabilities which is a desirable property for mobile.</p><p>Causal Convolution (CausalConv). By leveraging the translation equivariant property of convolution, we replace all temporal convolutions with CausalConvs <ref type="bibr" target="#b28">[55]</ref>, effectively making them unidirectional along the temporal dimension. Concretely, we first compute padding to balance the convolution across all axes and then move any padding after the final frame and merge it with any padding before the first frame. See Appendix Cfor an illustration of how the receptive field differs from standard convolution, as well as a description of the causal padding algorithm.</p><p>When using a stream buffer with CausalConv, we can replace causal padding with the buffer itself, carrying forward the last few frames from a previous subclip and copying them into the padding of the next subclip. If we have a temporal kernel size of k (and we do not use any strided sampling), then our padding and therefore buffer width becomes b = k − 1. Usually, k = 3 which implies b = 2, resulting in a small memory footprint. Stream buffers are only required before layers that aggregate features across multiple frames, so spatial and pointwise convolutions (e.g., 1x3x3, 1x1x1) can be left as-is, further saving memory.</p><p>Cumulative Global Average Pooling (CGAP). We use CGAP to approximate any global average pooling involving the temporal dimension. For any activations up to frame T , we can compute this as a cumulative sum:</p><formula xml:id="formula_3">CGAP(x, T ) = 1 T T t=1 x t ,<label>(3)</label></formula><p>where x represents a tensor of activations. To compute CGAP causally, we keep a single-frame stream buffer storing the cumulative sum up to T .</p><p>CausalSE with Positional Encoding. We denote CausalSE as the application of CGAP to SE, where we multiply the spatial feature map at frame t with the SE computed from CGAP(x, t). From our empirical results, CausalSE is prone to instability likely due to the SE projection layers have a difficult time determining the quality of the CGAP estimate, which has high variance early in the video. To combat this problem, we apply a sine-based fixed positional encoding (POSENC) scheme inspired by Transformers <ref type="bibr" target="#b50">[77,</ref><ref type="bibr" target="#b23">50]</ref>. We directly use frame index as the position and sum the vector with CGAP output before applying the SE projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training and Inference with Stream Buffers</head><p>Training. To reduce the memory requirements during training, we use a recurrent training strategy where we split a given batch of examples into n subclips, applying a forward pass that outputs a prediction for each subclip, using stream buffers to cache activations. However, we do not backpropagate gradients past the buffer so that the memory of previous subclips can be deallocated. Instead, we compute losses and accumulate computed gradients between subclips, similar to batch gradient accumulation. This allows the network to account for all T = nT clip frames, performing n forward passes before applying the gradients. This training strategy allows the network to learn longer term dependencies thus results in better accuracy than a model trained with shorter video length (see Appendix C).</p><p>We can set T clip to any value without affecting accuracy. However, ML accelerators (e.g., GPUs) benefit from multiplying large tensors, so for training we typically set a value of T clip ∈ {8, 16, 32}. This accelerates training while allowing careful control of memory cost.</p><p>Online Inference. One major benefit of using causal operations like CausalConv and CausalSE is to allow a 3D video CNN to work online. Similar to training, we use the stream buffer to cache activations between subclips. However, we can set the subclip length to a single frame (T clip = 1) for maximum memory savings. This also reduces the latency between frames, enabling the model to output predictions frame-by-frame on a streaming video, accumulating new information incrementally like a recurrent network (RNN) <ref type="bibr">[32]</ref>. But unlike traditional convolutional RNNs, we can input a variable number of frames per step to produce the same output. For streaming architectures with CausalConv, we predict a video's label by pooling the frame-by-frame output features using CGAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Ensembles</head><p>The stream buffers can reduce MoViNets' memory footprints up to an order of magnitude in the cost of about 1% accuracy drop on Kinetics 600. We can restore this accuracy using a simple ensembling strategy. We train two MoViNets independently with the same architecture, but halve the frame-rate, keeping the temporal duration the same (resulting in half the input frames). We input a video into both networks, with one network having frames offset by one frame and apply an arithmetic mean on the unweighted logits before applying softmax. This method results in a twomodel ensemble with the same FLOPs as a single model before halving the frame-rate, providing prediction with enriched representations. In our observations, despite the fact that both models in the ensemble may have lower accuracy than the single model individually, together when ensembled they can have higher accuracy than the single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Video Classification</head><p>In this section, we evaluate MoViNets' accuracy, efficiency, and memory consumption during inference on five representative action recognition datasets.</p><p>Datasets. We report results on all Kinetics datasets, including Kinetics 400 [12, 39], Kinetics 600 <ref type="bibr" target="#b8">[9]</ref>, and Kinetics 700 <ref type="bibr" target="#b9">[10]</ref>, which contain 10-second, 250-frame video sequences at 25 fps labeled with 400, 600, and 700 action classes, respectively. We use examples that are available at the time of writing, which is 87.5%, 92.8%, and 96.2% of the training examples respectively (see Appendix C). Additionally, we experiment with Moments in Time <ref type="bibr" target="#b27">[54]</ref>, containing 3-second, 75-frame sequences at 25fps in 339 action classes, and Charades <ref type="bibr" target="#b37">[64]</ref>, which has variable-length videos with 157 action classes where a video can contain multiple class annotations. We include Something-Something V2 [28] and Epic Kitchens 100 [19] results in Appendix C.</p><p>Implementation Details. For each dataset, all models are trained with RGB frames from scratch, i.e., we do not apply any pretraining. For all datasets, we train with 64 frames (except when the inference frames are fewer) at various frame-rates, and run inference with the same frame-rate.</p><p>We run TuNAS using Kinetics 600 and keep 7 MoViNets each having a FLOPs target used in <ref type="bibr">[24]</ref>. As our models get larger, our scaling coefficients increase the input resolution, number of frames, depth, and feature width of the networks. We also experiment with AutoAugment [16] augmentation used in image classification, i.e., we sample a random image augmentation for each video and apply the same augmentation for each frame. For the architectures of the 7 models as well as training hyperparameters, see Appendix B.</p><p>Single-Clip vs. Multi-Clip Evaluation. We evaluate all our models with a single clip sampled from input video with a fixed temporal stride, covering the entire video duration. When the single-clip and multi-clip evaluations use the same number of frames in total so that FLOPs are equivalent, we find that single-clip evaluation yields higher accuracy (see Appendix C). This can be due in part to 3D CNNs being able to model longer-range dependencies, even when evaluating on many more frames than it was trained on. Since existing models commonly use multi-clip evaluation, we report the total FLOPs per video, not per clip, for a fair comparison.</p><p>However, single-clip evaluation can greatly inflate a network's peak memory usage (as seen in <ref type="figure" target="#fig_0">Figure 1</ref>), which is likely why multi-clip evaluation is commonly used in previous work. The stream buffer eliminates this problem, allowing MoViNets to predict like they are embedding the full video, and incurs less peak memory than multi-clip evaluation.</p><p>We also reproduce X3D [24], arguably the most related work to ours, to test its performance under single-clip and 10-clip evaluation to provide more insights. We denote 30clip to be the evaluation strategy with 10 clips times three spatial crops for each video, while 10-clip just uses one spatial crop. We avoid any spatial augmentation or temporal sampling in MoViNets during inference to improve efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison Results on Kinetics 600</head><p>MoViNets without Stream Buffers. <ref type="table">Table 2</ref> presents the main results of seven MoViNets on Kinetics 600 before applying the stream buffer, mainly compared with various X3D models <ref type="bibr">[24]</ref>, which are recently developed for efficient video recognition. The columns of the table correspond to the Top-1 classification accuracy; GFLOPs per video a model incurs; resolution of the input video frame (where we shorten 224 2 to 224); input frames per video, where 30 × 4 means the 30-clip evaluation with 4 frames as input in each run; frames per second (FPS), determined by the temporal stride τ in the search space for MoViNets; and a network's number of parameters.</p><p>MoViNet-A0 has fewer GFLOPs and is 10% more accurate than the frame-based MobileNetV3-S [33] (where we train MobileNetV3 using our training setup, averaging logits across frames). MoViNet-A0 also outperforms X3D-S in terms of both accuracy and GFLOPs. MoViNet-A1 matches the GFLOPs of X3D-S, but its accuracy is 2% higher than X3D-S.</p><p>Growing the target GFLOPs to the range between X3D-S and 30-clip X3D-XS, we arrive at MoViNet-A2. We can achieve a little higher accuracy than 30-clip X3D-XS or X3D-M by using almost half of their GFLOPs. Additionally, we include the frame-by-frame MobileNetV3-L and verify that it can benefit from TSM <ref type="bibr" target="#b21">[48]</ref> by about 3%.</p><p>There are more significant margins between larger MoViNets (A3-A6) and their counterparts in the X3D family. It is not surprising because NAS should intuitively be more advantageous over the handcrafting method for X3D when the design space is large. MoViNet-A5 and MoViNet-A6 outperform several state-of-the-art video networks, including recent Transformer models like ViViT <ref type="bibr" target="#b1">[2]</ref> and TimeSformer <ref type="bibr" target="#b4">[5]</ref> (see the last 6 rows of <ref type="table">Table 2</ref>).  <ref type="table">Table 2</ref>. Accuracy of MoViNet on Kinetics 600. We measure total GFLOPs per video across all frames, and report the inference resolution (res), number of clips × frames per clip (frames), and frame rate (fps) of each video clip. * denotes our reproduced models. For X3D, we report inference resolution, which differs from training. We report all datapoints to the best knowledge available.</p><p>MoViNet-A6 with AutoAugment achieves 84.8% accuracy (without pretraining) while still being substantially more efficient than comparable models (most often by an order of magnitude). Even when compared to fully Transformer <ref type="bibr" target="#b50">[77]</ref> models like TimeSformer-HR <ref type="bibr" target="#b4">[5]</ref>, MoViNet-A6 outperforms it by 1% accuracy and using 40% of the FLOPs.</p><p>MoViNets with Stream Buffers. Our base MoViNet architectures may consume lots of memory in the absence of modifications, especially as the model sizes and input frames grow. Using the stream buffer with causal operations, we can have an order of magnitude peak memory reduction for large networks (MoViNets A3-A6), as shown in the last column of <ref type="table">Table 3</ref>. Moreover, <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the streaming architec-  <ref type="table">Table 3</ref>. Base vs. Streaming Architectures on Kinetics 600. We and report the inference resolution (res), number of clips × frames per clip (frames), and frame rate (fps) for each video. We measure the total GFLOPs per video across all frames. We denote "Stream" to be causal models using a stream buffer frame-by-frame, and "Ens" to be two ensembled models (with half the input frames so FLOPs are equivalent). Memory usage is measured in peak MB for a single video clip. * denotes our reproduced models.</p><p>tures' effect on memory. From the left panel at the top, we see that our MoViNets are more accurate and more memory-efficient across all model sizes compared to X3D, which employs multi-clip evaluation. We also demonstrate constant memory as we scale the total number of frames in the input receptive field at the top's right panel. The bottom panel indicates that the streaming MoViNets remain efficient in terms of the GFLOPs per input video. We also apply our stream buffer to ResNet3D-50 (see the last two rows in <ref type="table">Table 3</ref>). However, we do not see as much of a memory reduction, likely due to larger overhead when using full 3D convolution as opposed to the depthwise convolution in MoViNets.</p><p>MoViNets with Stream Buffers and Ensembling. We see from <ref type="table">Table 3</ref> only a small 1% accuracy drop across all models after applying the stream buffer. We can restore the accuracy using the temporal ensembling without any additional inference cost. <ref type="table">Table 3</ref> reports the effect of ensembling two models trained at half the frame rate of the original model (so that GFLOPs remain the same). We can see the accuracy improvements in all streaming architectures, showing that ensembling can bridge the gap between streaming and non-streaming architectures, especially as model sizes grow. It is worth noting that, unlike prior works, the ensembling balances accuracy and efficiency (GFLOPs) in the same spirit as <ref type="bibr" target="#b13">[40]</ref>, not just to boost the accuracy. We evaluate two versions of MoViNet: a base version without a stream buffer and a causal version with a stream buffer. Note that memory may be inflated due to padding and runtime overhead. Top right: comparison of max memory usage on a V100 GPU as a function of the number of input frames. Bottom: the classification accuracy. * denotes our reproduced models. <ref type="figure" target="#fig_4">Figure 4</ref> summarizes the main results of MoViNets on all the five datasets along with state-of-the-art models that have results reported on the respective datasets. We compare MoViNets with X3D [24], MSNet <ref type="bibr" target="#b14">[41]</ref>, TSM <ref type="bibr" target="#b21">[48]</ref>, ResNet3D [30], SlowFast [25], EfficientNet-L2 <ref type="bibr" target="#b56">[83]</ref>, TVN <ref type="bibr" target="#b30">[57]</ref>, SRTG <ref type="bibr" target="#b41">[68]</ref>, and AssembleNet <ref type="bibr" target="#b36">[63,</ref><ref type="bibr" target="#b35">62]</ref>. Appendix Ctabulates the results with more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison Results on Other Datasets</head><p>Despite only searching for efficient architectures on Kinetics 600, NAS yields models that drastically improve over prior work on other datasets as well. On Moments in Time, our models are 5-8% more accurate than Tiny Video Networks (TVNs) <ref type="bibr" target="#b30">[57]</ref> at low GFLOPs, and MoViNet-A5 achieves 39.9% accuracy, outperforming As-sembleNet <ref type="bibr" target="#b36">[63]</ref> (34.3%) which uses optical flow as additional input (while our models do not). On Charades, MoViNet-A5 achieves the accuracy of 63.2%, beating As-sembleNet++ <ref type="bibr" target="#b35">[62]</ref> (59.8%) which uses optical flow and object segmentation as additional inputs. Results on Charades provide evidence that our models are also capable of sophisticated temporal understanding, as these videos can have longer duration clips than what is seen in Kinetics and Moments in Time.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Additional Analyses</head><p>MoViNet Operations. We provide some ablation studies about some critical MoViNet operations in <ref type="table" target="#tab_2">Table 4</ref>. For the base network without the stream buffer, SE is vital for achieving high accuracy; MoViNet-A1's accuracy drops by 2.9% if we remove SE. We see a much larger accuracy drop when using CausalConv without SE than CausalConv with a global SE, which indicates that the global SE can take some of the role of standard Conv to extract information from future frames. However, when we switch to a fully streaming architecture with CausalConv and CausalSE, this information from future frames is no longer available, and we see a large drop in accuracy, but still significantly improved from CausalConv without SE. Using PosEnc, we can gain back some accuracy in the causal model.</p><p>MoViNet Architectures. We provide the architecture description of MoViNet-A2 in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Hardware Benchmark</head><p>MoViNets A0, A1, and A2 represent the fastest models that would most realistically be used on mobile devices. We compare them with MobileNetV3 in <ref type="figure">Figure 5</ref> with respect to both FLOPs and real-time latency on an x86 Intel Xeon W-2135 CPU at 3.70GHz. These models are comparable in per-frame computation cost, as we evaluate on 50 frames for all models. From these results we can conclude that streaming MoViNets can run faster on CPU while being more accurate at the same time, even with temporal modifications like TSM. While there is a discrepancy between FLOPs and latency, searching over a latency target explicitly in NAS can reduce this effect. However, we still see that FLOPs is a reasonable proxy metric for CPU latency, which would translate well for mobile devices.</p><p>We also show benchmarks for MoViNets running on an Nvidia V100 GPU in <ref type="table">Table 6</ref>. Similar to mobile CPU, our streaming model latency is comparable to single-clip X3D models. However, we do note that MobileNetV3 can run faster than our networks on GPU, showing that the FLOPs metric for NAS has its limitations. MoViNets can be made more efficient by targeting real hardware instead of FLOPs, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>MoViNets provide a highly efficient set of models that transfer well across different video recognition datasets. Coupled with stream buffers, MoViNets significantly reduce training and inference memory cost while also supporting online inference on streaming video. We hope our approach to designing MoViNets can provide improvements to future and existing models, reducing memory and computation costs in the process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>We supplement the main text by the following materials.</p><p>• Appendix A provides more details of the search space, the technique to scale the search space, and the search algorithm.</p><p>• Appendix B is about the neural architectures of MoViNets A0-A7.</p><p>• Appendix C reports additional results on the datasets studied in the main text along with ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MoViNet Architecture Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Scaling Algorithm for the Search Space</head><p>To produce models that scale well, we progressively expand the search space across width, depth, input resolution, and frame rate, like EfficientNet <ref type="bibr" target="#b45">[72]</ref>. Specifically, we use a single scaling parameter φ to define the size of our search space. Then we define the following coefficients:</p><formula xml:id="formula_4">depth: d = α φ = 1.36 φ base width: w = β φ = 1.18 φ resolution: r = γ φ = 1.16 φ frame-rate: f = δ φ = 1.24 φ such that αβ 2 γ 2 δ ≈ 4.</formula><p>This will ensure that an increase in φ by 1 will multiply the average model size in the search space by 4. Here we use a multiplier of 4 (instead of 2) to spread out our search spaces so that we can run the same search space with multiple efficiency targets and sample our desired target model size from it.</p><p>As a result, our parameters for a given search space is the following: depth: L ∈ {d, . . . , 10d} base width: c base ∈ {16w, 24w, 48w, 96w, 96w, 192w} resolution: S = 224r frame-rate: τ = 5f.</p><p>We round each of the above parameters to the nearest multiple of 8. If φ = 0, this forms the base search space for MoViNet-A2. Note that c expand is defined relative to c base , so we do not need coefficients for it.</p><p>We found coefficients α, β, γ, δ using a random search over these parameters. More specifically, we select values in the range [1.05, 1.40] at increments of 0.05 to represent possible values of the coefficients. We ensure that the choice of coefficients is such that αβ 2 γ 2 δ ≈ 4, where the initial computation target for each frame is 300 MFLOPs. For each combination, we scale the search space by the coefficients where φ = 1, and randomly sample three architectures from each search space. We train models for a selected search space for 10 epochs, averaging the results of the accuracy for that search space. Then we select the coefficients that maximize the resulting accuracy. Instead of selecting a single set of coefficients, we average the top 5 candidates to produce the final coefficients. While the sample size of models is small and would be prone to noise, we find that the small averages work well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Search Algorithm</head><p>During search, we train a one-shot model using Tu-NAS <ref type="bibr" target="#b3">[4]</ref> that overlaps all possible architectures into a hypernetwork. At every step during optimization, we alternate between learning the network weights and learning a policy π which we use to randomly sample a path through the hypernetwork to produce an initially random network architecture. π is learned using REINFORCE <ref type="bibr" target="#b54">[81]</ref>, optimized on the quality of sampled architectures, defined as the absolute reward consisting of the sampled network's accuracy and cost. At each stage, the RL controller must choose a single categorical decision to select an architectural component. The network architecture is a result of binding a value to each decision. For example, the decision might choose between a spatial 1x3x3 convolution and a temporal 5x1x1 convolution. We use FLOPs as the cost metric for architecture search, and use Kinetics 600 as the dataset to optimize for efficient video networks. During search, we obtain validation set accuracies on a held-out subset of the Kinetics 600 training set, training for a total of 90 epochs.</p><p>The addition of SE [34] to our search space increases FLOPs by such a small amount (&lt; 0.1%) that the search enables it for all layers. SE plays a similar role as the feature gating in S3D-G <ref type="bibr" target="#b57">[84]</ref>, except with a nonlinear squeeze inside the projection operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Stream Buffers for NAS</head><p>We apply the stream buffers to MoViNets as a separate step after NAS in the main text. We can also leverage them for NAS to reduce memory usage during search. Memory poses one of the biggest challenges for NAS, as models are forced to use a limited number of frames and small batch sizes to be able to keep the models in memory during optimization. While this does not prevent us from performing search outright, it requires the use of accelerators with very high memory requirements, requiring a high cost of entry. To circumvent this, we can use stream buffers with a small clip size to reduce memory. As a result, we can increase the total embedded frames and increase the batch size to provide better model accuracy estimation while running NAS. <ref type="table">Table 7</ref> provides an example of an experiment where the use of a stream buffer can reduce memory requirements in this manner. Using a stream buffer, we can reduce the input size from a single clip of 16 frames to 2 clips of 8 frames each,  <ref type="table">Table 7</ref>. Effect of Stream Buffer on NAS (Kinetics 600). We measure the effect of NAS on MoViNet-A0 when using a stream buffer vs. without on the same input. By embedding half the input at a time, we can double the batch size to improve the average NAS held-out accuracy without significantly increasing GPU memory per device. and double the batch size. This results in a relatively modest increase in memory, compared to not using the buffer where we can run into out-of-memory (OOM) issues. We note that the values of b in each layer influences the memory consumption of the model. This is dependent entirely on the temporal kernel width of the 3D convolution. If k = 5, then we only need to cache the last 4 frames. b could be larger, but but it will result in extra frames we will discard, so we set it to the smallest value to conserve memory. Therefore, it is not necessary to specify it directly with NAS, as NAS is only concerned with the kernel sizes. However, we can add an objective to NAS to minimize memory usage, which will apply pressure to reduce the temporal kernel widths and therefore will indirectly affect the value of b in each layer. Reducing memory consumption even further by keeping kernel sizes can be explored in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architectures of MoViNets</head><p>See <ref type="table">Tables 17, 18</ref>, 19, 20, 21, and 22 for the architecture definitions of MoViNet A0-A5 (we move the tables to the final pages of the Appendices to reduce clutter). For MoViNet-A6, we ensemble architectures A4 and A5 using the strategy described in the main text, i.e., we train both models independently and apply an arithmetic mean on the logits during inference. All layers of all models have SE layers enabled, so we remove this search hyperparameter from all tables for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 More Details of the Architectures and Training</head><p>We apply additional changes to our architectures and model training to improve performance even further. To improve convergence speed in searching and training we use ReZero <ref type="bibr" target="#b2">[3]</ref> by applying zero-initialized learnable scalar weights that are multiplied with features before the final sum in a residual block. We also apply skip connections that are traditionally used in ResNets, adding a 1x1x1 convolution in the first layer of each block which may change the base channels or downsample the input. However, we modify this to be similar to ResNet-D [31] where we apply 1x3x3 spatial average pooling before the convolution to Top-1 Accuracy (%) <ref type="bibr" target="#b23">50</ref> 80 120 <ref type="table" target="#tab_3">50  80  120   50   80  120  50   80   120  50  80  120</ref> MoViNet-A0 MoViNet-A1 MoViNet-A2 MoViNet-A3 MoViNet-A4 <ref type="figure">Figure 6</ref>. Effect of Frame-Rate on Efficiency on Kinetics 600. We train and evaluate each model with 50, 80, and 120 frames at 5fps, 8fps, and 12fps respectively.</p><p>improve feature representations.</p><p>We apply Polyak averaging <ref type="bibr" target="#b31">[58]</ref> to the weights after every optimization step, using an Exponential Moving Average (EMA) with decay 0.99. We adopt the Hard Swish activation function, which is a variant of SiLU/Swish <ref type="bibr">[21,</ref><ref type="bibr" target="#b34">61]</ref> proposed by MobileNetV3 [33] that is friendlier to quantization and CPU inference. We use the RMSProp optimizer with momentum 0.9 and a base learning rate of 1.8. We train for 240 epochs with a batch size of 1024 with synchronized batch normalization on all datasets and decay the learning rate using a cosine learning rate schedule <ref type="bibr" target="#b26">[53]</ref> with a linear warm-up of 5 epochs.</p><p>We use a softmax cross-entropy loss with label smoothing 0.1 during training, except for Charades where we apply sigmoid cross-entropy to handle the multiple-class labels per video. For Charades, we aggregate predictions across frames similar to AssembleNet <ref type="bibr" target="#b36">[63]</ref>, where we apply a softmax across frames before applying temporal global average pooling to find multiple action classes that may occur in different frames.</p><p>Some works also expand the resolution for inference. For instance, X3D-M trains with a 224 2 resolution while evaluating 256 2 when using spatial crops. We evaluate all of our models on the same resolution as training to make sure the FLOPs per frame during inference is unchanged from training.</p><p>Our choice of frame-rates can vary from model to model, providing different optimality depending on the architecture. We plot the accuracy of training various MoViNets on Kinetics 600 with different frame-rates in <ref type="figure">Figure 6</ref>. Most models have good efficiency at 50 frames (5fps) or 80 frames (8fps) per video. However, we can see MoViNet-A4 benefits from a higher frame-rate of 12fps. For Charades, we use 64 frames at 6fps for both training and inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Implementation Details and Experiments C.1 Implementing Causal Convolution by Padding</head><p>To make a temporal convolution operation causal, we can apply a simple padding trick which shifts the receptive field forward such that the convolutional kernel is centered at the frame furthest into the future. <ref type="figure" target="#fig_6">Figure 7</ref> illustrates this effect. With a normal 3D convolution operation with kernel size (k t , k h , k w ) and stride s = 1, the padding with respect to dimension i is given as:</p><formula xml:id="formula_5">p left i , p right i = ( ki−1 2 , ki−1 2 ) if x is odd ( ki−2 2 , ki 2 ) otherwise.<label>(4)</label></formula><p>where p left i , p right i are the left and right padding amounts respectively. For causal convolutions, we transform p t as:</p><formula xml:id="formula_6">p left causal t , p right causal t = (p left t + p right t , 0).<label>(5)</label></formula><p>such that the effective temporal receptive field of a voxel at time position t only spans (0, t].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Additional Details of Datasets</head><p>We note that for all the Kinetics datasets are gradually shrinking over time due to videos being taken offline, making it difficult to compare against less recent works. We report on the most recently available videos. While putting our work at a disadvantage compared to previous work, we wish to make comparisons more fair for future work. Nevertheless, we report the numbers as-is and report the reduction of examples in the datasets in <ref type="table">Table 8</ref>.</p><p>We report full results of all models in the following tables: <ref type="table">Table 9</ref> for Kinetics 400, <ref type="table">Table 10</ref> for Kinetics 600 (with top-5 accuracy), <ref type="table" target="#tab_10">Table 11</ref> for Kinetics 700, <ref type="table" target="#tab_5">Table 12</ref> for Moments in Time, <ref type="table">Table 13</ref> for Charades, <ref type="table" target="#tab_2">Table 14</ref> for Something-Something V2, and   <ref type="table">Table 10</ref>. Accuracy of MoViNet on Kinetics 600 with additional top-5 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Single-Clip vs. Multi-Clip Evaluation</head><p>We report all of our results on a single view without multi-clip evaluation. Additionally, we report the total number of frames used for evaluation and the frame rate (note that the evaluation frames can exceed the total number of frames in the reference video when subclips overlap).   <ref type="bibr" target="#b41">[68]</ref> 28.5 220 -</p><p>MoViNet-A6 40.2 274 31.4M ResNet3D-50 <ref type="bibr" target="#b36">[63]</ref> 27.2 --SRTG-R3D-50 <ref type="bibr" target="#b41">[68]</ref> 30.7 300 -SRTG-R3D-101 <ref type="bibr" target="#b41">[68]</ref> 33.6 350 -AssembleNet-50 (RGB+Flow) <ref type="bibr" target="#b36">[63]</ref> 31.4 480 37.3M AssembleNet-101 (RGB+Flow) <ref type="bibr" target="#b36">[63]</ref> 34.3 760 53.3M ViViT-L/16x2 <ref type="bibr" target="#b1">[2]</ref> 38.0 3410 100M As seen in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table">Table 2</ref> (in the main text), switching from a multi-clip to single-clip X3D model on Kinetics 600 (where we cover the entire 10-second clip) results in much higher computational efficiency per video. Existing work typically factors out FLOPs in terms of FLOPs per subclip, but it can hide the true cost of computation, since we can keep adding more clips to boost accuracy higher.</p><p>We also evaluate the differences between training the same MoViNet-A2 model on smaller clips vs. longer clips and evaluating the models with multi-clip vs. single-clip, as seen in <ref type="figure" target="#fig_7">Figure 8</ref>. For multi-clip evaluation, we can see that accuracy improves when the number of clips fill the whole duration of the video (this can be seen at 5 clips for 8 training frames and at 3 clips for 16 training frames), and only  very slightly improves as we add more clips. However, if we train MoViNet-A2 on 16 frames and evaluate on 80 frames (so that we cover all 10 seconds of the video), this results in higher accuracy than the same number of frames using multi-clip eval. Furthermore, we can boost this accuracy even higher if we use 48 frames to train our model. Using stream buffers, we can reduce memory usage of training so that we can train using 48 frames while only using the memory of embedding 16 frames at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Streaming vs. Non-Streaming Evaluation</head><p>One question we have wondered is if the distribution of features learned is different from streaming and nonstreaming architectures. In <ref type="figure" target="#fig_8">Figure 9</ref>, we plot the average accuracy across Kinetics 600 of a model evaluated on a single frame by embedding an entire video, pooling across spatial dimensions, and applying the classification layers independently on each frame.</p><p>We first notice that the accuracy MobileNetV3 and MoViNet-A2 exhibit a Laplace distribution, on average peaking at the center frame of each video. Since Mo-bileNetV3 is evaluated on each frame independently, we can observe that the most salient part of the actions is on average in the video's midpoint. This is a good indicator that the videos in Kinetics are trimmed very well to center around the most salient part of each action. Likewise, MoViNet-A2, with balanced 3D convolutions, has the same characteristics as MobileNetV3, just with higher accuracy.</p><p>However, the dynamics of streaming MoViNet-A2 with causal convolutions is entirely different. The distribution of accuracy fluctuates and varies more than non-streaming architectures. By removing the ability for the network to see all frames as a whole with causal convolutions, the aggregation of features is not the same as when using balanced convolutions. Despite this difference, overall, the accuracy dif- ference across all videos is only about 1%. And by looking at top-5 accuracy in <ref type="table">Table 10</ref>, we can notice that streaming architectures nearly perform the same, despite the apparent information loss when transitioning to a model with a timeunidirectional receptive field. <ref type="figure" target="#fig_0">Figure 10</ref> shows how training clip duration affects the accuracy of a model evaluated at different durations. We can see that MoViNet can generalize well beyond the original clip duration it was trained with, always improving in accuracy with more frames. However, the model does notably worse if evaluated on clips with shorter durations than it was trained on. Longer clip duration for training translates to better accuracy for evaluation on longer clips overall. And with a stream buffer, we can train on even longer sequences to boost evaluation performance even higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Long Video Sequences</head><p>However, we also see we can operate frame-by-frame with stream buffers, substantially saving memory, showing better memory efficiency than multi-clip approaches and requiring constant memory as the number of input frames increase (and therefore temporal receptive field). Despite the accuracy reduction, we can see MoViNet-Stream models perform very well on long video sequences and are still more efficient than X3D which requires splitting videos into smaller subclips. We encourage future work using multiclip evaluation to report results without overlapping subclips, which not only provides a much more representative accuracy measurement, tends to be more efficient as well.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Stream Buffers with Other Operations</head><p>WaveNet <ref type="bibr" target="#b28">[55]</ref> introduces causal convolution, where the receptive field on a stack of 1D convolutions is forced to only see activations up to the current time step, as opposed to balanced convolutions which expand their receptive fields in both directions. We take inspiration from causal convolutions [15, 13, 18] to design stream buffers. However, WaveNet only proposes 1D convolutions for generative modeling, using them for their autoregressive property. We generalize the idea of causal convolution to any local operation, and introduce stream buffers to be able to use causal operations for online inference, allowing frame-byframe predictions. In addition, Transformer-XL [17] caches activations in a temporal buffer much like our work, for use in long-range sequence modeling. However, the model is only causal across fixed sequences while our work can be causal across individual frames, and can even vary the number of frames in each clip (so long as frames are consecutive with no gaps or overlaps between clips). We can apply the same principle to other operations as well to generalize causal operations. Note that this approach is not inherently tied to any data type or modality. Stream buffers can also be used to model many kinds of temporal data, e.g., audio, text.</p><p>(2+1)D CNNs. Additionally, support for efficient 3D convolutions on mobile devices is currently fragmented, while 2D convolutions are well supported. We include the option to search for (2+1)D architectures, splitting up any 3D depthwise convolutions into a 2D spatial convolution followed by a 1D temporal convolution. We show that trivially changing a 3D architecture to (2+1)D decreases FLOPs while also keeping similar accuracy, as seen in    <ref type="table">Table 22</ref>. MoViNet-A5 Architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Accuracy vs. FLOPs and Memory on Kinetics 600.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effect of Streaming MoViNets on Memory on Kinetics 600. Top left: comparison of accuracy vs. max memory usage on a V100 GPU on our models, progressively increasing in size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Accuracy vs. FLOPs Comparison across 5 large-scale action recognition datasets. Each series represents a model family, with points representing individual models ordered by FLOPs. We report FLOPs per video using single-clip evaluation for all MoViNets and compare with competitive multi-clip (and reproduced single-clip) models, using a log-scale on the x-axis. * denotes reproduced models. For Charades, we evaluate on MoViNet A1, A4, A5, and A6 only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Standard Convolution vs. Causal Convolution. The figure illustrates the effective receptive field along a sequence of frames. The temporal kernel size is 3, with padding shown in white. Causal convolution can be performed efficiently by padding only on one side of the time axis thus to force the output causality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Single vs. Multi-Clip Evaluation on Kinetics 600. A comparison between the number of training frames and the number of eval frames and clips. The number of clips are shown inside each datapoint, where applicable. Other datapoints are evaluated on single clips. We use MoViNet-A2 with frame stride 3 for all datapoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Difference Between Streaming and Base MoViNets. The plot displays the average accuracy across the Kinetics 600 dataset of an embedded model by applying the classification layers independently on each frame. Shading around each solid line indicates one standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Generalization to Longer Clips. A display of how duration of a clip during training affects the evaluation accuracy of different clip durations during evaluation. We use MoViNet-A2 with frame stride 3 for all datapoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>MoViNet Operations Ablation on Kinetics 600. We compare different configurations on MoViNet-A1, including Conv/CausalConv, SE/CausalSE/No SE, and PosEnc, and report accuracy and GFLOPs per video.</figDesc><table><row><cell>MODEL</cell><cell cols="2">CAUSALCONV SE CAUSALSE POSENC TOP-1 GFLOPS</cell></row><row><cell></cell><cell>73.3</cell><cell>6.04</cell></row><row><cell></cell><cell>72.1</cell><cell>6.04</cell></row><row><cell></cell><cell>73.5</cell><cell>6.06</cell></row><row><cell>MoViNet-A1</cell><cell>74.0</cell><cell>6.06</cell></row><row><cell></cell><cell>74.9</cell><cell>6.06</cell></row><row><cell></cell><cell>75.2</cell><cell>6.06</cell></row><row><cell></cell><cell>77.7</cell><cell>56.9</cell></row><row><cell>MoViNet-A3</cell><cell>79.0</cell><cell>57.1</cell></row><row><cell></cell><cell>79.6</cell><cell>57.1</cell></row><row><cell></cell><cell>80.3</cell><cell>57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 -</head><label>5</label><figDesc>Appendix Bhas the detailed architectures of other MoViNets. Most notably, the network prefers large bottleneck width multipliers in the range [2.5, 3.5], often expanding or shrinking them after each layer. In contrast, X3D-M with similar compute requirements has a wider base feature width with a smaller</figDesc><table><row><cell>STAGE</cell><cell></cell><cell>OPERATION</cell><cell></cell><cell>OUTPUT SIZE</cell></row><row><cell>data</cell><cell></cell><cell>stride 5, RGB</cell><cell></cell><cell>50 × 224 2</cell></row><row><cell>conv 1</cell><cell></cell><cell>1 × 3 2 , 16</cell><cell></cell><cell>50 × 112 2</cell></row><row><cell>block 2</cell><cell></cell><cell>1×5 2 , 16, 40</cell><cell></cell></row><row><cell></cell><cell></cell><cell>3×3 2 , 16, 40</cell><cell></cell><cell>50 × 56 2</cell></row><row><cell></cell><cell></cell><cell>3×3 2 , 16, 64</cell><cell></cell></row><row><cell>block 3</cell><cell></cell><cell>3×3 2 , 40, 96</cell><cell></cell></row><row><cell></cell><cell>   </cell><cell>3×3 2 , 40, 120 3×3 2 , 40, 96 3×3 2 , 40, 96</cell><cell>   </cell><cell>50 × 28 2</cell></row><row><cell></cell><cell></cell><cell>3×3 2 , 40, 120</cell><cell></cell></row><row><cell>block 4</cell><cell></cell><cell>5×3 2 , 72, 240</cell><cell></cell></row><row><cell></cell><cell>   </cell><cell>3×3 2 , 72, 155 3×3 2 , 72, 240 3×3 2 , 72, 192</cell><cell>   </cell><cell>50 × 14 2</cell></row><row><cell></cell><cell></cell><cell>3×3 2 , 72, 240</cell><cell></cell></row><row><cell>block 5</cell><cell></cell><cell>5×3 2 , 72, 240</cell><cell></cell></row><row><cell></cell><cell>     </cell><cell>3×3 2 , 72, 240 3×3 2 , 72, 240 3×3 2 , 72, 240 1×5 2 , 72, 144</cell><cell>     </cell><cell>50 × 14 2</cell></row><row><cell></cell><cell></cell><cell>3×3 2 , 72, 240</cell><cell></cell></row><row><cell>block 6</cell><cell></cell><cell>5×3 2 , 144, 480</cell><cell></cell></row><row><cell></cell><cell>       </cell><cell>1×5 2 , 144, 384 1×5 2 , 144, 384 1×5 2 , 144, 480 1×5 2 , 144, 480 3×3 2 , 144, 480</cell><cell>       </cell><cell>50 × 7 2</cell></row><row><cell></cell><cell></cell><cell>1×3 2 , 144, 576</cell><cell></cell></row><row><cell>conv 7</cell><cell></cell><cell>1 × 1 2 , 640</cell><cell></cell><cell>50 × 7 2</cell></row><row><cell>pool 8</cell><cell></cell><cell>50 × 7 2</cell><cell></cell><cell>1 × 1 2</cell></row><row><cell>dense 9</cell><cell></cell><cell>1 × 1 2 , 2048</cell><cell></cell><cell>1 × 1 2</cell></row><row><cell>dense 10</cell><cell></cell><cell>1 × 1 2 , 600</cell><cell></cell><cell>1 × 1 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>MoViNet-A2 Architecture searched by TuNAS, running 50 frames on Kinetics 600. SeeTable 1for the search space definition detailing the meaning of each component. constant bottleneck multiplier of 2.25. The searched network prefers balanced 3x3x3 kernels, except at the first downsampling layers in the later blocks, which have 5x3x3 kernels. The final stage almost exclusively uses spatial ker-Runtime on an Nvidia V100 GPU on Kinetics 600. Latency is given for the entire video clip and per frame in ms.</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Top-1 Accuracy (%)</cell><cell>55 60 65 70 75</cell><cell>50</cell><cell>100 MFLOPs per Frame 150 200 MoViNet-Base MoViNet-Stream MobileNetV3 + TSM MobileNetV3</cell><cell cols="2">2 x86 CPU Latency per Frame (ms) 4 6 8 MoViNet-Stream MobileNetV3 + TSM MobileNetV3</cell></row><row><cell cols="6">Figure 5. CPU Latency Comparison on Kinetics 600. We</cell></row><row><cell cols="6">compare the efficiency of MoViNets (A0, A1, A2) vs. Mo-</cell></row><row><cell cols="6">bileNetV3 [33] using FLOPs and benchmarked latency on an x86</cell></row><row><cell cols="4">Xeon CPU at 3.70GHz.</cell><cell></cell></row><row><cell cols="3">MODEL</cell><cell cols="3">STREAMING VIDEO (MS) FRAME (MS) TOP-1</cell></row><row><cell cols="4">MoViNet-A0-Stream</cell><cell>183</cell><cell>3.7 70.3</cell></row><row><cell cols="4">MoViNet-A1-Stream</cell><cell>315</cell><cell>6.3 75.6</cell></row><row><cell cols="4">MoViNet-A2-Stream</cell><cell>325</cell><cell>6.5 76.5</cell></row><row><cell cols="4">MoViNet-A3-Stream</cell><cell>1110</cell><cell>9.2 79.6</cell></row><row><cell cols="4">MoViNet-A4-Stream</cell><cell>1130</cell><cell>14.1 80.5</cell></row><row><cell cols="4">MoViNet-A5-Stream</cell><cell>2310</cell><cell>19.2 82.0</cell></row><row><cell cols="4">MobileNetV3-S*</cell><cell>68</cell><cell>1.4 61.3</cell></row><row><cell cols="4">MobileNetV3-L*</cell><cell>81</cell><cell>1.6 68.1</cell></row><row><cell cols="4">X3D-M* (Single Clip)</cell><cell>345</cell><cell>6.9 76.9</cell></row><row><cell cols="4">X3D-XL* (Single Clip)</cell><cell>943</cell><cell>18.9 80.3</cell></row></table><note>nels of size 1x5x5, indicating that high-level features for classification benefit from mostly spatial features. This comes at a contrast to S3D [85], which reports improved efficiency when using 2D convolutions at lower layers and 3D convolutions at higher layers.* denotes reproduced models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>[ 12 ]</head><label>12</label><figDesc>Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299-6308, 2017. 2, 5 [13] Shuo-Yiin Chang, Bo Li, Gabor Simko, Tara N Sainath, Anshuman Tripathi, Aäron van den Oord, and Oriol Vinyals. Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203-213, 2020. 1, 2, 3, 5, 6, 7, 15 [25] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In</figDesc><table><row><cell>Temporal modeling using dilated convolution and gating for</cell><cell>Proceedings of the IEEE international conference on com-</cell></row><row><cell>voice-activity-detection. In 2018 IEEE International Confer-</cell><cell>puter vision, pages 6202-6211, 2019. 1, 2, 6, 7, 15, 16</cell></row><row><cell>ence on Acoustics, Speech and Signal Processing (ICASSP),</cell><cell>[26] Christoph Feichtenhofer, Axel Pinz, and Richard P Wildes.</cell></row><row><cell>pages 5549-5553. IEEE, 2018. 2, 18</cell><cell>Spatiotemporal multiplier networks for video action recog-</cell></row><row><cell>[14] Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and</cell><cell>nition. In Proceedings of the IEEE conference on computer</cell></row><row><cell>Rogerio Feris. Big-little net: An efficient multi-scale fea-</cell><cell>vision and pattern recognition, pages 4768-4777, 2017. 2</cell></row><row><cell>ture representation for visual and speech recognition. arXiv</cell><cell>[27] Tommaso Furlanello, Zachary C Lipton, Michael Tschan-</cell></row><row><cell>preprint arXiv:1807.03848, 2018. 2</cell><cell>nen, Laurent Itti, and Anima Anandkumar. Born again neural</cell></row><row><cell>[15] Changmao Cheng, Chi Zhang, Yichen Wei, and Yu-Gang</cell><cell>networks. arXiv preprint arXiv:1805.04770, 2018. 2</cell></row><row><cell>Jiang. Sparse temporal causal convolution for efficient ac-</cell><cell>[28] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-</cell></row><row><cell>tion modeling. In Proceedings of the 27th ACM International</cell><cell>ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,</cell></row><row><cell>Conference on Multimedia, pages 592-600, 2019. 2, 18</cell><cell>Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz</cell></row><row><cell>[16] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-van, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. 5 [17] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. 2, 18 [18] Divyanshu Daiya, Min-Sheng Wu, and Che Lin. Stock movement prediction that integrates heterogeneous data sources using dilated causal convolution networks with at-tention. In ICASSP 2020-2020 IEEE International Confer-ence on Acoustics, Speech and Signal Processing (ICASSP), pages 8359-8363. IEEE, 2020. 2, 18 [19] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision. arXiv preprint arXiv:2006.13256, 2020. 5</cell><cell>Mueller-Freitag, et al. The" something something" video database for learning and evaluating visual common sense. In Proceedings of the IEEE International Conference on Computer Vision, pages 5842-5850, 2017. 2, 5 [29] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and im-agenet? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6546-6555, 2018. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 2, 3, 7 [31] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-yuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-tion, pages 558-567, 2019. 14 [32] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. 5</cell></row><row><cell>[20] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,</cell><cell>[33] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh</cell></row><row><cell>Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,</cell><cell>Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,</cell></row><row><cell>and Trevor Darrell. Long-term recurrent convolutional net-</cell><cell>Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-</cell></row><row><cell>works for visual recognition and description. In Proceed-</cell><cell>bilenetv3. In Proceedings of the IEEE International Confer-</cell></row><row><cell>ings of the IEEE conference on computer vision and pattern</cell><cell>ence on Computer Vision, pages 1314-1324, 2019. 1, 2, 3,</cell></row><row><cell>recognition, pages 2625-2634, 2015. 2</cell><cell>6, 7, 9, 14</cell></row><row><cell>[21] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-</cell><cell>[34] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-</cell></row><row><cell>weighted linear units for neural network function approxima-</cell><cell>works. In Proceedings of the IEEE conference on computer</cell></row><row><cell>tion in reinforcement learning. Neural Networks, 107:3-11,</cell><cell>vision and pattern recognition, pages 7132-7141, 2018. 2,</cell></row><row><cell>2018. 14</cell><cell>3, 13</cell></row><row><cell>[22] Linxi Fan, Shyamal Buch, Guanzhi Wang, Ryan Cao, Yuke</cell><cell>[35] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-</cell></row><row><cell>Zhu, Juan Carlos Niebles, and Li Fei-Fei. Rubiksnet: Learn-</cell><cell>tional neural networks for human action recognition. IEEE</cell></row><row><cell>able 3d-shift for efficient video action recognition. In Pro-</cell><cell>transactions on pattern analysis and machine intelligence,</cell></row><row><cell>ceedings of the European Conference on Computer Vision</cell><cell>35(1):221-231, 2012. 2</cell></row><row><cell>(ECCV), 2020. 2</cell><cell>[36] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and</cell></row><row><cell></cell><cell>Junjie Yan.</cell></row></table><note>[23] Quanfu Fan, Chun-Fu Richard Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation. In Advances in Neural Information Processing Systems, pages 2264-2273, 2019. 2 [24] Christoph Feichtenhofer. X3d:Stm: Spatiotemporal and motion encoding for action recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 2000-2009, 2019. 2 [37] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 15</head><label>15</label><figDesc>for Epic Kitchens 100. For a table of results on Kinetics 600, seeTable 10and also the main text.</figDesc><table><row><cell>DATASET</cell><cell>TRAIN</cell><cell cols="2">VALID RELEASED</cell></row><row><cell>Kinetics 400</cell><cell cols="3">215,435 (87.5%) 17,686 (88.4%) May 2017</cell></row><row><cell>Kinetics 600</cell><cell cols="3">364,305 (92.8%) 27,764 (92.5%) Aug 2018</cell></row><row><cell>Kinetics 700</cell><cell cols="2">524,595 (96.2%) 33,567 (95.9%)</cell><cell>Jul 2019</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>The number of examples available for each of the Kinetics dataset splits at the time of writing (Sept 20, 2020) along with the percentages compared to examples available on release. Each dataset loses about 4% of its examples per year. Accuracy of MoViNet on Kinetics 400.</figDesc><table><row><cell>MODEL</cell><cell cols="3">TOP-1 TOP-5 GFLOPS PARAM</cell></row><row><cell>MoViNet-A0</cell><cell cols="2">65.8 87.4</cell><cell>2.71 3.1M</cell></row><row><cell>MoViNet-A1</cell><cell cols="2">72.7 91.2</cell><cell>6.02 4.6M</cell></row><row><cell>X3D-XS [24]</cell><cell>69.5</cell><cell>-</cell><cell>23.3 3.8M</cell></row><row><cell>MoViNet-A2</cell><cell cols="2">75.0 92.3</cell><cell>10.3 4.8M</cell></row><row><cell>X3D-S [24]</cell><cell>73.5</cell><cell>-</cell><cell>76.1 3.8M</cell></row><row><cell>MoViNet-A3</cell><cell cols="2">78.2 93.8</cell><cell>56.9 5.3M</cell></row><row><cell>X3D-M [24]</cell><cell cols="2">76.0 92.3</cell><cell>186 3.8M</cell></row><row><cell>MoViNet-A4</cell><cell cols="2">80.5 94.5</cell><cell>105 5.9M</cell></row><row><cell>X3D-L [24]</cell><cell cols="2">77.5 92.9</cell><cell>744 6.1M</cell></row><row><cell>MoViNet-A5</cell><cell cols="2">80.9 94.9</cell><cell>281 15.7M</cell></row><row><cell>X3D-XL [24]</cell><cell cols="2">79.1 93.9</cell><cell>1452 11.0M</cell></row><row><cell>MoViNet-A6</cell><cell cols="2">81.5 95.3</cell><cell>386 31.4M</cell></row><row><cell>X3D-XXL [24]</cell><cell cols="2">80.4 94.6</cell><cell>5800 20.3M</cell></row><row><cell>TimeSformer-L [5]</cell><cell cols="2">80.7 94.7</cell><cell>7140 120M</cell></row><row><cell>ViViT-L/16x2 [2]</cell><cell cols="2">81.3 94.7</cell><cell>3990 88.9M</cell></row><row><cell>MODEL</cell><cell cols="3">TOP-1 Top-5 GFLOPS Param</cell></row><row><cell>MoViNet-A0</cell><cell cols="2">71.5 90.4</cell><cell>2.71 3.1M</cell></row><row><cell>MoViNet-A0-Stream</cell><cell cols="2">70.3 90.1</cell><cell>2.73 3.1M</cell></row><row><cell>MoViNet-A1</cell><cell cols="2">76.0 92.6</cell><cell>6.02 4.6M</cell></row><row><cell>MoViNet-A1-Stream</cell><cell cols="2">75.6 92.8</cell><cell>6.06 4.6M</cell></row><row><cell>MoViNet-A2</cell><cell cols="2">77.5 93.4</cell><cell>10.3 4.8M</cell></row><row><cell>MoViNet-A2-Stream</cell><cell cols="2">76.5 93.3</cell><cell>10.4 4.8M</cell></row><row><cell>MoViNet-A3</cell><cell cols="2">80.8 94.5</cell><cell>56.9 5.3M</cell></row><row><cell cols="3">MoViNet-A3 + AutoAugment 81.3 95.3</cell><cell>56.9 5.3M</cell></row><row><cell>MoViNet-A4</cell><cell cols="2">81.2 94.9</cell><cell>105 4.9M</cell></row><row><cell cols="3">MoViNet-A4 + AutoAugment 83.0 96.0</cell><cell>105 4.9M</cell></row><row><cell>X3D-M [24]</cell><cell cols="2">78.8 94.5</cell><cell>186 3.8M</cell></row><row><cell>MoViNet-A5</cell><cell cols="2">82.7 95.7</cell><cell>281 15.7M</cell></row><row><cell cols="3">MoViNet-A5 + AutoAugment 84.3 96.4</cell><cell>281 15.7M</cell></row><row><cell>MoViNet-A6</cell><cell cols="2">83.5 96.2</cell><cell>386 15.7M</cell></row><row><cell cols="3">MoViNet-A6 + AutoAugment 84.8 96.5</cell><cell>386 15.7M</cell></row><row><cell>X3D-XL [24]</cell><cell cols="2">81.9 95.5</cell><cell>1452 11.0M</cell></row><row><cell>SlowFast-R50 [25]</cell><cell cols="2">78.8 94.0</cell><cell>1080 34.4M</cell></row><row><cell>SlowFast-R101 [25]</cell><cell cols="2">81.8 95.1</cell><cell>7020 59.9M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>Accuracy of MoViNet on Kinetics 700.</figDesc><table><row><cell>MODEL</cell><cell cols="2">TOP-1 GFLOPS PARAM</cell></row><row><cell>MoViNet-A0</cell><cell>27.5</cell><cell>4.07 3.1M</cell></row><row><cell>MoViNet-A1</cell><cell>32.0</cell><cell>9.03 4.6M</cell></row><row><cell>TVN-1 [57]</cell><cell>23.1</cell><cell>13.0 11.1M</cell></row><row><cell>MoViNet-A2</cell><cell>34.3</cell><cell>15.5 4.8M</cell></row><row><cell>MoViNet-A2-Stream</cell><cell>33.6</cell><cell>15.6 4.8M</cell></row><row><cell>TVN-2 [57]</cell><cell>24.2</cell><cell>17.0 110M</cell></row><row><cell>MoViNet-A3</cell><cell>35.6</cell><cell>35.6 5.3M</cell></row><row><cell>TVN-3 [57]</cell><cell>25.4</cell><cell>69.0 69.4M</cell></row><row><cell>MoViNet-A4</cell><cell>37.9</cell><cell>98.4 4.9M</cell></row><row><cell>TVN-4 [57]</cell><cell>27.8</cell><cell>106 44.2M</cell></row><row><cell>MoViNet-A5</cell><cell>39.1</cell><cell>175 15.7M</cell></row><row><cell>SRTG-R3D-34</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Accuracy of MoViNet on Moments in Time. All MoViNets are evaluated on 75 frames at 25 fps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 .</head><label>15</label><figDesc>Top-1 Accuracy of MoViNet on Epic Kitchens 100 on Action, Verb, and Noun classes. All MoViNets are evaluated on 32 frames at 12 fps.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 .</head><label>16</label><figDesc>3D vs. (2+1)D on Kinetics 600.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>table 16 .Table 18 .</head><label>1618</label><figDesc>Here we define MoViNet-A2b as a searched model similar to MoViNet-A2. MoViNet-A1 Architecture.</figDesc><table><row><cell>STAGE</cell><cell>OPERATION</cell><cell>OUTPUT SIZE</cell></row><row><cell>data</cell><cell>stride 5, RGB</cell><cell>50 × 172 2</cell></row><row><cell>conv1</cell><cell>1 × 3 2 , 8</cell><cell>50 × 86 2</cell></row><row><cell>block2</cell><cell>1×5 2 , 8, 40</cell><cell>50 × 43 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 19 .</head><label>19</label><figDesc>MoViNet-A2 Architecture.</figDesc><table><row><cell>STAGE</cell><cell></cell><cell>OPERATION</cell><cell></cell><cell>OUTPUT SIZE</cell></row><row><cell>data</cell><cell></cell><cell>stride 5, RGB</cell><cell></cell><cell>120 × 256 2</cell></row><row><cell>conv1</cell><cell></cell><cell>1 × 3 2 , 16</cell><cell></cell><cell>120 × 128 2</cell></row><row><cell>block2</cell><cell></cell><cell>1×5 2 , 16, 40</cell><cell></cell></row><row><cell></cell><cell> </cell><cell>3×3 2 , 16, 40 3×3 2 , 16, 64</cell><cell cols="2">  120 × 64 2</cell></row><row><cell>block3</cell><cell></cell><cell>3×3 2 , 16, 40 3×3 2 , 48, 112</cell><cell></cell></row><row><cell></cell><cell>     </cell><cell>3×3 2 , 48, 144 3×3 2 , 48, 112 1×5 2 , 48, 112 3×3 2 , 48, 144</cell><cell>     </cell><cell>120 × 32 2</cell></row><row><cell>block4</cell><cell></cell><cell>3×3 2 , 48, 144 5×3 2 , 80, 240</cell><cell></cell></row><row><cell></cell><cell>   </cell><cell>3×3 2 , 80, 152 3×3 2 , 80, 240 3×3 2 , 80, 192</cell><cell>   </cell><cell>120 × 16 2</cell></row><row><cell>block5</cell><cell></cell><cell>3×3 2 , 80, 240 5×3 2 , 88, 264</cell><cell></cell></row><row><cell></cell><cell>         </cell><cell>3×3 2 , 88, 264 3×3 2 , 88, 264 3×3 2 , 88, 264 1×5 2 , 88, 160 3×3 2 , 88, 264 3×3 2 , 88, 264</cell><cell>         </cell><cell>120 × 16 2</cell></row><row><cell>block6</cell><cell></cell><cell>3×3 2 , 88, 264 5×3 2 , 168, 560</cell><cell></cell></row><row><cell></cell><cell>             </cell><cell>1×5 2 , 168, 448 1×5 2 , 168, 448 1×5 2 , 168, 560 1×5 2 , 168, 560 3×3 2 , 168, 560 1×5 2 , 168, 448 1×5 2 , 168, 448 3×3 2 , 168, 560</cell><cell>             </cell><cell>120 × 8 2</cell></row><row><cell></cell><cell></cell><cell>1×3 2 , 168, 672</cell><cell></cell></row><row><cell>conv7</cell><cell></cell><cell>1 × 1 2 , 744</cell><cell></cell><cell>120 × 8 2</cell></row><row><cell>pool8</cell><cell></cell><cell>120 × 8 2</cell><cell></cell><cell>1 × 1 2</cell></row><row><cell>dense9</cell><cell></cell><cell>1 × 1 2 , 2048</cell><cell></cell><cell>1 × 1 2</cell></row><row><cell>dense10</cell><cell></cell><cell>1 × 1 2 , 600</cell><cell></cell><cell>1 × 1 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 20 .Table 21 .</head><label>2021</label><figDesc>MoViNet-A3 Architecture. MoViNet-A4 Architecture.</figDesc><table><row><cell>STAGE</cell><cell></cell><cell>OPERATION</cell><cell></cell><cell>OUTPUT SIZE</cell></row><row><cell>data</cell><cell></cell><cell>stride 5, RGB</cell><cell></cell><cell>80 × 290 2</cell></row><row><cell>conv1</cell><cell></cell><cell>1 × 3 2 , 24</cell><cell></cell><cell>80 × 145 2</cell></row><row><cell>block2</cell><cell></cell><cell>1×5 2 , 24, 64</cell><cell></cell></row><row><cell></cell><cell>     </cell><cell>3×3 2 , 24, 64 3×3 2 , 24, 96 3×3 2 , 24, 64 3×3 2 , 24, 96</cell><cell>     </cell><cell>80 × 72 2</cell></row><row><cell>block3</cell><cell></cell><cell>3×3 2 , 24, 64 5×3 2 , 56, 168</cell><cell></cell></row><row><cell></cell><cell>           </cell><cell>3×3 2 , 56, 168 3×3 2 , 56, 136 3×3 2 , 56, 136 3×3 2 , 56, 168 3×3 2 , 56, 168 3×3 2 , 56, 168 1×5 2 , 56, 136</cell><cell>           </cell><cell>80 × 36 2</cell></row><row><cell>block4</cell><cell></cell><cell>3×3 2 , 56, 136 5×3 2 , 96, 320</cell><cell></cell></row><row><cell></cell><cell>           </cell><cell>3×3 2 , 96, 160 3×3 2 , 96, 320 3×3 2 , 96, 192 3×3 2 , 96, 320 3×3 2 , 96, 152 3×3 2 , 96, 320 3×3 2 , 96, 256</cell><cell>           </cell><cell>80 × 18 2</cell></row><row><cell>block5</cell><cell></cell><cell>3×3 2 , 96, 320 5×3 2 , 96, 320</cell><cell></cell></row><row><cell></cell><cell>             </cell><cell>3×3 2 , 96, 320 3×3 2 , 96, 320 3×3 2 , 96, 320 1×5 2 , 96, 192 3×3 2 , 96, 320 3×3 2 , 96, 320 3×3 2 , 96, 192 3×3 2 , 96, 320</cell><cell>             </cell><cell>80 × 18 2</cell></row><row><cell>block6</cell><cell></cell><cell>3×3 2 , 96, 320 5×3 2 , 192, 640</cell><cell></cell></row><row><cell></cell><cell>                  </cell><cell>1×5 2 , 192, 512 1×5 2 , 192, 512 1×5 2 , 192, 640 1×5 2 , 192, 640 3×3 2 , 192, 640 1×5 2 , 192, 512 1×5 2 , 192, 512 1×5 2 , 192, 640 1×5 2 , 192, 768 1×5 2 , 192, 640 3×3 2 , 192, 640</cell><cell>                  </cell><cell>80 × 9 2</cell></row><row><cell></cell><cell></cell><cell>3×3 2 , 192, 768</cell><cell></cell></row><row><cell>conv7</cell><cell></cell><cell>1 × 1 2 , 856</cell><cell></cell><cell>80 × 9 2</cell></row><row><cell>pool8</cell><cell></cell><cell>80 × 9 2</cell><cell></cell><cell>1 × 1 2</cell></row><row><cell>dense9</cell><cell></cell><cell>1 × 1 2 , 2048</cell><cell></cell><cell>1 × 1 2</cell></row><row><cell>dense10</cell><cell></cell><cell>1 × 1 2 , 600</cell><cell></cell><cell>1 × 1 2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://activity-net.org/challenges/2020/tasks/guest_kinetics.html.2020.16" />
		<title level="m">Activitynet task b: Kinetics challenge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanru</forename><forename type="middle">Henry</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887,2020.14</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rezero is all you need: Fast convergence at large depth</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can weight sharing outperform random architecture search? an investigation with tunas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient video classification using fewer frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shweta</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukundhan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-the-shelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A short note about kinetics-600. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">search with bayesian optimisation and optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Mazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2016" to="2025" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">When ensembling smaller models is more efficient than single large models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00570</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motionsqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="345" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic multiple choice learning for training diverse deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Purushwalkam Shiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2119" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Diverse temporal aggregation and depthwise spatiotemporal factorization for efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Il</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00317</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent tubelet proposal and recognition networks for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="303" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13087</idno>
		<title level="m">Perf-net: Pose empowered rgb-flow net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Videolstm convolves, attends and flows for action recognition. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9605" to="9616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06803</idno>
		<title level="m">Temporal adaptive module for video recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Lobacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadezhda</forename><surname>Chirkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Kodryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<title level="m">On power laws in deep ensembles. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tiny video networks: Architecture search for efficient video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Assemblenet++: Assembling modality representations via attention connections-supplementary material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhana</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Kangaspunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13209</idno>
		<title level="m">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Swapout: Learning an ensemble of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learn to cycle: Time-consistent feature discovery for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08247</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lattice long short-term memory for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dynamic inference: A new approach toward efficient video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="676" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04851</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Faster recurrent networks for efficient video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13098" to="13105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
