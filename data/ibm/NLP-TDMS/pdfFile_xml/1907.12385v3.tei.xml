<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Space Factorisation and Manipulation via Matrix Subspace Projection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhe</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaozheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Guerin</surname></persName>
						</author>
						<title level="a" type="main">Latent Space Factorisation and Manipulation via Matrix Subspace Projection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle the problem disentangling the latent space of an autoencoder in order to separate labelled attribute information from other characteristic information. This then allows us to change selected attributes while preserving other information. Our method, matrix subspace projection, is much simpler than previous approaches to latent space factorisation, for example not requiring multiple discriminators or a careful weighting among their loss functions. Furthermore our new model can be applied to autoencoders as a plugin, and works across diverse domains such as images or text. We demonstrate the utility of our method for attribute manipulation in autoencoders trained across varied domains, using both human evaluation and automated methods. The quality of generation of our new model (e.g. reconstruction, conditional generation) is highly competitive to a number of strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We investigate the problem of manipulating multiple attributes of data samples. This can be applied to image data, for example to manipulate a picture of a face to add a beard, change gender, or age. It can also be applied to text, for example to change the style or sentiment of a text. We assume that we have a training dataset where attributes are labelled. However there is an unsupervised aspect because we do not have samples of the same individual with different attribute combinations, e.g., the same person with and without a beard. Furthermore the training samples have some attribute combinations that are highly correlated, while other combinations are completely absent; e.g., in the CelebA dataset blond hair and earrings are highly correlated with female <ref type="bibr" target="#b21">(Torfason et al., 2016)</ref>, while a female with beard is absent. Nevertheless we would like our system to somehow isolate the explanatory factors in pixel space, to understand, e.g., that blond hair corresponds only to colour changes to hair pixels, and no change elsewhere in the face.</p><p>This challenge of isolating multiple explanatory factors poses interesting problems for generative models. In images of faces for example, even if the training data has no bearded lady, a good generative model should be able to 'imagine' novel examples that combine attributes in ways not present in training data. As noted by <ref type="bibr" target="#b6">Higgins et al. (2016)</ref> "Models are unable to generalise to data outside of the convex hull of the training distribution . . . unless they learn about the data generative factors and recombine them in novel ways." Ideally we should fully disentangle and isolate the data generative factors, so that we can represent the generative factors of a sample with a vector that has one part labelled attribute information, and another part with the other characteristic information of the sample. This is in a small way part of a general trend to try to move deep neural network research towards explanatory models of the world <ref type="bibr" target="#b15">(LeCun, 2013;</ref><ref type="bibr" target="#b13">Lake et al., 2016;</ref><ref type="bibr" target="#b26">Yuille &amp; Liu, 2018)</ref>, which requires disentanglement. The problem is important because isolating explanatory factors is a way to overcome the combinatorial explosion of required training examples if such factors are not isolated <ref type="bibr" target="#b26">(Yuille &amp; Liu, 2018)</ref>.</p><p>A typical approach to the problem uses an autoencoder (AE) which encodes a given input (e.g. picture, text, etc.) into a latent vector, and then restores (decodes) the latent vector to the given input <ref type="bibr" target="#b14">(Lample et al., 2017;</ref><ref type="bibr" target="#b8">Hu et al., 2017;</ref><ref type="bibr" target="#b24">Xiao et al., 2018;</ref><ref type="bibr" target="#b17">Li et al., 2019)</ref>. The latent vector contains the attribute information as well as other characteristic information of the input. If one can change the attribute information in the latent space, then one can generate examples with the altered attributes. The difficulty here is twofold: (1) learn a latent space representation which separates the attributes from all other characteristic information, and (2) fully disentangle the attributes. If we fail in the separation part, then efforts to generate with specific attributes may conflict with other information in the latent space (as in <ref type="bibr" target="#b10">Kingma et al. (2014)</ref> etc., see §2). If we fail in the second part then examples generated with specified attributes will also be contaminated with spurious attributes (see <ref type="figure" target="#fig_0">Fig. 1</ref> Left).</p><p>Many recent approaches make use of auxiliary neural <ref type="bibr">net-arXiv:1907.12385v3 [cs.</ref>LG] 14 Aug 2020  <ref type="bibr" target="#b23">(Wu et al., 2019)</ref>, where the only attribute changed is hair colour, but we see significant changes in skin colour, eyebrows, eyes, and lips. Right: from Fader <ref type="bibr" target="#b14">(Lample et al., 2017)</ref>, where female was changed to male, but female eyebrows are retained above the male ones, due to skip connections. work structures with adversarial training in the style of Generative Adversarial Networks (GANs). These new networks can be used to remove attribute information from the latent space <ref type="bibr" target="#b14">(Lample et al., 2017)</ref>, or to feedback a loss term to impose the attributes they want to appear in the output <ref type="bibr" target="#b4">(He et al., 2019)</ref>. These adversarial approaches have competing loss terms (for example reconstruction loss, attribute classification loss, realistic output loss), and require a careful choice of hyperparameters to weight these loss functions. In the case of <ref type="bibr" target="#b14">Lample et al. (2017)</ref> a slowly increasing loss was critical. These hyperparameters and training schedules must be determined by trial and error, to avoid training instability. Even after successful training we have found that some models ignore the desired attributes and put too much weight on reconstruction and realistic output (see §4). This is partly because we push systems to the very difficult setting of training for multiple attributes together (e.g. 40 attributes for CelebA). This is a very demanding setting for disentanglement, e.g. to dissociate lipstick, make-up, and blond hair from female, and to dissociate beard, bushy eyebrows, and 5 o'clock shadow from male.</p><p>We propose a simple and generic method, Matrix Subspace Projection (MSP), which directly separates the attribute information from all other non-attribute information, without relying on weighting loss terms from auxiliary neural networks. Our variables representing attributes are fully disentangled, with one isolated variable for each attribute of the training set. Therefore, when we do conditional generation, we can assign pure attributes combined with other latent data which does not conflict, so that the generated pictures are of high quality and not contaminated with spurious attributes. Meanwhile, our model is a universal plugin. In theory, it can be applied to any existing AEs (if and only if the AEs use a latent vector). If the AE is a generative model (such as VAE), with our approach, it becomes a conditional generative model that generates content based on the given condition constraints. In the case of images, we add a Patch-GAN at the end of our generator to sharpen the image, but this is not connected with the attribute manipulation task and is not core to our model; it could be replaced with any super resolution and sharpening method.</p><p>Our plugin has two uses: (1) samples can be generated from a random seed, but with given attributes; (2) a given sample can be modified to have desired specified attributes. Our key contributions are: (1) A simple and universal plugin for conditional generation and content replacement, directly applicable to any AE architectures (e.g., image or text).</p><p>(2) Strong performance on learning disentangled latent representations of multiple (e.g. 40) attributes. (3) A principled weighting strategy for combining loss terms for training. The code for our model is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The first approaches to control of generation by attributes (conditional VAEs <ref type="bibr" target="#b10">(Kingma et al., 2014;</ref><ref type="bibr" target="#b20">Sohn et al., 2015;</ref><ref type="bibr" target="#b25">Yan et al., 2016)</ref>) simply added attribute information as an extra input to the encoder or the decoder. These approaches generate using a latent vector z and also an attribute vector y, where the z often conflicts with y, because attribute information has not been removed from z. With conflicting inputs the best the VAE can do is to produce a blurry image.</p><p>Generative Adversarial Networks (GANs) can be augmented with encoders. IcGAN trains separate encoders for the y and z vectors, but does not try to remove potentially conflicting information <ref type="bibr" target="#b19">(Perarnau et al., 2016)</ref>. The IcGAN authors also note that it can fail to generate unusual attribute combinations such as a woman with a moustache, because the GAN discriminator discourages the generator from generating samples outside the training distribution.</p><p>More recent work tackled the problem of separating the attribute information from the latent vector, using a new auxiliary network (like a GAN discriminator) <ref type="bibr" target="#b14">(Lample et al., 2017;</ref><ref type="bibr" target="#b2">Creswell et al., 2017;</ref><ref type="bibr" target="#b11">Klys et al., 2018)</ref>, which attempts to guess the attribute of the latent vector z, and penalise the generator if attribute information remains. A significant drawback of these adversarial approaches is that great care must be taken in training so that the loss from the discriminator (which is trying to remove attribute information) does not disturb the training to produce a good reconstruction. In the case of Fader networks <ref type="bibr" target="#b14">(Lample et al., 2017)</ref> it was necessary to start with a discriminator loss weight of zero, and linearly increase to 0.0001 over the first 500,000 iterations; the authors state "This scheduling turned out to be critical in our experiments. Without it, we observed that the encoder was too affected by the loss coming from the discriminator, even for low values of [loss coefficient]."</p><p>While this adversarial approach can successfully remove attribute information from z, there is nothing to stop the decoder (generator) from associating other spurious information with the attribute. For example the decoder might associate the attribute intended to be for 'glasses' with an older or more masculine face. This is what we see in the results of two of the adversarial approaches (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Most of the results in <ref type="bibr" target="#b2">Creswell et al. (2017)</ref> focus on the attribute 'smiling' (not reproduced here), and this is very well disentangled. It is only when the training dataset associates other attributes with the trained attribute that entanglement will arise. In <ref type="bibr" target="#b2">Creswell et al. (2017)</ref> the attribute vector is a single binary variable so that the system can only be trained to control (or classify) one attribute. It is not unexpected that a generator will associate spurious information with an attribute if the association is present in the training data and the system has been trained only on examples labelling a single attribute, e.g., glasses. The system cannot know that it should isolate 'wearing glasses', and not 'wearing glasses and older'. Fader Networks <ref type="bibr" target="#b14">(Lample et al., 2017)</ref> can train for multiple attributes together, however <ref type="bibr" target="#b4">He et al. (2019)</ref> state that "Although Fader Networks is capable for multiple attribute editing with one model, in practice, multiple attribute setting makes the results blurry."</p><p>The most recent works (2018-19) are GAN-based. They do not try to remove attribute information from the latent space, but instead add an additional attribute classifier after generation, and impose an attribute classification loss. This is in addition to a typical GAN discriminator for realistic images. AttGAN <ref type="bibr" target="#b4">(He et al., 2019)</ref> uses an endoder, decoder (generator), and the attribute classifier and discriminator applied to the output of the generator. StarGAN <ref type="bibr" target="#b1">(Choi et al., 2018)</ref> and RelGAN <ref type="bibr" target="#b23">(Wu et al., 2019)</ref> use no encoder, but use a singe generator twice, in a cycle; the first direction alters attributes like a conditional GAN, the second one attempts to reconstruct the image (using original attributes), and so requires that non-attribute information has been preserved. StarGAN uses a discriminator and attribute classifier, like AttGAN, while RelGAN adds a third network for interpolation.</p><p>All the works cited from 2017 to 2019 have an adversarial component (in the style of a GAN); they train auxiliary classifiers to feed back loss terms, to ensure they remove undesirable attributes, or enforce desired ones. They need a careful weighting among loss terms, but there is no principled method for determining these weighting hyperparameters. Our work does not rely on an adversarial component to manipulate attributes; we use a more direct method of matrix projection onto subspaces, in order to factorise the latent representation and separate attributes from other information. Furthermore, unlike the above works 2 we do not use any skip connections. Skip connections can introduce errors when a region of the source and target image is quite different, we illustrate this further in <ref type="figure" target="#fig_0">Fig. 1</ref> Right.</p><p>In addition to the above works using labelled attributes there is also work on the more difficult problem of unsupervised learning of disentangled generative factors of data <ref type="bibr" target="#b0">(Chen et al., 2016;</ref><ref type="bibr" target="#b7">Higgins et al., 2017;</ref><ref type="bibr" target="#b12">Kumar et al., 2018)</ref>. However the supervised (labelled) approaches generate much clearer samples of selected attributes, and superior disentanglement. An alternative approach to controlled generation is to simply train a deep convolutional network and do linear interpolation in deep feature space <ref type="bibr" target="#b22">(Upchurch et al., 2017)</ref>. This shows surprisingly good results, but in changing an attribute that should only affect a local area it can affect more image regions, and can produce unrealistic results for more rare face poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We are interested in factorising and manipulating multiple attributes from a latent representation learned by an arbitrary Autoencoder (AE). Suppose we are given a dataset</p><formula xml:id="formula_0">D of elements (x, y) with x ∈ R n and y ∈ Y = {0, 1} k representing k attributes of x.</formula><p>Let an arbitrary AE be represented by z = F (x) and x = G(z), where F (·) is the encoder, G(·) is the decoder, z is the latent vector encoding x, and x is the reconstruction of x (see <ref type="figure" target="#fig_2">Fig. 3</ref>). Note that when x is a good approximation of x (i.e., x ≈ x), the attribute information of x represented in y will also be captured in the latent encoding z. Attribute manipulation means that we replace the attributes y captured by z with new attributes y n . Let K(·) be a replacement function, then we have new latent space z n = K(z, y n ) and x n = G(K(z, y n )), where the attribute information encoded in y n can be predicted from x n and the non-attribute information of x will be preserved.</p><p>To give a concrete example, given an image of a face, x, we wish to manipulate x w.r.t. the presence or absence of a set of desired attributes encoded in y n (e.g., a face with or without smiles, wearing or not wearing glasses), producing the manipulated image x n , without changing the identify of the face (i.e., preserving the non-attribute information of x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Disentangled Latent Representations via Matrix Subspace Projection</head><p>To tackle the problem formulated in §3.1, we propose a generic method to factor out the information about attributes y from z based on the idea of performing orthogonal matrix projection onto subspaces. Our model works as a universal plugin and in theory, it can be applied to any existing AEs. The general architecture of the proposed MSP model is depicted in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>. Given a latent vector z encoding x and an arbitrarily complex invertible function H(·), H(·) transforms z to a new linear space (ẑ = H(z)) such that one can find a matrix M where (a) the projection ofẑ on M (denoted byŷ) approaches y (i.e.,ŷ captures attribute information),</p><formula xml:id="formula_1">M ·ẑ =ŷ;ŷ → y (1) and (b) there is an orthogonal matrix U ≡ [M; N],</formula><p>where N is the null space of M (i.e., M ⊥ N) and the projection ofẑ on N (denoted byŝ) captures non-attribute information.</p><p>As U is orthogonal, we also have U T ≡ U −1 . In other words, rather than fitting F and G, the encoder and decoder will fit H · F (·) and G · H −1 (·) instead. As M itself is an incomplete orthogonal matrix, similar operations cannot be applied to M.</p><p>Our main learning objective, in addition to the original AE objective (i.e. reconstruction loss L AE ; see §3.3 and Eq. 8), is then to estimate M which is nontrivial. We turn the problem of finding an optimal solution for M into an optimisation problem, in which we need to (1) enforceŷ to be as close to y (i.e., the vector encoding the ground truth attributes) as possible; and (2) minimise ||ŝ|| 2 so that s contains as little information fromẑ as possible. This can be formulated into the following loss function</p><formula xml:id="formula_2">L MSP = L 1 + L 2 (2) L 1 = ||ŷ − y|| 2 = ||M ·ẑ − y|| 2 (3) L 2 = ||ŝ|| 2<label>(4)</label></formula><p>Here L 1 and L 2 encode the above two constraints, respectively, andŷ is the predicted attributes. Given that the AE relies on the information ofẑ to reconstruct x, the optimisation constraints of L AE and L 2 essentially introduce an adversarial process: on the one hand, it discourages any information ofẑ to be stored inŝ due to the penalty from L 2 ; on the other hand, the AE requires information fromẑ to reconstruct x. So, the best solution is to only restore the essential information for reconstruction (except the attribute information) inŝ. By optimising L MSP , we causeẑ to be factorised, with the attribute information stored inŷ, whilê s only retains non-attribute information.</p><p>The first part of our loss function L 1 is relatively straightforward. The main obstacle here is to compute L 2 asŝ is unknown. We develop a strategy to compute ||ŝ|| 2 indirectly. According to the definition ofŷ andŝ we can derive:</p><formula xml:id="formula_3">L 2 =||ŝ|| 2 = ||ŝ − 0|| 2 =||[ŷ;ŝ] − [ŷ; 0]|| 2 Identical deformation =||U ·ẑ − [ŷ; 0]|| 2<label>(5)</label></formula><p>where the square brackets represent the vector concatenation. Because U is orthogonal, we have</p><formula xml:id="formula_4">L 2 =||U ·ẑ − [ŷ; 0]|| 2 =||U −1 · (U ·ẑ − [ŷ; 0])|| 2 =||ẑ − U −1 · [ŷ; 0]|| 2 = ||ẑ − U T · [ŷ; 0]|| 2 =||ẑ − [M; N] T · [ŷ; 0]|| 2 =||ẑ − M T ·ŷ|| 2 ≈ ||ẑ − M T · y|| 2<label>(6)</label></formula><p>With Eq. 6 (which makes use of the properties of orthogonal matrices), we avoid computingŝ and N directly when minimising ||ŝ|| 2 , and turn the minimisation problem into optimising M instead. Finally, we have:</p><formula xml:id="formula_5">L MSP = L 1 + L 2 = ||M ·ẑ − y|| 2 + ||ẑ − M T ·ŷ|| 2 ≈ ||M ·ẑ − y|| 2 + ||ẑ − M T · y|| 2<label>(7)</label></formula><p>Latent Space Factorisation and Manipulation via Matrix Subspace Projection</p><p>The loss function in Eq. 7 also guarantees that after training, the solution for M will be part of the orthogonal matrix U (see §4.5). When L MSP is small, the transposition of M becomes the inverse of M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Applying the Matrix Subspace Projection Framework to an AE</head><p>To apply our matrix subspace projection (MSP) framework to an existing AE, one only needs to derive a final loss function by combining the loss of the AE and the loss of our MSP framework.</p><formula xml:id="formula_6">L = L AE + αL MSP<label>(8)</label></formula><p>where α is the weight for L MSP . As illustrated in <ref type="figure" target="#fig_2">Fig. 3  (a)</ref> and <ref type="formula">(b)</ref>, one should note that applying our framework will not change the structure of the AE, where our MSP component simply takes the latent vectorẑ of the AE as input. L AE hopes thatŝ can store more information to reconstruct x, but L MSP wantsŝ to contain less information. When α is small, the model becomes a standard AE. When α is too large, the non-attribute information inẑ is reduced excessively, resulting in the generated products tending to the average of the training samples. Therefore, another challenge we face is how to set α appropriately.</p><p>We propose a principled strategy for effectively determining the value of α (this strategy is used in all experiments in this paper). Since L AE and L MSP essentially represent a competing relationship forẑ resources, we specify that L AE and L MSP have the same influence on updatingŝ. We use α to represent the "intensity" with which the AE updateŝ z during each back-propagation process. This intensity depends on the structure of the model and the loss function used by the model. For example, suppose an AE (for picture generation) uses a CNN decoder and L2-loss. During the training process, the error of each pixel between the generated picture and the true picture is backpropagated toẑ as the gradients ofẑ. The sum of these gradients is the final gradient ofẑ (i.e., corresponding to L AE ). For a picture with h × w pixels and c colour channels, there are h × w × c parts of gradients accumulated, so the intensity is h × w × c. The intensity ofŷ for updatingẑ is the total amount of attributes (i.e. the dimension ofŷ), because the error for each attribute is propagated back toẑ and accumulated (i.e., corresponding to L MSP ). Therefore, in order to balance the influence of L AE and L MSP on updatingẑ during training, we have:</p><formula xml:id="formula_7">α ≈ h × w × c size(attribute) + size(ẑ)<label>(9)</label></formula><p>When using the cross-entropy-loss (or NLL loss etc.), which is usually for textual generative models (e.g. the seq2seq model), each generated word produces only one intensity, regardless of the word embedding size. Meanwhile, loss values returned by the cross-entropy-loss are proportional to the error, but the loss values returned by the MSP loss (which is a MSE loss) are proportional to the error's square. Therefore, for a sentence of length k, the intensity of the entire sentence toẑ is k 2 , so that for cross-entropy-loss,</p><formula xml:id="formula_8">α ≈ k 2 size(attribute) + size(ẑ)<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Content Replacement and Conditional Generation</head><p>After MSP is trained (i.e., M is estimated), there are two ways to perform content replacement or change of attributes.</p><p>One way is to derive the orthogonal matrix U = [M; N] by solving the null space N of M (i.e., the null space is constituted of all the specific solutions for n w.r.t. equation M · n = 0, where n is an independent variable). Given an input x, we first encode it asẑ. We then use U to obtain the attribute vectorŷ of x and the non-attribute information vectorŝ as follows.</p><formula xml:id="formula_9">[ŷ;ŝ] = [M; N] ·ẑ = U ·ẑ = U · encoder(x)<label>(11)</label></formula><p>At this point, we can directly replace [ŷ;ŝ] with [y n ;ŝ], where y n is the new attribute vector. With [y n ;ŝ] and U T (note that U T approaches U −1 during training), we can derive the new latent code z n and then decode it into x n , which ultimately captures the desired new attributes.</p><p>x n = decoder(z n ) = decoder(U T · [y n ;ŝ])</p><p>Alternatively, we can avoid explicitly calculating matrix N (i.e. avoid calculatingŝ), for content replacement. According to Eqs.11 and 12, we define d as the distance betweenẑ and z n .</p><formula xml:id="formula_11">d =ẑ − z n = U T · [ŷ;ŝ] − U T · [y n ;ŝ] = U T · ([ŷ;ŝ] − [y n ;ŝ]) = U T · [ŷ − y n ; 0] = [M; N] T · [ŷ − y n ; 0] = M T · (ŷ − y n ) = M T · (M ·ẑ − y n )<label>(13)</label></formula><p>It should be noted that hereẑ = M T · M ·ẑ because the reconstruction loss does not allowsŝ to be zero. Thus, we have:  </p><formula xml:id="formula_12">z n =ẑ − d =ẑ − M T · (M ·ẑ − y n ) (14) x n = decoder(ẑ − M T · (M ·ẑ − y n ))<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>Here we evaluate the ability to disentangle. We also evaluate the orthogonality of M as it is an important indicator of how well our algorithm can approximate M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Matrix Subspace Projection in VAE</head><p>We apply our model on a vanilla VAE <ref type="bibr" target="#b9">(Kingma &amp; Welling, 2013)</ref> with the standard CNN encoder and decoder (the architectures are same as <ref type="bibr" target="#b14">Lample et al. (2017)</ref> ADAM optimiser with learning rate = 0.0002, mini-batch size of 256, and images are upsampled to 256 × 256. We add an additional PatchGAN <ref type="bibr" target="#b16">(Li &amp; Wand, 2016)</ref> to make the produced images sharp. The architecture of the PatchGAN discriminator also adopts the version of <ref type="bibr" target="#b14">Lample et al. (2017)</ref>. Our baselines are Fader networks <ref type="bibr" target="#b14">(Lample et al., 2017)</ref> and AttGAN <ref type="bibr" target="#b4">(He et al., 2019)</ref>, based on their published code and settings. We did not compare StarGAN <ref type="bibr" target="#b1">(Choi et al., 2018)</ref> because we feel it is superseded by AttGAN, which demonstrated better performance. We did not compare Rel-GAN <ref type="bibr" target="#b23">(Wu et al., 2019)</ref> as it does not disentangle attributes (see <ref type="figure" target="#fig_0">Fig. 1 (left)</ref>, also RelGAN cannot add a moustache or beard to a female face, instead it will transform it to a male face with beard).</p><p>We evaluated on the CelebA dataset <ref type="bibr" target="#b18">(Liu et al., 2015)</ref> (202,600 images) and trained one model on all 40 labelled attributes. The generated examples are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>  For a quantitative evaluation we trained a classifier (ResNet-CNN) to measure the accuracy with which attributes are changed. <ref type="table">Table 2</ref> shows that our MSP approach outperforms the competitors. Finally we calculated the average Fréchet Inception Distance (FID) <ref type="bibr" target="#b5">(Heusel et al., 2017)</ref> for each method: MSP=35.0, Fader=26.3, AttGAN=7.3 (lower is better, 0 is the best). The FID score tries to calculate the similarity of original images and generated images. Clearly AttGAN is a winner for quality while our MSP is a winner for accuracy of attribute modification. When AttGAN cannot handle the attribute modification it generates unchanged images and can get lower FID scores.</p><p>The results of attribute manipulation (both qualitative and quantitative) are surprisingly bad for Fader networks and AttGAN, especially relative to the examples displayed in their original papers. The primary reason for this is that we trained those models on all 40 attributes together. Fader networks works best when trained on a single attribute, as noted in Sec. 2. The original AttGAN paper trained on 13 attributes, and indeed it performs better at attribute manipulation than Fader in our pictures. For the 40-attribute-together version, when any attribute is changed all others must be unchanged. For example, when we transition from male to female <ref type="figure" target="#fig_4">(Fig. 4 left)</ref>, it is implicit that the female should keep no make-up or lipstick, etc. In the direction from female to male the male should keep no bushy eyebrows or 5 o'clock shadow. The original Fader network paper displays a beautiful example of interpolating between male and female, but the female does have make-up and lipstick and the male does have bushy eyebrows and 5 o'clock shadow. Our difficult 40-attribute setting is central to our aims, as stated in our introduction: we want to fully disentangle multiple attributes, because this gives a generative model the ability to 'imagine' novel examples that combine attributes in ways not present in training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human Evaluation of Generated Example Quality</head><p>We evaluated whether our MSP model reduces the quality of generated examples, using human evaluation via Amazon Mechanical Turk (hiring 150 participants in total).</p><p>For each model, we randomly generated 1,000 example pairs. Each pair contains a reconstructed example (from AE) and an example generated by AE+MSP with one or two random attribute modification (attributes were changed to −1 if they were originally &gt; 0, or changed to 1 if they were &lt; 0) 3 . The participants were shown the examples pair-bypair in the blind test, and they were asked to please choose the one with better text/image quality, or choose both if you think they perform similarly. The participants were told that the text quality means the fluency, semantic accuracy, and syntactic accuracy, and the image quality means the clarity and (face) recognisability. The results are shown in <ref type="table">Table 1</ref>. We treat the scores (i.e. participants' choices) of the result as a Likert scale, and we set our null hypothesis to be H 0 : generation quality of AE+MSP is worse than using the AE only, and, H 1 : generation quality of AE+MSP is equal or higher than using the AE only. The hypotheses are tested by a discrete Mann-Whitney U-test, rejecting H 0 with p &lt; 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Disentanglement</head><p>Disentanglement is also an important feature of our model. It means that when an attribute is modified, other at-tributes remain unchanged. We make the three models (VAE+GAN+MSP, AttGAN, and Fader Networks) generate images by manipulating two groups of highly correlated attributes, openness of mouth / smiling, and male / beard. For the two groups, the three models should respectively generate the images with closed mouth × no smiling, closed mouth × smiling, opened mouth × no smiling, opened mouth × smiling, female × no beard, female × beard, male × no beard, and male × beard. We hired 50 participants in Amazon Mechanical Turk; each of them was given 40 image blocks. A block contains four images, which were from the mouth / smiling group or the male / beard group, and which were generated by one of the three models. The participants were told which image should represent which attributes, and the participants evaluated whether it did for each image in the block by using a 3-level Likert scale (perfect, recognisable, and unrecognisable/unchanged). The results are shown in <ref type="table">Table 4</ref>. It shows that our model performs significantly better than the baseline. (p &lt; 0.0001).</p><p>In addition to human evaluation, we also conducted a quantitative evaluation to test how well a model can change an attribute in isolation. For some selected highly correlated attributes we change one target attribute, and measure the change in another non-target attribute. For instance (the row of gender/beard in <ref type="table" target="#tab_3">Table 3</ref>), when the gender attribute is changed manually, we measure the amount by which the beard attribute is consequently changed. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Note that, the scores show how much the non-target attributes are affected, but not whether the target attributes are correctly changed in the generated pictures. Therefore the scores need to be read in conjunction with <ref type="table">Table 2</ref>. According to both <ref type="table">Table 2</ref> and <ref type="table" target="#tab_3">Table 3</ref>, we can conclude that in both of the aspects of the manipulation of attributes and avoiding influence on non-target attributes, the performance of our model exceeds the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Matrix Subspace Projection in Seq2seq</head><p>We apply our model to a classic seq2seq model for textual content replacement, in order to determine if we can replace words according to the given attributes and keep other words unchanged. In this task, we adopt the E2E corpus <ref type="bibr" target="#b3">(Dušek et al., 2019)</ref>, which contains 50k+ reviews of restaurants (E2E dataset is developed for Natural Language Generation, but here we use it for content replacement). Each review is a single sentence that is labelled by the attribute-value pairs, for example, "name=[The Eagle]", "food=[French]", and "customerRating=[3/5]". We regard each attribute-value pair as a unique label. All the attributes constitute y whose entries are 1 or 0 to represent each value (the correct texts of the attribute name or value are NOT used).</p><p>Both the encoder and decoder of the seq2seq model are formed by two-layer LSTMs. The model is trained for 1000 near the six bells in the riverside area is a green man . it is a french coffee shop that is not family-friendly .  epochs (on a Tesla T4 around 12 hours). After training, we reconstruct the review sentences with randomly replaced attributes, for example replacing "name=[The Eagle]" by "name=[Burger King]", "customerRating=[3/5]" by "cus-tomerRating=[1/5]". 50% of attributes are changed in each sentence. The outcomes are shown in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Orthogonality Evaluation</head><p>The ability to disentangle attributes is ensured by the orthogonality of M in our model. Instead of directly using an orthogonal matrix, we train M to be orthogonal. Thus, we evaluate how close M is to the orthogonal matrix. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the heat map of M T · M and U T · U, which indicates that the production is fairly close to a unit matrix. It visualises M T · M in the seq2seq version of our model ( <ref type="figure" target="#fig_5">Fig. 5 (a)</ref>) and in the VAE version ( <ref type="figure" target="#fig_5">Fig. 5 (c)</ref>). The matrix U T · U (U is formed by M concatenating its null space) is also visualised (in <ref type="figure" target="#fig_5">Fig. 5 (b) and (d)</ref>). It is clear that when the matrices are multiplied by their transposes, the products do approximate the unit matrix. Although <ref type="bibr">Fig. 5 (c)</ref> shows that a small number of attributes remain slightly entangled (by the green and deep purple pixels), this is mainly caused by the few conflicting attributes in CelebA, for example the receding-hairline × bald × bangs, and the straight-hair × wavy-hair. Thus, M is indeed trained to be a (partial) orthogonal matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a matrix projection plugin that can be attached to various autoencoders (e.g. Seq2seq, VAE) to make the latent space factorised and disentangled, based on labelled attribute information, which ensures that manipulation in the latent space is much easier. We test the attribute manipulation ability of our model on an image dataset and text corpus, obtaining results that show clean disentanglement. In addition our model involves a simpler training process than adversarial approaches which need a long training with a very low weight on the loss coming from the discriminator that removes attribute information, to avoid the encoder being too affected by this loss term <ref type="bibr" target="#b14">(Lample et al., 2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Left: from RelGAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure showing the difficulty of disentangling attributes for supervised 'adversarial' approaches. (a) from Creswell et al. (2017) shows significant change in the eyebrows and eyes when adding facial hair. (b,c,d) from Klys et al. (2018). (b) moving across the glasses and facial hair subspace, from the female on the left, brings significant changes in eyebrows and eyes, and the shape of cheeks, making the face more masculine. (c) moving in glasses subspace shows changes around the eyes and mouth, looking older. (d) also moving in glasses subspace shows a narrower smile and a more masculine lower face. Note all of the pictures are generated by VAE, none are original photographs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) The general architecture of our model (MSP); (b) the simplified architecture of MSP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>(b) presents a simplified architecture of our MSP model, which is equivalent to the general architecture. This simplification exists because as explained earlier H(·) is invertible. So when the encoder and decoder of an AE have enough capacity, they can essentially absorb H and H −1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Examples of image attributes transformations using MSP (our model), Fader Networks and AttGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Measuring orthogonality: Heat map of M T · M and U T · U for Seq2seq+MSP (a,b), and VAE+GAN+MSP (c,d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative</figDesc><table><row><cell></cell><cell cols="3">evaluation of disentanglement (using ResNet-</cell></row><row><cell>CNN classifier).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">mouth open / smiling attributes morphing</cell></row><row><cell></cell><cell>Fader</cell><cell cols="2">AttGAN VAE+GAN</cell></row><row><cell></cell><cell>Network</cell><cell></cell><cell>MSP (ours)</cell></row><row><cell>perfect</cell><cell>36.7%</cell><cell>47.5%</cell><cell>68.3%</cell></row><row><cell>recognizable</cell><cell>20.8%</cell><cell>15.3%</cell><cell>4.9%</cell></row><row><cell cols="2">unreco/unchang 42.5%</cell><cell>37.2%</cell><cell>26.8%</cell></row><row><cell cols="4">male / beard attributes morphing</cell></row><row><cell></cell><cell>Fader</cell><cell cols="2">AttGAN VAE+GAN</cell></row><row><cell></cell><cell>Network</cell><cell></cell><cell>MSP (ours)</cell></row><row><cell>perfect</cell><cell>38.3%</cell><cell>55.9%</cell><cell>74.4%</cell></row><row><cell>recognizable</cell><cell>8.3%</cell><cell>11.2%</cell><cell>11.6%</cell></row><row><cell cols="2">unreco/unchang 53.3%</cell><cell>32.9%</cell><cell>14.0%</cell></row><row><cell cols="4">Table 4. Manual evaluation results of disentanglement. Numbers</cell></row><row><cell cols="4">in the table denote percentage of participants under the column</cell></row><row><cell cols="4">heading who felt the images represented the specified attribute (e.g.</cell></row><row><cell cols="4">smiling) in a way that was perfect, recognisable, or unrecognis-</cell></row><row><cell>able/unchanged.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">remove the glasses; FaderNetwork completely fails to add</cell></row><row><cell cols="4">glasses to the other two faces, and AttGan can only manage</cell></row><row><cell cols="4">weak rims on the final woman (bottom). FaderNetwork in</cell></row><row><cell cols="4">general struggles to change attributes, especially for the two</cell></row><row><cell cols="4">women, while AttGan does better, but struggles with certain</cell></row><row><cell cols="4">attributes, e.g. mostly it fails to change the final woman to</cell></row><row><cell cols="3">male, and struggles to remove makeup.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Orig-attribute eatType[pub], customer-rating[5-out-of-5], name[Blue-Spice], near[Crowne-Plaza-Hotel] familyFriendly[yes], area[city-centre], eatType[pub], food[Japanese], near[Express-by-Holiday-Inn], name[Green-Man] Orig-text the blue spice pub , near crowne plaza hotel , has a customer rating of 5 out of 5 .near the express by holiday inn in the city centre is green man . it is a japanese pub that is family-friendly .</figDesc><table><row><cell></cell><cell>Example 1</cell><cell>Example 2</cell></row><row><cell cols="2">New-attribute eatType[coffee-shop], customer-rating[5-out-</cell><cell>familyFriendly[no], area[riverside],</cell></row><row><cell></cell><cell>of-5], name[Blue-Spice], near[Avalon]</cell><cell>eatType[coffee-shop], food[French],</cell></row><row><cell></cell><cell></cell><cell>near[The-Six-Bells], name[Green-Man]</cell></row><row><cell>New-text</cell><cell>the blue spice coffee shop , near avalon has a</cell></row><row><cell></cell><cell>customer rating of 5 out of 5 .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results of changing attributes in E2E corpus.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: https://xiao.ac/proj/msp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Not mentioned in the Fader networks paper, but in the published code: https://github.com/facebookresearch/FaderNetworks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The generated examples were automatically filtered to prevent conflict attributes; e.g. images of woman with beard are not provided to the participants in this experiment.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank all the anonymous reviewers for their insightful comments. This work is supported by the award made by the UK Engineering and Physical Sciences Research Council (Grant number: EP/P011829/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stargan</surname></persName>
		</author>
		<idno>doi: 10. 1109/CVPR.2018.00916</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Conditional autoencoders with adversarial information factorization. CoRR, abs/1711.05175</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sengupta</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.05175" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating the stateof-the-art of end-to-end natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11528</idno>
		<ptr target="https://arxiv.org/abs/1901.11528" />
	</analytic>
	<monogr>
		<title level="m">The E2E NLG Challenge</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attgan: Facial attribute editing by only changing what you want</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2019.2916751</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5464" to="5478" />
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">66296640</biblScope>
		</imprint>
	</monogr>
	<note>ISBN 9781510860964</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Early visual concept learning with unsupervised deep learning. CoRR, abs/1606.05579</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.05579" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sy2fzU9gl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/hu17e.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning latent subspaces in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>nett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6444" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational inference of disentangled latent concepts from unlabeled observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1kG7GZAW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<idno>abs/1604.00289</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The power and limits of deep learning. Re-searchTechnology Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1080/08956308.2018.1516928</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A stable variational autoencoder for text modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation</title>
		<meeting>the 12th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="594" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Invertible conditional gans for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Andálvarez</surname></persName>
		</author>
		<idno>abs/1611.06355</idno>
		<ptr target="http://arxiv.org/abs/1611.06355" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From face images and attributes to attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Torfason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<idno>doi: 10.1007/ 978-3-319-54187-7 21</idno>
	</analytic>
	<monogr>
		<title level="m">13th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep feature interpolation for image content changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.645</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.645" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6090" to="6099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-domain image-to-image translation via relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Relgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dirichlet variational autoencoder for text modeling. CoRR, abs/1811.00135</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.00135" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_47</idno>
		<idno>doi: 10.1007/ 978-3-319-46493-0\ 47</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-0_47" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep nets: What have they ever done for vision? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1805.04025</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
