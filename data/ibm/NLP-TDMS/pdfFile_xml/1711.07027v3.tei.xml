<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences ‡ University of Technology Sydney § Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<email>liangzheng06@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences ‡ University of Technology Sydney § Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences ‡ University of Technology Sydney § Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
							<email>guoliang.kang@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences ‡ University of Technology Sydney § Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences ‡ University of Technology Sydney § Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
							<email>jiaojb@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences ‡ University of Technology Sydney § Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a "learning via translation" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation.</p><p>Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a Cy-cleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper considers domain adaptation in person re-ID. The re-ID task aims at searching for the relevant images to the query. In our setting, the source domain is fully annotated, while the target domain does not have ID labels. In the community, domain adaptation of re-ID is gaining increasing popularity, because 1) of the expensive labeling process and 2) when models trained on one dataset are directly used on another, the re-ID accuracy drops dramati- * Corresponding Author  In each triplet, left: an source-domain image, middle: a source-target translated version of the source image, right: an arbitrary target-domain image. We require that 1) a source image and its translated image should contain the same ID, i.e., self-similarity, and 2) the translated image should be of a different ID with any target image, i.e., domain dissimilarity. Note: the source and target domains contain entirely different IDs.</p><p>cally <ref type="bibr" target="#b5">[6]</ref> due to dataset bias <ref type="bibr" target="#b40">[41]</ref>. Therefore, supervised, single-domain re-ID methods may be limited in real-world scenarios, where domain-specific labels are not available.</p><p>A common strategy for this problem is unsupervised domain adaptation (UDA). But this line of methods assume that the source and target domains contain the same set of classes. Such assumption does not hold for person re-ID because different re-ID datasets usually contain entirely different persons (classes). In domain adaptation, a recent trend consists in image-level domain translation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28]</ref>. In the baseline approach, two steps are involved. First, labeled images from the source domain are transferred to the target domain, so that the transferred image has a similar style with the target. Second, the style-transferred images and their associated labels are used in supervised learning in the target domain. In literature, commonly used style transfer methods include <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57]</ref>. In this paper, we use CycleGAN <ref type="bibr" target="#b56">[57]</ref> following the practice in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In person re-ID, there is a distinct yet unconsidered re-  <ref type="figure">Figure 2</ref>: Pipeline of the "learning via translation" framework. First, we translate the labeled images from a source domain to a target domain in an unsupervised manner. Second, we train re-ID models with the translated images using supervised feature learning methods. The major contribution consists in the first step, i.e., similarity preserving image-image translation. quirement for the baseline described above: the visual content associated with the ID label of an image should be preserved after image-image translation. In our scenario, such visual content usually refers to the underlying (latent) ID information for a foreground pedestrian. To meet this requirement tailored for re-ID, we need additional constraints on the mapping function. In this paper, we propose a solution to this requirement, motivated from two aspects. First, a translated image, despite of its style changes, should contain the same underlying identity with its corresponding source image. Second, in re-ID, the source and target domains contain two entirely different sets of identities. Therefore, a translated image should be different from any image in the target dataset in terms of the underlying ID.</p><p>This paper introduces the Similarity Preserving cycleconsistent Generative Adversarial Network (SPGAN), an unsupervised domain adaptation approach which generates images for effective target-domain learning. SPGAN is composed of an Siamese network (SiaNet) and a Cycle-GAN. Using a contrastive loss, the SiaNet pulls close a translated image and its counter part in the source, and push away the translated image and any image in the target. In this manner, the contrastive loss satisfies the specific requirement in re-ID. Note that, the added constraint is unsupervised, i.e., the source labels are not used during domain adaptation. During training, in each mini-batch (batch size = 1), a training image is firstly used to update the Generator (of CycleGAN), then the Discriminator (of CycleGAN), and finally the layers in SiaNet. Through the coordination between CycleGAN and SiaNet, we are able to generate samples which not only possess the style of target domain but also preserve their underlying ID information.</p><p>Using SPGAN, we are able to create a dataset on the target domain in an unsupervised manner. The dataset inherits the labels from the source domain and thus can be used in supervised learning in the target domain. The contributions of this work are summarized below:</p><p>• Minor contribution: we present a "learning via translation" baseline for domain adaptation in person re-ID.</p><p>• Major contribution: we introduce SPGAN to improve the baseline. SPGAN works by preserving the underlying ID information during image-image translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image-image translation. Image-image translation aims at constructing a mapping function between two domains. A representative method is the conditional GAN <ref type="bibr" target="#b19">[20]</ref>, which using paired training data produces impressive transition results. However, the paired training data is often difficult to acquire. Unpaired image-image translation is thus more applicable. To tackle unpaired settings, a cycle consistency loss is introduced by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b56">57]</ref>. In <ref type="bibr" target="#b2">[3]</ref>, an unsupervised distance loss is proposed for one side domain mapping. In <ref type="bibr" target="#b26">[27]</ref>, a general framework is proposed by making a shared latent space assumption. A camera style adaptation method <ref type="bibr" target="#b55">[56]</ref> is proposed for re-ID based on Cycle-GAN. Our work aims to find a mapping function between the source domain and target domain, and we are more concerned with similarity preserving translation.</p><p>Neural style transfer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> is another strategy of image-image translation, which aims at replicating the style of one image, while our work focuses on learning the mapping function between two domains, rather than two images.</p><p>Unsupervised domain adaptation. Our work relates to unsupervised domain adaptation (UDA) where no labeled target images are available during training. In this community, some methods aim to learn a mapping between source and target distributions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">38]</ref>. Correlation Alignment (CORAL) <ref type="bibr" target="#b37">[38]</ref> proposes to match the mean and covariance of two distributions. Recent methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28]</ref> use an adversarial approach to learn a transformation in the pixel space from one domain to another. Other methods seek to find a domain-invariant feature space <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref>. Long et al. <ref type="bibr" target="#b29">[30]</ref> and Tzeng et al. <ref type="bibr" target="#b41">[42]</ref> use the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b14">[15]</ref> for this purpose. Ganin et al. <ref type="bibr" target="#b10">[11]</ref> and Ajakan et al. <ref type="bibr" target="#b1">[2]</ref> introduce a domain confusion loss to learn domain-invariant features. Different from the settings in this paper, most of the UDA methods assume that class labels are the same across domains, while different re-ID datasets contain entirely different person identities (classes). Therefore, the approaches mentioned above can not be utilized directly for domain adaptation in re-ID.</p><p>Unsupervised person re-ID. Hand-craft features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b50">51]</ref> can be directly employed for unsupervised re-ID. But these feature design methods do not fully exploit rich information from data distribution. Some methods are based on saliency statistics <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b43">44]</ref>. In <ref type="bibr" target="#b48">[49]</ref>, K-means clustering is used for learning an unsupervised asymmetric metric. Peng et al. <ref type="bibr" target="#b34">[35]</ref> propose an asymmetric multi-task dictionary learning for cross-data transfer.</p><p>Recently, several works focus on label estimation of unlabeled target dataset. Ye et al. <ref type="bibr" target="#b46">[47]</ref> use graph matching for cross-camera label estimation. Fan et al. <ref type="bibr" target="#b5">[6]</ref> propose a progressive method based on the iterations between K-means clustering and IDE <ref type="bibr" target="#b51">[52]</ref> fine-tuning. Liu et al. <ref type="bibr" target="#b28">[29]</ref> employ a reciprocal search process to refine the estimated labels. Wu et al. <ref type="bibr" target="#b45">[46]</ref> propose a dynamic sampling stragy for one-shot video-based re-ID. Our work seeks to learn re-ID models that can be utilized directly to target domain, and can potentially cooperate with label estimation methods in model initialization. Finally, we would like to refer the reader to the concurrent work named TJ-AIDL <ref type="bibr" target="#b44">[45]</ref> that utilizes additional attribute annotation to learn a feature representation space for the unlabeled target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Overview</head><p>Given an annotated dataset S from source domain and unlabeled dataset T from target domain, our goal is to use the labeled source images to train a re-ID model that generalizes well to target domain. <ref type="figure">Figure 2</ref> presents a pipeline of the "learning via translation" framework, which consists of two steps, i.e., source-target image translation for training data creation, and supervised feature learning for re-ID.</p><p>• Source-target image translation. Using a generative function G(·) that translates the annotated dataset S from the source domain to target domain in an unsupervised manner, we "create" a labeled training dataset G(S) on the target domain. In this paper, we use Cy-cleGAN <ref type="bibr" target="#b56">[57]</ref>, following the practice in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>• Feature learning. With the translated dataset G(S) that contains labels, feature learning methods are applied to train re-ID models. Specifically, we adopt the same setting as <ref type="bibr" target="#b51">[52]</ref>, in which the rank-1 accuracy and mAP on the fully-supervised Market-1501 dataset is 75.8% and 52.2%.</p><p>The focus of this paper is to improve Step 1, so that with better training samples, the overall re-ID accuracy can be improved. The experiment will validate the proposed Step 2 (G sp (·)) on several feature learning methods. A brief summary of different methods considered in this paper is presented in <ref type="table">Table 1</ref>. We denote the method "Direct Transfer" as directly using the training set S instead of G(S) for model learning. This method yields the lowest accuracy because the style difference between the source and target is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train. Set Test. Set Accuracy Supervised <ref type="table">Table 1</ref>: A brief summary of different methods considered in this paper. "G" and "G sp " denote the Generator in Cy-cleGAN and SPGAN, respectively. S train , T train , T test denote the training set of the source dataset, the training set and testing set of the target dataset, respectively. not resolved (to be shown in <ref type="table" target="#tab_2">Table 2</ref>). Using CycleGAN and SPGAN to generate a new training set, which is more style-consistent with the target, yields improvement.</p><formula xml:id="formula_0">T train T test +++++ Direct Transfer S train T test ++ CycleGAN (basel.) G(S train ) T test +++ SPGAN G sp (S train ) T test ++++</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SPGAN: Approach Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">CycleGAN Revisit</head><p>CycleGAN introduces two generator-discriminator pairs, {G, D T } and {F, D S }, which map a sample from source (target) domain to target (source) domain and produce a sample that is indistinguishable from those in the target (source) domain, respectively. For generator G and its associated discriminator D T , the adversarial loss is</p><formula xml:id="formula_1">L T adv (G, D T , p x , p y ) =E y∼py [(D T (y) − 1) 2 ] + E x∼px [(D T (G(x)) 2 ],<label>(1)</label></formula><p>where p x and p y denote the sample distributions in the source and target domain, respectively. For generator F and its associated discriminator D S , the adversarial loss is</p><formula xml:id="formula_2">L Sadv (F, D S , p y , p x ) =E x∼px [(D S (x) − 1) 2 ] + E y∼py [(D S (F (y)) 2 ].<label>(2)</label></formula><p>Considering there exist infinitely many alternative mapping functions due to the lack of paired training data, Cy-cleGAN introduces a cycle-consistent loss, which attempts to recover the original image after a cycle of translation and reverse translation, to reduce the space of possible mapping functions. The cycle-consistent loss is</p><formula xml:id="formula_3">L cyc (G, F ) =E x∼px [ F (G(x)) − x 1 ] + E y∼py [ G(F (y)) − y 1 ].<label>(3)</label></formula><p>Apart from cycle-consistent loss and adversarial loss, we use the target domain identity constraint <ref type="bibr" target="#b39">[40]</ref> as an auxiliary for image-image translation. Target domain identity constraint is introduced to regularize the generator to be the identity matrix on samples from target domain, written as As mentioned in <ref type="bibr" target="#b56">[57]</ref>, generators G and F may change the color of input images without L ide . In experiment, we observe that model may generate unreal results without L ide ( <ref type="figure" target="#fig_4">Fig. 4(b)</ref>). This is undesirable for re-ID feature learning.</p><formula xml:id="formula_4">L ide (G, F, p x , p y ) =E x∼px F (x) − x 1 + E y∼py G(y) − y 1 .<label>(4)</label></formula><p>Thus, we use L ide to preserve the color composition between the input and output (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">SPGAN</head><p>Applied in person re-ID, similarity preserving is an essential function to generate improved samples for domain adaptation. As analyzed in Section 1, we aim to preserve the IDrelated information for each translated image. We emphasize that such information should not be the background or image style, but should be underlying and latent. To fulfill this goal, we integrate a SiaNet with CycleGAN, as shown in <ref type="figure" target="#fig_2">Fig 3.</ref> During training, CyleGAN is to learn a mapping function between two domains, and SiaNet is to learn a latent space that constrains the learning of mapping function.</p><p>Similarity preserving loss function. We utilize the contrastive loss <ref type="bibr" target="#b15">[16]</ref> to train SiaNet:</p><formula xml:id="formula_5">L con (i, x 1 , x 2 ) =(1 − i){max(0, m − d)} 2 + id 2 ,<label>(5)</label></formula><p>where x 1 and x 2 are a pair of input vectors, d denotes the Euclidean distance between normalized embeddings of two input vectors, and i represents the binary label of the pair. Training image pair selection. In Eq. 5, the contrastive loss uses binary labels of input image pairs. The design of the pair similarities reflects the "self-similarity" and "domain-dissimilarity" principles. Note that, we select training pairs in an unsupervised manner, so that we use the contrastive loss without additional annotations.</p><p>Formally, CycleGAN has two generators, i.e., generator G which maps source-domain images to the style of the target domain, and generator F which maps target-domain images to the style of the source domain. Suppose two samples denoted as x S and x T come from the source domain and target domain, respectively. Given G and F , we define two positive pairs: 1) x S and G(x S ), 2) x T and F (x T ). In either image pair, the two images contain the same person; the only difference is that they have different styles. In the learning procedure, we encourage the whole network to pull these two images close.</p><p>On the other hand, for generators G and F , we also define two types of negative training pairs: 1) G(x S ) and x T , 2) F (x T ) and x S . Such design of negative training pairs is based on the prior knowledge that datasets in different re-ID domains have entirely different sets of IDs. Thus, a translated image should be of different ID from any target image. In this manner, the network pushes two dissimilar images away. Training pairs are shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Some positive pairs are also shown in (a) and (d) of each column in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>Overall objective function. The final SPGAN objective can be written as</p><formula xml:id="formula_6">L sp = L T adv + L Sadv + λ 1 L cyc + λ 2 L ide + λ 3 L con ,<label>(6)</label></formula><p>where λ t , t ∈ {1, 2, 3} controls the relative importance of four objectives. The first three losses belong to the Cycle-GAN formulation <ref type="bibr" target="#b56">[57]</ref>, and the contrastive loss induced by SiaNet imposes a new constraint on the system. SPGAN training procedure. In the training phase, SP-GAN are divided into three components which are learned alternately, the generators, discriminators and SiaNet. When the parameters of two components are fixed, the parameters of the third component is updated. We train the SPGAN until the convergence or the maximum iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Learning</head><p>Feature learning is the second step of the "learning via translation" framework. Once we have style-transferred dataset G(S) composed of the translated images and their associated labels, the feature learning step is the same as supervised methods. Since we mainly focus on Step 1 (source-target image translation), we adopt the baseline IDdiscriminative Embedding (IDE) following the practice in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>. We employ ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as the base model  <ref type="figure">Figure 5</ref>: Illustration of LMP. We partition the feature map into P (P = 2) parts horizontally. We conduct global max/avg pooling on each part and concatenate the feature vectors as the final representation. and only modify the output dimension of the last fullyconnected layer to the number of training identities. During testing, given an input image, we can extract the 2,048-dim Pool5 vector for retrieval under the Euclidean distance. Local Max Pooling. To further improve re-ID performance on the target dataset T , we introduce a feature pooling method named as local max pooling (LMP). It works on a well-trained IDE model and can reduce the impact of noisy signals incurred by the fake translated images. In the original ResNet-50, global average pooling (GAP) is conducted on Conv5. In our proposal ( <ref type="figure">Fig. 5</ref>), we first partition the Conv5 feature maps to P parts horizontally, and then conduct global max/avg pooling on each part. Finally, we concatenate the output of global max pooling (GMP) or GAP of each part as the final feature representation. The procedure is nonparametric, and can be directly used in the testing phase. In the experiment, we will compare local max pooling and local average pooling, and demonstrate the superiority of the former (LMP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We select two large-scale re-ID datasets for experiment, i.e., Market-1501 <ref type="bibr" target="#b50">[51]</ref> and DukeMTMC-reID <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b52">53]</ref>. Market-1501 is composed of 1,501 identities, 12,936 training images and 19,732 gallery images (with 2,793 distractors). It is split into 751 identities for training and 750 identities for testing. Each identity is captured by at most 6 cameras. All the bounding boxes are produced by DPM <ref type="bibr" target="#b7">[8]</ref>. DukeMTMC-reID is a re-ID version of the DukeMTMC dataset <ref type="bibr" target="#b35">[36]</ref>. It contains 34,183 image boxes of 1,404 identities: 702 identities are used for training and the remaining 702 for testing. There are 2,228 queries and 17,661 database images. For both datasets, we adopt rank-1 accuracy and mAP for re-ID evaluation <ref type="bibr" target="#b50">[51]</ref>. Sample images of the two datasets are shown in <ref type="figure" target="#fig_5">Fig.6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>SPGAN training and testing. We use Tensorflow <ref type="bibr" target="#b0">[1]</ref> to train SPGAN using the training images of Market-1501 and DukeMTMC-reID. Note that, we do not use any ID annotation during training procedure. In all experiments, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DukeMTMC-reID</head><p>Market-1501 rank-1 rank-5 rank-10 rank-20 mAP rank-1 rank-5 rank-10 rank- <ref type="bibr" target="#b19">20</ref>    Feature learning for re-ID. As described in Section 3.3, we adopt IDE for feature learning.</p><p>Specifically, ResNet-50 <ref type="bibr" target="#b16">[17]</ref> pretrained on ImageNet is used for fine-tuning on the translated training set. We modify the output of the last fully-connected layer to 751 and 702 for Market-1501 and DukeMTMC-reID, respectively. We use mini-batch SGD to train CNN models on a Tesla K80 GPU. Training parameters such as batch size, maximum number epochs, momentum and gamma are set to 16, 50, 0.9 and 0.1, respectively. The initial learning rate is set as 0.001, and decay to 0.0001 after 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation</head><p>Comparison between supervised learning and direct transfer. The supervised learning method and the direct transfer method are specified in <ref type="table">Table 1</ref>. When comparing the two methods in <ref type="table" target="#tab_2">Table 2</ref>, we can clearly observe a large performance drop when directly using a source-trained model on the target domain. For instance, the ResNet-50 model trained and tested on Market-1501 achieves 75.8% in rank-1 accuracy, but drops to 43.1% when trained on DukeMTMC-reID and tested on Market-1501. A similar drop can be observed when DukeMTMC-reID is used as the target domain, which is consistent with the experiments reported in <ref type="bibr" target="#b5">[6]</ref>. The reason behind the performance drop is the bias of data distributions in different domains.</p><p>The effectiveness of the "learning via translation" baseline using CycleGAN. In this baseline domain adaptation approach (Section 3.1), we first translate the label images from the source domain to the target domain and then use the translated images to train re-ID models. As shown in <ref type="table" target="#tab_2">Table 2</ref>, this baseline framework effectively improves the re-ID performance in the target dataset. Compared to the direct transfer method, the CycleGAN transfer baseline gains +2.5% improvements in rank-1 accuracy on Market-1501. When tested on DukeMTMC-reID, the performance gain is +5.0% in rank-1 accuracy. Through such an image-level domain adaptation method, effective domain adaptation baselines can be learned.</p><p>The impact of the target domain identity constraint. We conduct experiment to verify the influence of the identity loss on performance in <ref type="table" target="#tab_2">Table 2</ref>. We arrive at mixed observations. On the one hand, on DukeMTMC-reID, compared with the CycleGAN baseline, CycleGAN + L ide achieves similar rank-1 accuracy and mAP. On the other hand, on Market-1501, CycleGAN + L ide gains +2.5% and 1.6% improvement in rank-1 accuracy and mAP, respectively. The reason is that Market-1501 has a larger intercamera variance. When translating Duke images to the Market style, the translated images may be more prone to translation errors induced by the camera variances. Therefore, the identity loss is more effective when Market is the target domain. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, this loss prevents CycleGAN from generating strangely colored images.</p><p>SPGAN effect. On top of the CycleGAN baseline, we replace CycleGAN with SPGAN (m = 2). The effectiveness of the proposed similarity preserving constraint can be seen in <ref type="table" target="#tab_2">Table 2</ref>. Compared with Cycle + L ide , on DukeMTMC-reID, the similarity preserving constraint leads to +2.6% and +2.4% improvement over CycleGAN + L ide in rank-1 accuracy and mAP, respectively. On Market-1501, the gains are +3.4% and 2.1%. The working mechanism of SPGAN consists in preserving the underlying visual cues associated with the ID labels. The consistent improvement suggests that this working mechanism is critical for generating suitable samples for training in the target domain. Examples of translated images by SPGAN are shown in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>Comparison of different feature learning methods. In Step 2, we evaluate three feature learning methods, i.e., IDE <ref type="bibr" target="#b51">[52]</ref> (described in Section 3.3), IDE + <ref type="bibr" target="#b54">[55]</ref>, and SVDNet <ref type="bibr" target="#b38">[39]</ref>. Results are shown in <ref type="figure">Fig. 7</ref>. An interesting observation is that, while IDE + and SVDNet are superior to IDE under the scenario of "Direct Transfer", the three learning methods are basically on par with each other when using training samples generated by SPGAN.</p><p>A possible explanation is that some translated images are noisy, which has a large effect on better learning methods.</p><p>Sensitivity of SPGAN to key parameters. The margin m defined in Eq. 5 is a key parameter. If m = 0, the loss of negative pairs is not back propagated. If m gets larger, the weight of negative pairs in loss calculation increases. We conduce experiment to verify the impact of m, and results are shown in <ref type="table" target="#tab_2">Table 2</ref>. When turning off the contribution of negative pairs in Eq. 5, (m = 0), SPGAN only marginally improves the accuracy on Market-1501, and even compromises the system on Duke. When increasing m to 2, we have much superior accuracy. It indicates that the negative Rank-1 accuracy (%) Impact of on re-ID accuracy   pairs are critical to the system. Moreover, we evaluate the impact of λ 3 in Eq. 6 on Market-1501. λ 3 controls the relative importance of the proposed similarity preserving constraint. As shown in <ref type="figure">Fig.  9</ref>, the proposed constraint is proven effective when compared to λ 3 = 0, but a larger λ 3 does not bring more gains in accuracy. Specifically, λ 3 = 2 yields the best accuracy.</p><p>Local max pooling. We apply the LMP on the Conv5 layer to mitigate the influence of noise. Note that LMP is directly adopted in the feature extraction step for testing without fine-tuning. We empirically study how the number of parts and the pooling mode affect the performance. Experiment is conducted on SPGAN. The performance of various numbers of parts (P = 1, 2, 3, 7) and different pooling modes (max or average) is provided in <ref type="table" target="#tab_5">Table 3</ref>. When we use average pooling and P = 1, we have the original GAP used in ResNet-50. From these results, we speculate that with more parts, a finer partition leads to higher discriminative descriptors and thus higher re-ID accuracy.</p><p>Moreover, we test LMP on supervised learning and domain adaptation scenarios with three feature learning methods, i.e., IDE <ref type="bibr" target="#b51">[52]</ref>, IDE + <ref type="bibr" target="#b54">[55]</ref>, and SVDNet <ref type="bibr" target="#b38">[39]</ref>. As shown in <ref type="figure">Fig. 9</ref>, LMP does not guarantee stable improvement on supervised learning as observed in "IDE + " and SVDNet.</p><p>However, when applied in the scenario of domain adap-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art Methods</head><p>We compare the proposed method with the state-ofthe-art unsupervised learning methods on Market-1501 and DukeMTMC-reID in <ref type="table" target="#tab_7">Table 4</ref> and <ref type="table" target="#tab_9">Table 5</ref>, respectively.</p><p>Market-1501. On Market-1501, we first compare our results with two hand-crafted features, i.e., Bag-of-Words (BoW) <ref type="bibr" target="#b50">[51]</ref> and local maximal occurrence (LOMO) <ref type="bibr" target="#b25">[26]</ref>. Those two hand-crafted features are directly applied on test dataset without any training process, their inferiority can be clearly observed. We also compare existing unsupervised methods, including the Clustering-based Asymmetric MEtric Learning (CAMEL) <ref type="bibr" target="#b48">[49]</ref>, the Progressive Unsupervised Learning (PUL) <ref type="bibr" target="#b5">[6]</ref>, and UMDL <ref type="bibr" target="#b34">[35]</ref>. The results of UMDL are reproduced by Fan et al. <ref type="bibr" target="#b5">[6]</ref>. In the single-query setting, we achieve rank-1 accuracy = 51.5% and mAP = 22.8%. It</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>DukeMTMC-reID Rank-1 Rank-5 Rank-10 mAP Bow <ref type="bibr" target="#b50">[51]</ref> 17.  outperforms the second best method <ref type="bibr" target="#b5">[6]</ref> by +6.0% in rank-1 accuracy. In the multiple-query setting, we arrive at rank-1 accuracy = 57.0%, which is +2.5% higher than CAMEL <ref type="bibr" target="#b48">[49]</ref>. The comparisons indicate the competitiveness of the proposed method on Market-1501. DukeMTMC-reID. On DukeMTMC-reID, we compare the proposed method with BoW <ref type="bibr" target="#b50">[51]</ref>, LOMO <ref type="bibr" target="#b25">[26]</ref>, UMDL <ref type="bibr" target="#b34">[35]</ref>, and PUL <ref type="bibr" target="#b5">[6]</ref> under the single-query setting (there is no multiple-query setting in DukeMTMC-reID). The result obtained by the proposed method is rank-1 accuracy = 41.1%, mAP = 22.3%. Compared with the second best method, i.e., PUL <ref type="bibr" target="#b5">[6]</ref>, our result is +11.1% higher in rank-1 accuracy. Therefore, the superiority of SPGAN can be concluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper focuses on domain adaptation in person re-ID. When models trained on one dataset are directly transferred to another dataset, the re-ID accuracy drops dramatically due to dataset bias. To achieve improved performance in the new dataset, we present a "learning via translation" framework for domain adaptation, characterized by 1) unsupervised image-image translation and 2) supervised feature learning. We further propose that the underlying (latent) ID information for the foreground pedestrian should be preserved after image-image translation. To meet this requirement tailored for re-ID, we introduce the unsupervised self-similarity and domain-dissimilarity for similarity preserving image generation (SPGAN). We show that SPGAN better qualifies the generated images for domain adaptation and yields consistent improvement over the CycleGAN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of self-similarity and domaindissimilarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>SPGAN consists of two components: an SiaNet (top) and CycleGAN (bottom). CycleGAN learns mapping functions G and F between two domains, and the SiaNet learns a latent space that constrains the learning procedure of mapping functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>i = 1</head><label>1</label><figDesc>if x 1 and x 2 are positive pair; i = 0 if x 1 and x 2 are negative pair. m ∈ [0, 2] is the margin that defines the separability in the embedding space. When m = 0, the loss of negative training pair is not back-propagated in the system. When m &gt; 0, both positive and negative sample pairs are considered. A larger m means that the loss of negative training samples has a higher weight in back propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visual examples of image-image translation. The left four columns map Market images to the Duke style, and the right four columns map Duke images to the Market style. From top to bottom: (a) original image, (b) output of CycleGAN, (c) output of CycleGAN + L ide , and (d) output of SPGAN. Images produced by SPGAN have the target style while preserving the ID information in the source.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Sample images of (upper left:) DukeMTMC-reID dataset, (lower left:) Market-1501 dataset, (upper right:) Duke images which are translated to Market style, and (lower right:) Market images translated to Duke style. We use SPGAN for unpaired image-image translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>empirically set λ 1</head><label>1</label><figDesc>= 10, λ 2 = 5, λ 3 = 2 in Eq. 6 and m = 2 in Eq. 5. With an initial learning rate 0.0002, and model stop training after 5 epochs. During the testing procedure, we employ the Generator G for Market-1501 → DukeMTMC-reID translation and the Generative F for DukeMTMC-reID → Market-1501 translation. The translated images are used to fine-tune the model trained on source images. For CycleGAN, we adopt the architecture released by its authors. For SiaNet, it contains 4 convolutional layers, 4 max pooling layers and 1 fully connected (FC) layer, configured as below. (1) Conv. 4 × 4, stride = 2, #feature maps = 64; (2) Max pooling 2 × 2, stride = 2; (3) Conv. 4 × 4, stride = 2, #feature maps = 128; (4) Max pooling 2 × 2, stride = 2; (5) Conv. 4 × 4, stride = 2, feature maps = 256; (6) Max pool 2 × 2, stride = 2; (7) Conv. 4 × 4, stride = 2, #feature maps = 512; (8) Max pooling 2 × 2, stride = 2; (9) FC, output dimension = 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>λ 3 (Eq. 6) v.s re-ID accuracy. A larger λ 3 means larger weight of similarity preserving constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1711.07027v3 [cs.CV] 15 May 2018</figDesc><table><row><cell>Source domain</cell><cell></cell><cell>Target domain</cell><cell></cell><cell></cell></row><row><cell>…</cell><cell>similarity preserving image-to-image translation</cell><cell>…</cell><cell>feature learning</cell><cell>re-ID model</cell></row><row><cell>source domain</cell><cell></cell><cell>target domain</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>…</cell><cell></cell><cell></cell></row><row><cell></cell><cell>…</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of various methods on the target domains. When tested on DukeMTMC-reID, Market-1501 is used as source, and vice versa. "Supervised learning" denotes using labeled training images on the corresponding target dataset. "Direct Transfer" means directly applying the source-trained model on the target domain (see Section 3.1). By varying m specified in Eq. 5, the sensitivity of SPGAN to the relative importance of the positive and negative pairs is shown. When local max pooling (LMP) is applied, the number of parts is set to 7. We use IDE<ref type="bibr" target="#b51">[52]</ref> for feature learning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance of various pooling strategies with different numbers of parts (P ) and pooling modes (maximum or average) over SPGAN. The best results are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art on Market-1501. * denotes unpublished papers. "SQ" and "MQ" are the single-query and multiple-query settings, respectively. The best results are in bold.tation, LMP yields improvement over IDE, IDE + , and SVDNet. The superiority of LMP probably lies in that max pooling filters out some detrimental signals in the descriptor induced by noisy translated images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art on DukeMTMC-reID under the single-query setting. * denotes unpublished papers. The best results are in bold.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4446</idno>
		<title level="m">Domain-adversarial neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">One-sided unsupervised domain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast patch-based style transfer of arbitrary style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04337</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised person reidentification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10444</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycleconsistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diversified texture synthesis with feed-forward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stepwise metric promotion for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transfer sparse coding for robust image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Covariance descriptor based on bio-inspired features for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised crossdomain image generation. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
