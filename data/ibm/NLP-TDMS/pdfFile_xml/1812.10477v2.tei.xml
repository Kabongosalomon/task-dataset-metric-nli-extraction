<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Residual Dense Network for Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Yun</forename><surname>Fu</surname></persName>
						</author>
						<title level="a" type="main">Residual Dense Network for Image Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Residual dense network</term>
					<term>hierarchical features</term>
					<term>image restoration</term>
					<term>image super-resolution</term>
					<term>image denoising</term>
					<term>compression artifact reduction</term>
					<term>image deblurring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, deep convolutional neural network (CNN) has achieved great success for image restoration (IR) and provided hierarchical features at the same time. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images, thereby resulting in relatively-low performance. In this work, we propose a novel and efficient residual dense network (RDN) to address this problem in IR, by making a better tradeoff between efficiency and effectiveness in exploiting the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S INGLE image restoration (IR) aims to generate a visually pleasing high-quality (HQ) image from its degraded low-quality (LQ) measurement (e.g., downsaled, noisy, compressed, or/and blurred images). Image restoration plays an important and fundamental role and has been widely used in computer vision, ranging from security and surveillance imaging <ref type="bibr" target="#b0">[1]</ref>, medical imaging <ref type="bibr" target="#b1">[2]</ref>, to image generation <ref type="bibr" target="#b2">[3]</ref>. However, IR is very challenging, because the image degradation process is irreversible, resulting in an ill-posed inverse procedure. To tackle this problem, lots of works have been proposed, such as model-based <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and learning-based <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> methods. Recently, deep convolutional neural network (CNN) has been widely investigated and achieves promising performance in various image restoration tasks, such as image super-resolution (SR) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, image denoising (DN) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, image compression reduction (CAR) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and image deblurring <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>The first challenge is how to introduce deep CNN for image restoration. Dong et al. <ref type="bibr" target="#b14">[15]</ref>, for the first time, proposed SRCNN for image SR with three convolutional (Conv) layers and achieved significant improvement over previous methods. After firstly introducing CNN for image SR, Dong et al. <ref type="bibr" target="#b24">[25]</ref> further applied CNN for other image restoration   <ref type="bibr" target="#b26">[27]</ref>. (b) Dense block in SRDenseNet <ref type="bibr" target="#b18">[19]</ref>. (c) Our proposed residual dense block (RDB), which not only enables previous RDB to connect with each layer of current RDB, but also makes better use of local features.</p><p>but also increase the computation complexity quadratically. Later, dense block <ref type="figure" target="#fig_0">(Figure 1(b)</ref>) was introduced in SR-DenseNet for image SR <ref type="bibr" target="#b18">[19]</ref>. The growth rate is a key part in dense block. Based on the investigation in DenseNet <ref type="bibr" target="#b29">[30]</ref> and our experiments (see Section 5.1), higher growth rate contributes to better performance, resulting in wider networks. However, training a wider network with dense blocks would become harder. As investigated in EDSR <ref type="bibr" target="#b26">[27]</ref>, increasing feature map number above a certain level would make the training more difficult and numerically unstable.</p><p>To tackle the issues and limitations above, we propose a simple yet efficient residual dense network (RDN) ( <ref type="figure" target="#fig_1">Figure 2</ref>) by extracting the hierarchical features from the original LQ image. Our RDN is built on our proposed residual dense block <ref type="figure" target="#fig_0">(Figure 1(c)</ref>). A straightforward way to obtain hierarchical feature is to directly extract the output of each Conv layer in the LQ space. But, it's impractical, especially for a very deep network. Instead, we propose residual dense block (RDB), which consists of dense connected layers and local feature fusion (LFF) with local residual learning (LRL). Furthermore, the Conv layers of current RDB have direct access to the previous RDB, resulting in a contiguous state pass. We name it as contiguous memory mechanism, which further passes on information that needs to be preserved <ref type="bibr" target="#b29">[30]</ref>. So, in each RDB, LFF concatenates the states of preceding and current RDBs and adaptively extracts local dense features. Moreover, LFF helps to stabilize the training of wider networks with high growth rate. After obtaining multi-level local dense features, global feature fusion (GFF) is conducted to adaptively preserve the hierarchical features in a global way. As shown in <ref type="figure" target="#fig_1">Figures 2 and 3</ref>, the original LR input directly connects each layer, leading to an implicit deep supervision <ref type="bibr" target="#b30">[31]</ref>.</p><p>In summary, our main contributions are three-fold: <ref type="bibr">•</ref> We propose a unified framework residual dense network (RDN) for high-quality image restoration. The network makes full use of all the hierarchical features from the original LQ image. <ref type="bibr">•</ref> We propose residual dense block (RDB), which can not only read state from the preceding RDB via a contiguous memory (CM) mechanism, but also better utilize all the layers within it via local dense connections. The accumulated features are then adaptively preserved by local feature fusion (LFF). <ref type="bibr">•</ref> We propose global feature fusion to adaptively fuse hierarchical features from all RDBs in the LR space. With global residual learning, we combine the shallow features and deep features together, resulting in global dense features from the original LQ image.</p><p>A preliminary version of this work has been presented as a conference version <ref type="bibr" target="#b31">[32]</ref>. In the current work, we incorporate additional contents in significant ways: <ref type="bibr">•</ref> We investigate a flexible structure of RDN and apply it for different IR tasks. Such IR applications allow us to further investigate the potential breadth of RDN. <ref type="bibr">•</ref> We investigate more details and add considerable analyses to the initial version, such as block connection, network parameter number, and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We extend RDN for Gaussian image denoising, compression artifact reduction, and image deblurring. Extensive experiments on benchmark and real-world data demonstrate that our RDN still outperforms existing approaches in these IR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing IR methods mainly include model-based <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and learning-based <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> methods. Among them, deep learning (DL)-based methods have achieved dramatic advantages against conventional methods in computer vision <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b46">[46]</ref>. Here, we focus on several representative image restoration tasks, such as image super-resolution (SR), denoising (DN), compression artifact reduction (CAR), and image deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Super-Resolution</head><p>Deep learning was first investigated for image SR. The most challenging part is how to formulize the mapping between LR and HR images into deep neural network. After constructing the basic networks for image SR, there're still challenging problems to solve, such as how to train bery deep and wide network. We will show how these challenges are solved or alleviated.</p><p>Dong et al. <ref type="bibr" target="#b14">[15]</ref> proposed SRCNN, applying CNN into image SR for the first time. Based on SRCNN, lots of improvements have been down to pursue better performance. By using residual learning to ease the training of deeper networks, VDSR <ref type="bibr" target="#b17">[18]</ref> and IRCNN <ref type="bibr" target="#b7">[8]</ref> could train deeper networks with more stacked Conv layers. DRCN <ref type="bibr" target="#b25">[26]</ref> also achieved such a deep network by sharing network parameters and recursive learning, which was further employed in DRRN <ref type="bibr" target="#b47">[47]</ref>. However, these methods would pre-process the original input by interpolating it to desired size. Such a step would not only introduce extra artifacts (e.g., blurring artifacts), but also increase the computation complexity quadratically. As a result, extracting features from the interpolated LR images would not be able to build direct mapping from the original LR to HR images.</p><p>To address the problem above, Dong et al. <ref type="bibr" target="#b48">[48]</ref> built direct mapping from the original LR image to the HR target by introducing a transposed Conv layer (also known as deconvolution layer). Such a transposed Conv layer was further replaced by an efficient sub-pixel convolution layer in ESPCN <ref type="bibr" target="#b49">[49]</ref>. Due to its efficiency, sub-pixel Conv layer was adopted in SRResNet <ref type="bibr" target="#b50">[50]</ref>, which took advantage of residual learning <ref type="bibr" target="#b51">[51]</ref>. By extracting features from the original LR input and upscaling the final LR features with transposed or sub-pixel convolution layer, they could either be capable of real-time SR (e.g., FSRCNN and ESPCN), or be built to be very deep/wide (e.g., SRResNet and EDSR). However, all of these methods neglect to adequately utilize information from each Conv layer, but only upscale the high-level CNN features for the final reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Convolutional Neural Network (CNN)</head><p>LeCun et al. integrated constraints from task domain to enhance network generalization ability for handwritten zip code recognition <ref type="bibr" target="#b52">[52]</ref>, which can be viewed as the pioneering usage of CNNs. Later, various network structures were proposed with better performance, such as AlexNet <ref type="bibr" target="#b53">[53]</ref>, VGG <ref type="bibr" target="#b54">[54]</ref>, and GoogleNet <ref type="bibr" target="#b55">[55]</ref>. Recently, He et al. <ref type="bibr" target="#b51">[51]</ref> investigated the powerful effectiveness of network depth and proposed deep residual learning for very deep trainable networks. Such a very deep residual network achieves significant improvements on several computer vision tasks, like image classification and object detection. Huang et al. proposed DenseNet, which allows direct connections between any two layers within the same dense block <ref type="bibr" target="#b29">[30]</ref>. With the local dense connections, each layer reads information from all the preceding layers within the same dense block. The dense connection was introduced among memory blocks <ref type="bibr" target="#b8">[9]</ref> and dense blocks <ref type="bibr" target="#b18">[19]</ref>. More differences between DenseNet/SRDenseNet/MemNet and our RDN would be discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning for Image Restoration</head><p>Generally, there are two categories of methods for image restoration: model-based <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and learningbased <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> methods. The model-based methods often formulate the image restoration problems as optimization ones. Numerous regularizers have been investigated to search for better solutions, such as sparsity-based regularizers with learned dictionaries <ref type="bibr" target="#b3">[4]</ref> and nonlocal self-similarity inspired ones <ref type="bibr" target="#b5">[6]</ref>. Also, instead of using explicitly designed regularizers, Dong et al. <ref type="bibr" target="#b6">[7]</ref> proposed a denoising prior driven network for image restoration. Then, we give more details about learning-based methods.</p><p>Dong et al. <ref type="bibr" target="#b24">[25]</ref> proposed ARCNN for image compression artifact reduction (CAR) with several stacked convolutional layers. Mao et al. <ref type="bibr" target="#b21">[22]</ref> proposed residual encoderdecoder networks (RED) with symmetric skip connections, which made the network go deeper (up to 30 layers). Zhang et al. <ref type="bibr" target="#b22">[23]</ref> proposed DnCNN to learn mappings from noisy images to noise and further improved performance by utilizing batch normalization <ref type="bibr" target="#b56">[56]</ref>. Zhang et al. <ref type="bibr" target="#b7">[8]</ref> proposed to learn deep CNN denoiser prior for image restoration (IRCNN) by integrating CNN denoisers into model-based optimization method. However, such methods have limited network depth (e.g., 30 for RED, 20 for DnCNN, and 7 for IRCNN), limiting the network ability. Simply stacking more layers cannot reach better results due to gradient vanishing problem. On the other hand, by using short term and long-term memory, Tai et al. <ref type="bibr" target="#b8">[9]</ref> proposed MemNet for image restoration, where the network depth reached 212 but obtained limited improvement over results with 80 layers. For 31×31 input patches from 91 images, training an 80layer MemNet takes 5 days using 1 Tesla P40 GPU <ref type="bibr" target="#b8">[9]</ref>.</p><p>The aforementioned DL-based image restoration methods have achieved significant improvement over conventional methods, but most of them lose some useful hierarchical features from the original LQ image. Hierarchical features produced by a very deep network are useful for image restoration tasks (e.g., image SR). To fix this case, we propose residual dense network (RDN) to extract and adaptively fuse features from all the layers in the LQ space efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESIDUAL DENSE NETWORK FOR IR</head><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we show our proposed RDN for image restoration, including image super-resolution (see <ref type="figure" target="#fig_1">Figure 2</ref>(a)), denoising, compression artifact reduction, and deblurring (see <ref type="figure" target="#fig_1">Figure 2</ref>(b)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Structure</head><p>we mainly take image SR as an example and give specific illustrations for image DN and CAR cases.</p><p>RDN for image SR. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), our RDN mainly consists of four parts: shallow feature extraction net, residual dense blocks (RDBs), dense feature fusion (DFF), and finally the up-sampling net (UPNet). Let's denote I LQ and I HQ as the input and output of RDN. Specifically, we use two Conv layers to extract shallow features. The first Conv layer extracts features F −1 from the LQ input.</p><formula xml:id="formula_0">F −1 = H SF E1 (I LQ ) ,<label>(1)</label></formula><p>where H SF E1 (·) denotes convolution operation. F −1 is then used for further shallow feature extraction and global residual learning. So, we can further have</p><formula xml:id="formula_1">F 0 = H SF E2 (F −1 ) ,<label>(2)</label></formula><p>where H SF E2 (·) denotes convolution operation of the second shallow feature extraction layer and is used as input to residual dense blocks. Supposing we have D residual dense blocks, the output F d of the d-th RDB can be obtained by</p><formula xml:id="formula_2">F d = H RDB,d (F d−1 ) = H RDB,d (H RDB,d−1 (· · · (H RDB,1 (F 0 )) · · · )) ,<label>(3)</label></formula><p>where H RDB,d denotes the operations of the d-th RDB.</p><p>H RDB,d can be a composite function of operations, such as convolution and rectified linear units (ReLU) <ref type="bibr" target="#b57">[57]</ref>. As F d is produced by the d-th RDB fully utilizing each convolutional layers within the block, we can view F d as local feature. More details about RDB will be given in Section 3.2.</p><p>After extracting hierarchical features with a set of RDBs, we further conduct dense feature fusion (DFF), which includes global feature fusion (GFF) and global residual learning. DFF makes full use of features from all the preceding layers and can be represented as   where F DF is the output feature-maps of DFF by utilizing a composite function H DF F . More details about DFF will be shown in Section 3.3. After extracting local and global features in the LQ space, we stack an up-sampling net (UPNet) in the HQ space. Inspired by <ref type="bibr" target="#b26">[27]</ref>, we utilize ESPCN <ref type="bibr" target="#b49">[49]</ref> in UPNet followed by one Conv layer. The output of RDN can be obtained by</p><formula xml:id="formula_3">F DF = H DF F (F −1 , F 0 , F 1 , · · · , F D ) ,<label>(4)</label></formula><formula xml:id="formula_4">I HQ = H RDN (I LQ ) ,<label>(5)</label></formula><p>where H RDN denotes the function of our RDN. RDN for image DN, CAR, and Deblurring. When we apply our RDN to image DN, CAR, and Deblurring, the resolution of the input and output keep the same. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), we remove the upscaling module in UPNet and obtain the final HQ output via residual learning</p><formula xml:id="formula_5">I HQ = H RDN (I LQ ) + I LQ .<label>(6)</label></formula><p>Unlike image SR, here, we use residual learning connecting input and output for faster training. It should be noted that the network structure in <ref type="figure" target="#fig_1">Figure 2</ref>(b) is also suitable for image SR. But, it would consume more GPU memory and running time. As a result, we choose the network structure in <ref type="figure" target="#fig_1">Figure 2</ref>(a) for image SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Residual Dense Block</head><p>We present details about our proposed residual dense block (RDB) shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Our RDB contains dense connected layers, local feature fusion (LFF), and local residual learning, leading to a contiguous memory (CM) mechanism.</p><p>Contiguous memory (CM) mechanism. The idea behind CM is that we aim at fusing information from all the Conv layers as much as possible. But, directly fuse feature maps from all the Covn layers is not practical, because it would stack huge amount of features. Instead, we turn to first adaptively fuse information locally and then pass them to the following feature fusions. It is realized by passing the state of preceding RDB to each layer of the current RDB. Let F d−1 and F d be the input and output of the d-th RDB respectively and both have G 0 feature-maps. The output of c-th Conv layer of d-th RDB can be formulated as</p><formula xml:id="formula_6">F d,c = σ (W d,c [F d−1 , F d,1 , · · · , F d,c−1 ]) ,<label>(7)</label></formula><p>where σ denotes the ReLU <ref type="bibr" target="#b57">[57]</ref> activation function. W d,c is the weights of the c-th Conv layer, where the bias term is omitted for simplicity. We assume F d,c consists of G (also known as growth rate <ref type="bibr" target="#b29">[30]</ref>) feature-maps.</p><formula xml:id="formula_7">[F d−1 , F d,1 , · · · , F d,c−1 ]</formula><p>refers to the concatenation of the feature-maps produced by the (d − 1)-th RDB, convolutional layers 1, · · · , (c − 1) in the d-th RDB, resulting in G 0 +(c − 1) ×G feature-maps. The outputs of the preceding RDB and each layer have direct connections to all subsequent layers, which not only preserves the feed-forward nature, but also extracts local dense feature. Local feature fusion (LFF). We apply LFF to adaptively fuse the states from preceding RDB and the whole Conv layers in current RDB. As analyzed above, the feature-maps of the (d − 1)-th RDB are introduced directly to the d-th RDB in a concatenation way, it is essential to reduce the feature number. On the other hand, inspired by MemNet <ref type="bibr" target="#b8">[9]</ref>, we introduce a 1 × 1 convolutional layer to adaptively control the output information. We name this operation as local feature fusion (LFF) formulated as</p><formula xml:id="formula_8">F d,LF = H d LF F ([F d−1 , F d,1 , · · · , F d,c , · · · , F d,C ]) ,<label>(8)</label></formula><p>where H d LF F denotes the function of the 1 × 1 Conv layer in the d-th RDB. We also find that as the growth rate G becomes larger, very deep dense network without LFF would be hard to train. However, larger growth rate further contributes to the performance, which will be detailed in Section 5.1.</p><p>Local residual learning (LRL). We introduce LRL in RDB to further improve the information flow and allow larger growth rate, as there are several convolutional layers in one RDB. The final output of the d-th RDB can be obtained by</p><formula xml:id="formula_9">F d = F d−1 + F d,LF .<label>(9)</label></formula><p>It should be noted that LRL can also further improve the network representation ability, resulting in better performance.</p><p>We introduce more results about LRL in Section 5.2. Because of the dense connectivity and local residual learning, we refer to this block architecture as residual dense block (RDB). More differences between RDB and original dense block <ref type="bibr" target="#b29">[30]</ref> would be summarized in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dense Feature Fusion</head><p>After extracting local dense features with a set of RDBs, we further propose dense feature fusion (DFF) to exploit hierarchical features in a global way. DFF consists of global feature fusion (GFF) and global residual learning. Global feature fusion (GFF). We propose GFF to extract the global feature F GF by fusing features from all the RDBs</p><formula xml:id="formula_10">F GF = H GF F ([F 1 , · · · , F D ]) ,<label>(10)</label></formula><p>where [F 1 , · · · , F D ] refers to the concatenation of feature maps produced by residual dense blocks 1, · · · , D. H GF F is a composite function of 1 × 1 and 3 × 3 convolution. The 1 × 1 convolutional layer is used to adaptively fuse a range of features with different levels. The following 3 × 3 convolutional layer is introduced to further extract features for global residual learning, which has been demonstrated to be effective in <ref type="bibr" target="#b50">[50]</ref>. Global residual learning. We then utilize global residual learning to obtain the feature-maps before conducting upscaling by</p><formula xml:id="formula_11">F DF = F −1 + F GF ,<label>(11)</label></formula><p>where F −1 denotes the shallow feature-maps. All the other layers before global feature fusion are extensively utilized with our proposed residual dense blocks (RDBs). RDBs produce multi-level local dense features, which are further adaptively fused to form F GF . After global residual learning, we obtain deep dense feature F DF . It should be noted that Tai et al. <ref type="bibr" target="#b8">[9]</ref> utilized longterm dense connections in MemNet to recover more highfrequency information. However, in the memory block <ref type="bibr" target="#b8">[9]</ref>, the preceding layers don't have direct access to all the subsequent layers. The local feature information is not fully used, limiting the ability of long-term connections. In addition, MemNet extracts features in the HQ space, increasing computational complexity. While, inspired by <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref>, we extract local and global features in the LQ space. More differences between our proposed RDN and MemNet would be shown in Section 4. We would also demonstrate the effectiveness of global feature fusion in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>In our proposed RDN, we set 3 × 3 as the size of all convolutional layers except that in local and global feature fusion, whose kernel size is 1 × 1. For convolutional layer with kernel size 3 × 3, we pad zeros to each side of the input to keep size fixed. Shallow feature extraction layers, local and global feature fusion layers have G 0 =64 filters. Other layers in each RDB has G=64 filters and are followed by ReLU <ref type="bibr" target="#b57">[57]</ref>. For image SR, following <ref type="bibr" target="#b26">[27]</ref>, we use ES-PCNN <ref type="bibr" target="#b49">[49]</ref> to upscale the coarse resolution features to fine ones for the UPNet. For image DN and CAR, the up-scaling module is removed from UPNet. The final Conv layer has 3 output channels, as we output color HQ images. However, the network can also process gray images, for example, when we apply RDN for gray-scale image denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DIFFERENCES WITH PRIOR WORKS</head><p>Here, we give more details about the differences between our RDN and several related representative works. We further demonstrate those differences make our RDN more effective in Sections 5 and 6.</p><p>Difference to DenseNet. First, DenseNet <ref type="bibr" target="#b29">[30]</ref> is widely used in high-level computer vision tasks (e.g., image classification). While RDN is designed more specific for image restoration without using some operations in DenseNet, such as batch normalization and max pooling. Second, DenseNet places transition layers into two adjacent dense blocks. In RDN, the Conv layers are connected by local feature fusion (LFF) and local residual learning. Consequently, the (d − 1)-th RDB has direct access to each layer in the d-th RDB and also contributes to the input of (d + 1)-th RDB. Third, we adopt GFF to make better use of hierarchical features, which are neglected in DenseNet.</p><p>Difference to SRDenseNet. First, SRDenseNet <ref type="bibr" target="#b18">[19]</ref> introduces the basic dense block with batch normalization from DenseNet <ref type="bibr" target="#b29">[30]</ref>. Our residual dense block (RDB) improves it in several ways: <ref type="bibr" target="#b0">(1)</ref>. We propose contiguous memory (CM) mechanism, building direct connections between the preceding RDB and each layer of the current RDB. <ref type="bibr" target="#b1">(2)</ref>. Our RDB can be easily extended to be wider and be easy to train with usage of local feature fusion (LFF). <ref type="bibr" target="#b2">(3)</ref>. RDB utilizes local residual learning (LRL) to further encourage the information flow. Second, SRDenseNet densely connects dense blocks, while there are no dense connections among RDBs. We use global feature fusion (GFF) and global residual learning to extract hierarchical features, based on the fully extracted extracted local features by RDBs. Third, SRDenseNet aims to minimize L 2 loss. However, bing more powerful for performance and convergence <ref type="bibr" target="#b26">[27]</ref>, L 1 loss function is utilized in RDN for each image restoration task.</p><p>Difference to MemNet. In addition to the different choices for loss fuction between MemNet <ref type="bibr" target="#b8">[9]</ref> and RDN, we mainly summarize additional three differences. First, MemNet has to interpolate the original LR to the desired size, introducing extra blurry artifacts. While, RDN directly extracts features from the original LR input, which helps to reduce computational complexity and contribute to better performance. Second, in MemNet, most Conv layers within one recursive unit have no direct access to their preceding layers or memory block. While, each Conv layer receives  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NETWORK INVESTIGATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Study of D, C, and G.</head><p>In this subsection, we investigate the basic network parameters: the number of RDB (denote as D for short), the number of Conv layers per RDB (denote as C for short), and the growth rate (denote as G for short). We use the performance of SRCNN [61] as a reference. As shown in Figures 4(a) and 4(b), larger D or C would lead to higher performance. This is mainly because the network becomes deeper with larger D or C. As our proposed LFF allows larger G, we also observe larger G (see <ref type="figure" target="#fig_3">Figure 4</ref>(c)) contributes to better performance. On the other hand, RND with smaller D, C, or G would suffer some performance drop in the training, but RDN would still outperform SRCNN <ref type="bibr" target="#b61">[61]</ref>. More important, our RDN allows deeper and wider network, where more hierarchical features are extracted for higher performance. <ref type="table">Table 2</ref> shows the ablation investigation on the effects of contiguous memory (CM), local residual learning (LRL), and global feature fusion (GFF). The eight networks have the same RDB number (D = 20), Conv number (C = 6) per RDB, and growth rate (G = 32). We find that local feature  fusion (LFF) is needed to train these networks properly, so LFF isn't removed by default. The baseline (denote as RDN CM0LRL0GFF0) is obtained without CM, LRL, or GFF and performs very poorly (PSNR = 34.87 dB). This is caused by the difficulty of training <ref type="bibr" target="#b61">[61]</ref> and also demonstrates that stacking many basic dense blocks <ref type="bibr" target="#b29">[30]</ref> in a very deep network would not result in better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Investigation</head><p>We then add one of CM, LRL, or GFF to the baseline, resulting in RDN CM1LRL0GFF0, RDN CM0LRL1GFF0, and RDN CM0LRL0GFF1 respectively (from 2 nd to 4 th combination in <ref type="table">Table 2</ref>). We can validate that each component can efficiently improve the performance of the baseline. This is mainly because each component contributes to the flow of information and gradient.</p><p>We further add two components to the baseline, resulting in RDN CM1LRL1GFF0, RDN CM1LRL0GFF1, and RDN CM0LRL1GFF1 respectively (from 5 th to 7 th combination in <ref type="table">Table 2</ref>). It can be seen that two components would perform better than only one component. Similar phenomenon can be seen when we use these three components simultaneously (denote as RDN CM1LRL1GFF1). RDN using three components performs the best. We also visualize the convergence process of these eight combinations in <ref type="figure">Figure 5</ref>. The convergence curves are consistent with the analyses above and show that CM, LRL, and GFF can further stabilize the training process without obvious performance drop. These quantitative and visual analyses demonstrate the effectiveness and benefits of our proposed CM, LRL, and GFF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Size, Performance, and Test Time</head><p>We also compare the model size, performance, and test time with other methods on Set14 (×2) in <ref type="table" target="#tab_4">Table 3</ref>. Compared with EDSR, our RDN has half less amount of parameter and obtains better results. Although our RDN has more parameters than that of other methods, RDN achieves comparable (e.g., MDSR) or even less test time (e.g., MemNet). We further visualize the performance and test time comparison in <ref type="figure">Figure 6</ref>. We can see that our RDN achieves good tradeoff between the performance and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>The source code and models of the proposed method can be downloaded at https://github.com/yulunzhang/RDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Settings</head><p>Here we provide details of experimental settings, such as training/testing data for each tasks.</p><p>Training Data. Recently, Timofte et al. have released a high-quality (2K resolution) dataset DIV2K for image restoration applications <ref type="bibr" target="#b36">[37]</ref>. DIV2K consists of 800 training images, 100 validation images, and 100 test images. We use DIV2K as training data for image SR (except for Bicubic degradation model), color and gray image denoising, CAR, and deblurring. For image SR with Bicubic degradation (BI), some compared methods (e.g., D-DBPN <ref type="bibr" target="#b37">[38]</ref>) further use Flickr2K <ref type="bibr" target="#b26">[27]</ref> as well as DIV2K for training. We also train RDN by using larger training data to investigate whether RDN can further improve performance.</p><p>Testing Data and Metrics. For testing, we use five standard benchmark datasets: Set5 <ref type="bibr" target="#b58">[58]</ref>, Set14 <ref type="bibr" target="#b59">[59]</ref>, B100 <ref type="bibr" target="#b60">[60]</ref>, Urban100 <ref type="bibr" target="#b16">[17]</ref>, and Manga109 <ref type="bibr" target="#b64">[64]</ref> for image SR. We use Ko-dak24 (http://r0k.us/graphics/kodak/), BSD68 <ref type="bibr" target="#b60">[60]</ref>, and Urban100 <ref type="bibr" target="#b16">[17]</ref> for color and gray image DN. LIVE1 <ref type="bibr" target="#b65">[65]</ref> and Classic5 <ref type="bibr" target="#b66">[66]</ref> are used for image CAR. We use Mc-Master18 <ref type="bibr" target="#b7">[8]</ref>, Kodak24, and Urban100 as testing data for image deblurring. The quantitative results are evaluated with PSNR and SSIM <ref type="bibr" target="#b67">[67]</ref> on Y channel (i.e., luminance) of transformed YCbCr space.</p><p>Degradation Models. For image SR, in order to fully demonstrate the effectiveness of our proposed RDN, we use three degradation models to simulate LR images for image SR. The first one is bicubic downsampling by adopting the Matlab function imresize with the option bicubic (denote as BI for short). We use BI model to simulate LR images with scaling factor ×2, ×3, ×4, and ×8. Similar to <ref type="bibr" target="#b7">[8]</ref>, the second one is to blur HR image by Gaussian kernel of size 7 × 7 with standard deviation 1.6. The blurred image is then downsampled with scaling factor ×3 (denote as BD for short). We further produce LR image in a more challenging way. We first bicubic downsample HR image with scaling factor ×3 and then add Gaussian noise with noise level 30 (denote as DN for short). For color and gray image denoising, we add Gaussian noise with noise level σ to the ground truth to obtain the noisy inputs. For image CAR, we use Matlab JPEG encoder <ref type="bibr" target="#b68">[68]</ref> to generate compressed inputs. For image deburring, the commonly-used 25×25 Gaussian blur kernel of standard deviation 1.6 is used to blur images first. Then, additive Gaussian noise (σ = 2) is added to the blurry images to obtain the final inputs.</p><p>Training Setting. Following settings of <ref type="bibr" target="#b26">[27]</ref>, in each training batch, we randomly extract 16 LQ RGB patches with the size of 48 × 48 as inputs for image SR, DN, CAR, and deblurring. We randomly augment the patches by flipping horizontally or vertically and rotating 90 • . 1,000 iterations of back-propagation constitute an epoch. We implement our RDN with the Torch7 framework and update it with Adam optimizer <ref type="bibr" target="#b69">[69]</ref>. The learning rate is initialized to 10 −4 for all layers and decreases half for every 200 epochs. Training a RDN roughly takes 1 day with a Titan Xp GPU for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Image Super-Resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Results with BI Degradation Model</head><p>Simulating LR image with BI degradation model is widely used in image SR settings. For BI degradation model, we compare our RDN with state-of-the-art image SR methods: SRCNN <ref type="bibr" target="#b61">[61]</ref>, FSRCNN <ref type="bibr" target="#b48">[48]</ref>, SCN <ref type="bibr" target="#b62">[62]</ref>, VDSR <ref type="bibr" target="#b17">[18]</ref>, Lap-SRN <ref type="bibr" target="#b28">[29]</ref>, MemNet <ref type="bibr" target="#b8">[9]</ref>, SRDenseNet <ref type="bibr" target="#b18">[19]</ref>, MSLapSRN <ref type="bibr" target="#b63">[63]</ref>, EDSR <ref type="bibr" target="#b26">[27]</ref>, SRMDNF <ref type="bibr" target="#b19">[20]</ref>, D-DBPN <ref type="bibr" target="#b37">[38]</ref>, and DPDNN <ref type="bibr" target="#b6">[7]</ref>. Similar to <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b70">[70]</ref>, we also adopt self-ensemble strategy <ref type="bibr" target="#b26">[27]</ref> to further improve our RDN and denote the selfensembled RDN as RDN+. Here, we also additionally use Flickr2K <ref type="bibr" target="#b36">[37]</ref> as training data, which is also used in SR-MDNF <ref type="bibr" target="#b19">[20]</ref>, and D-DBPN <ref type="bibr" target="#b37">[38]</ref>. As analyzed above, a deeper and wider RDN would lead to a better performance. On the other hand, as most methods for comparison only use about  <ref type="table" target="#tab_5">Table 4</ref> shows quantitative comparisons for ×2, ×3, and ×4 SR. Results of SRDenseNet <ref type="bibr" target="#b18">[19]</ref> are cited from their paper. When compared with persistent CNN models ( SRDenseNet <ref type="bibr" target="#b18">[19]</ref> and MemNet <ref type="bibr" target="#b8">[9]</ref>), our RDN performs the best on all datasets with all scaling factors. This indicates the better effectiveness of our residual dense block (RDB) over dense block in SRDensenet <ref type="bibr" target="#b18">[19]</ref> and the memory block in MemNet <ref type="bibr" target="#b8">[9]</ref>. When compared with the remaining models, our RDN also achieves the best average results on most datasets. Specifically, for the scaling factor ×2, our RDN performs the best on all datasets. EDSR <ref type="bibr" target="#b26">[27]</ref> uses far more filters (i.e., 256) per Conv layer, leading to a very wide network with a large number of parameters (i.e., 43 M). Our RDN has about half less network parameter number and achieves better performance.</p><p>In <ref type="figure" target="#fig_5">Figure 7</ref>, we show visual comparisons on scales ×4 and ×8. We observe that most of compared methods cannot recover the lost details in the LR image (e.g., "img 004"), even though EDSR and D-DBPN can reconstruct partial details. In contrast, our RDN can recover sharper and clearer edges, more faithful to the ground truth. In image "img 092", some unwanted artifacts are generated in the degradation process. All the compared methods would fail to handle such a case, but enlarge the mistake. However, our RDN can alleviate the degradation artifacts and recover correct structures. When scaling factor goes larger (e.g., ×8), more structural and textural details are lost. Even we human beings can hardly distinguish the semantic content in the LR images. Most compared methods cannot recover the lost details either. However, with the usage of hierarchical features through dense feature fusion, our RDN reconstruct better visual results with clearer structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Results with BD and DN Degradation Models</head><p>Following <ref type="bibr" target="#b7">[8]</ref>, we also show the SR results with BD degradation model and further introduce DN degradation model. Our RDN is compared with SPMSR <ref type="bibr" target="#b13">[14]</ref>, SRCNN <ref type="bibr" target="#b61">[61]</ref>, FSR-CNN <ref type="bibr" target="#b48">[48]</ref>, VDSR <ref type="bibr" target="#b17">[18]</ref>, IRCNN G <ref type="bibr" target="#b7">[8]</ref>, and IRCNN C <ref type="bibr" target="#b7">[8]</ref>. We re-train SRCNN, FSRCNN, and VDSR for each degradation model. <ref type="table" target="#tab_7">Table 5</ref> shows the average PSNR and SSIM results on Set5, Set14, B100, Urban100, and Manga109 with scaling factor ×3. Our RDN and RDN+ perform the best on all the datasets with BD and DN degradation models. The Urban100: img 004 (×4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Bicubic SRCNN <ref type="bibr" target="#b61">[61]</ref> FSRCNN <ref type="bibr" target="#b48">[48]</ref> VDSR <ref type="bibr" target="#b17">[18]</ref> MemNet <ref type="bibr" target="#b8">[9]</ref> LapSRN <ref type="bibr" target="#b28">[29]</ref> EDSR <ref type="bibr">[</ref>   performance gains over other state-of-the-art methods are consistent with the visual results in <ref type="figure">Figures 8 and 9</ref>. For BD degradation model <ref type="figure">(Figure 8</ref>), the methods using interpolated LR image as input would produce noticeable artifacts and be unable to remove the blurring artifacts. In contrast, our RDN suppresses the blurring artifacts and recovers sharper edges. This comparison indicates that extracting hierarchical features from the original LR image would alleviate the blurring artifacts. It also demonstrates the strong ability of RDN for BD degradation model. For DN degradation model <ref type="figure">(Figure 9</ref>), where the LR image is corrupted by noise and loses some details. We observe that the noised details are hard to recovered by other methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b61">[61]</ref>. However, our RDN can not only handle the noise efficiently, but also recover more details. This comparison indicates that RDN is applicable for jointly image denoising and SR. These results with BD and DN degradation models demonstrate the effectiveness and robustness of our RDN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Super-Resolving Real-World Images</head><p>To further demonstrate the effectiveness of our proposed RDN, we super-resolve historical images with JPEG compression artifacts by ×4. We compare with two state-of-theart methods: SRMDNF <ref type="bibr" target="#b19">[20]</ref> and D-DBPN <ref type="bibr" target="#b37">[38]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 10</ref>, the historical image contains letters "HOME" and GT Bicubic SPMSR <ref type="bibr" target="#b13">[14]</ref> SRCNN <ref type="bibr" target="#b61">[61]</ref> IRCNN <ref type="bibr" target="#b7">[8]</ref> RDN (ours) <ref type="figure">Fig. 8</ref>. Visual results using BD degradation model with scaling factor ×3.</p><p>GT Bicubic SRCNN <ref type="bibr" target="#b61">[61]</ref> VDSR <ref type="bibr" target="#b17">[18]</ref> IRCNN <ref type="bibr" target="#b7">[8]</ref> RDN (ours) <ref type="figure">Fig. 9</ref>. Visual results using DN degradation model with scaling factor ×3.</p><p>Historical: img004 SRMDNF <ref type="bibr" target="#b19">[20]</ref> D-DBPN <ref type="bibr" target="#b37">[38]</ref> RDN (ours) <ref type="figure" target="#fig_0">Fig. 10</ref>. Visual results on real-world images with scaling factor ×4.</p><p>"ANTI". Both SRMDNF and D-DBPN suffer from blurring and distorted artifacts. On the other hand, our RDN reconstruct sharper and more accurate results. These comparisons indicate the benefits of learning hierarchical features from the original input, allowing our RDN to perform robustly for different or unknown degradation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Image Denoising</head><p>We compare our RDN with recently leading Gaussian denoising methods: BM3D <ref type="bibr" target="#b71">[71]</ref>, CBM3D <ref type="bibr" target="#b72">[72]</ref>, TNRD <ref type="bibr" target="#b20">[21]</ref>, RED <ref type="bibr" target="#b21">[22]</ref>, DnCNN <ref type="bibr" target="#b22">[23]</ref>, MemNet <ref type="bibr" target="#b8">[9]</ref>, IRCNN <ref type="bibr" target="#b7">[8]</ref>, MWCNN <ref type="bibr" target="#b44">[44]</ref>, N 3 Net <ref type="bibr" target="#b45">[45]</ref>, NLRN <ref type="bibr" target="#b46">[46]</ref>, and FFDNet <ref type="bibr" target="#b23">[24]</ref>. Kodak24 1 , BSD68 <ref type="bibr" target="#b60">[60]</ref>, and Urban100 <ref type="bibr" target="#b16">[17]</ref> are used for grayscale and color image denoising. Set12 <ref type="bibr" target="#b22">[23]</ref> is also used to test gray-scale image denoising. Noisy images are obtained by adding AWGN noises of different levels to clean images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Gray-scale Image Denoising</head><p>The PSNR results are shown in  <ref type="bibr" target="#b23">[24]</ref> on four test sets respectively. Gains on Urban100 become larger, which is mainly because our method takes advantage of a larger scope of context information with hierarchical features. Moreover, for noise levels σ = 30, 50, and 70, the gains over BM3D of RDN are larger than 0.7 dB, breaking the estimated PSNR bound (0.7 dB) over BM3D in <ref type="bibr" target="#b73">[73]</ref>. It should also be noted that our RDN achieves moderate gains over MemNet on BSD68. This is mainly because BSD68 is formed from 100 validation images in Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b60">[60]</ref>. But MemNet <ref type="bibr" target="#b8">[9]</ref> used these 100 validation images and 200 training images in BSD as training data. So it's reasonable for MemNet to perform pretty well on BSD68. Moreover, MWCNN <ref type="bibr" target="#b44">[44]</ref>, N 3 Net <ref type="bibr" target="#b45">[45]</ref>, and NLRN <ref type="bibr" target="#b46">[46]</ref> also have achieved good performance for some noise levels, which indicates that Wavelet transformation and non-local processing are promissing candidates for further image denoising improvements.</p><p>We show visual gray-scale denoised results of different methods in <ref type="figure" target="#fig_0">Figure 11</ref>. We can see that BM3D preserves image structure to some degree, but fails to remove noise deeply. TNRD <ref type="bibr" target="#b20">[21]</ref> tends to generate some artifacts in the smooth region. RED <ref type="bibr" target="#b21">[22]</ref>, DnCNN <ref type="bibr" target="#b22">[23]</ref>, MemNet <ref type="bibr" target="#b8">[9]</ref>, and IRCNN <ref type="bibr" target="#b7">[8]</ref> would over-smooth edges. The main reason should be the limited network ability for high noise level (e.g., σ = 50). In contrast, our RDN can remove noise greatly and recover more details (e.g., the tiny lines in "img 061"). Also, the gray-scale visual results by our RDN in the smooth BSD68: 119082 GT Noisy (σ=50) BM3D <ref type="bibr" target="#b71">[71]</ref> TNRD <ref type="bibr" target="#b20">[21]</ref> RED <ref type="bibr" target="#b21">[22]</ref> DnCNN <ref type="bibr" target="#b22">[23]</ref> MemNet <ref type="bibr" target="#b8">[9]</ref> IRCNN <ref type="bibr" target="#b7">[8]</ref> FFDNet <ref type="bibr" target="#b23">[24]</ref> RDN (ours) Urban100: img 061 GT Noisy (σ=50) BM3D <ref type="bibr" target="#b71">[71]</ref> TNRD <ref type="bibr" target="#b20">[21]</ref> RED <ref type="bibr" target="#b21">[22]</ref> DnCNN <ref type="bibr" target="#b22">[23]</ref> MemNet <ref type="bibr" target="#b8">[9]</ref> IRCNN <ref type="bibr" target="#b7">[8]</ref> FFDNet <ref type="bibr" target="#b23">[24]</ref> RDN (ours) <ref type="figure" target="#fig_0">Fig. 11</ref>. Gray-scale image denoising results with noise level σ = 50.</p><p>region are more faithful to the clean images (e.g., smooth regions in "119082" and "img 061").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Color Image Denoising</head><p>We generate noisy color images by adding AWGN noise to clean RGB images with different noise levels σ = 10, 30, 50, and 70. The PSNR results are listed in <ref type="table" target="#tab_10">Table 7</ref>. We apply gray image denoising methods (e.g., MemNet <ref type="bibr" target="#b8">[9]</ref>) for color image denoising channel by channel. Lager gains over MemNet <ref type="bibr" target="#b8">[9]</ref> of our RDN indicate that denoising color images jointly perform better than denoising each channel separately. Take σ=50 as an example, our RDN obtains 0.56 dB, 0.35, and 1.24 dB improvements over FFDNet <ref type="bibr" target="#b23">[24]</ref> on three test sets respectively. Residual learning and dense feature fusion allows RDN to go wider and deeper, obtain hierarchical features, and achieve better performance. We also show color image denoising visual results in <ref type="figure" target="#fig_0">Figure 12</ref>. CBM3D <ref type="bibr" target="#b72">[72]</ref> tends to produce artifacts along the edges. TNRD <ref type="bibr" target="#b20">[21]</ref> produces artifacts in the smooth area and is unable to recover clear edges. RED <ref type="bibr" target="#b21">[22]</ref>, DnCNN <ref type="bibr" target="#b22">[23]</ref>, MemNet <ref type="bibr" target="#b8">[9]</ref>, IRCNN <ref type="bibr" target="#b7">[8]</ref>, and FFDNet <ref type="bibr" target="#b23">[24]</ref> could produce blurring artifacts along edges (e.g., the structural lines in "img 039"). Because RED <ref type="bibr" target="#b21">[22]</ref> and MemNet <ref type="bibr" target="#b8">[9]</ref> were designed for gray image denoising. In our experiments on color image denoising, we conduct RED <ref type="bibr" target="#b21">[22]</ref> and Mem-Net <ref type="bibr" target="#b8">[9]</ref> in each channel. Although DnCNN <ref type="bibr" target="#b22">[23]</ref>, IRCNN <ref type="bibr" target="#b7">[8]</ref>, and FFDNet <ref type="bibr" target="#b23">[24]</ref> directly denoise noisy color images in three channels, they either fail to recover sharp edges and clean smooth area. In contrast, our RDN can recover shaper edges and cleaner smooth area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Real-World Color Image Denoising</head><p>To further demonstrate the effectiveness of our proposed RDN, we compare it with recent leading method MCWNNM <ref type="bibr" target="#b74">[74]</ref> on real noisy image. Following the same settings as <ref type="bibr" target="#b74">[74]</ref>, we estimate the noise levels (σ r , σ g , σ b ) via some noise estimation methods <ref type="bibr" target="#b75">[75]</ref>. Inspired by <ref type="bibr" target="#b74">[74]</ref>, we finetune RDN with the noise level σ= σ 2 r + σ 2 g + σ 2 b /3. Due to limited space, we only show the denoised results on the real noisy image "Dog" (with 192×192 pixels) <ref type="bibr" target="#b76">[76]</ref>, which doesn't have ground truth.</p><p>We show real-world color image denoising in <ref type="figure" target="#fig_0">Fig. 14</ref>. Although MCWNNM considers the specific noise levels of each channel, its result still suffers from over-smoothing artifacts and loses some details (e.g., the moustache). In contrast, our RDN handles the noise better and preserves more details and shaper edges, which indicates that RDN is also suitable for real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Image Compression Artifact Reduction</head><p>We further apply our RDN to reduce image compression artifacts. We compare our RDN with SA-DCT <ref type="bibr" target="#b66">[66]</ref>, AR-CNN <ref type="bibr" target="#b24">[25]</ref>, TNRD <ref type="bibr" target="#b20">[21]</ref>, and DnCNN <ref type="bibr" target="#b22">[23]</ref>. We use Matlab JPEG encoder <ref type="bibr" target="#b68">[68]</ref> to generate compressed test images from LIVE1 <ref type="bibr" target="#b65">[65]</ref> and Classic5 <ref type="bibr" target="#b66">[66]</ref>. Four JPEG quality settings q = 10, 20, 30, 40 are used in Matlab JPEG encoder. Here, we only focus on the compression artifact reduction (CAR) of Y channel (in YCbCr space) to keep fair comparison with other methods.</p><p>We report PSNR/SSIM values in <ref type="table" target="#tab_11">Table 8</ref>. As we can see, our RDN and RDN+ achieve higher PSNR and SSIM values on LIVE1 and Classic5 with all JPEG qualities than other compared methods. Taking q = 10 as an example, our RDN achieves 0.48 dB and 0.60 dB improvements over DnCNN <ref type="bibr" target="#b22">[23]</ref> in terms of PSNR. Even in such a challenging case (very low compression quality), our RDN can still obtain great performance gains over others. Similar improvements are also significant for other compression qualities. These comparisons further demonstrate the effectiveness of our proposed RDN.</p><p>Visual comparisons are further shown in <ref type="figure" target="#fig_0">Figure 13</ref>, where we provide comparisons under very low image quality (q=10). Although ARCNN <ref type="bibr" target="#b24">[25]</ref>, TNRD <ref type="bibr" target="#b20">[21]</ref>, and DnCNN <ref type="bibr" target="#b22">[23]</ref> can remove blocking artifacts to some degree, they also over-smooth some details (e.g., 1st and 2nd rows in <ref type="figure" target="#fig_0">Figure 13</ref>) and cannot deeply remove the compression artifacts around content structures (e.g., 3rd and 4th rows in <ref type="figure" target="#fig_0">Figure 13</ref>). While, RDN has stronger network representation ability to distinguish compression artifacts and content information better. As a result, RDN recovers more details with consistent content structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Image Deblurring</head><p>A common practice to synthesize blurry images is first apply a blur kernel and then add Gausian noise with noise       <ref type="bibr" target="#b7">[8]</ref>. There are several blur kernels, such as Gaussian and motion blur kernels. Here, we focus on the commonly-used 25×25 Gaussian blur kernel of standard deviation 1.6. The additive Gaussian noise (σ = 2) is added to the blurry images. We compare with IRCNN <ref type="bibr" target="#b7">[8]</ref> and use McMaster18 <ref type="bibr" target="#b7">[8]</ref>, Kodak24, and Urban100 as test sets. We provide quantitative results in <ref type="table" target="#tab_12">Table 9</ref>, where our RDN achieves large improvements over IRCNN for each test set. We further show visual results in <ref type="figure" target="#fig_0">Figure 15</ref>, where IRCNN still outputs some blurry structures. While, our RDN reconstructs much sharper edges and tiny details. These quantitative and qualitative comparisons demonstrate that our RDN also performs well for image deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSIONS</head><p>Here, we give a brief view of the benefits and limitations of our RDN and challenges in image restoration.</p><p>Benefits of RDN. RDB serves as the basic build module in RDN, which takes advantage of local and global feature fusion, obtaining very powerful representational ability. RDN uses less network parameters than residual network while achieves better performance than dense network, leading to a good tradeoff between model size and performance. RDN can be directly applied or generilized to several image restoration tasks with promissing performance.</p><p>Limitations of RDN. In some challenging cases (e.g., large scaling factor), RDN may fail to obtain proper details.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 16</ref>, although other methods fail to recover proper structures, our RDN cannot generates right structures either. The main reasons of this failure case may be that our RDN cannot recover similar textures based on the limited input information. Instead, RDN would generate most likely texture patterns learned from the training data.</p><p>Challenges in Image Restoration. Extreme cases make the image restoration tasks much harder, such as very large scaling factors for image SR, heavy noise for image DN, low JPEG quality in image CAR, and heavy blurring artifacts in image deblurring. Complex desegregation processes in the real world make it difficult for us to formulate the degradation process. Then it may make the data preparation and network training harder.</p><p>Future Works. We believe that RDN can be further applied to other image restoration tasks, such as image demosaicing, derain, and dehazing. On the other hand, it's worth to further improve the performance of RDN. As indicated in <ref type="figure" target="#fig_0">Figure 16</ref>, RDN generates better local structures than others, while the global recovered structures are wrong. How to learn stronger local and global feature representations is worth to investigate. Plus, RDN may further benefit from adversarial training, which may help to alleviate some blurring and over-smoothing artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In this work, based on our proposed residual dense block (RDB), we propose deep residual dense network (RDN) for image restoration. RDB allows direct connections from preceding RDB to each Conv layer of current RDB, which results in a contiguous memory (CM) mechanism. We proposed LFF to adaptively preserve the information from the current and previous RDBs. With the usage of LRL, the flow of gradient and information can be further improved and training wider network becomes more stable. Extensive benchmark and real-world evaluations well demonstrate that our RDN achieves superiority over state-of-the-art methods for several image restoration tasks.  <ref type="figure" target="#fig_0">Fig. 16</ref>. Failure cases for image super-resolution (×8).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison of prior network structures (a,b) and our residual dense block (c). (a) Residual block in MDSR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of our proposed residual dense network (RDN) for image restoration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Residual dense block (RDB) architecture. We denote the connections between the (d-1)-th RDB and the following convolutional layers as contiguous memory (CM) mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Convergence analysis of RDN for image SR (×2) with different values of D, C, and G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Fig. 5 .</head><label>25</label><figDesc>Ablation investigation of contiguous memory (CM), local residual learning (LRL), and global feature fusion (GFF). We observe the best performance (PSNR) on Set5 with scaling factor ×2 in 200 epochs.Different combinations of CM, LRL, and GFF CM # ! # # ! ! # ! LRL # # ! # ! # ! ! GFF # # # ! # ! ! ! PSNR 34.87 37.89 37.92 37.78 37.99 37.98 37.97 38.06 Convergence analysis on CM, LRL, and GFF. The curves for each combination is based on the PSNR on Set5 (×2) in 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Image super-resolution results (BI degradation model) with scaling factors s = 4 (first two rows) and s = 8 (last two rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 12 .</head><label>12</label><figDesc>Color image denoising results with noise level σ = 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Image compression artifacts reduction results with JPEG quality q = 10. Noisy MCWNNM [74] RDN (ours) Visual denoised results of the real noisy image "Dog". The noise level of R, G, B channels are estimated as 16.8, 17.0, and 16.6 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 PSNR</head><label>1</label><figDesc>(dB) comparisons under different block connections. The results are obtained with Bicubic (BI) degradation model for image SR (×4). RDB and outputs feature for the subsequent layers within same RDB. Plus, LRL further improves the information flow and performance. Third, in MemNet, the memory block neglects to fully utilize the features from preceding block and Conv layers within it.</figDesc><table><row><cell>Block connection</cell><cell cols="2">Dense connections</cell><cell cols="2">Contiguous memory</cell></row><row><cell>Datasets</cell><cell cols="2">SRDenseNet MemNet [30] [25]</cell><cell cols="2">RDN (w/o LRL) (with LRL) RDN</cell></row><row><cell>Set5 [58]</cell><cell>32.02</cell><cell>31.74</cell><cell>32.54</cell><cell>32.61</cell></row><row><cell>Set14 [59]</cell><cell>28.50</cell><cell>28.26</cell><cell>28.87</cell><cell>28.93</cell></row><row><cell>B100 [60]</cell><cell>27.53</cell><cell>27.40</cell><cell>27.75</cell><cell>27.80</cell></row><row><cell>Urban100 [17]</cell><cell>26.05</cell><cell>25.50</cell><cell>26.72</cell><cell>26.85</cell></row><row><cell cols="2">information from preceding</cell><cell></cell><cell></cell><cell></cell></row></table><note>Although dense connections are adopted to connect mem- ory blocks, MemNet fails to extract hierarchical features from the original LR images. However, with the local dense features extracted by RDBs, our RDN further fuses the hierarchical features from the whole preceding layers in a global way in the LR space.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>Parameter number, PSNR, and test time comparisons. The PSNR values are based on Set14 with Bicubic (BI) degradation model (×2).</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="5">LapSRN DRRN MemNet MDSR EDSR [29] [47] [9] [27] [27]</cell><cell>RDN (ours)</cell></row><row><cell></cell><cell># param.</cell><cell>812K</cell><cell>297K</cell><cell>677K</cell><cell>8M</cell><cell>43M</cell><cell>22M</cell></row><row><cell></cell><cell>PSNR (dB)</cell><cell>33.08</cell><cell>33.23</cell><cell>33.28</cell><cell>33.85</cell><cell>33.92</cell><cell>34.14</cell></row><row><cell></cell><cell>Time (s)</cell><cell>0.10</cell><cell>17.81</cell><cell>13.84</cell><cell>0.53</cell><cell>1.64</cell><cell>1.56</cell></row><row><cell></cell><cell>34.4</cell><cell cols="2">RDN+</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>34.2</cell><cell></cell><cell></cell><cell></cell><cell>RDN</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">EDSR+</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (dB)</cell><cell>33.6 33.8</cell><cell></cell><cell></cell><cell>MDSR+</cell><cell>EDSR</cell><cell>MDSR</cell></row><row><cell></cell><cell>33.4</cell><cell cols="2">MemNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>33.2</cell><cell>DRRN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LapSRN</cell></row><row><cell></cell><cell>33 10 2</cell><cell cols="4">10 0 Slower ← Test time (s) → Faster 10 1</cell><cell></cell><cell>10 −1</cell></row></table><note>Fig. 6. PSNR and test time on Set14 with BI model (×2).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Quantitative results with BI degradation model. Best and second best results are highlighted and underlined.</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell cols="2">Set5 PSNR</cell><cell>SSIM</cell><cell cols="2">Set14 PSNR SSIM</cell><cell cols="2">B100 PSNR SSIM</cell><cell cols="2">Urban100 PSNR SSIM</cell><cell cols="2">Manga109 PSNR SSIM</cell></row><row><cell>Bicubic</cell><cell>×2</cell><cell>33.66</cell><cell cols="2">0.9299</cell><cell>30.24</cell><cell>0.8688</cell><cell>29.56</cell><cell>0.8431</cell><cell>26.88</cell><cell>0.8403</cell><cell>30.80</cell><cell>0.9339</cell></row><row><cell>SRCNN [61]</cell><cell>×2</cell><cell>36.66</cell><cell cols="2">0.9542</cell><cell>32.45</cell><cell>0.9067</cell><cell>31.36</cell><cell>0.8879</cell><cell>29.50</cell><cell>0.8946</cell><cell>35.60</cell><cell>0.9663</cell></row><row><cell>FSRCNN [48]</cell><cell>×2</cell><cell>37.05</cell><cell cols="2">0.9560</cell><cell>32.66</cell><cell>0.9090</cell><cell>31.53</cell><cell>0.8920</cell><cell>29.88</cell><cell>0.9020</cell><cell>36.67</cell><cell>0.9710</cell></row><row><cell>VDSR [18]</cell><cell>×2</cell><cell>37.53</cell><cell cols="2">0.9590</cell><cell>33.05</cell><cell>0.9130</cell><cell>31.90</cell><cell>0.8960</cell><cell>30.77</cell><cell>0.9140</cell><cell>37.22</cell><cell>0.9750</cell></row><row><cell>LapSRN [29]</cell><cell>×2</cell><cell>37.52</cell><cell cols="2">0.9591</cell><cell>33.08</cell><cell>0.9130</cell><cell>31.08</cell><cell>0.8950</cell><cell>30.41</cell><cell>0.9101</cell><cell>37.27</cell><cell>0.9740</cell></row><row><cell>MemNet [9]</cell><cell>×2</cell><cell>37.78</cell><cell cols="2">0.9597</cell><cell>33.28</cell><cell>0.9142</cell><cell>32.08</cell><cell>0.8978</cell><cell>31.31</cell><cell>0.9195</cell><cell>37.72</cell><cell>0.9740</cell></row><row><cell>DPDNN [7]</cell><cell>×2</cell><cell>37.75</cell><cell cols="2">0.9600</cell><cell>33.30</cell><cell>0.9150</cell><cell>32.09</cell><cell>0.8990</cell><cell>31.50</cell><cell>0.9220</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>EDSR [27]</cell><cell>×2</cell><cell>38.11</cell><cell cols="2">0.9602</cell><cell>33.92</cell><cell>0.9195</cell><cell>32.32</cell><cell>0.9013</cell><cell>32.93</cell><cell>0.9351</cell><cell>39.10</cell><cell>0.9773</cell></row><row><cell>SRMDNF [20]</cell><cell>×2</cell><cell>37.79</cell><cell cols="2">0.9601</cell><cell>33.32</cell><cell>0.9159</cell><cell>32.05</cell><cell>0.8985</cell><cell>31.33</cell><cell>0.9204</cell><cell>38.07</cell><cell>0.9761</cell></row><row><cell>D-DBPN [38]</cell><cell>×2</cell><cell>38.09</cell><cell cols="2">0.9600</cell><cell>33.85</cell><cell>0.9190</cell><cell>32.27</cell><cell>0.9000</cell><cell>32.55</cell><cell>0.9324</cell><cell>38.89</cell><cell>0.9775</cell></row><row><cell>RDN (ours)</cell><cell>×2</cell><cell>38.30</cell><cell cols="2">0.9617</cell><cell>34.14</cell><cell>0.9235</cell><cell>32.41</cell><cell>0.9025</cell><cell>33.17</cell><cell>0.9377</cell><cell>39.60</cell><cell>0.9791</cell></row><row><cell>RDN+ (ours)</cell><cell>×2</cell><cell>38.34</cell><cell cols="2">0.9618</cell><cell>34.28</cell><cell>0.9241</cell><cell>32.46</cell><cell>0.9030</cell><cell>33.36</cell><cell>0.9388</cell><cell>39.74</cell><cell>0.9794</cell></row><row><cell>Bicubic</cell><cell>×3</cell><cell>30.39</cell><cell cols="2">0.8682</cell><cell>27.55</cell><cell>0.7742</cell><cell>27.21</cell><cell>0.7385</cell><cell>24.46</cell><cell>0.7349</cell><cell>26.95</cell><cell>0.8556</cell></row><row><cell>SRCNN [61]</cell><cell>×3</cell><cell>32.75</cell><cell cols="2">0.9090</cell><cell>29.30</cell><cell>0.8215</cell><cell>28.41</cell><cell>0.7863</cell><cell>26.24</cell><cell>0.7989</cell><cell>30.48</cell><cell>0.9117</cell></row><row><cell>FSRCNN [48]</cell><cell>×3</cell><cell>33.18</cell><cell cols="2">0.9140</cell><cell>29.37</cell><cell>0.8240</cell><cell>28.53</cell><cell>0.7910</cell><cell>26.43</cell><cell>0.8080</cell><cell>31.10</cell><cell>0.9210</cell></row><row><cell>VDSR [18]</cell><cell>×3</cell><cell>33.67</cell><cell cols="2">0.9210</cell><cell>29.78</cell><cell>0.8320</cell><cell>28.83</cell><cell>0.7990</cell><cell>27.14</cell><cell>0.8290</cell><cell>32.01</cell><cell>0.9340</cell></row><row><cell>LapSRN [29]</cell><cell>×3</cell><cell>33.82</cell><cell cols="2">0.9227</cell><cell>29.87</cell><cell>0.8320</cell><cell>28.82</cell><cell>0.7980</cell><cell>27.07</cell><cell>0.8280</cell><cell>32.21</cell><cell>0.9350</cell></row><row><cell>MemNet [9]</cell><cell>×3</cell><cell>34.09</cell><cell cols="2">0.9248</cell><cell>30.00</cell><cell>0.8350</cell><cell>28.96</cell><cell>0.8001</cell><cell>27.56</cell><cell>0.8376</cell><cell>32.51</cell><cell>0.9369</cell></row><row><cell>DPDNN [7]</cell><cell>×3</cell><cell>33.93</cell><cell cols="2">0.9240</cell><cell>30.02</cell><cell>0.8360</cell><cell>29.00</cell><cell>0.8010</cell><cell>27.61</cell><cell>0.8420</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>EDSR [27]</cell><cell>×3</cell><cell>34.65</cell><cell cols="2">0.9280</cell><cell>30.52</cell><cell>0.8462</cell><cell>29.25</cell><cell>0.8093</cell><cell>28.80</cell><cell>0.8653</cell><cell>34.17</cell><cell>0.9476</cell></row><row><cell>SRMDNF [20]</cell><cell>×3</cell><cell>34.12</cell><cell cols="2">0.9254</cell><cell>30.04</cell><cell>0.8382</cell><cell>28.97</cell><cell>0.8025</cell><cell>27.57</cell><cell>0.8398</cell><cell>33.00</cell><cell>0.9403</cell></row><row><cell>RDN (ours)</cell><cell>×3</cell><cell>34.78</cell><cell cols="2">0.9299</cell><cell>30.63</cell><cell>0.8477</cell><cell>29.33</cell><cell>0.8107</cell><cell>29.02</cell><cell>0.8695</cell><cell>34.58</cell><cell>0.9502</cell></row><row><cell>RDN+ (ours)</cell><cell>×3</cell><cell>34.84</cell><cell cols="2">0.9303</cell><cell>30.74</cell><cell>0.8495</cell><cell>29.38</cell><cell>0.8115</cell><cell>29.18</cell><cell>0.8718</cell><cell>34.81</cell><cell>0.9512</cell></row><row><cell>Bicubic</cell><cell>×4</cell><cell>28.42</cell><cell cols="2">0.8104</cell><cell>26.00</cell><cell>0.7027</cell><cell>25.96</cell><cell>0.6675</cell><cell>23.14</cell><cell>0.6577</cell><cell>24.89</cell><cell>0.7866</cell></row><row><cell>SRCNN [61]</cell><cell>×4</cell><cell>30.48</cell><cell cols="2">0.8628</cell><cell>27.50</cell><cell>0.7513</cell><cell>26.90</cell><cell>0.7101</cell><cell>24.52</cell><cell>0.7221</cell><cell>27.58</cell><cell>0.8555</cell></row><row><cell>FSRCNN [48]</cell><cell>×4</cell><cell>30.72</cell><cell cols="2">0.8660</cell><cell>27.61</cell><cell>0.7550</cell><cell>26.98</cell><cell>0.7150</cell><cell>24.62</cell><cell>0.7280</cell><cell>27.90</cell><cell>0.8610</cell></row><row><cell>VDSR [18]</cell><cell>×4</cell><cell>31.35</cell><cell cols="2">0.8830</cell><cell>28.02</cell><cell>0.7680</cell><cell>27.29</cell><cell>0.0726</cell><cell>25.18</cell><cell>0.7540</cell><cell>28.83</cell><cell>0.8870</cell></row><row><cell>LapSRN [29]</cell><cell>×4</cell><cell>31.54</cell><cell cols="2">0.8850</cell><cell>28.19</cell><cell>0.7720</cell><cell>27.32</cell><cell>0.7270</cell><cell>25.21</cell><cell>0.7560</cell><cell>29.09</cell><cell>0.8900</cell></row><row><cell>MemNet [9]</cell><cell>×4</cell><cell>31.74</cell><cell cols="2">0.8893</cell><cell>28.26</cell><cell>0.7723</cell><cell>27.40</cell><cell>0.7281</cell><cell>25.50</cell><cell>0.7630</cell><cell>29.42</cell><cell>0.8942</cell></row><row><cell>DPDNN [7]</cell><cell>×4</cell><cell>31.72</cell><cell cols="2">0.8890</cell><cell>28.28</cell><cell>0.7730</cell><cell>27.44</cell><cell>0.7290</cell><cell>25.53</cell><cell>0.7680</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>SRDenseNet [19]</cell><cell>×4</cell><cell>32.02</cell><cell cols="2">0.8930</cell><cell>28.50</cell><cell>0.7780</cell><cell>27.53</cell><cell>0.7337</cell><cell>26.05</cell><cell>0.7819</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>EDSR [27]</cell><cell>×4</cell><cell>32.46</cell><cell cols="2">0.8968</cell><cell>28.80</cell><cell>0.7876</cell><cell>27.71</cell><cell>0.7420</cell><cell>26.64</cell><cell>0.8033</cell><cell>31.02</cell><cell>0.9148</cell></row><row><cell>SRMDNF [20]</cell><cell>×4</cell><cell>31.96</cell><cell cols="2">0.8925</cell><cell>28.35</cell><cell>0.7787</cell><cell>27.49</cell><cell>0.7337</cell><cell>25.68</cell><cell>0.7731</cell><cell>30.09</cell><cell>0.9024</cell></row><row><cell>D-DBPN [38]</cell><cell>×4</cell><cell>32.47</cell><cell cols="2">0.8980</cell><cell>28.82</cell><cell>0.7860</cell><cell>27.72</cell><cell>0.7400</cell><cell>26.38</cell><cell>0.7946</cell><cell>30.91</cell><cell>0.9137</cell></row><row><cell>RDN (ours)</cell><cell>×4</cell><cell>32.61</cell><cell cols="2">0.8999</cell><cell>28.93</cell><cell>0.7894</cell><cell>27.80</cell><cell>0.7436</cell><cell>26.85</cell><cell>0.8089</cell><cell>31.45</cell><cell>0.9187</cell></row><row><cell>RDN+ (ours)</cell><cell>×4</cell><cell>32.69</cell><cell cols="2">0.9007</cell><cell>29.01</cell><cell>0.7909</cell><cell>27.85</cell><cell>0.7447</cell><cell>27.01</cell><cell>0.8120</cell><cell>31.74</cell><cell>0.9208</cell></row><row><cell>Bicubic</cell><cell>×8</cell><cell>24.40</cell><cell cols="2">0.6580</cell><cell>23.10</cell><cell>0.5660</cell><cell>23.67</cell><cell>0.5480</cell><cell>20.74</cell><cell>0.5160</cell><cell>21.47</cell><cell>0.6500</cell></row><row><cell>SRCNN [61]</cell><cell>×8</cell><cell>25.33</cell><cell cols="2">0.6900</cell><cell>23.76</cell><cell>0.5910</cell><cell>24.13</cell><cell>0.5660</cell><cell>21.29</cell><cell>0.5440</cell><cell>22.46</cell><cell>0.6950</cell></row><row><cell>FSRCNN [48]</cell><cell>×8</cell><cell>20.13</cell><cell cols="2">0.5520</cell><cell>19.75</cell><cell>0.4820</cell><cell>24.21</cell><cell>0.5680</cell><cell>21.32</cell><cell>0.5380</cell><cell>22.39</cell><cell>0.6730</cell></row><row><cell>SCN [62]</cell><cell>×8</cell><cell>25.59</cell><cell cols="2">0.7071</cell><cell>24.02</cell><cell>0.6028</cell><cell>24.30</cell><cell>0.5698</cell><cell>21.52</cell><cell>0.5571</cell><cell>22.68</cell><cell>0.6963</cell></row><row><cell>VDSR [18]</cell><cell>×8</cell><cell>25.93</cell><cell cols="2">0.7240</cell><cell>24.26</cell><cell>0.6140</cell><cell>24.49</cell><cell>0.5830</cell><cell>21.70</cell><cell>0.5710</cell><cell>23.16</cell><cell>0.7250</cell></row><row><cell>LapSRN [29]</cell><cell>×8</cell><cell>26.15</cell><cell cols="2">0.7380</cell><cell>24.35</cell><cell>0.6200</cell><cell>24.54</cell><cell>0.5860</cell><cell>21.81</cell><cell>0.5810</cell><cell>23.39</cell><cell>0.7350</cell></row><row><cell>MemNet [9]</cell><cell>×8</cell><cell>26.16</cell><cell cols="2">0.7414</cell><cell>24.38</cell><cell>0.6199</cell><cell>24.58</cell><cell>0.5842</cell><cell>21.89</cell><cell>0.5825</cell><cell>23.56</cell><cell>0.7387</cell></row><row><cell>MSLapSRN [63]</cell><cell>×8</cell><cell>26.34</cell><cell cols="2">0.7558</cell><cell>24.57</cell><cell>0.6273</cell><cell>24.65</cell><cell>0.5895</cell><cell>22.06</cell><cell>0.5963</cell><cell>23.90</cell><cell>0.7564</cell></row><row><cell>EDSR [27]</cell><cell>×8</cell><cell>26.96</cell><cell cols="2">0.7762</cell><cell>24.91</cell><cell>0.6420</cell><cell>24.81</cell><cell>0.5985</cell><cell>22.51</cell><cell>0.6221</cell><cell>24.69</cell><cell>0.7841</cell></row><row><cell>D-DBPN [38]</cell><cell>×8</cell><cell>27.21</cell><cell cols="2">0.7840</cell><cell>25.13</cell><cell>0.6480</cell><cell>24.88</cell><cell>0.6010</cell><cell>22.73</cell><cell>0.6312</cell><cell>25.14</cell><cell>0.7987</cell></row><row><cell>RDN (ours)</cell><cell>×8</cell><cell>27.23</cell><cell cols="2">0.7854</cell><cell>25.25</cell><cell>0.6505</cell><cell>24.91</cell><cell>0.6032</cell><cell>22.83</cell><cell>0.6374</cell><cell>25.14</cell><cell>0.7994</cell></row><row><cell>RDN+ (ours)</cell><cell>×8</cell><cell>27.40</cell><cell cols="2">0.7900</cell><cell>25.38</cell><cell>0.6541</cell><cell>25.01</cell><cell>0.6057</cell><cell>23.04</cell><cell>0.6439</cell><cell>25.48</cell><cell>0.8058</cell></row><row><cell cols="7">64 filters per Conv layer, we report results of RDN by using</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">D = 16, C = 8, and G = 64 for a fair comparison.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Benchmark results with BD and DN degradation models. Average PSNR/SSIM values for scaling factor ×3.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Bicubic</cell><cell>SRCNN [61]</cell><cell>FSRCNN [48]</cell><cell>VDSR [18]</cell><cell>IRCNN G [8]</cell><cell>IRCNN C [8]</cell><cell>RDN (ours)</cell><cell>RDN+ (ours)</cell></row><row><cell>Set5</cell><cell>BD DN</cell><cell cols="8">28.78/0.8308 32.05/0.8944 26.23/0.8124 33.25/0.9150 33.38/0.9182 33.17/0.9157 34.58/0.9280 34.70/0.9289 24.01/0.5369 25.01/0.6950 24.18/0.6932 25.20/0.7183 25.70/0.7379 27.48/0.7925 28.47/0.8151 28.55/0.8173</cell></row><row><cell>Set14</cell><cell>BD DN</cell><cell cols="8">26.38/0.7271 28.80/0.8074 24.44/0.7106 29.46/0.8244 29.63/0.8281 29.55/0.8271 30.53/0.8447 30.64/0.8463 22.87/0.4724 23.78/0.5898 23.02/0.5856 24.00/0.6112 24.45/0.6305 25.92/0.6932 26.60/0.7101 26.67/0.7117</cell></row><row><cell>B100</cell><cell>BD DN</cell><cell cols="8">26.33/0.6918 28.13/0.7736 24.86/0.6832 28.57/0.7893 28.65/0.7922 28.49/0.7886 29.23/0.8079 29.30/0.8093 22.92/0.4449 23.76/0.5538 23.41/0.5556 24.00/0.5749 24.28/0.5900 25.55/0.6481 25.93/0.6573 25.97/0.6587</cell></row><row><cell>Urban100</cell><cell>BD DN</cell><cell cols="8">23.52/0.6862 25.70/0.7770 22.04/0.6745 26.61/0.8136 26.77/0.8154 26.47/0.8081 28.46/0.8582 28.67/0.8612 21.63/0.4687 21.90/0.5737 21.15/0.5682 22.22/0.6096 22.90/0.6429 23.93/0.6950 24.92/0.7364 25.05/0.7399</cell></row><row><cell>Manga109</cell><cell>BD DN</cell><cell cols="8">25.46/0.8149 29.47/0.8924 23.04/0.7927 31.06/0.9234 31.15/0.9245 31.13/0.9236 33.97/0.9465 34.34/0.9483 23.01/0.5381 23.75/0.7148 22.39/0.7111 24.20/0.7525 24.88/0.7765 26.07/0.8253 28.00/0.8591 28.18/0.8621</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>. One can see that</cell></row><row><cell>on all the 4 test sets with 4 noise levels, our RDN+ achieves</cell></row><row><cell>1. http://r0k.us/graphics/kodak/</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Quantitative results about gray-scale image denoising. Best and second best results are highlighted and underlined</figDesc><table><row><cell>Method</cell><cell>10</cell><cell>30</cell><cell cols="2">Set12</cell><cell>50</cell><cell>70</cell><cell>10</cell><cell cols="2">Kodak24 30 50</cell><cell>70</cell><cell>10</cell><cell>30</cell><cell cols="2">BSD68</cell><cell>50</cell><cell>70</cell><cell>10</cell><cell>Urban100 30 50</cell><cell>70</cell></row><row><cell>BM3D [71]</cell><cell>34.38</cell><cell cols="2">29.13</cell><cell cols="2">26.72</cell><cell>25.22</cell><cell>34.39</cell><cell>29.13</cell><cell>26.99</cell><cell>25.73</cell><cell>33.31</cell><cell cols="2">27.76</cell><cell cols="2">25.62</cell><cell>24.44</cell><cell>34.47</cell><cell>28.75</cell><cell>25.94</cell><cell>24.27</cell></row><row><cell>TNRD [21]</cell><cell>34.27</cell><cell cols="2">28.63</cell><cell cols="2">26.81</cell><cell>24.12</cell><cell>34.41</cell><cell>28.87</cell><cell>27.20</cell><cell>24.95</cell><cell>33.41</cell><cell cols="2">27.66</cell><cell cols="2">25.97</cell><cell>23.83</cell><cell>33.78</cell><cell>27.49</cell><cell>25.59</cell><cell>22.67</cell></row><row><cell>RED [22]</cell><cell>34.89</cell><cell cols="2">29.70</cell><cell cols="2">27.33</cell><cell>25.80</cell><cell>35.02</cell><cell>29.77</cell><cell>27.66</cell><cell>26.39</cell><cell>33.99</cell><cell cols="2">28.50</cell><cell cols="2">26.37</cell><cell>25.10</cell><cell>34.91</cell><cell>29.18</cell><cell>26.51</cell><cell>24.82</cell></row><row><cell>DnCNN [23]</cell><cell>34.78</cell><cell cols="2">29.53</cell><cell cols="2">27.18</cell><cell>25.52</cell><cell>34.90</cell><cell>29.62</cell><cell>27.51</cell><cell>26.08</cell><cell>33.88</cell><cell cols="2">28.36</cell><cell cols="2">26.23</cell><cell>24.90</cell><cell>34.73</cell><cell>28.88</cell><cell>26.28</cell><cell>24.36</cell></row><row><cell>MemNet [9]</cell><cell>N/A</cell><cell cols="2">29.63</cell><cell cols="2">27.38</cell><cell>25.90</cell><cell>N/A</cell><cell>29.72</cell><cell>27.68</cell><cell>26.42</cell><cell>N/A</cell><cell cols="2">28.43</cell><cell cols="2">26.35</cell><cell>25.09</cell><cell>N/A</cell><cell>29.10</cell><cell>26.65</cell><cell>25.01</cell></row><row><cell>IRCNN [8]</cell><cell>34.72</cell><cell cols="2">29.45</cell><cell cols="2">27.14</cell><cell>N/A</cell><cell>34.76</cell><cell>29.53</cell><cell>27.45</cell><cell>N/A</cell><cell>33.74</cell><cell cols="2">28.26</cell><cell cols="2">26.15</cell><cell>N/A</cell><cell>34.60</cell><cell>28.85</cell><cell>26.24</cell><cell>N/A</cell></row><row><cell>MWCNN [44]</cell><cell>N/A</cell><cell cols="2">N/A</cell><cell cols="2">27.74</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">N/A</cell><cell cols="2">26.53</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>27.42</cell><cell>N/A</cell></row><row><cell>N 3 Net [45]</cell><cell>N/A</cell><cell cols="2">N/A</cell><cell cols="2">27.43</cell><cell>25.90</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">N/A</cell><cell cols="2">26.39</cell><cell>25.14</cell><cell>N/A</cell><cell>N/A</cell><cell>26.82</cell><cell>25.15</cell></row><row><cell>NLRN [46]</cell><cell>N/A</cell><cell cols="2">N/A</cell><cell cols="2">27.64</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">N/A</cell><cell cols="2">26.47</cell><cell>N/A</cell><cell>N/A</cell><cell>29.94</cell><cell>27.38</cell><cell>25.66</cell></row><row><cell>FFDNet [24]</cell><cell>34.65</cell><cell cols="2">29.61</cell><cell cols="2">27.32</cell><cell>25.81</cell><cell>34.81</cell><cell>29.70</cell><cell>27.63</cell><cell>26.34</cell><cell>33.76</cell><cell cols="2">28.39</cell><cell cols="2">26.30</cell><cell>25.04</cell><cell>34.45</cell><cell>29.03</cell><cell>26.52</cell><cell>24.86</cell></row><row><cell>RDN (ours)</cell><cell>35.06</cell><cell cols="2">29.94</cell><cell cols="2">27.60</cell><cell>26.05</cell><cell>35.17</cell><cell>30.00</cell><cell>27.85</cell><cell>26.54</cell><cell>34.00</cell><cell cols="2">28.56</cell><cell cols="2">26.41</cell><cell>25.10</cell><cell>35.41</cell><cell>30.01</cell><cell>27.40</cell><cell>25.64</cell></row><row><cell>RDN+ (ours)</cell><cell>35.08</cell><cell cols="2">29.97</cell><cell cols="2">27.64</cell><cell>26.09</cell><cell>35.19</cell><cell>30.02</cell><cell>27.88</cell><cell>26.57</cell><cell>34.01</cell><cell cols="2">28.58</cell><cell cols="2">26.43</cell><cell>25.12</cell><cell>35.45</cell><cell>30.08</cell><cell>27.47</cell><cell>25.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 37.33 31.98 29.70 28.24 36.49 30.70 28.34 26.88 36.75 31.78 29.38 27.74</head><label>7</label><figDesc>Quantitative results about color image denoising. Best and second best results are highlighted and underlined 30.89 28.63 27.27 35.91 29.73 27.38 26.00 36.00 30.36 27.94 26.31 TNRD [21] 34.33 28.83 27.17 24.94 33.36 27.64 25.96 23.83 33.60 27.40 25.52 22.63 RED [22] 34.91 29.71 27.62 26.36 33.89 28.46 26.35 25.09 34.59 29.02 26.40 24.74 DnCNN [23] 36.98 31.39 29.16 27.64 36.31 30.40 28.01 26.56 36.21 30.28 28.16 26.17 31.39 29.10 27.68 36.14 30.31 27.96 26.53 35.77 30.53 28.05 26.39 RDN (ours) 37.31 31.94 29.66 28.20 36.47 30.67 28.31 26.85 36.69 31.69 29.29 27.63 RDN+ (ours)</figDesc><table><row><cell>Method</cell><cell>10</cell><cell>Kodak24 30 50</cell><cell>70</cell><cell>10</cell><cell>BSD68 30 50</cell><cell>70</cell><cell>10</cell><cell>Urban100 30 50</cell><cell>70</cell></row><row><cell cols="2">CBM3D [72] 36.57 MemNet [9] N/A</cell><cell cols="2">29.67 27.65 26.40</cell><cell>N/A</cell><cell cols="2">28.39 26.33 25.08</cell><cell>N/A</cell><cell cols="2">28.93 26.53 24.93</cell></row><row><cell>IRCNN [8]</cell><cell cols="2">36.70 31.24 28.93</cell><cell>N/A</cell><cell cols="2">36.06 30.22 27.86</cell><cell>N/A</cell><cell cols="2">35.81 30.28 27.69</cell><cell>N/A</cell></row><row><cell cols="2">FFDNet [24] 36.81 BSD68: 163085</cell><cell>GT DnCNN [23]</cell><cell cols="2">Noisy (σ=50) MemNet [9]</cell><cell cols="2">CBM3D [72] IRCNN [8]</cell><cell cols="2">TNRD [21] FFDNet [24]</cell><cell>RED [22] RDN (ours)</cell></row><row><cell></cell><cell></cell><cell>GT</cell><cell cols="2">Noisy (σ=50)</cell><cell cols="2">CBM3D [72]</cell><cell cols="2">TNRD [21]</cell><cell>RED [22]</cell></row><row><cell>Urban100: img 039</cell><cell></cell><cell>DnCNN [23]</cell><cell cols="2">MemNet [9]</cell><cell cols="2">IRCNN [8]</cell><cell cols="2">FFDNet [24]</cell><cell>RDN (ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Quantitative results about image compression artifact reduction. Best and second best results are highlighted and underlined .7905 28.65 0.8093 28.98 0.8217 29.15 0.8111 29.19 0.8123 29.67 0.8247 29.70 0.8252 20 30.07 0.8683 30.81 0.8781 31.29 0.8871 31.46 0.8769 31.59 0.8802 32.07 0.8882 32.10 0.8886 30 31.41 0.9000 32.08 0.9078 32.69 0.9166 32.84 0.9059 32.98 0.9090 33.51 0.9153 33.54 0.9156 40 32.35 0.9173 32.99 0.9240 33.63 0.9306 N/A N/A 33.96 0.9247 34.51 0.9302 34.54 0.9304 Classic5 10 27.82 0.7800 28.88 0.8071 29.04 0.8111 29.28 0.7992 29.40 0.8026 30.00 0.8188 30.03 0.8194 20 30.12 0.8541 30.92 0.8663 31.16 0.8694 31.47 0.8576 31.63 0.8610 32.15 0.8699 32.19 0.8704 30 31.48 0.8844 32.14 0.8914 32.52 0.8967 32.78 0.8837 32.91 0.8861 33.43 0.8930 33.46 0.8932 40 32.43 0.9011 33.00 0.9055 33.34 0.9101 N/A N/A 33.77 0.9003 34.27 0.9061 34.29 0.9063</figDesc><table><row><cell>Dataset</cell><cell>Quality</cell><cell>JPEG PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM SA-DCT [66] ARCNN [25] TNRD [21] DnCNN [23] RDN (ours) RDN+ (ours)</cell></row><row><cell></cell><cell>10</cell><cell>27.77 0</cell></row><row><cell>LIVE1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 9</head><label>9</label><figDesc>PSNR (dB)/SSIM results about image deblurring.</figDesc><table><row><cell>Set</cell><cell cols="2">McMaster18 PSNR SSIM</cell><cell cols="2">Kodak24 PSNR SSIM</cell><cell cols="2">Urban100 PSNR SSIM</cell></row><row><cell>Blurry</cell><cell>27.00</cell><cell>0.7817</cell><cell>26.09</cell><cell>0.7142</cell><cell>22.38</cell><cell>0.6732</cell></row><row><cell>IRCNN [8]</cell><cell>32.50</cell><cell>0.8961</cell><cell>30.40</cell><cell>0.8513</cell><cell>27.70</cell><cell>0.8577</cell></row><row><cell>RDN (ours)</cell><cell>33.60</cell><cell>0.9157</cell><cell>30.88</cell><cell>0.8718</cell><cell>29.34</cell><cell>0.8886</cell></row><row><cell>level σ [7],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Fig. 15. Visual results on image deblurring.</figDesc><table><row><cell>Urban100: img 012</cell><cell>HQ</cell><cell>Blurry</cell><cell></cell><cell>IRCNN [8]</cell><cell>RDN (ours)</cell></row><row><cell></cell><cell>GT</cell><cell>Bicubic</cell><cell>SRCNN [61]</cell><cell>SCN [62]</cell><cell>VDSR [18]</cell></row><row><cell>Urban100: img 095 (×8)</cell><cell>MemNet [9]</cell><cell>MSLapSRN [63]</cell><cell>EDSR [27]</cell><cell>D-DBPN [38]</cell><cell>RDN (ours)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported in part by the NSF IIS award 1651902, ONR Young Investigator Award N00014-14-1-0484, and U.S. Army Research Office Award W911NF-17-1-0367.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cardiac image super-resolution with global correspondence using multiatlas patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M S M</forename><surname>De Marvao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image restoration via simultaneous sparse coding: Where structured sparsity meets gaussian scale mixture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="217" to="232" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denoising prior driven deep neural network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2305" to="2318" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Asian Conf. Comput. Vis</title>
		<meeting>IEEE Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A statistical prediction model based on sparse representations for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ffdnet: Toward a fast and flexible solution for cnn based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04026</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshop</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Density-aware single image de-raining using a multi-stream dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10171</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshop</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on image dehazing: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshop</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">2018 pirm challenge on perceptual image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. Workshop</title>
		<meeting>Eur. Conf. Comput. Vis. Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Crafting a toolchain for image restoration by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2443" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. Workshop</title>
		<meeting>Eur. Conf. Comput. Vis. Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. Workshop</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Curves Surf</title>
		<meeting>7th Int. Conf. Curves Surf</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<title level="m">Live image quality assessment database release</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive dct for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012-10" />
			<biblScope unit="page" from="112" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Color image denoising via sparse 3d collaborative filtering with grouping constraint in luminance-chrominance space</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Patch complexity, finite pixel correlations and optimal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">An efficient statistical method for image noise level estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The noise clinic: a blind image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
