<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijian</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueliang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
						</author>
						<title level="a" type="main">Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-based Dialogues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building an intelligent dialogue system with the ability to select a proper response according to a multi-turn context is a great challenging task. Existing studies focus on building a context-response matching model with various neural architectures or PLMs and typically learning with a single response prediction task. These approaches overlook many potential training signals contained in dialogue data, which might be beneficial for context understanding and produce better features for response prediction. Besides, the response retrieved from existing dialogue systems supervised by the conventional way still faces some critical challenges, including incoherence and inconsistency. To address these issues, in this paper, we propose learning a context-response matching model with auxiliary self-supervised tasks designed for the dialogue data based on pre-trained language models. Specifically, we introduce four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination, and jointly train the PLM-based response selection model with these auxiliary tasks in a multi-task manner. By this means, the auxiliary tasks can guide the learning of the matching model to achieve a better local optimum and select a more proper response. Experiment results on two benchmarks indicate that the proposed auxiliary self-supervised tasks bring significant improvement for multi-turn response selection in retrieval-based dialogues, and our model achieves new state-of-the-art results on both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Building a dialogue system that can converse with people naturally and meaningfully is one of the most challenging problems towards high-level artificial intelligence, and has been drawing increasing interests from both academia and industry area. Most existing dialogue systems are either generation-based <ref type="bibr" target="#b16">(Vinyals &amp; Le, 2015;</ref><ref type="bibr" target="#b11">Serban et al., 2016)</ref> or retrieval-based <ref type="bibr" target="#b17">(Wang et al., 2013;</ref><ref type="bibr" target="#b9">Lowe et al., 2015;</ref><ref type="bibr" target="#b21">Wu et al., 2017;</ref><ref type="bibr" target="#b14">Tao et al., 2019b)</ref>. Given the dialogue context, generation-based approaches synthesize a response word by word with a conditional language model, while retrievalbased methods select a proper response from a candidate pool. In this paper, we focus on retrieval-based approaches that are superior in providing informative responses and have been widely applied in several famous commercial products such as XiaoIce <ref type="bibr" target="#b12">(Shum et al., 2018)</ref> from Microsoft and AliMe Assist  from Alibaba.</p><p>We consider the response selection task in multi-turn dialogues, where the retrieval model ought to select a most proper response by measuring the matching degree between a multi-turn dialogue context and a number of response candidates. Earlier studies <ref type="bibr" target="#b17">(Wang et al., 2013;</ref><ref type="bibr" target="#b5">Hu et al., 2014;</ref><ref type="bibr" target="#b9">Lowe et al., 2015)</ref> concatenate the context to a single utterance and calculate the matching score with the utterance-level representations. Later, most response selection models <ref type="bibr" target="#b29">(Zhou et al., 2016;</ref><ref type="bibr" target="#b21">Wu et al., 2017;</ref><ref type="bibr" target="#b27">Zhang et al., 2018)</ref> perform context-response matching within the representation-matching-aggregation paradigm, where each turn of utterance is represented individually and sequential information is aggregated among a sequence of utteranceresponse matching features. To further improve the performance of response selection, some recent approaches consider multiple granularities (or layers) of representations <ref type="bibr" target="#b30">(Zhou et al., 2018;</ref><ref type="bibr" target="#b14">Tao et al., 2019b;</ref><ref type="bibr" target="#b19">Wang et al., 2019b)</ref> for matching or propose more complicated interaction mechanisms between the context and the response <ref type="bibr" target="#b13">(Tao et al., 2019a)</ref>.</p><p>Recently, a wide range of studies have shown that pretrained language models (PLMs), such as BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, XLNET <ref type="bibr" target="#b23">(Yang et al., 2019)</ref> and RoBERTa , on the large corpus can learn universal language representations, which are helpful for various downstream natural language processing tasks and can get rid of training a new model from scratch. To adapt pre-trained models for multi-turn response selection, <ref type="bibr" target="#b20">Whang et al. (2020)</ref> and <ref type="bibr" target="#b3">Gu et al. (2020)</ref> make the first attempt to utilize BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> to learn a matching model, where context and the candidate response are first concate-nated and then fed into the PLMs for calculating the final matching score. These pre-trained language models can well capture the interaction information among inter-utterance and intra-utterance through multiple transformer layers. Although PLM-based response selection models demonstrate superior performance due to its strong representation ability, it is still challenging to effectively learn task-related knowledge during the training process, especially when the size of training corpora is limited. Naturally, these studies typically learn the response selection model with only the context-response matching task and overlook many potential training signals contained in dialogue data. Such training signals might be beneficial for context understanding and produce better features for response prediction. Besides, the response retrieved by existing dialogue systems supervised by the conventional way still faces some critical challenges, including incoherence and inconsistency.</p><p>On account of the above issues, in this paper, instead of configuring complex context-response matching models, we propose learning the context-response matching model with auxiliary self-supervised tasks designed for dialogue data based on pre-trained language models (e.g., BERT). Specifically, we introduce four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination, and jointly train the PLM-based response selection model with these auxiliary tasks in a multi-task manner. On the one hand, these auxiliary tasks help improve the capability of the response selection model to understand the dialogue context and measure the semantic relevance, consistency or coherent between the context and the response candidates. On the other hand, they can guide the matching model to effectively learn taskrelated knowledge with a fixed amount of train corpora and produce better features for response prediction.</p><p>We conduct experiments on two benchmark data sets for multi-turn response selection: the Ubuntu Dialog Corpus <ref type="bibr" target="#b9">(Lowe et al., 2015)</ref> and the E-commerce Dialogue Corpus <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref>. Evaluation results show that our proposed approach is significantly better than all state-of-the-art models on both datasets. Compared with the previous state-of-the-art methods, our model achieves 2.9% absolute improvement in terms of R 10 @1 for the Ubuntu dataset and 4.8% absolute improvement for the E-commerce dataset. Furthermore, we applied our proposed self-supervised learning schema to some non-PLMbased response selection models, e.g., dual LSTM <ref type="bibr" target="#b9">(Lowe et al., 2015)</ref> and ESIM <ref type="bibr" target="#b0">(Chen &amp; Wang, 2019)</ref>. Experimental results indicate that our learning schema can also bring consistent and significant improvement to the performance of the existing matching models. Surprisingly, with selfsupervised learning, a simple ESIM even performs better than BERT on the ubuntu dataset, demonstrating that our approach is beneficial for various matching architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>[CLS]  In summary, our contributions are three-fold:</p><formula xml:id="formula_0">! [EOT] " [EOT] # [EOT] [SEP] ! [SEP] !,# !,$! ··· Context Response Pre-trained Language Model (BERT) E !"# $! E %&amp;' $" E %&amp;' $# E %&amp;' E #%( E) ! E #%( ,</formula><p>• We propose learning a context-response matching model with multiple auxiliary self-supervised tasks to fully utilize various training signals in the multi-turn dialogue context.</p><p>• We design four self-supervised tasks, aiming at enhancing the capability of a PLM-based response prediction model in capturing the semantic relevance, coherence or consistency.</p><p>• We achieve new state-of-the-art results on two benchmark datasets. Besides, with the help of auxiliary self-supervised tasks, a simple ESIM model can even achieve better performance than BERT on the Ubuntu dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Task Formalization</head><p>Suppose that there is a multi-turn dialogue dataset D =</p><formula xml:id="formula_1">{c i , r i , y i } N i=1 , where c i = {u i,1 , u i,2 , .</formula><p>. . , u i,mi } denotes a dialogue context with u i,t representing the utterance of the t-th turn, r i denotes a response candidate, and y i ∈ {0, 1} denotes a label with y i = 1 indicating that r i is a proper response for c i (otherwise, y i = 0). The task is to learn a matching model g(·, ·) from D so that for any new context c = {u 1 , u 2 , . . . , u m } and a response candidate r, g(c, r) ∈ [0, 1] can measure the matching degree between c and r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Matching with PLMs</head><p>We consider building the context-response matching model with the pre-trained language models, as it is trained on large amounts of unlabelled data and provides strong universal representations" that can be finetuned on task-specific training data to achieve good performance on downstream tasks. Following previous studies <ref type="bibr" target="#b3">(Gu et al., 2020;</ref><ref type="bibr" target="#b20">Whang et al., 2020)</ref>, we select BERT as the base model for a fair comparison. 2 Specifically, given a context c = {u 1 , u 2 , . . . , u m }, where the t-th utterance u t = {w t,1 , . . . , w t,lt } is a sequence with l t words, a response candidate r = {r 1 , r 2 , . . . , r lr } consisting of l r words and a label y ∈ {0, 1}, we first concatenate all utterances in the context and the response candidate as a single consecutive token sequence with special tokens separating them, which can be formu-</p><formula xml:id="formula_2">lated as x = {[CLS], u 1 , [EOT], u 2 , [EOT], . . . , [EOT], u m , [EOT], [SEP], r, [SEP]}.</formula><p>Here [CLS] and [SEP] are the classification symbol and the segment separation symbol of BERT, [EOT] is the "End Of Turn" tag designed for multiturn context. For each word of x, token, position and segment embeddings of x are summated and fed into pre-trained transformer layer (a.k.a. BERT), giving us the contextualized embedding sequence {E <ref type="bibr">[CLS]</ref> , E 2 , . . . , E lx }. E <ref type="bibr">[CLS]</ref> is an aggregated representation vector that contains the semantic interaction information for the context-response pair. We then fed E [CLS] into a multi-perception layer to obtain the final matching score for the context-response pair:</p><formula xml:id="formula_3">g(c, r) = σ(W 2 · f (W 1 E [CLS] + b 1 ) + b 2 )<label>(1)</label></formula><p>where W {1,2} and b {1,2} are trainable parameters for response prediction task, f (·) is a tanh activation function, σ(·) stands a sigmoid function.</p><p>Finally, cross-entropy loss function is utilized as the training objective of the context-response matching task:</p><formula xml:id="formula_4">L crm = −y log(g(c, r)) − (1 − y) log(1 − g(c, r))<label>(2)</label></formula><p>Before the fine-tuning procedure with the above contextresponse matching task, for a fair comparison, we follow previous studies <ref type="bibr" target="#b20">(Whang et al., 2020;</ref><ref type="bibr" target="#b3">Gu et al., 2020;</ref><ref type="bibr" target="#b4">Gururangan et al., 2020)</ref> and carry out domain-adaptive posttraining to incorporate in-domain knowledge into BERT. In the rest of this section, we will introduce our proposed four auxiliary self-supervised tasks, and then present the final learning objective of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-Supervised Tasks</head><p>Heading for a matching model that can effectively learn domain knowledge with a fixed amount of training corpora and produce better features for response prediction, we design four auxiliary self-supervised tasks, i.e. session-level matching, utterance restoration, incoherence detection and consistency classification. These self-supervised tasks try to enhance the capability of the model to measure the semantic relevance, coherent, and consistency between the context and the response candidate. On the other hand, they can also guide the learning of the model to achieve a better local optimum. <ref type="figure">Figure 2</ref> illustrates the sketches of four types of self-supervised tasks.</p><p>[CLS]</p><formula xml:id="formula_5">! ··· +,#- ··· BERT E !"# $! ··· $" ··· [CLS] [EOT] [SEP] BERT E !"# . E %&amp;' / %%&amp; , MLP E #%( # [SEP] $# E #%( w0,2 w0,3 " ··· [CLS] ! ··· ' ' '(! BERT E !"# $! ··· $" $"&amp;! ' ) [SEP] $# E #%( ··· ··· *+, -./" , 0123" MLP [CLS] ! ··· ( ' ··· BERT E !"# $! ··· $" ··· # [SEP] $# E #%( | ' = 1 ! , ⋯ , ) ··· ··· Incoherent utterance [EOT] E %&amp;' / w ',5" #$% Masked utterance &amp;'()% (a) Next session prediction (b) Utterance restoration (c) Incoherence detection (d) Consistency discrimination Figure 2.</formula><p>Sketches of four types of self-supervised tasks. Gray square stands for various embeddings for each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">NEXT SESSION PREDICTION</head><p>Due to the natural sequential relationship between dialogue turns, the latter turns usually show a strong semantic relevance with the previous turns in the context. Inspired by such a characteristic, we design a more general response prediction task with the dialogue context, name next session prediction (NSP), to fully utilize the sequential relationship of the dialogue data and enhance the capability of the model to measure the semantic relevance. Specifically, the next session prediction task requires the model to predict whether two sequences are consecutive and relevant. However, instead of matching a context with a response utterance, the model needs to calculate the matching degree between two pieces of dialogue session.</p><p>Formally, given a context c = {u 1 , u 2 , . . . , u m }, we randomly 1 split c into two consecutive pieces c left = {u 1 , . . . , u t } and c right = {u t+1 , . . . , u m }. Then, with a 50% chance, we replace c left or c right with a piece of context sampled from the whole training corpus 2 . If one of the two piece is replaced, we set the label y nsp = 0, otherwise y nsp = 1. The next session prediction task requires the model to discriminate whether c left and c right can form a consecutive context.</p><p>To train PLMs with the proposed self-supervised task, we first concatenate all utterances of each piece as a single sequence with [EOT] appended to the end of each utterance. Similar to the main task, we fed two segments into BERT encoder and obtain the aggregated representation of the piece pair E nsp <ref type="bibr">[CLS]</ref> . We further compute the final matching score g nsp (c left , c right ) with a non-linear transformation. Finally, the objective function of context alignment task can be formulated as</p><formula xml:id="formula_6">L nsp = −y nsp log(g nsp (c left , c right )) − (1 − y nsp ) log(1 − g nsp (c left , c right )) (3) 2.3.2. UTTERANCE RESTORATION</formula><p>As one of the common self-supervised tasks in PLMs, tokenlevel masked language modeling is usually utilized to guide the model to learn semantic and syntactic features of word sequences with the bidirectional context. Here we further introduce utterance-level masked language modeling, i.e. utterance restoration (UR) task to encourage the model to be aware of the semantic connections among utterances in the context. Specifically, we mask all the tokens in an utterance randomly sampled from the dialogue session and let the model restore it with the information from the rest context. By learning to predict a proper utterance that fits its surrounding dialogue context, the model can produce better representations that can well adapt to dialogues, similar to the idea of continuous bag-of-words model <ref type="bibr" target="#b10">(Mikolov et al., 2013)</ref>.</p><p>Formally, given a context c = {u 1 , u 2 , . . . , u m }, we randomly select an utterance u t and replace all tokens in the utterance with a special token <ref type="bibr">[MASK]</ref>. The model is required to restore u t based on c = {u 1 , . . . , u t−1 , u mask , u t+1 , . . . , u m }. </p><formula xml:id="formula_7">E t,j = GLEU(W ur E t,j + b ur ) p(w t,j |ĉ) = softmax W ur E t,j + b ur<label>(4)</label></formula><p>where W ur , W ur , b ur , b ur are trainable parameters, w t,j is the j-th token of the t-th utterance, and GLEU(·) is an activation function. Then, the training objective of utterance restoration task is to minimize the following negative loglikelihood (NLL):</p><formula xml:id="formula_8">L ur = − 1 l t lt j=1 log p(w t,j |ĉ)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">INCOHERENCE DETECTION</head><p>Inspired by the concept of discourse coherence <ref type="bibr" target="#b6">(Jurafsky, 2000)</ref> in linguistics, we further introduce the incoherence detection (ID) task which requires the model to recognize the incoherent utterance within a dialogue session, so as to enhance the capability of a model on capturing the sequential relationship among utterances and selecting coherent response candidates. Specifically, given a dialogue context c = {u 1 , . . . , u m }, we randomly select one of the utterances u k ∈ {u 1 , . . . , u m } and replace it with an utterance randomly sampled from the whole training corpus. Then, the model should find the incoherent utterance among the context. For each sample, we define a one-hot label {z 1 , . . . , z m } , where z t = 1 if t = k, indicating that the t-th utterance is been replaced, otherwise z t = 0.</p><p>To model this task, BERT encoder takes an input</p><formula xml:id="formula_9">x id = {[CLS], u 1 , [EOT], . . . , u m , [EOT], [SEP]} and out- puts E id = {E [EOT] , E 1,1 , . . . , E m,lm , E [SEP] },</formula><p>where E t,j denotes the contextualized embedding of the j-th word in the k-th utterance and l t is the length of t-th utterance. We calculate the aggregated representation of the k-th utterance by fusing the mean and max value of the embedding sequence {E t,1 , . . . , E t,lt }, which can be formulated as</p><formula xml:id="formula_10">U t = 1 l t lt j=1 E t,j ; max 1≤j≤lt E t,j<label>(6)</label></formula><p>Then, the model makes a prediction based on the aggregated representations of each utterance, the probability of the t-th utterance being replaced is</p><formula xml:id="formula_11">p(z t = 1|u 1 , . . . , u m ) = softmax(W id U t + b id ) = exp(W id U t + b id ) m s=1 exp(W id U s ) + b id (7) where W id and b id are trainable parameters.</formula><p>Finally, the learning objective of inconsistency detection task is defined as</p><formula xml:id="formula_12">L id = − m t=1 z t log p(z t = 1|u 1 , . . . , u m )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4.">CONSISTENCY DISCRIMINATION</head><p>Selecting responses that are consistent with the dialogue context is one of the major challenges in building engaging conversational agents. However, most previous studies focused on modeling the semantic relevance between the context and the response candidate. Intuitively, utterances from the same dialogue session tend to share similar topics, and utterances from the same interlocutor tend to share the same personality or style. According to the characteristics, we attempt to enhance the capability of a response 4 prediction model to measure the consistency with a selfsupervised discriminative training scheme that utilizes the natural structure of dialogue data.</p><p>Formally, given a dialog context c = {u 1 , u 2 , . . . , u m }, we sample two utterances from the same interlocutor 3 , and denote them as u and v respectively. Then, we randomly sample an utteranceṽ from another context in the training corpus. The model is required to measure the consistency degree of u, v and u,ṽ and give a higher score to u, v . Since u and v are not consecutive in the dialogue context and from the same interlocutor, the model is encouraged to capture the features about the consistency (such as topic, personality and style) between two sequences, rather than semantic relevance or coherence.</p><p>To calculate the consistency score of a sequence pair u, v , we first concatenate the two utterances as</p><formula xml:id="formula_13">x cd = {[CLS], u, [SEP]</formula><p>, v, [SEP]}, and then fed the sequence into BERT. As described in previous tasks, BERT returns an aggregated representation E cd <ref type="bibr">[CLS]</ref> . Then, the consistency score g cd (u, v) is computed with a non-linear transformation over E cd <ref type="bibr">[CLS]</ref> . Likewise, we can obtain the consistency score of u,ṽ , i.e. g cd (u,ṽ). Finally, we would like g cd (u, v) to be larger than g cd (u,ṽ) by at least a margin ∆ and define the learning objective as a hing loss function:</p><formula xml:id="formula_14">L cd = max{0, ∆ − g cd (u, v) + g cd (u,ṽ)}<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Learning Objective</head><p>We adopt a multi-task learning manner and define the final objective function as:</p><formula xml:id="formula_15">L final = L crm + αL self L self = L slm + L ur + L id + L cd<label>(10)</label></formula><p>where α is a hyper-parameter as a trade-off between the objective of the main task and those of the auxiliary tasks.</p><p>In this way, all tasks are joint learned so that the model can effectively leverage the training corpus and learn both characteristics of dialogue text and implicit knowledge contained in the dialogue data. The auxiliary tasks can be regarded as regularization in model estimation for enhancing the model's generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Evaluation Metrics</head><p>we evaluate the proposed method on two benchmark datasets for multi-turn dialogue response selection. The first dataset is the Ubuntu Dialogue Corpus (v1.0) <ref type="bibr" target="#b9">(Lowe et al., 2015)</ref>, which consists of multi-turn English dialogues about technical support and is collected from chat logs of the Ubuntu forum. We use the copy shared by <ref type="bibr" target="#b3">Gu et al. (2020)</ref>, in which numbers, paths and URLs are replaced by placeholders. The Ubuntu dataset contains 1 million context-response pairs for training, and 0.5 million pairs for validation and test.</p><p>The ratio of positive candidates and negative candidates is 1 : 1 in the training set, and 1 : 9 in the validation set and the test set. The second dataset is the E-commerce Dialogue Corpus <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref>, which consists of real-world multi-turn dialogues between customers and customer service staff on Taobao 4 , the largest e-commerce platform in China. The E-commerce dataset contains 1 million contextresponse pairs for training, and 10 thousand pairs for validation and test. The ratio of positive candidates and negative candidates is 1 : 1 in the training set and the validation set, and 1 : 9 in the test set.</p><p>Following <ref type="bibr" target="#b9">Lowe et al. (2015)</ref> and <ref type="bibr" target="#b27">Zhang et al. (2018)</ref>, we employ R n @ks as evaluation metrics, where R n @k denotes recall at position k in n candidates and measures the probability of the positive response being ranked in top k positions among n candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baseline Models</head><p>We compared BERT-SL with the following models:</p><p>DualLSTM <ref type="bibr" target="#b9">(Lowe et al., 2015)</ref>: the model concatenates all utterances in the context to form a single sequence and calculates a matching score based on the representations produced by an LSTM.</p><p>Multi-View <ref type="bibr" target="#b29">(Zhou et al., 2016)</ref>: the model measures the matching degree between the context and the response candidate in both a word view and an utterance view.</p><p>SMN <ref type="bibr" target="#b21">(Wu et al., 2017)</ref>: the model lets each utterance in the context interacts with the response candidate, and the matching vectors of all utterance-response pairs are aggregated with an RNN to calculate a final matching score.</p><p>DUA <ref type="bibr" target="#b27">(Zhang et al., 2018)</ref>: the model formulates previous utterances into context using a deep utterance aggregation model, and performs context-response similar to SMN.</p><p>DAM <ref type="bibr" target="#b30">(Zhou et al., 2018)</ref>: the model is similar to SMN, but utterances in the context and the response candidate are represented with stacked self-attention and cross-attention layers. The matching vectors are aggregated with a 3-D CNN.</p><p>MRFN <ref type="bibr" target="#b14">(Tao et al., 2019b)</ref>: the model employs multiple types of representations for context-response interaction, where each type encodes semantics of units from a kind of granularity or dependency among the units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>Ubuntu Corpus E-commerce Corpus R 2 @1 R 10 @1 R 10 @2 R 10 @5 R 10 @1 R 10 @2 R 10 @5 DualLSTM <ref type="bibr" target="#b9">(Lowe et al., 2015)</ref> 0.901 0.638 0.784 0.949 0.365 0.536 0.828 Multi-View <ref type="bibr" target="#b29">(Zhou et al., 2016)</ref> 0.908 0.662 0.801 0.951 0.421 0.601 0.861 SMN <ref type="bibr" target="#b21">(Wu et al., 2017)</ref> 0 ESIM <ref type="bibr" target="#b0">(Chen &amp; Wang, 2019)</ref>: the model first concatenates all utterances in the context into a single sequence, and then employs ESIM structure derived from NLI for contextresponse matching.</p><p>IMN <ref type="bibr" target="#b2">(Gu et al., 2019)</ref>: following <ref type="bibr" target="#b21">Wu et al. (2017)</ref>, the model enhances the representations at both the wordand sentence-level and collects matching information of utterance-response pairs bidirectionally.</p><p>IoI <ref type="bibr" target="#b13">(Tao et al., 2019a)</ref>: the model lets the context-response matching process goes deep along the interaction block chain via representations in an iterative fashion.</p><p>MSN <ref type="bibr" target="#b25">(Yuan et al., 2019)</ref>: the model utilizes a multi-hop selector to select the relevant utterances in context and then matches the filtered context with the response candidate to obtain a matching score.</p><p>BERT <ref type="bibr" target="#b20">(Whang et al., 2020)</ref>: the model fine-tunes the BERT with the concatenation of the context and the response candidates as the input.</p><p>BERT-VFT <ref type="bibr" target="#b20">(Whang et al., 2020)</ref>: before fine-tuning, the model also carries out a post-training on training corpora in the same manner as BERT.</p><p>SA-BERT <ref type="bibr" target="#b3">(Gu et al., 2020)</ref>: the model follows BERT-VFT, and further incorporates speaker-aware embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Following <ref type="bibr" target="#b3">Gu et al. (2020)</ref>, we select English uncased BERT base (110M) as the context-response matching model for the Ubuntu dataset and Chinese BERT base model for the E-commerce dataset. We implement the models with the code in https://github.com/huggingface/ transformers. The maximum lengths of the context and response were set to 448 and 64 as the maximum length of input sequence in BERT is 512. Intuitively, the last tokens in the context and the previous tokens in the response candidate are more important, so we cut off the previous tokens for the context but do the cut-off in the reverse direction for the response candidate if the sequences are longer than the maximum length. We choose 32 as the size of minibatches for training. On both the Ubuntu dataset and the Douban dataset, we applied domain adaptive post-training before the finetuning procedure following the settings of <ref type="bibr" target="#b20">Whang et al. (2020)</ref>. Training instances of auxiliary tasks are generated dynamically. We select ∆ (Equation <ref type="formula" target="#formula_14">(9)</ref>) in {0.2, 0.4, 0.6, 0.8} and find that 0.6 is the best choice. We vary α (Equation <ref type="formula" target="#formula_3">(10)</ref>) in {0.1, 0.2, 0.5, 1.0} and choose α = 1.0 as the trade-off between the learning objectives. The model is optimized using Adam optimizer with a learning rate set as 3e − 5. Early stopping on the validation data is adopted as a regularization strategy. All the experiment results except ours are cited from previous works. <ref type="table" target="#tab_3">Table 1</ref> reports the results of BERT-SL and all baseline models on the Ubuntu datasets and the E-commerce dataset. From the evaluation results, we can easily observe that the PLM-based response selection models generally perform better than the models based on various neural architectures. The phenomenon shows the advantage of the pre-trained models on providing strong universal representations for 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>Ubuntu Corpus E-Commerce Corpus R 2 @1 R 10 @1 R 10 @2 R 10 @5 R 10 @1 R 10 @2 R 10 @5 DualLSTM <ref type="bibr" target="#b9">(Lowe et al., 2015)</ref>   <ref type="table">Table 2</ref>. Evaluation results of two matching models trained with the proposed self-supervised tasks. Numbers marked with * mean that the improvement is statistically significant compared with the baseline (t-test with p-value &lt; 0.05).</p><p>response selection. Among those PLM-based response selection models, our BERT-SL outperforms the best baseline BERT-VFT in terms of all metrics on both data sets. Specifically, compared to the previous state-of-the-art model, our BERT-SL achieves 2.9% absolute improvement in terms of R 10 @1 on the Ubuntu dataset and 4.8% absolute improvement on the E-commerce dataset. We conduct statistical tests, and the results indicate that the improvement on all metrics except R 10 @5 on the E-commerce data is statistically significant. The significant improvement demonstrates the effectiveness of our proposed self-supervised learning schema. Notably, our method does not increase the inference time compared with existing PLM-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussions</head><p>Ablation study. To investigate the impact of different selfsupervised tasks, we conducted a comprehensive ablation study. We keep the architecture of the matching model and remove each self-supervised task individually from the model, and denote the model as "BERT-SL w/o. T ", where T ∈ {NSP, UR, ID, CD} stand for next session prediction, utterance restoration, incoherence detection and consistency discrimination respectively. The detailed results are reported in the last four lines of <ref type="table" target="#tab_3">Table 1</ref>. First of all, we find that all four self-supervised tasks are useful as removing any of them causes a performance drop on both datasets. Second, we can conclude that on the Ubuntu data, the rank of the tasks in terms of R 10 @1 is that ID &gt; NSP &gt; CD &gt; UR; and on the E-commerce data, the rank of the tasks is that CD &gt; ID &gt; NSP &gt; UR 5 . Among the four tasks, ID plays an important role in improving the response selection task. The reason might be that the ID task can encourage the model to consider the coherence between the context and a response candidate, which acts as complementary to the main task. It is also noted that removing the utterance restoration task leads to the slightest decrease of the performance on both datasets, as the feature learned by UR may be redundant with that learned by the token-level mask language modeling in pre-training. Besides, the representation learned by the generative task might have a considerable <ref type="bibr">5</ref> We select R10@1 as target metrics in the study of the importance of different tasks because they are more critical than other metrics in real systems of response selection. discrepancy with the discrimination task. Finally, the CD task is much more important on the E-commerce data than it is on the Ubuntu data, as E-commerce corpora contain more diverse content.</p><p>Self-supervised learning for ESIM/DualLSTM. We are curious about whether the effectiveness of the proposed selfsupervised learning schema depends on the architecture of the response selection model. Therefore, we test our proposed learning schema on some non-PLM-based response selection models, such as dual LSTM <ref type="bibr" target="#b9">(Lowe et al., 2015)</ref> and ESIM <ref type="bibr" target="#b0">(Chen &amp; Wang, 2019)</ref>. The original two models treat the multi-turn context as a long sequence and are trained with only the context-response task. Thus, we implement two models and jointly train them with the proposed four self-supervised tasks in a multi-task manner. <ref type="table">Table 2</ref> reports the comparison results on both data sets. We observe a consistent and significant improvement of the performance for both DualLSTM and ESIM. Particularly, with the help of auxiliary self-supervised tasks, a simple ESIM model can even achieve better performance on the Ubuntu dataset than BERT, which is a bigger pre-trained model. The results imply that our learning schema is beneficial for various matching architectures, and indicate the effectiveness and generality of the proposed method.</p><p>Performance across different lengths of context. To analyze how the performance of our proposed BERT-SL varies with different context lengths, we compare BERT-SL with BERT, BERT-VFT and the state-of-the-art non-PLM-based response selection models (a.k.a. MSN). In this work, context length is measured by (1) number of turns and (2) number of all tokens in a context. <ref type="figure" target="#fig_1">Figure 3</ref> shows how the performance of the four models varies across contexts with different lengths. We can observe that the performance of all models first increases monotonically when the context length increases, and then fluctuates or even drops when context length keeps increasing. The reason might be that when only a few utterances are available in the context, the model could not capture enough information for matching, but when the context becomes long enough, noises will be brought to matching due to the topic shift in dialogue. Across the different lengths of the context, our BERT-SL can always achieve better performance than BERT-VFT as well as other baselines. It is worth noting that the perfor-7  mance of our BERT-SL is more stable than other models across different turns of the context, and drops more slightly than other models for a long context. The results imply that our learning schema improves the capability of the matching model to deal with long contexts or short context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Works</head><p>With the advance of natural language processing, building an intelligent dialogue system with data-driven approaches <ref type="bibr" target="#b16">(Vinyals &amp; Le, 2015;</ref><ref type="bibr" target="#b9">Lowe et al., 2015)</ref> has drawn increasing interests in recent years. Most existing approaches are either generation-based or retrieval-based. The former synthesize a response word by word via natural language generation techniques <ref type="bibr" target="#b16">(Vinyals &amp; Le, 2015;</ref><ref type="bibr" target="#b11">Serban et al., 2016)</ref>, while the latter select the most appropriate response from a set of candidates <ref type="bibr" target="#b17">(Wang et al., 2013;</ref><ref type="bibr" target="#b21">Wu et al., 2017;</ref><ref type="bibr" target="#b20">Whang et al., 2020)</ref>. We focus on retrievalbased methods in this paper. Earlier studies pay attention to constructing single-turn context-response matching models where only a single utterance is considered or multiple utterances in the context are concatenated into a long sequence for response selection <ref type="bibr" target="#b17">(Wang et al., 2013;</ref><ref type="bibr" target="#b5">Hu et al., 2014;</ref><ref type="bibr" target="#b9">Lowe et al., 2015)</ref>. Recently, most studies focus on the multi-turn scenario where each utterance in the context first interacts with the response candidate, and then the matching features are aggregated according to the sequential dependencies of the multi-turn context <ref type="bibr" target="#b29">(Zhou et al., 2016;</ref><ref type="bibr" target="#b21">Wu et al., 2017;</ref><ref type="bibr" target="#b22">Yan et al., 2016;</ref><ref type="bibr" target="#b30">Zhou et al., 2018;</ref><ref type="bibr" target="#b13">Tao et al., 2019a)</ref>, and they usually adopt the representation-matchingaggregation paradigm to build the matching models. Following the paradigm, <ref type="bibr" target="#b14">Tao et al. (2019b)</ref> and <ref type="bibr" target="#b19">Wang et al. (2019b)</ref> further consider multiple granularities of representations for matching. Besides, to tackle the side effect of using too much context, <ref type="bibr" target="#b25">Yuan et al. (2019)</ref> utilizes a multi-hop selector to select the relevant utterances in the context for response matching.</p><p>Recently, pre-trained language models <ref type="bibr" target="#b1">(Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Yang et al., 2019;</ref><ref type="bibr" target="#b8">Liu et al., 2019)</ref> have shown impressive benefits for various downstream NLP tasks, and some researchers tried to apply them on response selection. <ref type="bibr" target="#b15">Vig &amp; Ramea (2019)</ref> utilizes BERT to represent each utteranceresponse pair and aggregate these representations to calculate the matching score. <ref type="bibr" target="#b20">Whang et al. (2020)</ref> treat the context as a long sequence and perform context-response matching with the BERT. Besides, the model also introduces the next utterance prediction and mask language modeling tasks borrowed from BERT during the post-training on dialogue corpus to incorporate in-domain knowledge for the matching model. Following <ref type="bibr" target="#b20">Whang et al. (2020)</ref>, <ref type="bibr" target="#b3">Gu et al. (2020)</ref> propose to heuristically incorporate speaker-aware embeddings into BERT to promote the capability of context understanding in multi-turn dialogues.</p><p>Self-supervised learning has become a significant direction in the AI community and has contributed to the success of pre-trained language models <ref type="bibr" target="#b1">(Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Yang et al., 2019;</ref><ref type="bibr" target="#b8">Liu et al., 2019)</ref>. Inspired by this, some researchers propose to learn down-stream tasks with auxiliary self-supervised tasks. In this manner, models can effectively learn task-related knowledge with a fixed amount of training data and produce better features for the primary task. Existing works have explored self-supervised tasks in text classification <ref type="bibr" target="#b24">(Yu &amp; Jiang, 2016)</ref>, summarization <ref type="bibr" target="#b18">(Wang et al., 2019a)</ref> and response generation <ref type="bibr" target="#b26">(Zhang et al., 2019;</ref><ref type="bibr" target="#b28">Zhao et al., 2020)</ref>. The work is unique in that we design several self-supervised tasks according to the characteristics of the dialogue data to improve the multi-turn response selection and our learning schema can bring consistent and significant improvement for both traditional context-response matching models and large-scale pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose learning a context-response matching model with four auxiliary self-supervised tasks designed for the dialogue data. Jointly trained with these auxiliary tasks, the matching model can effectively learn task-related knowledge contained in dialogue data, achieve a better local optimum and produce better features for response selection. Experiment results on two benchmarks indicate that the proposed auxiliary self-supervised tasks bring significant improvement for multi-turn response selection in retrievalbased dialogues, and our PLM-based model achieves new state-of-the-art results on both datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overall architecture of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Performance of BERT-SL and its variants across different lengths of contexts. (a) context length is measured by the average number of turns; (b) context length is measured by the total length of the context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Equal contribution 1 Wangxuan Institute of Computer Technology, Peking University, Beijing, China 2 Microsoft Corporation, Beijing 3 Center for Data Science, Peking University, Beijing, China. Correspondence to: Rui Yan &lt;ruiyan@pku.edu.cn&gt;, Chongyang Tao &lt;chongyangtao@gmail.com&gt;.</figDesc><table><row><cell>Preliminary work.</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>To adapt the task in BERT, we formulate input of BERT encoder as x ur = {[CLS], u 1 , [EOT], . . . , u mask , [EOT], . . . , u m , [EOT], [SEP]}, where u mask consists of only [MASK] tokens and has the same length with u t . After being processed by BERT, the top layer output a representation sequence E ur = {E [CLS] , E 1,1 , . . . , E 1,l1 , E [EOT] , . . . , E m,1 , . . . , E m,lm , E [EOT] , E [SEP]}, where l t is the length of the t-th utterance. The model will predict the masked utterance conditioned on the contextualized representations of each word. The probability distribution of each masked word can be calculated as</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results on the two data sets. Numbers marked with * mean that the improvement is statistically significant compared with the baseline (t-test with p-value &lt; 0.05). Numbers in bold indicate the best strategies for the corresponding models on specific metrics.</figDesc><table><row><cell></cell><cell></cell><cell>.926</cell><cell>0.726</cell><cell>0.847</cell><cell>0.961</cell><cell>0.453</cell><cell>0.654</cell><cell>0.886</cell></row><row><cell></cell><cell>DUA (Zhang et al., 2018)</cell><cell>-</cell><cell>0.752</cell><cell>0.868</cell><cell>0.962</cell><cell>0.501</cell><cell>0.700</cell><cell>0.921</cell></row><row><cell cols="2">Non-PLM-based DAM (Zhou et al., 2018)</cell><cell>0.938</cell><cell>0.767</cell><cell>0.874</cell><cell>0.969</cell><cell>0.526</cell><cell>0.727</cell><cell>0.933</cell></row><row><cell>Models</cell><cell>MRFN (Tao et al., 2019b)</cell><cell>0.945</cell><cell>0.786</cell><cell>0.886</cell><cell>0.976</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>IMN (Gu et al., 2019)</cell><cell>0.946</cell><cell>0.794</cell><cell>0.889</cell><cell>0.974</cell><cell>0.621</cell><cell>0.797</cell><cell>0.964</cell></row><row><cell></cell><cell>ESIM (Chen &amp; Wang, 2019)</cell><cell>0.950</cell><cell>0.796</cell><cell>0.874</cell><cell>0.975</cell><cell>0.570</cell><cell>0.767</cell><cell>0.948</cell></row><row><cell></cell><cell>IoI (Tao et al., 2019a)</cell><cell>0.947</cell><cell>0.796</cell><cell>0.894</cell><cell>0.974</cell><cell>0.563</cell><cell>0.768</cell><cell>0.950</cell></row><row><cell></cell><cell>MSN (Yuan et al., 2019)</cell><cell>-</cell><cell>0.800</cell><cell>0.899</cell><cell>0.978</cell><cell>0.606</cell><cell>0.770</cell><cell>0.937</cell></row><row><cell></cell><cell>BERT (Whang et al., 2020)</cell><cell>0.954</cell><cell>0.817</cell><cell>0.904</cell><cell>0.977</cell><cell>0.610</cell><cell>0.814</cell><cell>0.973</cell></row><row><cell></cell><cell>SA-BERT (Gu et al., 2020)</cell><cell>0.965</cell><cell>0.855</cell><cell>0.928</cell><cell>0.983</cell><cell>0.704</cell><cell>0.879</cell><cell>0.985</cell></row><row><cell></cell><cell>BERT-VFT (Whang et al., 2020)</cell><cell>-</cell><cell>0.855</cell><cell>0.928</cell><cell>0.985</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>BERT-VFT (Ours)</cell><cell>0.969</cell><cell>0.867</cell><cell>0.939</cell><cell>0.987</cell><cell>0.717</cell><cell>0.884</cell><cell>0.986</cell></row><row><cell>PLM-based</cell><cell>BERT-SL</cell><cell cols="6">0.975* 0.884* 0.946* 0.990* 0.776* 0.919*</cell><cell>0.991</cell></row><row><cell>Models</cell><cell>BERT-SL w/o. NSP</cell><cell>0.973</cell><cell>0.879</cell><cell>0.944</cell><cell>0.989</cell><cell>0.760</cell><cell>0.914</cell><cell>0.988</cell></row><row><cell></cell><cell>BERT-SL w/o. UR</cell><cell>0.974</cell><cell>0.881</cell><cell>0.945</cell><cell>0.990</cell><cell>0.763</cell><cell>0.916</cell><cell>0.991</cell></row><row><cell></cell><cell>BERT-SL w/o. ID</cell><cell>0.972</cell><cell>0.877</cell><cell>0.942</cell><cell>0.989</cell><cell>0.755</cell><cell>0.911</cell><cell>0.987</cell></row><row><cell></cell><cell>BERT-SL w/o. CD</cell><cell>0.973</cell><cell>0.880</cell><cell>0.945</cell><cell>0.989</cell><cell>0.742</cell><cell>0.897</cell><cell>0.986</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this work, all random sampling operations are carried out according to uniformly distribution.2 If cleft is replaced, the new piece should be the left part of another context with a random length, and vice versa.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We assume that utterances in a dialogue context are posed one by one, therefore we can simply sample utterances from only the odd turns or even turns.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.taobao.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequential matching model for end-to-end multi-turn response selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7350" to="7354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive matching network for multi-turn response selection in retrievalbased chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2321" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speaker-aware bert for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Speech &amp; language processing. Pearson Education India</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Alime assist: An intelligent assistant for creating an innovative e-commerce experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2495" to="2498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">From eliza to xiaoice: challenges and opportunities with social chatbots. Frontiers of Information Technology &amp; Electronic Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="10" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">One time of interaction may not be enough: Go deep with an interaction-over-interaction network for response selection in dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-representation fusion network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison of transfer-learning approaches for response selection in multi-turn conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on DSTC7</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised learning for contextualized extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2221" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-turn response selection in retrieval-based chatbots with iterated attentive convolution matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1081" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An effective domain adaptive post-training method for bert in response selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrieval-based human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">XLNET: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-hop selector network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Consistent dialogue generation with self-supervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05759</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling multi-turn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning a simple and effective model for multi-turn response generation with auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01972</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
