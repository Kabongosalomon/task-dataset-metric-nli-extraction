<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A method for detecting text of arbitrary shapes in natural scenes that improves text spotting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qitong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University Boston</orgName>
								<address>
									<postCode>02215</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zheng</surname></persName>
							<email>yizheng@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University Boston</orgName>
								<address>
									<postCode>02215</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Betke</surname></persName>
							<email>betke@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University Boston</orgName>
								<address>
									<postCode>02215</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A method for detecting text of arbitrary shapes in natural scenes that improves text spotting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding the meaning of text in images of natural scenes like highway signs or store front emblems is particularly challenging if the text is foreshortened in the image or the letters are artistically distorted. We introduce a pipeline-based text spotting framework that can both detect and recognize text in various fonts, shapes, and orientations in natural scene images with complicated backgrounds. The main contribution of our work is the text detection component, which we call UHT, short for UNet, Heatmap, and Textfill. UHT uses a UNet to compute heatmaps for candidate text regions and a textfill algorithm to produce tight polygonal boundaries around each word in the candidate text. Our method trains the UNet with groundtruth heatmaps that we obtain from text bounding polygons provided by groundtruth annotations. Our text spotting framework, called UHTA, combines UHT with the state-of-the-art text recognition system ASTER. Experiments on four challenging and public scene-text-detection datasets (Total-Text, SCUT-CTW1500, MSRA-TD500, and COCO-Text) show the effectiveness and generalization ability of UHT in detecting not only multilingual (potentially rotated) straight but also curved text in scripts of multiple languages. Our experimental results of UHTA on the Total-Text dataset show that UHTA outperforms four state-of-theart text spotting frameworks by at least 9.1 percent points in the F-measure, which suggests that UHTA may be used as a complete text detection and recognition system in real applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text detection is an important task in computer vision with application significance such as helping people with visual impairments to understand text in images (e.g., of medicine bottles or supermarket shelves) or helping selfdriving cars understand the meaning of traffic and street signs. Building computer vision systems that can detect text is not easy due to the variety of sizes, fonts, styles, sizes, and orientations in which text can occur in natural scene images and their often complex backgrounds (e.g., <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>In the past few years, computer vision researchers have developed methods that identify oriented straight text in natural scene images accurately <ref type="bibr" target="#b7">[14,</ref><ref type="bibr" target="#b19">26,</ref><ref type="bibr" target="#b38">45,</ref><ref type="bibr" target="#b40">47,</ref><ref type="bibr" target="#b41">48]</ref> ("oriented" means not necessarily aligned with the image rows). More recently, detection of arbitrarily-shaped text, such as curved or deformed text, has received attention from computer vision researchers, not only because detecting such text is more challenging than oriented straight text, but also because it commonly appears in daily life. For arbitrarilyshaped text detection, for example, a weakly supervised learning algorithm <ref type="bibr" target="#b0">[7]</ref> was recently proposed to extract character-based pseudo ground truth to help deep learning models effectively extract each character of such a text in a natural scene image. Whether it is detection of oriented straight or curved text, we found that most state-of-the-art methods rely on multiple deep-learned geometric properties of the text, such as angle attributes <ref type="bibr" target="#b17">[24,</ref><ref type="bibr" target="#b41">48]</ref> or text center line regions <ref type="bibr" target="#b39">[46]</ref>. Others use multiple output models <ref type="bibr" target="#b39">[46]</ref> to produce high evaluation scores on widely-used benchmarks. While state-of-the-art text detection methods can solve many challenging problems with these techniques, as far as we know, there is no method that simply and effectively uses a "text region feature map," even when given a variety of text shapes, sizes, and lengths. Moreover, many words are located so close to each other in the images that detection methods do not separate them correctly but grouped into one consecutive-word text region. These challenges make relying on only the text region feature map to effectively detect text in natural scene images seemingly impossible. But is it really impossible to accurately detect text using only one text region feature map in the scene text detection field? Our work shows that the answer is no. Using only one channel, the text region feature map, our method effectively detects text in images of natural scenes. With the help of new pre-processing and post-processing algorithms, we make accurately detecting text in images possible, relying on a relatively small amount of geometric information.</p><p>The contributions of our research work are five-fold:</p><p>• We propose a new text detection framework, called UHT, that outputs only one text region heatmap channel. UHT can solve challenging problems in the field of scene text detection, such as accurately detecting and separating multiple text regions that "stick" together.</p><p>• We propose a new text region feature map representation, which here is a special kind of heat map ( <ref type="figure" target="#fig_0">Fig. 1</ref>), that enables UHT to detect text in natural scene image.</p><p>• We propose a new algorithm called the Textfill Algorithm that can accurately extract multi-vertex bounding polygons that tightly define the outline of each word in the scene text region.</p><p>• UHT obtained evaluation scores that are higher than most of state-of-the-art scene text detection methods when fine-tuned on specific benchmark datasets. UHT outperforms all state-of-the-art methods in its generalization ability, as shown in one of the experiments.</p><p>• "Spotting" text in images means detecting and recognizing it. We introduce a complete pipeline-based text spotting system, called UHTA, showing that our UHT can be used for text spotting as long as an effective text recognition model is given. <ref type="figure">Figure 2</ref>. Pipeline of UHT. The process of text detection of UHT can be divided into three steps: 1) Pre-processing is used to generate a heatmap text region ground truth, which is used as a training label of UHT-Net. 2) A trained UHT-Net can output predicted text region heatmaps. 3) In the post-processing step, the Textfill algorithm outputs the final predicted text bounding polygons interpreting the outputs of the UHT-Net.</p><p>Our code is available at http://www.cs.bu.edu/faculty/ betke/UHT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The task of detecting text in images of everyday scenes, also known as "Scene Text Detection" is attracting more and more attention from researchers in the computer vision field. Initially, researchers focused on detecting oriented straight text in scene images <ref type="bibr" target="#b41">[48,</ref><ref type="bibr" target="#b3">10,</ref><ref type="bibr" target="#b40">47,</ref><ref type="bibr" target="#b38">45,</ref><ref type="bibr" target="#b7">14]</ref>. However, detecting text with arbitrary shapes is more and more popular recently <ref type="bibr" target="#b17">[24,</ref><ref type="bibr" target="#b33">40,</ref><ref type="bibr" target="#b34">41,</ref><ref type="bibr" target="#b39">46,</ref><ref type="bibr" target="#b0">7,</ref><ref type="bibr" target="#b42">49,</ref><ref type="bibr" target="#b35">42]</ref>.</p><p>Before methodologies in deep learning field are widely used in text detection field, SWT <ref type="bibr" target="#b4">[11]</ref> and MSER <ref type="bibr" target="#b20">[27]</ref> were two eye-catching algorithms which had influenced many text detection methodologies. In recent years, modern methodologies, which make use of deep learning backbones, can be coarsely classified into two categories: regression-based methodologies and segmentationbased methodologies.</p><p>Regression-based methodologies are largely influenced by some popular general object detection frameworks such as Faster-RCNN <ref type="bibr" target="#b23">[30]</ref>. TextBoxes <ref type="bibr" target="#b11">[18]</ref> was inspired by SSD <ref type="bibr" target="#b13">[20]</ref> and included long default boxes that had large aspect ratios to better detect text with different variation in natural scene images. In the text detection branch of Mask-TextSpotter <ref type="bibr" target="#b18">[25]</ref>, many text proposals were firstly generated by region proposal network to get text candidate boxes, then the RoI features of the text proposals were sent into the Fast R-CNN module.</p><p>Segmentation-based methodologies are mainly inspired by FCN <ref type="bibr" target="#b16">[23]</ref>, The FCN classifies the image at the pixel level, thus solving the problem of image segmentation at the semantic level. In the text detection field, people see text regions in natural scene images as positive samples and background as negative samples. TextSnake <ref type="bibr" target="#b17">[24]</ref> was proposed to detect text in the natural scene by predicting the text region and various geometry attributes of text to detect oriented straight and curve text effectively. Recently, instead of detecting whole text in images, CRAFT <ref type="bibr" target="#b0">[7]</ref> was proposed to detect individual characters, connecting them to get each text bounding polygon. The proposed method provides the character region score and the character affinity score that, together, effectively cover various kinds of text shapes. In this method, a weakly-supervised framework was implemented to generate character-level pseudo annotations.</p><p>As we can see, state-of-the-art frameworks make full use of a large volume of geometric information to effectively detect text in natural scene images. Our methodology, however, is based on only using text region information to effectively extract text bounding polygons from images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The pipeline of our model is shown in <ref type="figure">Figure 2</ref>. We now introduce our methodology in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-processing: Heatmap Text Region Groundtruth Generation</head><p>Our method represents each word or set of words in an image as an arbitrary-length text skeleton surrounded by a fixed-width region whose pixels have values defined by their distance ("radius") to the skeleton <ref type="figure" target="#fig_1">(Figure 3e</ref>). This "heatmap" representation for text is sufficiently flexible to represent both straight and curved text.</p><p>The way we generate heatmaps was inspired by previous work <ref type="bibr" target="#b0">[7,</ref><ref type="bibr" target="#b21">28]</ref>. Instead of simply marking the pixels of the text region as 1 and the background pixels as 0 (e.g., <ref type="bibr" target="#b17">[24,</ref><ref type="bibr" target="#b41">48]</ref>), our method assigns a probability to each pixel position in the feature map, indicating the probability that this pixel belongs to the text region ( <ref type="figure" target="#fig_1">Figure 3e</ref>). Naturally, the closer the pixel is to the center of the text, the closer the probability is to 1, and the farther the pixel is to the center of the text, the closer the probability is to 0.</p><p>Text Skeleton and Radius. Each annotated text polygon is defined by K vertices, where K is an even number. First, we use a skeleton to represent each polygon. We expand the original number of center points on the skeleton to σ = K + (m − 1) × (K − 2) points, where m is a positive integer (see <ref type="figure" target="#fig_1">Figure 3b</ref> and 3c for more details). In our experiments, m is set to 5. We then pair the down ). For the ith pair of points, we compute the center points as follows:</p><formula xml:id="formula_0">P (i) center = P (i) up + P (i) down 2</formula><p>.</p><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Then, P</head><p>(1) center , P</p><p>(2) center , ..., P ( σ 2 ) center are defined as "Text Center Points (TCP)." The TCPs are essential for building the "Text Skeleton (TS)." We delete the two end groups of TCPs, so that the TCP set is changed from P</p><p>(1)</p><formula xml:id="formula_1">center , P (2) center , ..., P ( σ 2 ) center to P (3) center , P (4) center , ..., P ( σ 2 −2)</formula><p>center . Connecting the center points in the TCP set, then we compute the final polygon skeleton. For each point in the TCP set, we also need their "Radius (R)." For the ith pair of points, R (i) is defined as follows:</p><formula xml:id="formula_2">R (i) = dis(P (i) center , P (i) up ) + dis(P (i) center , P (i) down ) 2 ,<label>(2)</label></formula><p>where function dis(A, B) is the Euclidean distance between A and B.</p><p>2D Gaussian Heatmap Ground Truth Representation. First we need to compute every point of TS using the Bresenham algorithm [1], which yields the "All Text Skeleton Points Set (ATSPS)". Given the set R, which is</p><formula xml:id="formula_3">R (3) , R (4) , ...R ( σ 2 −2)</formula><p>, and a 2D Gaussian kernel, we can compute the heatmap representation for each text bounding polygon. In addition, the range of the values of the generated Heatmap Text Region Groundtruth is set to [0.0, 1.0]. These are then used as training labels for the UHT-Net.</p><p>Since the generated heatmap groundtruth is the text skeleton convolved with several 2D Gaussian kernels, each text region is proportional to the length of its text skeleton. So due to our pre-processing algorithm, we suggest that UHT has the potential to be more accurate than other methods when detecting long text regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">UHT-Net Architecture and Training Objectives</head><p>UHT-Net is a UNet-based <ref type="bibr" target="#b24">[31]</ref> network that predicts score heatmaps of text regions. First, images are contracted to different feature maps. In the expanding process, our method employs either VGG-16 <ref type="bibr" target="#b27">[34]</ref> or ResNet-50 <ref type="bibr" target="#b6">[13]</ref> as backbone networks. Then these feature maps are gradually bilinearly expanded to the original size and mixed with the corresponding output of the previous stage in order to accurately detect text of different sizes.</p><p>UHT-Net uses an end-to-end training strategy. The definition of the loss function is</p><formula xml:id="formula_4">L = L reg + λ 1 L center + λ 2 L region ,<label>(3)</label></formula><p>where λ 1 and λ 2 are both set to 1.0. We define L reg as the weighted MSE-Loss (because the ratio between positive and negative samples is unbalanced in the scene text detection datasets), which is defined for an input image χ to be:</p><formula xml:id="formula_5">Lreg = text text + BG (YBG − f θ (χBG)) 2 + BG text + BG (Ytext − f θ (χtext)) 2 ,<label>(4)</label></formula><p>where text denotes the positive pixels in the heatmap, BG denotes the negative pixels in the heatmap, text denotes the total number of positive pixels in the heatmap, BG denotes the total number of negative pixels in the heatmap, Y means pixels in the groundtruth heatmap generated by the pre-process, and f θ (χ) means pixels in the output of the UHT-Net, where θ are parameters in the UHT-Net.</p><p>The dice loss <ref type="bibr" target="#b29">[36]</ref> for the text center and text region is denoted by L center and L region respectively. The text center is defined as the text region pixels in the output of the UHT-Net and generated groundtruth heatmap pixels that are higher than 0.9. The text region is defined as the text region pixels in the output of the UHT-Net and generated Algorithm 1 Textfill Algorithm while stack do <ref type="bibr">11:</ref> x, y = stack.pop() <ref type="bibr">12:</ref> if judgeF low(x − 1, y, x, y) then <ref type="bibr">13:</ref> stack.add((x − 1, y)) <ref type="bibr">14:</ref> if judgeF low(x + 1, y, x, y) then <ref type="bibr">15:</ref> stack.add((x + 1, y)) <ref type="bibr">16:</ref> if judgeF low(x, y − 1, x, y) then <ref type="bibr">17:</ref> stack.add((x, y − 1)) <ref type="bibr">18:</ref> if judgeF low(x, y + 1, x, y) then <ref type="bibr">19:</ref> stack.add((x, y + 1))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>A[stack] = 1.0 <ref type="bibr" target="#b14">21</ref>:</p><formula xml:id="formula_6">C = f indCoutour(A) 22:</formula><p>V .append(contourExpand(C)) <ref type="bibr" target="#b16">23</ref>: Output: Polygon vertices V groundtruth heatmap pixels that are higher than 0.05, which are:</p><formula xml:id="formula_7">L center = 1 − 2 |f θ (χ center ) Y center | |f θ (χ center )| + |Y center | ,<label>(5)</label></formula><formula xml:id="formula_8">L region = 1 − 2 |f θ (χ region ) Y region | |f θ (χ region )| + |Y region | .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Post-processing: Textfill Algorithm</head><p>Extracting the final predicted text bounding polygons from the output of the UHT-Net is accomplished by our novel post-processing method, the Textfill Algorithm, which is inspired by the floodfill algorithm [2]. Details are shown in Algorithm 1, which uses computer vision tools that can be found in OpenCV. The function judgeF low(x 1 , y 1 , x 2 , y 2 ) is used to expand CR in Algorithm 1 to compute the complete text bounding polygon. The definition of CR is at Line 3 of Algorithm 1. where S is the pixel area of the polygonal region A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section 1 , we introduce details of our experiments, including the datasets we use and our training strategy, and provide experimental results and their analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text Detection Datasets Used in Experiments</head><p>SynthText <ref type="bibr" target="#b5">[12]</ref> is a large scale dataset with 800k synthetic images that are created by adding English oriented straight text with random fonts, sizes, colors, and orientations to natural images. These synthetic images are quite similar to natural scene images with text.</p><p>Total-Text <ref type="bibr" target="#b1">[8]</ref> is a dataset with images that contain oriented straight and curved text and whose labels are annotated by bounding polygons. The image backgrounds are quite similar to real scenes. This dataset contains 1,255 training and 300 testing images.</p><p>SCUT-CTW1500 <ref type="bibr" target="#b14">[21]</ref> is another text detection dataset which includes both English and Chinese scripts. SCUT-CTW1500 contains 1,000 training and 500 testing images in which text shape is arbitrary (as for Total-Text). Each text annotation is marked as polygon containing 14 points.</p><p>MSRA-TD500 <ref type="bibr" target="#b37">[44]</ref> focuses on multilingual oriented straight text in natural scenes. It contains 500 images with English and Chinese scripts, which are split into 300 training and 200 testing images. Text region annotations are marked as rotated rectangles.</p><p>COCO-Text <ref type="bibr" target="#b32">[39]</ref> is one of the challenges of ICDAR 2017 Robust Reading Competition. Its text instances in the images are English straight text regions distributed in various orientations. It contains 63,686 images in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Data Augmentation. The process of training UHT-Net can be divided into two steps: 1) Pretraining with the Synth-Text dataset, and 2) fine-tuning using the Total-Text, SCUT-CTW1500, MSRA-TD500 or COCO-Text datasets respectively. To further improve training, we randomly rotated the training images and cropped with areas ranging from 0.24 to 1.0 and aspect ratios ranging from 0.33 to 3. Data augmentation is implemented in both pretraining and fine-tuning processes.</p><p>Training Strategy of UHT-Net. Our methodology was implemented in Pytorch 1.0.1 <ref type="bibr" target="#b22">[29]</ref>. UHT-Net was pre-trained on SynthText with one epoch and then finetuned using Total-Text, SCUT-CTW1500, MSRA-TD500, or COCO-Text. We adopted the Adam optimizer <ref type="bibr" target="#b10">[17]</ref> as the learning rate scheme. In the pretraining process, inspired by Smith <ref type="bibr" target="#b28">[35]</ref>, we set the initial learning rate to 3 × 10 −5 for VGG-16 based UHT and 10 −4 for ResNet-50 based UHT. We did not change it during the pretraining process. In the fine-tuning process, except for COCO-Text, we set the initial learning rate to 10 −4 (the fine-tuning learning rate for COCO-Text is set to 5 × 10 −4 ). The decay rate was 0.8 every 10 epochs. Single-scale training was used. In the pretraining and fine-tuning training processes, we set the batch size to 8 on a single RTX-2080Ti GPU. In the evaluation process, the batch size was set to 1 on a single RTX-2080Ti GPU.</p><p>Hyperparameters of Textfill Algorithm. To show general results across datasets, two sets of thresholds T top , T end were tested: (0.7, 0.2) for Total-Text &amp; SCUT-CTW1500 and (0.75, 0.2) for MSRA-TD500 &amp; COCO-Text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Results on Curved Text Detection</head><p>Our results on the benchmarks Total-Text <ref type="bibr" target="#b1">[8]</ref> and SCUT-CTW1500 <ref type="bibr" target="#b14">[21]</ref> are given in Tables 1 and 2, respectively. We found that some state-of-the-art methods <ref type="bibr" target="#b35">[42,</ref><ref type="bibr" target="#b39">46]</ref> included multi-scale testing. To conduct a peer comparison, we ran experiments on curved text detection datasets with singlescale and multi-scale testing (abbreviated as "MS" below) separately. Except for the baselines <ref type="bibr" target="#b1">[8,</ref><ref type="bibr" target="#b14">21]</ref>, listed methods without + used the same pre-training and fine-tuning data as we did, otherwise were different.</p><p>Results on Total-Text Dataset <ref type="table">(Table 1)</ref>. Fine-tuning on Total-Text stops at 300 epochs. During the testing process, each image is set to 700 × 700. In single-scale testing, our UHT beats all of the state-of-the-art methodologies and keeps the same F-measure score with the newest and most competitive model, CharNet H-88 <ref type="bibr" target="#b35">[42]</ref>. UHT V16 even yields a higher recall rate than all the other state-ofthe-art methods, 85.6%. This indicates that UHT is able to detect text that is missed by other methods.</p><p>Results on SCUT-CTW1500 Dataset <ref type="table">(Table 2</ref>) Finetuning on SCUT-CTW1500 stops at 307 epochs for UHT V16 and 300 epochs for UHT R50. During the testing process, each image is set to 512 × 512 because the average size of images in SCUT-CTW1500 is relatively smaller than that of Total-Text. Experimental results show that UHT also performs well on SCUT-CTW1500. Surprisingly, UHT found image text that did not appear in the ground truth annotation (see <ref type="figure" target="#fig_2">Figure 4)</ref>. We fixed the SCUT-CTW1500 ground truth to include missed words. To ensure fairness in evaluation, we ran experiments on two different versions of text annotations on the SCUT-CTW1500 dataset, with and without updated ground truth (  <ref type="table">Table 2</ref>. Experimental results on the SCUT-CTW1500 dataset: "P" means Precision, "R" Recall, "F" F-measure, * denotes results on updated groundtruth annotations, and "MS" multi-scale testing.</p><p>truth was updated, the recall score of UHT is almost unchanged but the precision score improves a little. We welcome other researchers to run experiments on the updated SCUT-CTW1500 and therefore make it publicly available, see http://www.cs.bu.edu/faculty/betke/UHT. Analysis on Multi-scale Testing. We tested our model with images of different sizes (500×500, 700×700, and 900×900), relying on the Fast NMS algorithm to screen out excess text bounding polygons. A reason for the increase of the recall score of multi-scale compared to single-scale testing may be that multi-scale testing combines image information of different sizes, making it easier for UHT to detect text regions that are difficult to detect in single-scale testing. However, false-positive samples are more likely to appear in multi-scale testing. We think that might be caused by the effort of the Fast NMS algorithm or text detection effect on very large images, all of which cause decrease of precision score of multi-scale testing (see <ref type="table">Tables 1 and 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results for Oriented Straight Text Detection</head><p>Results on MSRA-TD500 Dataset <ref type="table">(Table 3)</ref>. Finetuning on MSRA-TD500 stops at 200 epochs. During the testing process, each image is set to 512 × 512. For UHT, only single-scale texting is implemented in this experiment. The results show that our UHT beats most of the state-ofthe-art methods on MSRA-TD500.</p><p>Results on COCO-Text Dataset <ref type="table" target="#tab_3">(Table 4</ref>). With the help of the official COCO-Text API, we extracted 15,124 training images and 3,346 validation images, all of which contain at least one text region. However, testing images cannot be extracted. So in the experiments with COCO-Text, experimental results are given based on tests on the validation images for state-of-the-art frameworks and UHT. Fine-tuning on COCO-Text stops at 300 epochs. During the testing process, each image is set to 768 × 768. For UHT, only single-scale testing is implemented in this experiment. The results (  IC13 and IC17-MLT datasets, please refer to Section 4.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Generalization Ability</head><p>A powerful text detection framework should have good generalization ability instead of just overfitting to a particular dataset and reaching high evaluation scores for that dataset. To further verify the generalization ability of UHT, we conducted two additional experiments: (1) we pre-trained and fine-tuned our model on datasets without curved text, here ICDAR-2015 <ref type="bibr" target="#b9">[16]</ref> with 200 epochs, and then evaluated it on the Total-Text and SCUT-CUW1500 datasets. We chose state-of-the-art baselines which were also only fine-tuned on ICDAR-2015 <ref type="bibr" target="#b9">[16]</ref>. (2) We finetuned TextSnake <ref type="bibr" target="#b17">[24]</ref> and UHT using the same fine-tuning data as CRAFT <ref type="bibr" target="#b0">[7]</ref>.</p><p>Experimental results <ref type="table">(Table 5)</ref> show that UHT method performs well on two curved and one oriented straight text datasets, even if it is not fine-tuned on them. It outperforms all listed state-of-the-art baseline methods. We suggest that the powerful generalization ability of UHT is due to its flexibility in text expression as well as the effectiveness of the Textfill Algorithm in extracting text bounding polygons by making full use of the UHT-Net output.  <ref type="table">Table 5</ref>. Generalization ability on Total-Text and SCUT-CTW1500 datasets Results of Seglink <ref type="bibr" target="#b25">[32]</ref>, EAST <ref type="bibr" target="#b41">[48]</ref>, PixelLink <ref type="bibr" target="#b3">[10]</ref> and TextSnake <ref type="bibr" target="#b17">[24]</ref> were reported by Long et al. <ref type="bibr" target="#b17">[24]</ref>. Results of CRAFT <ref type="bibr" target="#b0">[7]</ref> are from the official CRAFT model <ref type="bibr">[4]</ref>, which we fine-tuned on the ICDAR-2015 dataset. * denotes results with respect to the annotations that we modified for SCUT-CTW1500. We used single-scale testing. <ref type="figure">Figure 5</ref>. Pipeline of the proposed text spotting model UHTA. The model first calls the proposed UHT Detector and then converts UHT's polygonal output regions into horizontally-aligned rectangular regions of text. These text regions are then passed to the state-of-the-art text recognition model ASTER <ref type="bibr" target="#b26">[33]</ref>, which can accurately recognize the text in these regions and output text strings. So, like a person, UHTA does not only know where the text regions are, but also recognize the content of each text region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental Results on Text Spotting</head><p>So far, we have shown that the proposed UHT model can accurately localize text in natural scene images. The application range of UHT can be widen when it is embedded into a text recognition framework <ref type="figure">(Fig. 5)</ref>. After all, a sighted person would not stop at the task of localizing text but also aim to identify its content. To provide a computer vision system who can take on both tasks of detecting and recognizing text in images, we here propose the model UHTA (short for UHT + ASTER <ref type="bibr" target="#b26">[33]</ref>).</p><p>We now present a peer comparison for UHTA, showing Venue F-measure (%) Textboxes <ref type="bibr" target="#b11">[18]</ref> AAAI-2017 36.3 Mask TextSpotter <ref type="bibr" target="#b18">[25]</ref> ECCV-2018 52.9 TextNet <ref type="bibr" target="#b30">[37]</ref> ACCV-2018 54.0 CharNet H-88 <ref type="bibr" target="#b35">[42]</ref> ICCV-2019 66.6 TSA <ref type="bibr" target="#b17">[24,</ref><ref type="bibr" target="#b26">33]</ref> ECCV-2018 58.1 UHTA V16 (Ours) <ref type="bibr" target="#b26">[33]</ref> -75.7 UHTA R50 (Ours) <ref type="bibr" target="#b26">[33]</ref> -77.6 the contribution of UHT in a second text spotting system, called TSA (TextSnake [24] + ASTER <ref type="bibr" target="#b26">[33]</ref>) that applies the pretrained TextSnake model <ref type="bibr">[3]</ref> with the same training data as UHT. The reason why we use ASTER as our text recognizor is that it can efficiently recognize curved and straight text and its code is available.</p><p>We ran experiments using the Total-Text dataset, where annotations of text spotting are included. Single-scale and lexicon-free testing were implemented in the evaluation for all models. Experimental results are detailed in <ref type="table" target="#tab_6">Table 6</ref>, which show that UHTA has a powerful ability to spot text and outperforms state-of-the-art text spotting systems on the Total-Text dataset. Moreover, since the same conditions were applied for UHTA and TSA, the superior results of UHTA shows the effectiveness of UHT as the text detection module of the text spotting pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis and Discussion</head><p>Framework Features. UHT is robust to different scales and shapes of text from natural scene images. UHT treats the text in the natural scene images directly as positive regions instead of the composition of different geometry attributes, whether it is oriented straight text or curved text. Textfill algorithm can also flexibly and accurately extract the text in the images according to the output of UHT-Net. Even if text regions are very close to each other, UHT can accurately separate words, outperforming most of the stateof-the-art methodologies in the text detection field.</p><p>Multilingual Ability. SCUT-CTW1500 and MSRA-T-D500 contain English and Chinese scripts. Our results show the effectiveness of UHT in detecting scripts in a Latin language like English and Sino-Tibetan language like Chinese.</p><p>Generalization Ability. UHT outperforms state-of-theart text detection frameworks by at least 1.5 percent points Backbone Total-Text SCUT-CTW1500 MSRA-TD500 COCO-Text UHT V16 1.6 2.1 2.6 5.2 UHT R50 1.9 1.8 3.7 4.5 <ref type="table">Table 7</ref>. Speed of UHT. The unit of measure is FPS.</p><p>in the F-measure of Total-Text <ref type="bibr" target="#b1">[8]</ref>  <ref type="table">(Table 5)</ref>, at least 10.4 percent points in the F-measure of SCUT-CTW1500 <ref type="bibr" target="#b14">[21]</ref> ( <ref type="table">Table 5)</ref>, and 8.5 percent points in the F-measure of COCO-Text <ref type="bibr" target="#b32">[39]</ref>  <ref type="table" target="#tab_3">(Table 4)</ref>, even when not fine-tuned on them. This shows that UHT not only performs well when fine-tuned to a specific dataset, but also performs well when not. We thus conclude that UHT is robust and has a strong generalization ability. Speed Analysis. <ref type="table">Table 7</ref> reveals that the speed of UHT when dealing with curved text is slower than with oriented straight text. We think this might be caused by the original ground truth representation of the text region. Oriented straight text is represented by a rectangle but curved text by a more complicated multi-vertex polygon.</p><p>The backbone UHT-Net. Interestingly, analyzing all experimental results, we found that usually UHT R50 performs slightly better than UHT V16; but sometimes the opposite occurs. We think this might be caused by the hyperparameter setting: our choice of hyperparameters may not allow UHT V16 or UHT R50 to exert their optimal abilities compared with another model for a particular dataset.</p><p>Text Spotting Analysis. The strong experimental results of UHTA <ref type="table" target="#tab_6">(Table 6)</ref> show the strength of UHT as an application -UHT performs excellent when used as a text detector in a pipeline-based text spotting system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new text detection model called UHT that, with little information, can effectively detect text in natural scene images. UHT performs well in experiments with publicly available datasets. This includes experiments when UHT is fined-tuned and tested on a specific dataset and when fine-tuned and tested on different datasets. We fixed ground truth annotation errors of the SCUT-CTW1500 dataset and make the corrected ground truth publicly available. We further showed the scope of UHT by implementing a pipeline-based text spotting system that improves the results of other state-of-the-art text spotting frameworks by a range of 9.1-41.3 percent points in the F-measure. In the future, we plan to explore the possibility of detecting scripts of languages other than English and Chinese, such as Korean and Arabic, with UHT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Polygonal text annotations of curved text in images are often so imprecise (bottom left) that heat maps (bottom right), computed as an intermediate step to interpret the text, yield inaccurate results. The proposed UHT method computes and interprets deep learned heat maps (top right) that result in much more accurate polygonal text outlines (top left), which in turn yield better text recognition results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Process of creating the heatmap groundtruth. (a) Input: Text bounding polygon annotation, defined here by K = 10 vertices. (b) This polygon consists of K−2 2 = 4 quadrilaterals (shown in different colors). (c) Each quadrilateral is divided into m equal parts, here m = 3. The Text Center Points (TCP) are marked as red dots, and the Text Skeleton (TS) is drawn in orange and coffee colors Using knowledge from mathematical geometry, we can get original text annotation points expanded to σ = K + (m − 1)(K − 2) points, here 26. (d) To focus on the text center region, we delete the two ends of the TS (orange lines). This yields the final TS, here drawn in coffee color. The pink line exemplifies the radius R in our text representation method (Equation 2). (e) The final heatmap ground truth. The range of the 2D Gaussian kernel is set to [0.0, 1.0].coordinates of the upper part of the vertices of the polygon with the coordinates of the lower part of the vertices of the polygon. This yields the following pairs of points:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples of updated ground truth annotations: Left: The groundtruth annotation in SCUT-CTW1500 missed "89" (top) and " c Roberto Herrett" (bottom). Right: Our updated annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input: Output heatmap H from UHT-Net; thresholds T top , T end . 2: // Extract center points for each text region: 3: Set pixel values in regions where heatmap H pixel values &gt; T top to 1.0, otherwise to 0.0. These regions are defined as CR. 4: Find center points CP for each CR.</figDesc><table><row><cell cols="2">5: // Extract text region:</cell></row><row><cell cols="2">6: V = []</cell></row><row><cell cols="2">7: for each CP do</cell></row><row><cell>8:</cell><cell>Initialize zero-valued canvas A with same shape as</cell></row><row><cell></cell><cell>heatmap H.</cell></row><row><cell>9:</cell><cell>stack=set(A[x][y]).</cell></row><row><cell>10:</cell><cell></cell></row></table><note>1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>FunctioncontourExpand is defined as a dilating process (see morphology tools in OpenCV) with the following kernel:</figDesc><table><row><cell>k =</cell><cell>8 + S 750 35</cell><cell>S ∈ (0, 2 × 10 4 ] S &gt; 2 × 10 4 ,</cell></row></table><note>Function judgeF low(x 1 , y 1 , x 2 , y 2 ) returns true if H[x 1 ][y 1 ] &lt;= H[x 2 ][y 2 ] and H[x 1 ][y 1 ] &gt; T end , or re- turn true if H[x 1 ][y 1 ] &gt;= T end /2, where the H[x][y] de- notes the pixel in the output from the UHT-Net.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 )Table 1</head><label>21</label><figDesc>. After the ground</figDesc><table><row><cell>Methodology</cell><cell>Venue</cell><cell cols="3">P (%) R (%) F (%)</cell></row><row><cell cols="3">Single-scale Testing</cell><cell></cell><cell></cell></row><row><cell>Poly-FRCNN-3 [8]</cell><cell cols="2">IJDAR-2019 78.0</cell><cell>68.0</cell><cell>73.0</cell></row><row><cell>TextSnake [24]</cell><cell>ECCV-2018</cell><cell>82.7</cell><cell>74.5</cell><cell>78.4</cell></row><row><cell>CSE + [22]</cell><cell>CVPR-2019</cell><cell>81.4</cell><cell>79.7</cell><cell>80.2</cell></row><row><cell>TextField [43]</cell><cell>TIP-2019</cell><cell>81.2</cell><cell>79.9</cell><cell>80.6</cell></row><row><cell>PSENet-1s + [40]</cell><cell cols="4">CVPR-2019 84.02 77.96 80.87</cell></row><row><cell>FTSN [9]</cell><cell>ICPR-2018</cell><cell>84.7</cell><cell>78.0</cell><cell>81.3</cell></row><row><cell>ICG [38]</cell><cell>PR-2019</cell><cell>82.9</cell><cell>80.9</cell><cell>81.5</cell></row><row><cell>LOMO [46]</cell><cell>CVPR-2019</cell><cell>88.6</cell><cell>75.7</cell><cell>81.6</cell></row><row><cell>CRAFT + [7]</cell><cell>CVPR-2019</cell><cell>87.6</cell><cell>79.9</cell><cell>83.6</cell></row><row><cell>PSENet v2 [41]</cell><cell>ICCV-2019</cell><cell>89.3</cell><cell>81.0</cell><cell>85.0</cell></row><row><cell>CharNet H-88 [42]</cell><cell>ICCV-2019</cell><cell>89.9</cell><cell>81.7</cell><cell>85.6</cell></row><row><cell>UHT V16 (Ours)</cell><cell>-</cell><cell>88.8</cell><cell>82.6</cell><cell>85.6</cell></row><row><cell>UHT R50 (Ours)</cell><cell>-</cell><cell>88.2</cell><cell>81.8</cell><cell>84.9</cell></row><row><cell cols="3">Multi-scale Testing</cell><cell></cell><cell></cell></row><row><cell>LOMO MS [46]</cell><cell>CVPR-2019</cell><cell>87.6</cell><cell>79.3</cell><cell>83.3</cell></row><row><cell cols="2">CharNet H-88 MS [42] ICCV-2019</cell><cell>88.0</cell><cell>85.0</cell><cell>86.5</cell></row><row><cell>UHT V16 MS (Ours)</cell><cell>-</cell><cell>85.0</cell><cell>85.6</cell><cell>85.3</cell></row><row><cell>UHT R50 MS (Ours)</cell><cell>-</cell><cell>85.4</cell><cell>84.2</cell><cell>84.8</cell></row><row><cell>Methodology</cell><cell>Venue</cell><cell cols="3">P (%) R (%) F (%)</cell></row><row><cell cols="3">Single-scale Testing</cell><cell></cell><cell></cell></row><row><cell>CTD [21]</cell><cell>PR-2019</cell><cell>74.3</cell><cell>65.2</cell><cell>69.5</cell></row><row><cell>CTD+TLOC [21]</cell><cell>PR-2019</cell><cell>74.3</cell><cell>69.8</cell><cell>73.4</cell></row><row><cell>TextSnake [24]</cell><cell>ECCV-2018</cell><cell>67.9</cell><cell>85.3</cell><cell>75.6</cell></row><row><cell>CSE + [22]</cell><cell>CVPR-2019</cell><cell>78.7</cell><cell>76.1</cell><cell>77.4</cell></row><row><cell>LOMO [46]</cell><cell>CVPR-2019</cell><cell>89.2</cell><cell>69.6</cell><cell>78.4</cell></row><row><cell>ICG [38]</cell><cell>PR-2019</cell><cell>82.8</cell><cell>79.8</cell><cell>81.3</cell></row><row><cell>TextField [43]</cell><cell>TIP-2019</cell><cell>83.0</cell><cell>79.8</cell><cell>81.4</cell></row><row><cell>CRAFT [7]</cell><cell>CVPR-2019</cell><cell>86.0</cell><cell>81.1</cell><cell>83.5</cell></row><row><cell>PSENet v2 [41]</cell><cell>ICCV-2019</cell><cell>86.4</cell><cell>81.2</cell><cell>83.7</cell></row><row><cell cols="3">PAN Mask R-CNN + [15] WACV-2019 86.8</cell><cell>83.2</cell><cell>85.0</cell></row><row><cell>UHT V16 (Ours)</cell><cell>-</cell><cell>84.3</cell><cell>84.8</cell><cell>84.5</cell></row><row><cell>UHT V16* (Ours)</cell><cell>-</cell><cell>86.2</cell><cell>84.1</cell><cell>85.2</cell></row><row><cell>UHT R50 (Ours)</cell><cell>-</cell><cell>85.9</cell><cell>83.3</cell><cell>84.6</cell></row><row><cell>UHT R50* (Ours)</cell><cell>-</cell><cell>87.4</cell><cell>82.3</cell><cell>84.8</cell></row><row><cell cols="3">Multi-scale Testing</cell><cell></cell><cell></cell></row><row><cell>LOMO MS [46]</cell><cell>CVPR-2019</cell><cell>85.7</cell><cell>76.5</cell><cell>80.8</cell></row><row><cell>UHT V16 MS (Ours)</cell><cell>-</cell><cell>83.3</cell><cell>85.4</cell><cell>84.4</cell></row><row><cell>UHT V16 MS* (Ours)</cell><cell>-</cell><cell>85.2</cell><cell>84.7</cell><cell>85.0</cell></row><row><cell>UHT R50 MS (Ours)</cell><cell>-</cell><cell>81.9</cell><cell>86.1</cell><cell>84.0</cell></row><row><cell>UHT R50 MS* (Ours)</cell><cell>-</cell><cell>84.0</cell><cell>85.5</cell><cell>84.7</cell></row></table><note>. Experimental results on the Total-Text dataset. "P" means Precision, "R" Recall, "F" F-measure,* denotes results on updated groundtruth annotations, and "MS" multi-scale testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>) show that our UHT beats state-of-theart methods when fine-tuned on COCO-Text training images. For more details of the fine-tuning experiments on</figDesc><table><row><cell>Methodology</cell><cell>Venue</cell><cell cols="3">P (%) R (%) F (%)</cell></row><row><cell cols="2">Zhang et al. [47] CVPR-2016</cell><cell>83</cell><cell>67</cell><cell>74</cell></row><row><cell>He et al. [14]</cell><cell>CVPR-2017</cell><cell>77</cell><cell>70</cell><cell>74</cell></row><row><cell>EAST  † [48]</cell><cell cols="2">CVPR-2017 87.3</cell><cell>67.4</cell><cell>76.1</cell></row><row><cell>SegLink [32]</cell><cell>CVPR-2017</cell><cell>86</cell><cell>70</cell><cell>77</cell></row><row><cell>PixelLink  † [10]</cell><cell>AAAI-2018</cell><cell>83.0</cell><cell>73.2</cell><cell>77.8</cell></row><row><cell>TextSnake [24]</cell><cell>ECCV-2018</cell><cell>83.2</cell><cell>73.9</cell><cell>78.3</cell></row><row><cell>RRD  † [19]</cell><cell>CVPR-2018</cell><cell>87</cell><cell>73</cell><cell>79</cell></row><row><cell>Lyu et al.  † [26]</cell><cell cols="2">CVPR-2018 87.6</cell><cell>76.2</cell><cell>81.5</cell></row><row><cell>CRAFT [7]</cell><cell cols="2">CVPR-2019 88.2</cell><cell>78.2</cell><cell>82.9</cell></row><row><cell>UHT V16 (Ours)</cell><cell>-</cell><cell>84.2</cell><cell>76.2</cell><cell>80.0</cell></row><row><cell>UHT R50 (Ours)</cell><cell>-</cell><cell>83.2</cell><cell>77.0</cell><cell>80.0</cell></row><row><cell cols="5">Table 3. Experimental results on MSRA-TD500; "P" means Preci-</cell></row><row><cell cols="5">sion, "R" Recall, "F" F-measure.  † denotes multi-scale testing. In</cell></row><row><cell cols="5">this table, training data and testing scale of different methods may</cell></row><row><cell cols="5">not be the same, which inadvertently hinders comparison.</cell></row><row><cell>Methodology</cell><cell>Venue</cell><cell cols="3">P (%) R (%) F (%)</cell></row><row><cell cols="3">Fine-tuned using COCO-Text</cell><cell></cell><cell></cell></row><row><cell>TextSnake[24]</cell><cell>ECCV-2018</cell><cell>54.7</cell><cell>36.3</cell><cell>43.6</cell></row><row><cell>UHT V16 (Ours)</cell><cell>-</cell><cell>62.2</cell><cell>47.7</cell><cell>54.0</cell></row><row><cell>UHT R50 (Ours)</cell><cell>-</cell><cell>60.8</cell><cell>49.0</cell><cell>54.2</cell></row><row><cell cols="4">Fine-tuned using IC13 and IC17-MLT</cell><cell></cell></row><row><cell>TextSnake[24]</cell><cell>ECCV-2018</cell><cell>35.3</cell><cell>33.7</cell><cell>34.5</cell></row><row><cell>CRAFT[7]</cell><cell cols="2">CVPR-2019 44.0</cell><cell>28.9</cell><cell>34.9</cell></row><row><cell>UHT V16 (Ours)</cell><cell>-</cell><cell>43.8</cell><cell>43.1</cell><cell>43.4</cell></row><row><cell>UHT R50 (Ours)</cell><cell>-</cell><cell>46.4</cell><cell>41.5</cell><cell>43.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Experimental results on COCO-Text; "P" means Preci-</cell></row><row><cell>sion, "R" Recall, "F" F-measure. As far as we know, no new work</cell></row><row><cell>conducted experiments on testing datasets of COCO-Text since</cell></row><row><cell>2019. So in this table, instead of copying directly from other pa-</cell></row><row><cell>pers, experimental results from state-of-the-art methodologies are</cell></row><row><cell>reimplemented by us using their official code [7] and [3].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Experimental results of TSA and UHTA on the Total-Text</figDesc><table><row><cell>Dataset. Pretrained ASTER [33] model is downloaded from offi-</cell></row><row><cell>cial pytorch reimplementation [5]. Evaluation method for UHTA</cell></row><row><cell>and TSA is end-to-end recognition from [6]. Annotations are</cell></row><row><cell>horizontal text-bounding rectangles because UHT and TextSnake</cell></row><row><cell>outputs horizontal text-bounding rectangles in UHTA and TSA.</cell></row><row><cell>No distinction between uppercase and lowercase was made when</cell></row><row><cell>we evaluated UHTA and TSA. The listed F-measures of the prior</cell></row><row><cell>works were reported in their original papers. UHTA V16 denotes</cell></row><row><cell>UHTA with VGG-16backbone; UHTA R50 denotes UHTA with</cell></row><row><cell>ResNet-50 backbone.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In tables of this section, UHT V16 denotes UHT with VGG-16 backbone; UHT R50 denotes UHT with ResNet-50 backbone.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been partially supported by the National Science Foundation, grant 1838193, and the Boston University Hariri Institute for Computing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character region awareness for text detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Total-text: A comprehensive dataset for scene text detection and recognition. CoRR, abs/1710.10400</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fused text segmentation networks for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1709.03272</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<idno>abs/1801.01315</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1604.06646</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep direct regression for multi-oriented scene text detection. CoRR, abs/1703.08289</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mask R-CNN with pyramid attention network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<idno>abs/1811.09058</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1611.06779</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rotationsensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>abs/1803.05265</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1512.02325</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Detecting curve text in the wild: New dataset and new solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1712.02170</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards robust curve text detection with conditional spatial expansion. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4038</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>abs/1807.02242</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>abs/1802.08948</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Robust widebaseline stereo from maximally stable extremal regions. Image and vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1603.06937</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1703.06520</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">No more pesky learning rate guessing games. CoRR, abs/1506.01186</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno>abs/1707.03237</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Textnet: Irregular text reading from images with an end-toend trainable network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<idno>abs/1812.09900</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting dense and arbitrary-shaped scene text by instanceaware component grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Shape robust text detection with progressive scale expansion network. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05900</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07954</idno>
		<title level="m">Convolutional character networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Textfield: Learning A deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>abs/1812.01393</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Look more than once: An accurate detector for text of arbitrary shapes. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>abs/1604.04018</idno>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Textmountain: Accurate scene text detection via instance segmentation. CoRR, abs/1811.12786</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
