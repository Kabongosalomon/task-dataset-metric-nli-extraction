<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Guided Convolutional Network for Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Jie</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Fei-Peng</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wei</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Ping</forename><surname>Tan</surname></persName>
						</author>
						<title level="a" type="main">Learning Guided Convolutional Network for Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SUBMISSION TO IEEE TRANSACTIONS ON IMAGE PROCESSING, 2019 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Depth completion</term>
					<term>depth estimation</term>
					<term>guided filtering</term>
					<term>multi-modal fusion</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense depth perception is critical for autonomous driving and other robotics applications. However, modern Li-DAR sensors only provide sparse depth measurement. It is thus necessary to complete the sparse LiDAR data, where a synchronized guidance RGB image is often used to facilitate this completion. Many neural networks have been designed for this task. However, they often naïvely fuse the LiDAR data and RGB image information by performing feature concatenation or element-wise addition. Inspired by the guided image filtering, we design a novel guided network to predict kernel weights from the guidance image. These predicted kernels are then applied to extract the depth image features. In this way, our network generates content-dependent and spatially-variant kernels for multi-modal feature fusion. Dynamically generated spatially-variant kernels could lead to prohibitive GPU memory consumption and computation overhead. We further design a convolution factorization to reduce computation and memory consumption. The GPU memory reduction makes it possible for feature fusion to work in multi-stage scheme. We conduct comprehensive experiments to verify our method on real-world outdoor, indoor and synthetic datasets. Our method produces strong results. It outperforms state-of-the-art methods on the NYUv2 dataset and ranks 1st on the KITTI depth completion benchmark at the time of submission. It also presents strong generalization capability under different 3D point densities, various lighting and weather conditions as well as cross-dataset evaluations. The code will be released for reproduction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D ENSE depth perception is critical for many robotics applications, such as autonomous driving or other mobile robots. Accurate dense depth perception of the observed image is the prerequisite for solving the following tasks such as obstacle avoidance, object detection or recognition and 3D scene reconstruction. While depth cameras can be easily adopted in indoor scenes, outdoor dense depth perception mainly relies on stereo vision or LiDAR sensors. Stereo vision algorithms <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> still have many difficulties in reconstructing thin and discontinuous objects. So far, LiDAR sensors provide the most reliable and most accurate depth sensing and have been widely integrated into many robots and autonomous vehicles. However, current LiDAR sensors only obtain sparse depth measurements, e.g. 64 scan lines in the vertical direction. Such a sparse depth sensing is insufficient for real applications like robotic navigation. Thus, estimating dense depth map from the sparse LiDAR input is of great value for both academic research and industrial applications.</p><p>Many recent works <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref> on this topic take deep learning as approach and exploit an additional synchronized RGB image for depth completion. These methods have achieved significantly improvements over conventional approaches <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. For example, Qiu et al. <ref type="bibr" target="#b5">[6]</ref> train a network to estimate surface normal from both the RGB image and LiDAR data and further use the recovered surface normal to guide depth completion. Ma et al. <ref type="bibr" target="#b6">[7]</ref> exploit photo-consistency between neighboring video frames for depth completion. Jaritz et at. <ref type="bibr" target="#b10">[11]</ref> adopt a depth loss as well as a semantic loss for supervision. Despite the different methods proposed by these works, they basically share the same scheme in multi-modal feature fusion. Specifically, these works adopt the operation like concatenation or element-wise addition to fuse the feature vectors from sparse depth and RGB image together directly for further processing. However, the commonly used concatenation or element-wise addition operation is not such appropriate when considering the heterogenous data and the complex environments. The potentiality of RGB image as guidance is difficult to be fully exploited by applying in such a simple way. In contrast, we suggest a more sophisticated fusion module to improve the performance of the depth completion task.</p><p>Our work is inspired by the guided image filtering <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In guided image filtering, the output at a pixel is a weighted average of nearby pixels, where the weights are functions of the guidance image. This strategy has been adopted for generic completion/super-resolution of RGB and range images <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Inspired by the success of guided image filtering, we seek to learn a guided network to automatically generate spatiallyvariant convolution kernels according to the input image and then apply them to extract features from sparse depth image by our guided convolution module. Compared with the hand-crafted function for kernel generation in guided image filtering <ref type="bibr" target="#b12">[13]</ref>, our end-to-end learned network structure has a potential to produce more powerful kernels with agreement of scene context for depth completion. The proposed network architecture. The whole network architecture includes two sub-networks: GuideNet in orange and DepthNet in blue. We add a standard convolution layer at the beginning of both GuideNet and DepthNet as well as the end of DepthNet. The light orange and blue are the encoder stages, while corresponding dark ones are decoder stage of GuideNet and DepthNet, respectively. The ResBlock represents the basic residual block structure with two sequential 3 × 3 convolutional layers from <ref type="bibr" target="#b11">[12]</ref>. and pixels at all the positions share the same kernels, our guided convolutional module has spatially-variant kernels that are automatically generated according to the content. Thus, our network is more powerful to handle various challenging situations in depth completion task. An obvious drawback of using spatially-variant kernels is the large GPU memory consumption, which is also the original motivation of parameter sharing in the convolutional neural network. Especially when applying the spatially-variant convolution module in the multi-stage fusion for depth completion, the massive GPU memory consumption is even unaffordable for computational platforms (See subsection III-C for memory and computation discussion). Thus, it's non-trivial to look for a practical way to make the network available. Inspired by recent network compression technique <ref type="bibr" target="#b17">[18]</ref>, we factorize the convolution operation in our guided convolution module to two stages, a spatially-variant channel-wise convolution stage and a spatially-invariant cross-channel convolution stage. By using such a novel factorization, we get an enormous reduction of GPU memories such that the guided convolution module can be integrated with the powerful encoder-decoder network in multi-stages in a modern GPU device.</p><p>The proposed method is evaluated on both outdoor and indoor datasets, from real-world and synthetic scenes. It outperforms the state-of-the-art methods on KITTI depth completion benchmark and rank 1st at the time of paper submission. Comprehensive ablation studies demonstrate the effectiveness of each component and the fusion strategy used in our method. Compared with other depth completion methods, our method also achieves the best performance on the indoor NYUv2 datset. Last but not least, our model presents strong generalization capability under different depth point densities, various lighting and weather conditions as well as cross-dataset evaluations. Our code will be released at https://github.com/kakaxi314/GuideNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Depending on whether there is an RGB image to guide the depth completion, previous methods can be roughly divided into two categories: depth-only methods and image-guided methods. We briefly review these techniques and other literatures relevant to our network design.</p><p>Depth-only Methods These methods use a sparse or lowresolution depth image as input to generate a full-resolution depth map. Some early methods reconstruct dense disparity maps <ref type="bibr" target="#b7">[8]</ref> or depth maps <ref type="bibr" target="#b8">[9]</ref> based on the compressive sensing theory <ref type="bibr" target="#b7">[8]</ref> or a combined wavelet-contourlet dictionary <ref type="bibr" target="#b8">[9]</ref>. Ku et al. <ref type="bibr" target="#b9">[10]</ref> use a series of hand-crafted conventional operators like dilation, hole closure, hole filling, and blurring, etc., to transform sparse depth maps into dense. More recently, deep learning based approaches demonstrate promising results. Uhrig et al. <ref type="bibr" target="#b4">[5]</ref> propose a sparsity invariant CNN to deal with sparse data or features by using an observation mask. Eldesokey et al. <ref type="bibr" target="#b18">[19]</ref> solve depth completion via generating a full depth as well as a confidence map with normalized convolution. Chodosh et al. <ref type="bibr" target="#b19">[20]</ref> combine compressive sensing with deep learning for depth prediction. The main focus of these methods is to design appropriate operators, e.g. sparsity invariant CNN <ref type="bibr" target="#b4">[5]</ref>, to deal with sparse inputs and propagate these spare information to the whole image.</p><p>In terms of depth super-resolution, some methods exploit a database <ref type="bibr" target="#b20">[21]</ref> of paired low-resolution and high-resolution depth image patches or self-similarity searching <ref type="bibr" target="#b21">[22]</ref> to generate a high resolution depth image. Some methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> further propose to solve depth super-resolution by dictionary learning. Riegler et al. <ref type="bibr" target="#b24">[25]</ref> use a deep network to produce a high-resolution depth map as well as depth discontinuities and feed them into a variational model to refine the depth. Unlike these depth super-resolution methods, which take the dense and regular depth image as input. Instead, the depth input in our method is sparse and irregular, and also we train our model end-to-end without any further optimization or postprocessing.</p><p>Image-guided Methods These methods usually achieve better results, since they utilize an additional RGB image, which provides strong cues on semantic information, edge information, or surface information, etc. Earlier works mainly address depth super-resolution with bilateral filtering <ref type="bibr" target="#b15">[16]</ref>, or global energy minimization <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>, where the depth completion is guided by image <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>, semantic segmentation <ref type="bibr" target="#b28">[29]</ref> or edge information <ref type="bibr" target="#b29">[30]</ref>.</p><p>Recently, Zhang et al. <ref type="bibr" target="#b30">[31]</ref> propose to predict surface normal and occlusion boundary from a deep network and further utilize them to help depth completion in indoor scenes. Qiu et al. <ref type="bibr" target="#b5">[6]</ref> extend a similar surface normal as guidance idea to the outdoor environment and recover dense depth from sparse LiDAR data. Ma et al. <ref type="bibr" target="#b6">[7]</ref> propose a self-supervised network to explore photo-consistency among neighboring video frames for depth completion. Huang et al. <ref type="bibr" target="#b31">[32]</ref> propose three sparsityinvariant operations to deal with sparse inputs. Eldesokey et al. <ref type="bibr" target="#b32">[33]</ref> combine their confidence propagation <ref type="bibr" target="#b18">[19]</ref> with RGB information to solve this problem. Gansbeke et al. <ref type="bibr" target="#b33">[34]</ref> use two parallel networks to predict depth and learn an uncertainty to fuse two results. Cheng et al. <ref type="bibr" target="#b34">[35]</ref> use CNN to learn the affinity among neighboring pixels to help depth estimation.</p><p>Although various approaches have been proposed for depth completion with a reference RGB image, they almost share the same strategy in fusing depth and image features, which is simple concatenation or element-wise addition operation. In this paper, inspired by guided image filtering <ref type="bibr" target="#b12">[13]</ref>, we propose a novel guided convolution module for feature fusion, to better utilize the guidance information from the RGB image.</p><p>Joint Filtering and Guided Filtering Our method is also relevant to joint bilateral filtering <ref type="bibr" target="#b13">[14]</ref> and guided image filtering <ref type="bibr" target="#b12">[13]</ref>. Joint/guided image filtering utilizes a reference or guidance image as prior and aims to transfer the structures from the reference image to the target image for color/depth image super-resolution <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, image restoration <ref type="bibr" target="#b35">[36]</ref>, etc.</p><p>Early joint filtering methods <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref> explore common structures between target and reference images and formulate the problem as iterative energy minimization. Recently, Li et al. <ref type="bibr" target="#b39">[40]</ref> propose a CNNs based joint filtering for image noise reduction, depth upsampling etc., but the joint filtering is implemented as a simple feature concatenation. Gharbi et al. <ref type="bibr" target="#b40">[41]</ref> generate affine parameters by a deep network to perform color transforms for image enhancement. Lee et al. <ref type="bibr" target="#b41">[42]</ref> adopt a similar bilateral learning scheme of <ref type="bibr" target="#b40">[41]</ref> but generate bilateral weights and apply them once on a pre-obtained depth map for depth refinement. In contrast, our guided convolution module works on image features and serves as a flexibly pluggable component in multiple stages of an encoder-decoder network.</p><p>In <ref type="bibr" target="#b42">[43]</ref>, Wu et al. propose a guided filtering layer to perform joint upsampling, which is close to our work. It directly reformulates the conventional guided filter <ref type="bibr" target="#b12">[13]</ref> and make it differentiable as a neural network layer. As a result, the kernel weights are generated by the same close-form equation of guided filter <ref type="bibr" target="#b12">[13]</ref> to filter the input image. This kind of operator is inapplicable to fill-in sparse LiDAR points, as commented by the authors of guided filter in their conference paper <ref type="bibr" target="#b43">[44]</ref>. Our method is also inspired by guided filter <ref type="bibr" target="#b12">[13]</ref>. Rather than generating guided filter kernels from a specific close-form equation, we consider to learns more general and powerful kernels from the guidance image and applies the kernels to fuse multi-modal features for depth completion task.</p><p>Dynamic Filtering On the other hand, in convolutional neural networks, Dynamic Filtering Network (DFN) <ref type="bibr" target="#b44">[45]</ref> is a broad category of methods where the network generates filter kernels dynamically based on the input image to enable operations like local spatial transformation on the input features. The general concept first proposed in <ref type="bibr" target="#b44">[45]</ref> is mainly evaluated on video (and stereo) prediction with previous frames as input.</p><p>Recently, several applications and extensions of DFN have been developed. 'Deformable convolution' <ref type="bibr" target="#b45">[46]</ref> dynamically generates the offsets to the fixed geometric structure which can be seen as an extension of DFN by focusing on the sampling locations. Simonovsky et al. <ref type="bibr" target="#b46">[47]</ref> extends DFN into the graph signals in spatial domain, where the filter weights are dynamically generated for each specific input sample and conditioned on the edge labels. Wu et al. <ref type="bibr" target="#b47">[48]</ref> propose an extension of DFN by using multiple sampled neighbor regions to dynamically generate weights with larger receptive fields.</p><p>Our kernel generating approach shares the same philosophy with DFN and can be considered as a variant and extension, focusing on multi-stage feature fusion of multi-modal data. The spatially-variant kernels generated by DFN <ref type="bibr" target="#b44">[45]</ref> consume large GPU memories and thus are only applied once on low resolution images or features. However, multi-stage feature fusion is critical for feature extraction from sparse depth and color image on the depth completion task, but has not been studied by previous DFN papers. To address it, we design a novel network structure with convolution factorization and further discuss the impact of fusion strategies on depth completion results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED METHOD</head><p>Given a sparse depth map S generated by projecting the LiDAR points to the image plane with calibration parameters and a RGB image I as guidance reference, depth completion aims to produce a dense depth map D of the whole image. The RGB image can provide extremely useful information for depth completion task, as it depicts object boundaries and scene contents.</p><p>To explain our guided convolutional network to upgrade S to D with the guidance of I, we first briefly review the guided image filtering which inspires our guided convolution module in subsection III-A. Then we elaborate the design of the guided convolution module in subsection III-B and introduce a novel convolution factorization in subsection III-C. In the next, we explain how this module can be used in a common encoder-decoder network, and the multi-stage fusion scheme used in our method in subsection III-D. Finally, we give implementation details including hyperparameter settings in subsection III-E.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Guided Image Filtering</head><p>The guided image filtering <ref type="bibr" target="#b12">[13]</ref> generates spatially-variant filters according to a guidance image. In our setting of depth completion task, this method would compute the value at a pixel i in D as a weighted average of nearby pixels from S, i.e.</p><formula xml:id="formula_0">D i = j∈N (i) W ij (I)S j .<label>(1)</label></formula><p>Here, i, j are pixels indexes and N (i) is a local neighborhood of the pixel i. The kernel weights W ij are computed according to the guidance image I and a hand-crafted closed-form equation similar to the matting Laplacian from <ref type="bibr" target="#b48">[49]</ref>. Unless specifically indicating, we omit the index of the image or feature channel for simplifying notations. This guided image filtering might be applied to image superresolution like in <ref type="bibr" target="#b16">[17]</ref>. However, our input LiDAR points are sparse and irregular. As pointed by the authors of <ref type="bibr" target="#b43">[44]</ref>, the guided image filtering cannot work well on sparse inputs. This motivates us to learn more general and powerful filter kernels from the guidance image I, rather than using the hand-crafted function for kernel generation. And then we apply the kernels to fuse the multi-modal features, not directly filtering on the input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Guided Convolution Module</head><p>Here, we elaborate the design of our guided convolution module that generates content-dependent and spatially-variant kernels for depth completion.</p><p>As shown in <ref type="figure">Figure 1</ref>, our guided convolution module would server as a flexibly pluggable component to fuse the features from RGB and depth image in multiple stages. It would generate convolutional kernels automatically from the guidance image feature I and apply them to the sparse depth map feature S. Here, I and S are features extracted from the guidance image I and sparse depth map S respectively. We denote the output from this guided convolution module as D, which is the extracted feature of depth image. Formally,</p><formula xml:id="formula_1">D = W G (I; Θ) ⊗ S,<label>(2)</label></formula><p>where W G is the kernel generated by our network according to the input guidance image feature I, and further depends on the network parameter Θ. Here, ⊗ indicates the convolution operation. <ref type="figure" target="#fig_2">Figure 2</ref> (a) illustrates the design of our learnable guided convolution module. There is a 'Kernel-Generating Layer' (KGL) to generate the kernel W G according to the image features I. The parameters of the KGL are Θ. We can employ any differentiable operations for this KGL in principle. Since we deal with grid images, convolution layers are preferable for this task. Thus, the most naïve implementation is to directly apply convolution layers to generate all the kernel weights required for convolution operation on the depth feature map. Please note that the kernel W G is content-dependent and spatially-variant. Content-dependent means the guided kernel W G is dynamically generated, depending on the image content I. Spatially-variant means different kernels are applied to different spatial positions of the sparse depth feature S. In comparison, Θ is fixed spatially and across different input images once it is learned.</p><p>The advantages of content-dependent and spatially-variant kernels are two-folds. Firstly, this kind of kernels allows the network to apply different filters to different objects (and different image regions). It is useful because, for example, the depth distribution on a car would be different from that on the road (also, nearby and faraway cars own different depth distributions). Thus, generating the kernels dynamically according to the image content and spatial position would be helpful. Secondly, during training, the gradient of a spatiallyinvariant kernel is computed as the average over all image pixels from the next layer. Such an average is more likely leading to gradient closing to zero, even thought the learned kernel is far from optimal for every position, which could generate sub-optimal results as pointed by <ref type="bibr" target="#b47">[48]</ref>. In comparison, spatially variant kernels can alleviate this problem and make the training better behaved, which towards to stronger results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convolution Factorization</head><p>However, generating and applying these spatially-variant kernels naïvely would consume a large amount of GPU memory and computation resources. The enormous GPU memory consumption is unaffordable for modern GPU device, when integrating the guided convolution module into multistage fusion of an encoder-decoder network. To address this challenge, inspired by recent network compression techniques, e.g. MobileNets <ref type="bibr" target="#b17">[18]</ref>, we design a novel factorization as well as a matched network structure to split the guided convolution module into two stages for better memory and computation efficiency. This step is critical to make the network practical.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref> (b), the first stage is a channelwise convolution layer where the m-th channel of the depth feature S m is convolved with the corresponding channel of the generated filter kernel W G m . These convolutions are still spatially-variant. The output depth feature D m after the first stage then becomes</p><formula xml:id="formula_2">D m = W G m (I; Θ ) ⊗ S m ,<label>(3)</label></formula><p>where Θ and W G are the KGL parameter and the guided kernel in the first stage, respectively. In this stage, the KGL is implemented by a standard convolution layer. The second stage is a cross-channel convolution layer where a 1 × 1 convolution aggregates features across different channels. This stage is still content-dependent but spatiallyinvariant. The kernel weights are also generated from the guidance image feature I, but are shared among all pixels. Specifically, we first use an average pooling over the guidance image feature I at each channel individually to obtain an intermediate image feature I with size M × 1 × 1, where M is the number of channels of I. We then feed I into a fullyconnected layer to generate the guided kernel W G , whose</p><formula xml:id="formula_3">size is M × N × 1 × 1,</formula><p>where N is the number of channels of the dense depth feature D. Finally, we apply W G to the depth feature D from the channel-wise convolution layer to obtain the final depth feature D. Formally,</p><formula xml:id="formula_4">D = W G (I ; Θ ) ⊗ D ,<label>(4)</label></formula><p>where Θ is the parameter in the fully-connected layer. In Equation <ref type="formula" target="#formula_4">(4)</ref>, W G is spatially invariant and shared by all pixels. The convolution applied to D is a 1 × 1 convolution to aggregate this M -channel features to a N -channel features in D and can be executed quickly.</p><p>Memory &amp; Computation Efficiency Analysis. Now, we analyze the improvement of this two-stage strategy in terms of memory and computation efficiency. If the convolution operations ⊗ in Equqation <ref type="formula" target="#formula_1">(2)</ref> is implemented naïvly, the target depth feature D p,n at a pixel p and the channel n can be formalized explicitly as</p><formula xml:id="formula_5">D p,n = m k W G p,k,m,n (I; Θ) · S p+k,m ,<label>(5)</label></formula><p>where k is the offset in a K ×K filter kernel window centered at p and m is the channel index of S. Suppose the height and width of the input depth feature S are H and B respectively. It is easy to figure out that the size of the generated kernel is (M × N × K 2 × H × B). In an encoder-decoder network, H and B are usually very large in the initial scales of the encoder or the end scales of the decoder. M and N usually go up to hundreds or even thousands in the latent space. Hence, the memory consumption is high and unaffordable even for modern GPUs. By our convolution factorization, we split convolution in Equation <ref type="formula" target="#formula_5">(5)</ref> into a channel-wise convolution in Equation <ref type="formula" target="#formula_2">(3)</ref> and a cross-channel convolution in Equation <ref type="bibr" target="#b3">(4)</ref>. We can explicitly re-formulate these two equations in detail as</p><formula xml:id="formula_6">D p,m = k W G p,k,m (I; Θ ) · S p+k,m<label>(6)</label></formula><p>and</p><formula xml:id="formula_7">D p,n = m W G m,n (I ; Θ ) · D p,m ,<label>(7)</label></formula><p>The computation complexities of Equation <ref type="formula" target="#formula_6">(6)</ref> and Equation <ref type="bibr" target="#b6">(7)</ref> are O(K 2 ) and O(M ) respectively. Therefore, by this novel convolution factorization, we reduce the computational</p><formula xml:id="formula_8">complexity of D p,n from O(M × K 2 ) to O(M + K 2 ).</formula><p>Moreover, the proposed factorization can reduce GPU memory consumption enormously. This is extremely important for networks with multi-stage fusions. Suppose the memory consumption by the proposed factorization and naïve convolution are M f and M s respectively, then</p><formula xml:id="formula_9">M f M s = M × K 2 × H × B + M × N M × N × K 2 × H × B = 1 N + 1 K 2 × H × B .<label>(8)</label></formula><p>As an example, when using 4-byte floating precision and taking M = N = 128, H = 64, B = 304, and K = 3, which is the setting of the second fusion stage of our network, the proposed two-stage convolution reduces GPU memory from 10.7GB to 0.08GB, nearly 128 times lower for just a single layer. In this way, our guided convolution module can be applied on multiple scales of a network, e.g. in an encoderdecoder network. <ref type="figure">Figure 1</ref> illustrates the overall structure of the proposed network, which is based on two encoder-decoder networks with skip layers. Here, we refer the two networks taking the RGB image I and sparse LiDAR depth image S as GuideNet and DepthNet respectively. The GuidedNet aims to learn hierarchical feature representations with both lowlevel and high-level information from RGB image. Such image features are used to generate spatially-variant and contentdependent kernels automatically for depth feature extractions. The DepthNet takes the LiDAR depth image as input and progressively fuse hierarchical image features by the guided convolution module in encoder stage. It then regresses dense depth image at the decoder stage. Both encoders of Guided-Net and DepthNet consist of a trail of ResNet blocks <ref type="bibr" target="#b11">[12]</ref>. Convolution layer with stride is used to aggregate feature to low resolution in encoder stage, and deconvolution layer in decoder stage upsamples the feature map to high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Architecture</head><p>We also add standard convolution layers at the beginning of both GuideNet and DepthNet as well as the end of DepthNet.</p><p>Please note that during feature fusion, instead of the early or late fusion scheme widely used in the existing methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b33">[34]</ref>, we utilize a novel fusion scheme which fuse the decoder features of the GuidedNet to the encoder features of the DepthNet. In our network, image features act as guidance for the generation of depth feature representations. Thus, compared with encoder features, features from the decoder stage in the GuideNet are preferable, as they own more highlevel context information. In addition, in contrast to fuse only once, we fuse the two sources in multi-stage, which shows stronger and more reliable results. More comparisons and analyses can be found in subsection IV-D.</p><p>E. Implementation Details 1) Loss Function: During training, we adopt the mean squared error (MSE) to compute the loss between ground truth and predicted depth. For real-world data, the ground truth depth is often semi-dense, because it is difficult to collect ground truth depth for every pixel. Therefore, we only consider valid pixels in the reference ground truth depth map when computing the training loss. The final loss function is</p><formula xml:id="formula_10">L = p∈Pv D gt p − D p 2 ,<label>(9)</label></formula><p>where P v represents the set of valid pixels. D gt p and D p denote the ground truth and predicted depth at the pixel p, respectively.</p><p>2) Training Setting: We use ADAM <ref type="bibr" target="#b49">[50]</ref> as the optimizer with a starting learning rate of 10 −3 and weight decay of 10 −6 . The learning rate drops by half every 50k iterations. We utilize 2 GTX 1080Ti GPUs for training with batch size of 8. Synchronized Cross-GPU Batch Normalization <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> is used in the network training stage. Our method is trained end-to-end FROM SCRATCH. In contrast, some stateof-the-art methods employ extra datasets for training, e.g. DeepLiDAR <ref type="bibr" target="#b5">[6]</ref> utilizes synthetic data to train the network for obtaining scene surface normal, and the authors of <ref type="bibr" target="#b33">[34]</ref> use a pretrained model on Cityscapes 1 as network initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct comprehensive experiments to verify our method on both outdoor and indoor datasets, captured in realworld and synthetic scenes. We first introduce all the datasets and evaluation metrics used in our experiments in subsection IV-A and IV-B respectively. Then, as autonomous driving is the major application of depth completion, we compare our method with the state-of-the-art methods on the outdoor scene KITTI dataset in subsection IV-C. It follows by extensive ablation studies on the KITTI validation set in subsection IV-D to investigate the impact of each network component and the fusion scheme used in our method. In subsection IV-E, we verify the performance of proposed method on the indoor scene NYUv2 dataset. Finally, in subsection IV-F, we perform experiments under various settings including input depth with different densities, RGB images captured under various lighting and weather conditions and cross-dataset evaluations to prove generalization capability of our method. 1 https://www.cityscapes-dataset.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI Dataset</head><p>The KITTI depth completion dataset <ref type="bibr" target="#b4">[5]</ref> contains 86, 898 frames for training, 1, 000 frames for validation, and another 1, 000 frames for testing. It provides public leaderboard 2 for ranking submissions. The ground truth depth is generated by registering LiDAR scans temporally. These registered points are further verified with the stereo image pairs to get rid of noisy points. As there are rare LiDAR points at the top of an image, following <ref type="bibr" target="#b33">[34]</ref>, input images are cropped to 256 × 1216 for both training and testing.</p><p>Virtual KITTI Dataset Virtual KITTI dataset <ref type="bibr" target="#b52">[53]</ref> is a synthetic dataset, where the virtual scenes are cloned from the real world KITTI video sequences. Besides the 5 virtual image sequences cloned from KITTI sequence, it also generates the corresponding image sequences under various lighting conditions (like morning, sunset) and weather conditions (like fog, rain), totally 17,000 image frames. To generate sparse LiDAR points, instead of random sampling from the dense depth map, we use the sparse depth of the corresponding image frame in KITTI dataset as a mask to obtain sparse samples from dense ground truth depth, which makes the distribution of sparse depth on image is close to real-world situation. We split the whole Virtual KITTI dataset to train and test set to finetune and evaluate our model respectively. Since the destination is to verify the robustness of our model under various lighting and weather condition, we only fine-tune our model under the original 'clone' condition whose weather is good, using sequence of '0001', '0002', '0006' and '0018' for training. And the sequence '0020' with various weather and lighting conditions is used for evaluation. In summary, we have 1289 frames for fine-tuning and 837 frames for each condition to evaluate.</p><p>NYUv2 Dataset NYUv2 dataset <ref type="bibr" target="#b56">[57]</ref> consists of RGB images and depth images captured by Microsoft Kinect in 464 indoor scenes. Following the similar setting of previous depth completion methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b53">[54]</ref>, our method is trained on 50k images uniformly sampled from the training set, and tested on the 654 official labeled test set for evaluation. As a preprocessing, the depth values are in-painted using the official toolbox, which adopts the colorization scheme <ref type="bibr" target="#b57">[58]</ref> to fillin missing values. For both train and test set, the original frames of size 640 × 480 are half down-sampled with bilinear interpolation, and then center-cropped to 304×228. The sparse input depth is generated by random sampling from the dense ground truth. Due to the input resolution for our network must be a multiple of 32, we futher pad the images to 320 × 256 as input for our method but evaluate only the valid region of size 304 × 228 to keep fair comparison with other methods.</p><p>SUN RGBD Dataset The SUN RGBD dataset <ref type="bibr" target="#b58">[59]</ref> is an indoor dataset containing RGB-D images from many other datasets <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. We only use SUN RGBD dataset for cross-dataset evaluation. Since NYUv2 dataset is a subset of SUN RGBD dataset, we exclude them in evaluation to avoid repetition. We keep all images with the same resolution of NYUv2 dataset as 640 × 480, captured under different scenes. Totally, we evaluate our model on 3944 image frames, with  <ref type="bibr" target="#b53">[54]</ref>, 'DDP' <ref type="bibr" target="#b54">[55]</ref>, 'DeepLiDAR' <ref type="bibr" target="#b5">[6]</ref>, 'CSPN' <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b55">[56]</ref> and 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref>. In the zoomed regions, our method recovers better 3D details. 555 frames captured by Kinect V1 and 3389 captured by Asus Xtion camera. The same pre-processing method for NYUv2 dataset is used to fill depth map. Note, frames captured by Asus Xtion camera are more challenging, because the data comes from a different device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>Following the KITTI benchmark and exiting depth completion methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b34">[35]</ref>, for outdoor scene, we use these four standard metrics for evaluation: root mean squared error (RMSE), mean absolute error (MAE), root mean squared error of the inverse depth (iRMSE) and mean absolute error of the inverse depth (iMAE). Among them, RMSE and MAE directly measure depth accuracy, while RMSE is more sensitive and chosen as the dominant metric to rank submissions on the KITTI leaderboard. iRMSE and iMAE compute the mean error of inverse depth, which gives less weight for far-away points.</p><p>For indoor scene, to be consistent with comparative depth completion methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>, the evaluation metrics are selected as root mean squared error (RMSE), mean absolute relative error (REL) and δ i which means the percentage of predicted pixels where the relative error is less a threshold i. Specifically, i is chosen as 1.25, 1.25 2 and 1.25 3 separately for evaluation. Here, a higher i indicates a softer constraint and a higher δ i represents a better prediction. RMSE is chosen as the primary metric for all the experiment evaluations as it is sensitive to large errors on distant regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on KITTI Dataset</head><p>We first evaluate our method on the KITTI depth completion dataset <ref type="bibr" target="#b4">[5]</ref>. Our method is trained end-to-end from scratch on the train set and compared the performance with stateof-the-art methods on test set. <ref type="table" target="#tab_0">Table I</ref> lists the quantitative comparison of our method and other top-ranking published methods on the KITTI leaderboard. Our method ranks 1st and exceed all other methods under the primary RMSE metric at the time of paper submission, and presents comparable performance on other evaluation metrics. <ref type="figure" target="#fig_3">Figure 3</ref> shows some visual comparison results with several state-of-the-art methods on the KITTI test set. Our results are shown in the last row. While all methods provide visually plausible results in general, our estimated depth maps reveal more details and are more accurate around object boundaries. For example, our method can better recover depth of background between the arms of a person as highlighted by the magenta circle in <ref type="figure" target="#fig_3">Figure 3</ref>. The predicted depth of our method owns the most accurate contour in the black car region.</p><p>Furthermore, to verify whether the guided convolution Image Guided kernel <ref type="figure">Fig. 4</ref>. Visualization of the guided kernels, where a kernel is visualized as a 2D vector by applying the Prewitt operator <ref type="bibr" target="#b61">[62]</ref>. Similar pixels tend to have the similar kernels.</p><p>module really learns content-dependent and spatially-variant information to benefit depth completion, we visualize one selected channel of the guided kernels W G from the most early fusion stage in <ref type="figure">Figure 4</ref>. This is done by applying the Prewitt operator [62] on each K×K kernel to get the weighted sum of x-axis shift and y-axis shift, respectively. We then obtain a 2D vector at each pixel and visualize it by a color code, like the way optical flow is visualized. We can easily see that the boundary with similar gradient direction or surface with similar normal direction share similar color code. Please note that the method used here to visualize the guided kernels is extremely rough due to the difficulty of kernel weight interpretation in deep neural networks. Also, the network is only supervised by semi-dense depth, it's almost impossible for each object has it own color code in visualization without direct semantic supervision, as semantic information is defined by human beings and owns little relationship with the depth supervision. To some extent, this visualization confirms the guided kernels are consistent with image content. Hence the guided kernels are likely helpful for depth completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>To investigate the impact of each network component and fusion scheme on the final performance, we conduct ablation studies on the KITTI validation dataset. Specifically, we evaluate several different variations of our network. The quantitative comparisons are summarized in <ref type="table" target="#tab_0">Table II.</ref> 1) Comparison with Feature Addition/Concatenation: Existing methods often use addition or concatenation for multimodality feature fusion. To compare with them, we replace all the guided convolution modules in our network by feature addition or concatenation but keep the other network components and settings unchanged. The results are indicated as 'Add.' and 'Concat.' respectively. Compared with our guided convolution module, the simple feature addition or concatenation significantly worsen the results, with the RMSE increasing 31.59 mm and 24.35 mm respectively.</p><p>We can see that the results of 'Add.' is a slightly worse than that of 'Concat.'. This is also reasonable, because image and depth features are heterogeneous data from different sources. By applying addition, we implicitly treat these two different features in the same way, which leads to performance drops. Indeed, most of state-of-the-art methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[33]</ref> adopt concatenation to fuse the heterogeneous depth and image features while apply addition to fuse homogeneous depth features from different stages.</p><p>2) Fusion Scheme of GuideNet and DepthNet: As described in subsection III-D, instead of using early or late feature fusion like existing methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b33">[34]</ref>, our approach fuses the decoder features of the GuideNet to the encoder features of the DepthNet. To verify the effectiveness of such a fusion scheme, we train and evaluate the performance of fusing the decoder features of the GuideNet to the decoder features of the DepthNet (referred as 'D-D Fusion') and fusing the encoder features of the GuideNet to the encoder features of the DepthNet (referred as 'E-E Fusion'). In the later one, the decoder structure of the GuideNet is removed since it is not used anymore. In this way, our method can be seen as 'D-E Fusion'. <ref type="table" target="#tab_0">Table II</ref> compares the results of 'E-E Fusion' and 'D-D Fusion' with our method. The performance drop of the 'E-E Fusion' verifies our earlier analysis that the decoder image features own more high-level context information thus can better guide depth feature extraction. The 'D-D Fusion', fusing image and depth features in the decoder stage, suffers from even larger performance drop. Comparing the 'D-D Fusion' and our final model, we conclude that the image guidance is more effective at encoder stage of depth feature extraction. It's also reasonable and easy to understand, as feature extracted in early stage can influence the following feature extraction, especially for sparse depth image.</p><p>On the other hand, even the weaker fusion strategy in the 'E-E Fusion' outperforms conventional feature addition or concatenation. This attributes to our guided convolution module that can generate content-dependent and spatiallyvariant kernels to promote the depth completion. This obser-  <ref type="bibr" target="#b53">[54]</ref> and 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref> on NYUv2 test set. We present the results of these three methods under 200 samples and 500 samples. Depth images are showed as grey images for clear visualization. The most notable regions are selected with cyan rectangles for easy comparisons. vation further proves the effectiveness of the proposed guided convolution module.</p><p>3) Fusion Scheme of Multi-stage Guidance: We also design two other variants to verify the effectiveness of multi-stage guidance scheme. For comparison, based on our guided network, we replace all the guided modules with concatenation except the one in the first fusion stage, and refer it as 'First Guide'. From the same view, we use 'Last Guide' to refer the condition only the guided module in the last fusion stage is remained. Using concatenation for the feature fusion of other stages is from the result, that concatenation can perform a little better than addition operation as shown in <ref type="table" target="#tab_0">Table II</ref>.</p><p>We can see that both the results of 'First Guide' and 'Last Guide' are worse than our multi-stage guidance scheme. This demonstrates the effectiveness of our multistage guidance design. Also, the 'First Guide' performs a little bit better than 'Last Guide'. It also consists with our early analysis that image guidance is more effective at early stage, since feature extracted in early stage can influence the following feature extraction. Moreover, both the results of 'First Guide' and 'Last Guide' perform better than the 'Concat.'. It once more verifies that the designed Guided Convolution Module is a much powerful fusion scheme for depth completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments on NYUv2 Dataset</head><p>To verify the performance of our method on indoor scene, we directly train and evaluate our guided network on the NYUv2 dataset <ref type="bibr" target="#b56">[57]</ref>, without any specific modification. Following existing methods, we train and evaluate our method with the settings of 200 and 500 sparse LiDAR samples separately. The quantitative comparisons with other methods are shown in <ref type="table" target="#tab_0">Table III</ref>. The results of 'Bilateral' <ref type="bibr" target="#b56">[57]</ref>, and 'CSPN' <ref type="bibr" target="#b34">[35]</ref> come from the CSPN <ref type="bibr" target="#b34">[35]</ref>. The results of 'TGV' <ref type="bibr" target="#b27">[28]</ref>, 'Zhang et al.' <ref type="bibr" target="#b30">[31]</ref> and 'DeepLiDAR' <ref type="bibr" target="#b5">[6]</ref> are obtained from DeepLiDAR <ref type="bibr" target="#b5">[6]</ref>. By using the released implementations, we get the results of 'Ma et al.' <ref type="bibr" target="#b53">[54]</ref> with 500 samples and 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref> with 200 samples. We can see from the results, our method outperforms all other methods in both settings of 500 samples and 200 samples. Without specific modification, our method ranks top under all these 5 evaluation metrics. The performance of our method and the 'Sparse-to-Dense' <ref type="bibr" target="#b6">[7]</ref> degrades more gently comparing to that of 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref>.</p><p>We also show some qualitative comparisons on the test set in <ref type="figure" target="#fig_4">Figure 5</ref>. Our method is compared with 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref> and 'Ma et al.' <ref type="bibr" target="#b53">[54]</ref> on the settings of 200 samples and 500 samples. The most notable regions are selected with cyan rectangles for easy comparisons. From the predicted depth, we can see the results of 'Ma et al.' over-smooth the whole image and blur small objects. Even though 'NConv-CNN' shows much clear depth predictions, it also suffers obvious detail loss at object structures, especially the thin object boundaries. Our method show sharp transitions aligning to local details and generate the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Generalization Capability</head><p>To prove the generalization capability of our method, we test its performance under different point densities, various lighting and weather conditions as well as cross-dataset evaluations.</p><p>1) Different Point Densities: We test the performance of our method under different point densities. Our model is the same one trained from scratch only on the KITTI train set without any fine-tuning, to faithfully reflect its generalization capability. For a comparison, we also evaluate another two state-of-the-art methods, 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref> and 'Sparse-to-Dense' <ref type="bibr" target="#b6">[7]</ref>, using their open-source code and the best performed model trained by their authors.</p><p>Firstly, we vary the LiDAR input with 5 different levels of density on the KITTI validation set. The KITTI dataset is captured with a 64-line Velodyne LiDAR. However, real industrial applications may only adopt a 32-line or even 16line LiDAR considering the high sensor cost. To analyze the impact of the sparsity level on the final result, we test with 5 different levels of LiDAR density on the KITTI validation dataset, where the input LiDAR points are randomly sampled according to a given ratio. Specifically, the density ratios of 0.2, 0.4, 0.6, 0.8 and 1.0 are adopted in our evaluation. <ref type="figure" target="#fig_5">Figure 6</ref> shows the RMSE of our network, 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref> and 'Sparse-to-Dense' <ref type="bibr" target="#b6">[7]</ref>   <ref type="bibr" target="#b32">[33]</ref> shows significant performance drop and its RMSE increases quickly. In comparison, our method and the 'Sparseto-Dense' <ref type="bibr" target="#b6">[7]</ref>, on the other hand, degrade gradually and are consistently better than the 'NConv-CNN' <ref type="bibr" target="#b32">[33]</ref>. The results demonstrate the strong generalization capability of our method under various LiDAR points density ratios.</p><p>2) Various Lighting and Weather Conditions: KITTI dataset is collected in the similar lighting condition and in good weather condition. However, varied weather and lighting conditions always occur in practice and may bring the potential impact on the performance of depth completion. To verify whether our guided network can still work well in these kinds of challenging situations, we conduct evaluation experiments on Virtual KITTI dataset <ref type="bibr" target="#b52">[53]</ref> with various lighting (e.g., sunset) and weather (e.g., fog) conditions, and compare our method with other two variants of 'Add.' and 'Concat' introduced in subsection IV-D. Based on the trained model on KITTI dataset, we fine-tune our method under good 'clone' condition, then test its performance under various lighting and weather condition in a different sequence.</p><p>We evaluate our methods and two variants under the 'clone', 'fog', 'morning', 'overcast', 'rain' and 'sunset' conditions separately. <ref type="figure">Figure 7</ref> depicts the results of three methods under various conditions. We can easily find, compared with 'Add.' and 'Concat', our method achieves the best RMSE among all the conditions. Also, the RMSE results of our method keep stable across all the situations, which can verify the generalization capability of our method under various lighting and weather conditions.</p><p>3) Cross-dataset Evaluation: In order to show the generalization of our method, we also conduct cross-dataset evaluations by using the models trained on NYUv2 dataset to directly test on SUN RGBD dataset <ref type="bibr" target="#b58">[59]</ref>.</p><p>The comparison results are listed in <ref type="table" target="#tab_0">Table IV and Table V  for</ref>   NYUv2 dataset. We can see our method still outperforms other methods with the best RMSE and reports close results with NYUv2 dataset. The results demonstrate the strong cross-dataset generalization capability of our method. We also present some quantitative results in <ref type="figure" target="#fig_6">Figure 8</ref>. The first three rows selected in red rectangle are results on images captured by Kinect V1, and the last three rows in green rectangle are results from Xtion. The priority of our method can be found easily from the predicted depth, especially the selected regions. By comparing the results in <ref type="table" target="#tab_0">Table III, Table IV and Table V</ref>, we can find that all these three methods yield a little worse results on the dataset collected by Xtion, which may be caused by different camera intrinsic parameters and the extrinsic parameters between image sensor and depth sensor. How to design method with better generalization capability between different devices is an interesting direction for the future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We propose a guided convolutional network to recover dense depth from sparse and irregular LiDAR points with an RGB image as guidance. Our novel guided network can dynamically predict content-dependent and spatially-variant kernel weights according to the guidance image to facilitate depth completion. We further design a convolution factorization to reduce GPU memory consumption such that our guided convolution module can be applied in powerful encoderdecoder network with multi-stage fusion scheme. Extensive experiments and ablation studies verify the superior performance of our guided convolutional network and the effectiveness of the feature fusion strategy on depth completion. Our method not only shows strong results on both indoor and outdoor scenes, but also presents strong generalization capability under different point densities, various lighting and weather conditions as well as cross-dataset evaluations. While this paper specifically focuses on the problem of depth completion, we believe that other tasks in computer vision involving multisources as input can also benefit from the design of our guided convolution module and the fusion scheme in our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Compared with standard convolutional module, where the kernel is spatially-invariant arXiv:1908.01238v1 [cs.CV] 3 Aug 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Guided Convolution Module. (a) shows the overall pipeline of guided convolution module. Given image features I as input, filter generation layer dynamically produces guided kernels W G (including W G and W G ), which are further applied on input depth features S and output new depth features D. (b) shows the details of convolution between guided kernels W G and input depth features S. We factorize it into two-stage convolutions: channel-wise convolution and cross-channel convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative comparison with state-of-the-art methods on KITTI test set. The results are from the KITTI depth completion leaderboard in which depth images are colorized along with depth range. Our results are shown in the bottom row and compared with top-ranking methods 'Sparse-to-Dense'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative comparison with 'Ma et al.'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>RMSE (in mm) under different levels of input LiDAR point density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>dataset captured by Kinect V1 and Asus Xtion camera respectively. Both settings of 500 samples and 200 samples are evaluated by using the comparison models trained on Qualitative comparison with 'Ma et al.' [54] and 'NConv-CNN' [33] on SUN RGBD dataset. Images in red rectangle are captured by Kinect V1 and Images in green rectangle are collected by Xtion. Depth results of these three methods under 200 samples and 500 samples are showed as grey images for clear visualization. The most notable regions are selected with cyan rectangles for easy comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc>Performance on the KITTI dataset. THE RESULT IS EVALUATED BY THE KITTI TESTING SERVER AND DIFFERENT METHODS ARE RANKED BY THE RMSE (IN mm).</figDesc><table><row><cell></cell><cell>RMSE</cell><cell>MAE</cell><cell cols="2">iRMSE iMAE</cell></row><row><cell>CSPN [35], [56]</cell><cell cols="2">1019.64 279.46</cell><cell>2.93</cell><cell>1.15</cell></row><row><cell>DDP [55]</cell><cell>832.94</cell><cell>203.96</cell><cell>2.10</cell><cell>0.85</cell></row><row><cell>NConv-CNN [33]</cell><cell>829.98</cell><cell>233.26</cell><cell>2.60</cell><cell>1.03</cell></row><row><cell>Sparse-to-Dense [7]</cell><cell>814.73</cell><cell>249.95</cell><cell>2.80</cell><cell>1.21</cell></row><row><cell>RGB certainty [34]</cell><cell>772.87</cell><cell>215.02</cell><cell>2.19</cell><cell>0.93</cell></row><row><cell>DeepLiDAR [6]</cell><cell>758.38</cell><cell>226.50</cell><cell>2.56</cell><cell>1.15</cell></row><row><cell>Ours</cell><cell>736.24</cell><cell>218.83</cell><cell>2.25</cell><cell>0.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc>Ablation study on KITTI's validation set. SEE TEXT IN SUBSECTION IV-D FOR MORE DETAILS.</figDesc><table><row><cell></cell><cell>RMSE</cell><cell>MAE</cell><cell cols="2">iRMSE iMAE</cell></row><row><cell>Add.</cell><cell cols="2">809.37 233.18</cell><cell>3.98</cell><cell>1.11</cell></row><row><cell>Concat.</cell><cell cols="2">802.13 226.87</cell><cell>2.53</cell><cell>1.02</cell></row><row><cell>E-E Fusion</cell><cell cols="2">783.35 222.43</cell><cell>2.51</cell><cell>1.01</cell></row><row><cell>D-D Fusion</cell><cell cols="2">795.64 223.95</cell><cell>6.73</cell><cell>1.15</cell></row><row><cell cols="3">First Guide 799.03 224.27</cell><cell>2.66</cell><cell>1.01</cell></row><row><cell>Last Guide</cell><cell cols="2">800.60 226.07</cell><cell>2.68</cell><cell>1.03</cell></row><row><cell>Ours</cell><cell cols="2">777.78 221.59</cell><cell>2.39</cell><cell>1.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc>Performance on the NYUv2 dataset. BOTH SETTINGS OF 200 SAMPLES AND 500 SAMPLES ARE EVALUATED.</figDesc><table><row><cell cols="2">samples method</cell><cell cols="3">RMSE↓ REL↓ δ1.25↑</cell><cell>δ 1.25 2 ↑</cell><cell>δ 1.25 3 ↑</cell></row><row><cell></cell><cell>Bilateral [57]</cell><cell>0.479</cell><cell>0.084</cell><cell>92.4</cell><cell>97.6</cell><cell>98.9</cell></row><row><cell></cell><cell>TGV [28]</cell><cell>0.635</cell><cell>0.123</cell><cell>81.9</cell><cell>93.0</cell><cell>96.8</cell></row><row><cell></cell><cell>Zhang et al. [31]</cell><cell>0.228</cell><cell>0.042</cell><cell>97.1</cell><cell>99.3</cell><cell>99.7</cell></row><row><cell>500</cell><cell>Ma et al. [54]</cell><cell>0.204</cell><cell>0.043</cell><cell>97.8</cell><cell>99.6</cell><cell>99.9</cell></row><row><cell></cell><cell>NConv-CNN [33]</cell><cell>0.129</cell><cell>0.018</cell><cell>99.0</cell><cell>99.8</cell><cell>100</cell></row><row><cell></cell><cell>CSPN [35]</cell><cell>0.117</cell><cell>0.016</cell><cell>99.2</cell><cell>99.9</cell><cell>100</cell></row><row><cell></cell><cell>DeepLiDAR [6]</cell><cell>0.115</cell><cell>0.022</cell><cell>99.3</cell><cell>99.9</cell><cell>100</cell></row><row><cell></cell><cell>Ours</cell><cell>0.101</cell><cell>0.015</cell><cell>99.5</cell><cell>99.9</cell><cell>100</cell></row><row><cell></cell><cell>Ma et al. [54]</cell><cell>0.230</cell><cell>0.044</cell><cell>97.1</cell><cell>99.4</cell><cell>99.8</cell></row><row><cell>200</cell><cell>NConv-CNN [33]</cell><cell>0.173</cell><cell>0.027</cell><cell>98.2</cell><cell>99.6</cell><cell>99.9</cell></row><row><cell></cell><cell>Ours</cell><cell>0.142</cell><cell>0.024</cell><cell>98.8</cell><cell>99.8</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>under various LiDAR</figDesc><table><row><cell></cell><cell>2200</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Add.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Concat.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2000</cell><cell>Ours</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1800</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RMSE(mm)</cell><cell>1600</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1400</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1200</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>clone</cell><cell>fog</cell><cell>morning overcast</cell><cell>rain</cell><cell>sunset</cell></row><row><cell cols="6">Fig. 7. RMSE (in mm) on Virtual KITTI test set under various lighting and</cell></row><row><cell cols="6">weather conditions. Our guided network are compared with the 'Add.' and</cell></row><row><cell cols="3">'Concat' variants.</cell><cell></cell><cell></cell></row><row><cell cols="6">point densities. With the density decreasing, the 'NConv-</cell></row><row><cell cols="2">CNN'</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc>Performance on the SUN RGBD dataset collected by Kinect V1. THE EVALUATION FRAMES ARE CAPTURED WITH SAME DEVICE AS NYUV2</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DATASET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>samples</cell><cell>method</cell><cell>RMSE↓</cell><cell>REL↓</cell><cell cols="3">δ1.25↑ δ 1.25 2 ↑ δ 1.25 3 ↑</cell></row><row><cell></cell><cell>Ma et al. [54]</cell><cell>0.180</cell><cell>0.053</cell><cell>97.0</cell><cell>99.3</cell><cell>99.7</cell></row><row><cell>500</cell><cell>Nconv-CNN [33]</cell><cell>0.119</cell><cell>0.019</cell><cell>98.7</cell><cell>99.7</cell><cell>99.9</cell></row><row><cell></cell><cell>Ours</cell><cell>0.096</cell><cell>0.020</cell><cell>99.0</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell></cell><cell>Ma et al. [54]</cell><cell>0.206</cell><cell>0.044</cell><cell>97.1</cell><cell>99.4</cell><cell>99.8</cell></row><row><cell>200</cell><cell>Nconv-CNN [33]</cell><cell>0.159</cell><cell>0.029</cell><cell>97.8</cell><cell>99.4</cell><cell>99.8</cell></row><row><cell></cell><cell>Ours</cell><cell>0.139</cell><cell>0.036</cell><cell>97.6</cell><cell>99.5</cell><cell>99.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc>Performance on the SUN RGBD dataset collected by Xtion. THE EVALUATION FRAMES ARE CAPTURED WITH DIFFERENT DEVICE FROM</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NYUV2 DATASET.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">samples method</cell><cell cols="3">RMSE↓ REL↓ δ1.25↑</cell><cell>δ 1.25 2 ↑</cell><cell>δ 1.25 3 ↑</cell></row><row><cell></cell><cell>Ma et al. [54]</cell><cell>0.206</cell><cell>0.050</cell><cell>97.0</cell><cell>99.3</cell><cell>99.8</cell></row><row><cell>500</cell><cell>Nconv-CNN [33]</cell><cell>0.136</cell><cell>0.020</cell><cell>98.6</cell><cell>99.6</cell><cell>99.9</cell></row><row><cell></cell><cell>Ours</cell><cell>0.119</cell><cell>0.020</cell><cell>98.9</cell><cell>99.8</cell><cell>99.9</cell></row><row><cell></cell><cell>Ma et al. [54]</cell><cell>0.238</cell><cell>0.055</cell><cell>95.8</cell><cell>99.0</cell><cell>99.7</cell></row><row><cell>200</cell><cell>Nconv-CNN [33]</cell><cell>0.180</cell><cell>0.030</cell><cell>97.6</cell><cell>99.4</cell><cell>99.8</cell></row><row><cell></cell><cell>Ours</cell><cell>0.160</cell><cell>0.032</cell><cell>97.9</cell><cell>99.5</cell><cell>99.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.cvlibs.net/datasets/kitti/eval depth.php?benchmark</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of stereo matching costs on images with radiometric differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1582" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1592" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-supervised sparseto-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00275</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dense disparity maps from sparse disparity measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleinsteuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2126" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth reconstruction from sparse samples: Representation, algorithm, and sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1983" to="1996" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Conference on Computer and Robot Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial-depth super resolution for range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint geodesic upsampling of depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Patch based synthesis for single depth image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth super resolution by rigid body self-similarity in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hornacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1123" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational depth superresolution using example-based edge representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint super resolution and denoising from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1525" to="1537" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Atgv-net: Accurate depth superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High quality depth map upsampling for 3d-tof cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1623" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highquality depth map upsampling and completion for rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5559" to="5572" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image guided depth upsampling using anisotropic total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Edge-guided single depth image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="428" to="438" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08685</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Confidence propagation through cnns for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05356</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crossfield joint image restoration via scale map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1537" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rolling guidance filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="815" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mutual-structure for joint filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3406" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust image filtering using joint static and dynamic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4823" to="4831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep joint image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep bilateral learning for real-time image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Depth completion with deep geometry and context guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-U</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast end-to-end trainable guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1838" to="1847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic filtering with large sampling field for convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10034</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning depth with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02695</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="689" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Object enhancement and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Prewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Picture processing and Psychopictorics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="19" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
