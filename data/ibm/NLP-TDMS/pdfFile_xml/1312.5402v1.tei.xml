<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Some Improvements on Deep Convolutional Neural Network Based Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
							<email>andrewgeraldhoward@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Andrew Howard Consulting Ventura</orgName>
								<address>
									<postCode>93003</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Some Improvements on Deep Convolutional Neural Network Based Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techniques include adding more image transformations to the training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Vis ual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>In trod u cti on</p><p>Deep convolutional neural networks have recently been substantially improving upon the state of the art in image classification and other recognition tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>. Since their introduction in the early 1990s <ref type="bibr" target="#b6">[7]</ref>, convolutional neural networks have consistently been competitive with other techniques for image classification and recognition. Recently, they have pulled away from competing methods due the availability of larger data sets, better models and training algorithms and the availability of GPU computing to enable investigation of larger and deeper models.</p><p>The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b7">[8]</ref> is a venue for evaluating what the current state of the art for image classification and recognition is. It is large dataset of 1.2 million images with 1000 classes that are a subset of the Imagenet dataset <ref type="bibr" target="#b2">[3]</ref>. The 2012 competition demonstrated a large step forward for convolutional neural networks where a convnet based system <ref type="bibr" target="#b5">[6]</ref> soundly beat the competing methodology based on Fisher Vectors <ref type="bibr" target="#b3">[4]</ref> by roughly 10% in absolute terms. The convolution neural network based system was made up of an ensemble of deep, eight layer networks. It also incorporated important features such as pooling and normalizing layers, data transformations to expand the training data size, data transformations at test time, and drop out <ref type="bibr" target="#b4">[5]</ref> to avoid over fitting.</p><p>We investigate useful additions to the winning system from 2012 <ref type="bibr" target="#b5">[6]</ref> and improve upon its results by 20% in relative terms. This paper summarizes our entry in the 2013 ILSVRC which achieved a 13.55% top 5 error rate compared to the previous year's 16.4% top 5 error rate. The winning entry, Clarifai, which is partially detailed in <ref type="bibr" target="#b9">[10]</ref>, achieved a top 5 error rate of 11.74%. The methods outlined in this paper should be able to improve upon it and other convolutional neural network based systems.</p><p>Our models are based on the 2012 winning system <ref type="bibr" target="#b5">[6]</ref> and use the code provided at http://code.google.com/p/cuda-convnet as a starting point. Our model structure is identical with the exception that the fully connected layers are twice as big. It turns out, that this change does not improve the top 5 error rate. We use the same training methodology of training the net until the validation error plateaus and reducing the step size by 10 at each plateau. Additional changes are detailed in the follow sections.</p><p>The paper is organized as follows. Section 2 describes the additional transformations used to increase the effective number of training examples. Section 3 describes the additional transformations used at test time to improve prediction and a method to reduce the total number of predictions to a manageable size. Section 4 describes complementary high resolution models. Section 5 reports the results of our system and we conclude with a summary and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ad d i ti on al Data Tran sf ormati on s f or Trai n in g</head><p>Deep neural networks are greatly improved with the addition of more training data. When more training data is not available, transformations to the existing training data which reflect the variation found in images can synthetically increase the training s et size. In the previous Imagenet classification system <ref type="bibr" target="#b5">[6]</ref>, three types of image transformations were used to augment the training set. The first was to take a randomly located crop of 224x224 pixels from a 256x256 pixel image capturing some translation i nvariance. The second was to flip the image horizontally to capture the reflection invariance. The final data transformation was to add randomly generated lighting which tries to capture invariance to the change in lighting and minor color variation. We add additional transformations that extend the translation invariance and color invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">.1 E x t e n d i n g I ma g e C ro p s i n t o E x t r a P i x e l s</head><p>Previously <ref type="bibr" target="#b5">[6]</ref>, translated image crops of 224x224 pixels were selected from a training image of 256x256. The 256x256 image was generated by rescaling the largest image dimension to 256 and then cropping the other side to be 256. This results in a loss of information by not considering roughly 30% of the pixels. While the cropped pixels are most likely less informative than the middle pixels we found that making use of these additional pixels improved the model.</p><p>To use the whole image, we first scale the smallest side to 256 leaving us with a 256xN or Nx256 sized image. We then select a random crop of 224x224 as a training im age. This yields a large number of additional training examples and helps the net learn more extensive translation invariance. <ref type="figure">Figure 1</ref> shows a comparison of a square cropped image versus using the full image. The square training image of the cat will never generate training examples with a tail in it or with both ears compared to selecting crops from the full image. <ref type="figure">Figure 1</ref>: Even well centered images when cropped lose information like the cat's ear and tail compared to the full image on the right. We select training patches from the full image to avoid loss of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">.A d d i t i o n a l C o l o r M a n i p u l a t i o n s</head><p>In addition to the random lighting noise that has been used in previous pipelines <ref type="bibr" target="#b5">[6]</ref>, we also add additional color manipulations. We randomly manipulate the contrast, brightness and color using the python image library (PIL). This helps generate training examples that cover the span of image variations helping the neural network to learn invariance to changes in these properties. We randomly choose an order for the three manipulations and then choose a number between 0.5 and 1.5 for the amount of enhancement (a setting of 1 leaves the image unchanged). After manipulating the contrast, brightness and color, we then add the random lighting noise similar to <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ad d i ti on al Data Tran sf ormati on s f or Testi n g</head><p>Previous methods combined the predictions of 10 image transformations into a final prediction. They used the central crop and the four corners as well as the horizontal flip of these five. We found that predicting at three different scales improved the joint prediction. We also made predictions on three different views of the data captur ing the extra pixels that are previously cropped out. The combination of 5 translations, 2 flips, 3 scales, and 3 views leads to 90 predictions which slow predictions down by almost an order of magnitude. To rectify this, we show that a simple greedy algorithm can choose a subset of 10 transforms that predicts almost as well as all 90 and a subset of 15 that predicts slightly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">.1 P re d i c t i o n s a t M u l t i p l e S c a l e s</head><p>Images contain useful predictive elements at different scales. To capture this we make predictions at three different scales. We use the original 256 scale as well as 228 and 284. Note that when scaling an image up, it is important to use a good interpolation method like bicubic scaling and not to use anti aliasing filters designed for scaling images down. When scaling down, anti aliasing seems to help a little bit but in practice we used bicubic scaling for up scaling and down scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">.2 P re d i c t i o n s w i t h M u l t i p l e Vi e w s</head><p>In order to make use of all of the pixels in the image when making predi ctions, we generate three different square image views. For an 256xN (Nx256) image, we generate a left (upper), center, and right (lower) view of 256x256 pixels and then apply all crops, flips and scales to each of these views. <ref type="figure">Figure 2</ref> demonstrates how the three views are constructed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">.R e d u c i n g t h e N u m b e r o f P re d i c t i o n s</head><p>Our final combination of 5 crops, 2 flips, 3 scales and 3 views yields a combination of 90 predictions per model. This is almost an order of magnitude larger than the 10 predictions that were used previously and may cause an unacceptable delay in real world applications. We used a simple greedy algorithm to reduce the number of predictions to an acceptable number.</p><p>The simple greedy algorithm starts with the best prediction and at each step adds another prediction until there is no additional improvement. This algorithm finds that the first 10 predictions are almost as accurate as using all 90 which would have t he same run time as previous methods that also only use 10 predictions. It is also able to find a slightly improved combination using 15 predictions that improves on the 90 prediction baseline. <ref type="figure">Figure 2</ref> shows a plot of accuracy as more predictions are added.</p><p>This simple greedy algorithm is easy to implement, quick to run and has no parameters to tune. It avoids some of the pitfalls inherent in various weight based learning methods for combining predictions. Because most of the predictions are very similar (almost collinear), methods such as stacking <ref type="bibr" target="#b8">[9]</ref> or similar algorithms tend to have trouble and can create large weights of opposing sign. Penalization based method (l1, l2 etc) can help but not completely mitigate this effect. <ref type="table" target="#tab_3">Table 2</ref> shows the effect of the new test transformations and the results of the greedy algorithm. <ref type="figure" target="#fig_0">Figure 3</ref> shows the progression of the greedy algorithm as it adds models until convergence.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">H i gh er Resol u ti on Mod el s</head><p>Objects in images can exist at different scales. In the previous section we run our trained network on scaled up and scaled down versions of the image. However, once the image is scaled up too much, the trained network no longer performs well. To search fo r objects in images in scaled up higher resolution images we need to retrain the network at that scale. In practice, previously trained models can be used to initialize higher resolution models and cut training time down substantially to 30 epochs from 90 epochs. Higher resolution models are complementary to base models and a single high resolution model is as valuable as four additional base models as shown in <ref type="table">Table 3</ref>. Complementary models have been found to be valuable in other large scale data contests such as the Netflix Prize <ref type="bibr" target="#b0">[1]</ref>. In previous sections, models are trained on 224x224 patches cropped from 256xN (Nx256) images. For the higher resolution models, we wish to train on larger images using the same model structure. We build models on 448xN (Nx448) images using crops of 224x224. This ensures that at test time, the four corner crops do not overlap. In practice it may not be practical to store images at multiple resolutions or they may not be available. So to reuse 256xN (Nx256) training images we take 128x128 sized patches and scale them up to be 224x224. This simulates using 224x224 patches in a 448xN (Nx448) image without needing access to separate training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">.1 M o d e l D e t a i l s</head><p>We use an identical model structure and training methods as the lower resolution models. Because the model is identical in structure, we can initialize the higher resolution models with fully trained low resolution models. This allows us to train the new model in 30 epochs rather than the previous 90 epochs. We start with a step size of 0.001 and divide the step size by 10 once the validation score plateaus. We reduce the step size twice before convergence. Because there are effectively more training patches due to the smaller patch size relative to image size, drop out is not as important. In practice we use drop out for the initial training and the first step size reduction to 0.0001, we then turn off drop out to finish the training at 0.0001. Then the model is finished training with drop out turned off and the step size reduced to 0.00001. This gives better results than training with drop out alone or without drop out.</p><p>When making predictions on new images, we can make use of more image patches.  <ref type="table">Table 3</ref>: The combination of base model and high resolution model is better than five base models for top-1 classification and equivalent for top-5 classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Resu l ts</head><p>The final image classification system submitted to ILSVRC2013 was composed of 10 neural networks made up of 5 base models and 5 high resolution models and had a test set top 5 error rate of 13.6%. This is an improvement on the previous state of the art of 16.4% but short of the best result of 11.7%. The methods described in this paper should be able improve on the current state of the art of 11.7%. Results are summarized in <ref type="table" target="#tab_6">Table 4</ref>.   <ref type="bibr" target="#b5">[6]</ref> and current year's winner Clarifai. The improvements proposed could also improve on the Clarifai system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Con cl u si on</head><p>In this paper we showed a number of ways to improve neural network based image classification systems. We first showed some new useful image transformations to increase the effective size of the training set. These were based on using more of the image to select training crops and additional color manipulations. We also showed useful image transformations for generating testing predictions. We made predictions at different scales and generated predictions on different views of the image. These additional pr edictions can slow down the system so we showed a simple greedy algorithm that reduces the number of predictions needed. Finally, we showed an efficient way to train higher resolution models that generate useful complementary predictions. A single base model and a single high resolution model are as good as 5 base models. These improvements to the image classification pipeline are easy to implement and should be able to improve other convolutional neural network based image classification systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>This figure shows the accuracy of the greedy selection algorithm as it adds more predictions compared to the baseline 10 predictions and the full 90 predictions .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>shows the effect of using the new training and testing transforms compared to the previous baseline result. It also shows that the new model architecture with doubled fully connected layers does not improve the top 5 error rate.Figure 2: We generate predictions based on three different square views of the image to incorporate all of the pixels and to take into account differing image sizes.</figDesc><table><row><cell></cell><cell>Val Top-1</cell><cell>Val Top-5</cell></row><row><cell>Krizhevsky et al Single Convnet [6]</cell><cell>40.7%</cell><cell>18.2%</cell></row><row><cell>New Training, Test Transforms</cell><cell>37.5%</cell><cell>15.9%</cell></row><row><cell>Double FC + New Training, Test Transforms</cell><cell>37.0%</cell><cell>15.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results for new training and testing transforms using the architecture of Krizhevsky et al and the same model with double the size for all fully connected layers. The larger fully connected layers do not improve the top 5 validation error substantively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>This table shows results for additional test time transformations and combinations with a greedy algorithm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Previously 5 crops (or translations) are used when making predictions. Because these crops have less overlap for the higher resolution model, we find it useful to increase this to 9 crops by adding the middle top, middle bottom, left middle and right middle. This brings th e total number of predictions per model up to 162 (9 crops, 2 flips, 3 scales, 3 views) making the greedy selection algorithm from section 3.3 very important.</figDesc><table><row><cell></cell><cell>Val Top-1</cell><cell>Val Top-5</cell></row><row><cell>One Base Net</cell><cell>37.0%</cell><cell>15.8%</cell></row><row><cell>Two Base Nets</cell><cell>35.9%</cell><cell>15.1%</cell></row><row><cell>One High Resolution Net</cell><cell>36.8%</cell><cell>16.2%</cell></row><row><cell>One Base Net + One High Resolution Net</cell><cell>34.9%</cell><cell>14.5%</cell></row><row><cell>Five Base Nets</cell><cell>35.2%</cell><cell>14.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>This table shows a comparison of the proposed methods to the previous year's winner Krizhevsky et al</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lessons from the Netflix prize challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="79" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Imagenet: A Large-Scale Hierarchical Image Database. CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gunji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yasumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
		<title level="m">Classifier entry. ILSVRC-2012</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.image-net.org/challenges/LSVRC/2013/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">ILSVRC-2013</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901v3</idno>
		<title level="m">Visualizing and Understanding Convolutional Networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
