<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-branch Attentive Transformer *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufang</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
							<email>xiangyangli@ustc.edu.cn2shufxi</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
						</author>
						<title level="a" type="main">Multi-branch Attentive Transformer *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While the multi-branch architecture is one of the key ingredients to the success of computer vision tasks, it has not been well investigated in natural language processing, especially sequence learning tasks. In this work, we propose a simple yet effective variant of Transformer [26] called multi-branch attentive Transformer (briefly, MAT), where the attention layer is the average of multiple branches and each branch is an independent multi-head attention layer. We leverage two training techniques to regularize the training: drop-branch, which randomly drops individual branches during training, and proximal initialization, which uses a pretrained Transformer model to initialize multiple branches. Experiments on machine translation, code generation and natural language understanding demonstrate that such a simple variant of Transformer brings significant improvements. Our code is available at https://github.com/HA-Transformer. * This work is conducted at Microsoft Research Asia. Preprint. Under review. arXiv:2006.10270v2 [cs.CL] 26 Jul 2020 Dropout, during inference, all branches are used. (2) Proximal initialization: We initialize MAT with the corresponding parameters trained on a standard single-branch Transformer.</p><p>Our contributions are summarized as follows:</p><p>(1) We propose a simple yet effective variant of Transformer, multi-branch attentive Transformer (MAT). We leverage two techniques, drop branch and proximal initialization, to train this new variant.</p><p>(2) We conduct experiments on three sequence learning tasks: neural machine translation, code generation and natural language understanding. On these tasks, MAT significantly outperforms the standard Transformer baselines, demonstrating the effectiveness of our method.</p><p>(3) We explore another variant which introduces multiple branches into feed-forward layers. We find that such a modification slightly hurts the performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The multi-branch architecture of neural networks, where each block consists of more than one parallel components, is one of the key ingredients to the success of deep neural models and has been well studied in computer vision. Typical structures include the inception architectures <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b23">23]</ref>, ResNet <ref type="bibr" target="#b6">[6]</ref>, ResNeXt <ref type="bibr" target="#b32">[32]</ref>, DenseNet <ref type="bibr" target="#b8">[8]</ref>, and the network architectures discovered by neural architecture search algorithms <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b13">13]</ref>. Models for sequence learning, a typical natural language processing problem, also benefit from multi-branch architectures. Bi-directional LSTM (BiLSTM) models can be regarded as a two-branch architecture, where a left-to-right LSTM and a right-to-left LSTM are incorporated into one model. BiLSTM has been applied in neural machine translation (briefly, NMT) <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b31">31]</ref> and pre-training <ref type="bibr" target="#b17">[17]</ref>. Transformer <ref type="bibr" target="#b26">[26]</ref>, the state-of-the-art model for sequence learning, also leverages multi-branch architecture in its multi-head attention layers.</p><p>Although multi-branch architecture plays an important role in Transformer, it has not been well studied and explored. Specifically, using hybrid structures with both averaging and concatenation operations, which has been widely used in image classification tasks <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b25">25]</ref>, are missing in current literature for sequence learning. This motivates us to explore along this direction. We propose a simple yet effective variant of Transformer, which treats a multi-head attention layer as a branch, duplicates such an attention branch for multiple times, and averages the outputs of those branches. Since both the concatenation operation (for the multiple attention heads) and the averaging operation (for the multiple attention branches) are used in our model, we call our model multi-branch attentive Transformer (briefly, MAT) and such an attention layer with both the concatenation and adding operations as the multi-branch attention layer.</p><p>Due to the increased structure complexity, it is challenging to directly train MAT. Thus, we leverage two techniques for MAT training. (1) Drop branch: During training, each branch should be independently dropped so as to avoid possible co-adaption between those branches. Note that similar to 2 Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Introduction to Transformer</head><p>A Transformer model consists of an encoder and a decoder. Both the encoder and the decoder are stacks of blocks. Each block is mainly made up of two types of layers: the multi-head attention layer and the feed-forward layer (briefly, FFN). We will mathematically describe them.</p><p>Let concat(· · · ) denote the concatenation operation, where all inputs are combined into a larger matrix along the last dimension. Let attn M and attn denote a multi-head attention layer with M heads (M ∈ Z + ) and a standard attention layer. A standard attention layer <ref type="bibr">[3,</ref><ref type="bibr" target="#b26">26]</ref> takes three elements as inputs, including query Q, key K and value V , whose sizes are T q × d, T × d and T × d (T q , T, d are integers). attn is defined as follows:</p><formula xml:id="formula_0">attn(Q, K, V ) = softmax QK √ d V,<label>(1)</label></formula><p>where softmax is the softmax operation. A multi-head attention layer aggregates multiple attention layers in the concatenation way:</p><p>attn M (Q, K, V ) = concat(H 1 , H 2 , · · · , H M );</p><formula xml:id="formula_1">H i = attn(QW i Q , KW i K , V W i V ), i ∈ [M ],<label>(2)</label></formula><p>where [M ] denotes the set {1, 2, · · · , M }. In Eqn. <ref type="bibr" target="#b2">(2)</ref>, the W 's are the parameters to be learned. Each W is of dimension T × (d/M ). The output of attn M is the same size as Q, i.e., T q × d.</p><p>In standard Transformer, an FFN layer, denoted by FFN, is implemented as follows:</p><formula xml:id="formula_2">FFN(x) = max(xW 1 + b 1 , 0)W 2 + b 2 ,<label>(3)</label></formula><p>where x is a 1 × d-dimension input, max is an element-wise operator, W 1 and W 2 are d × d h and d h × d matrices, b 1 and b 2 are d h and d dimension vectors. Usually, d h &gt; d.</p><p>In the encoder of a Transformer, each block consists of a self-attention layer, implemented as a attn M where the query Q, key K and value V are the outputs of previous layer, and an FFN layer. In the decoder side, an additional multi-head attention is inserted between the self-attention layer and FFN layer, which is known as the encoder-decoder attention: Q is the output of the previous block, K and V are the outputs of the last block in the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-branch architectures for sequence learning</head><p>As shown in Section 1, multi-branch architectures have been well investigated in image processing. In comparison, the corresponding work for sequence learning is limited. The bidirectional LSTM, a two-branch architecture, has been applied in machine translation <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b31">31]</ref> and pre-training techniques like ELMo <ref type="bibr" target="#b17">[17]</ref>. <ref type="bibr" target="#b21">[21]</ref> proposed two use both convolutional neural networks and Transformer in the encoder and decoder. Similar idea is further expanded in <ref type="bibr" target="#b33">[33]</ref>. A common practice of the previous work is that they focus on which network components (convolution, attention, etc.) should be used in a multi-branch architecture. In this work, we do not want to introduce additional operations into Transformer but focus on how to boost Transformer with its own components, especially, where and how to apply the multi-branch topology, and figure out several useful techniques to train multi-branch architectures for sequence learning. Such aspects are missing in previous literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-branch attentive Transformer</head><p>In this section, we first introduce the structure of multi-branch attentive Transformer (MAT) in Section 3.1, and then we introduce the drop branch technique in Section 3.2. The proximal initialization is described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network architecture</head><p>The network architecture of our proposed MAT adopts the backbone of standard Transformer, except that all multi-head attention layers (including both self-attention layers and encoder-decoder attention layers) are replaced with the multi-branch attention layers.</p><p>Let mAttn Na,M (Q, K, V ) denote a multi-branch attention layer, where N a represents the number of branches and each branch is a multi-head attention layer with M heads attn M . Q, K and V denote query, key and value, which are defined in Section 2. Mathematically, mAttn Na,M (Q, K, V ) works as follows: <ref type="formula" target="#formula_1">(2)</ref>) is the collection of all parameters of the i-th multi-head attention layer.</p><formula xml:id="formula_3">mAttn Na,M (Q, K, V ) = Q + 1 N a Na i=1 attn M (Q, K, V ; θ i ),<label>(4)</label></formula><formula xml:id="formula_4">where θ i = {W i Q , W i K , W i V } M i=1 (see Eqn.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Drop branch technique</head><p>As mentioned above, multiple branches in multi-branch attention layers have the same structure. To avoid the co-adaptation among them, we leverage the drop branch technique. The inspiration comes from the dropout <ref type="bibr" target="#b22">[22]</ref> and drop path technique <ref type="bibr" target="#b12">[12]</ref>, where some branches are randomly dropped during training.</p><p>Let mAttn Na,M (Q, K, V ; ρ) denote a multi-branch attention layer with drop branch rate ρ ∈ [0, 1], which is an extension of that in Section 3.1. Equipped with drop branch technique, the i-th branch of the multi-branch attention layer, denoted as β M i (Q, K, V ; ρ, θ i ), works as follows:</p><formula xml:id="formula_5">β M i (Q, K, V ; ρ, θ i ) = I{U i ≥ ρ} 1 − ρ attn M (Q, K, V ; θ i ),<label>(5)</label></formula><p>where U i is uniformly sampled from [0, 1] (briefly, U i ∼ unif[0, 1]); I is the indicator function. During training, we may set ρ ≥ 0, where any branch might be skipped with probability ρ. During inference, we must set ρ = 0, where all branches are leveraged with equal weights 1. The multi-branch attention layer mAttn Na,M (Q, K, V ; ρ) works as follows:</p><formula xml:id="formula_6">mAttn Na,M (Q, K, V ; ρ) = Q + 1 N a Na i=1 β M i (Q, K, V ; ρ, θ i ).<label>(6)</label></formula><p>Note when ρ = 0, Eqn.(6) degenerates to Eqn.(4).</p><p>During training, it is possible that all branches in multi-branch attention layer are dropped. The residual connection ensures that even if all branches are dropped, the output from the previous layer can be fed to top layers through the identical mapping. That is, the network is never blocked. When N a = 1 and ρ = 0, a multi-branch attention layer degenerates to a vanilla multi-head attention layer. When N a = 1 and ρ &gt; 0, it is the standard Transformer with randomly dropped attention layers during training, which is similar to the drop layer <ref type="bibr" target="#b5">[5]</ref>. (See Section 4.2 for more discussions.)</p><p>We also apply the drop branch technique to FFN layers. That is, the revised FFN layer is defined as</p><formula xml:id="formula_7">x + I{U ≥ ρ} 1 − ρ FFN(x), U ∼ unif[0, 1]<label>(7)</label></formula><p>where x is the output of the previous layer. An illustration of multi-branch attentive Transformer is in <ref type="figure" target="#fig_0">Figure 1</ref>. Each block in the encoder consists of a multi-branch attentive self-attention layer and an FFN layer. Each block in the decoder consists of another multi-branch attentive encoder-decoder attention layer. Layer normalization <ref type="bibr" target="#b2">[2]</ref> remains unchanged. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proximal initialization</head><p>MAT can be optimized like the standard version by first randomly initializing all parameters and training until convergence. However, the multi-branch attention layer increases the training complexity of MAT, since there are multiple branches to be optimized.</p><p>Recently, proximal algorithms <ref type="bibr" target="#b16">[16]</ref> have been widely used in pretraining-and-finetuning framework to regularize training <ref type="bibr" target="#b9">[9]</ref>. Then main idea of proximal algorithms is to balance the trade-off between minimizing the objective function and minimizing the weight distance with a pre-defined weight. Inspired by those algorithms, we design a two-stage warm-start training strategy:</p><p>(1) Train a standard Transformer with embedding dimension d, FFN dimension d h .</p><p>(2) Duplicate both the self-attention layers and encoder-decoder attention layers for N a times to initialize MAT. Train this new model until convergence.</p><p>We empirically find that the proximal initialization scheme can boost the performance than that obtained by training from scratch. For WMT'14 English→German translation, we follow <ref type="bibr" target="#b15">[15]</ref> to preprocess the data , and eventually obtain 4.5M training data. We use newstest 2013 as the validation set, and choose newstest 2014 as the test set. The number of BPE merge operation is 32k. The preprocess steps for WMT'19 De→Fr are the same as those for WMT'14 En→De, and we eventually get 9M training data in total. We concatenate newstest2008 to newstest 2014 together as the validation set and use newstest 2019 as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application to neural machine translation</head><p>Model configuration and training strategy: For the three IWSLT tasks, we choose the default setting provided by fairseq official code 4 as the baseline with embedding dimension d = 512, hidden dimension d h = 1024 and number of heads M = 4. For WMT'14 En→De, we mainly follow the big transformer setting, where the above three numbers are 1024, 4096 and 16 respectively. The dropout rates are 0.3. For the more detailed parameters like ρ and N a , we will introduce the details in corresponding subsections. We use the Adam <ref type="bibr" target="#b10">[10]</ref> optimizer with initial learning rate 5 × 10 −4 , β 1 = 0.9, β 2 = 0.98 and the inverse_sqrt learning rate scheduler <ref type="bibr" target="#b26">[26]</ref> to control training. Each model is trained until convergence. The source embedding, target embedding and output embedding of each task are shared. The batch size is 4096 for both IWSLT and WMT tasks. For IWSLT tasks, we train on single P40 GPU; for WMT tasks, we train on eight P40 GPUs.</p><p>Evaluation We evaluate the translation quality by BLEU scores. For IWSLT'14 De→En and WMT'14 En→De, following the common practice, we use multi-bleu.perl. For other tasks, we choose sacreBLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Exploring hyper-parameters of MAT</head><p>Due to resource limitation, we first explore hyper-parameters and proximal initialization on IWSLT'14 De→En dataset to get some empirical results, then transfer them to larger datasets.</p><p>We try different combination of N a , d and d h . We ensure the number of total parameters not exceeding 36.7M , the size of the default model for IWSLT <ref type="bibr">'14</ref> De→En. All results are reported in <ref type="table" target="#tab_1">Table 1</ref>, where the network architecture is described by a four-element tuple, with each position representing the number of branches N a , embedding dimension d, hidden dimension d h and model sizes. The baselines correspond to the architectures with N a = 1 and ρ = 0. The most widely adopted baseline (marked with ) is 34.95 and the model size is 36.7M. Reducing d to 256 results in slightly BLEU score 35.04. We have the following observations:</p><p>(1) Using multi-branch attention layers with more than one branches can boost the translation BLEU scores, with a proper ρ. We also try to turn off the drop branch at the FFN layers. In this case, we found that by ranging ρ from 0 to 0.3, architecture 3/256/2048 can achieve 34.63, 34.78 and 34.98 BLEU scores, which are worse than those obtained by using drop branch at all layers. This shows that the drop branch at every layers is important for our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Exploring proximal initialization</head><p>In this section, we explore the effect of proximal initialization, i.e., warm start from an existing standard Transformer model. The results are reported in <ref type="table" target="#tab_3">Table 2</ref>. Generally, compared with the results without proximal initialization, the models can achieve more than 0.5 BLEU score improvement. Specifically, with N a = 3 and d h = 2048, we can achieve a 36.22 BLEU score, setting a state-ofthe-art record on this task. We also evaluate the validation perplexity of all architectures in <ref type="table" target="#tab_3">Table 2</ref>, and the model with N a = 3 and d h = 2048 still get the lowest perplexity 4.56. In summary, according to the exploration in Section 4.2 and Section 4.3, we empirically get the following conclusions: (1) A multi-branch attention layer is helpful to improve Transformer. We suggest to try N a = 2 or N a = 3 first. (2) A larger d h is helpful to bring better performance.</p><p>Enlarging d h to consume the remaining parameters of reducing d (of the standard Transformer) is a better choice. <ref type="formula" target="#formula_2">(3)</ref> The drop branch and proximal initialization are two key techniques to the success of MAT in NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Application to other NMT tasks</head><p>Results of other IWSLT tasks: We apply the discoveries to Spanish↔English and French↔English, and report the results in <ref type="table" target="#tab_5">Table 3</ref>. For each setting, both the highest BLEU scores according to validation performance as well as the corresponding ρ's are reported. We choose 1/512/1024 as the default baseline, whose model size is the upper bound of our MAT models. We also implement two other baselines, one with smaller d and the other with larger d h . In baselines, the ρ is fixed as zero.   We summarize previous results on WMT'14 En→De in <ref type="table" target="#tab_7">Table 5</ref>. MAT can achieve comparable or slightly better results than carefully designed architectures like weighted Transformer and Dynamic-Conv, and than the evolved Transformer discovered by neural architecture search.</p><p>The results of WMT'19 De→Fr are shown in <ref type="table" target="#tab_8">Table 6</ref>. Compared with standard big transformer model of architecture 1/1024/4096, MAT with ρ = 0.1 can improve the baseline by 0.58 point.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Application to code generation</head><p>We verify our proposed method on code generation, which is to map natural language sentences into code.</p><p>Datasets Following <ref type="bibr" target="#b29">[29]</ref>, we conduct experiments on a Java dataset 6 <ref type="bibr" target="#b7">[7]</ref> and a Python dataset <ref type="bibr" target="#b27">[27]</ref>. In the Java dataset, the numbers of training, validation and test sequences are 69708, 8714 and 8714 respectively, and the corresponding numbers for Python are 55538, 18505 and 18502. All samples are tokenized. We use the downloaded Java dataset without further processing and use Python standard AST moduleto further process the python code. The source and target vocabulary sizes in natural language to Java code generation are 27k and 50k, and those for natural language to Python code generation are 18k and 50k. In this case, following <ref type="bibr" target="#b29">[29]</ref>, we do not apply subword tokenization like BPE to the sequences. Evaluation Following <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b7">7]</ref>, we use sentence-level BLEU scores, which is the average BLEU score of all sequences to evaluate the generation quality. We choose the percentage of valid code (PoV) as another metric for evaluation, which is the percentage of code that can be parsed into an AST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model configuration</head><p>We report the results on code generation task in <ref type="table" target="#tab_10">Table 8</ref>. It is obvious that Transformer-based models (standard Transformer and MAT) is significantly better than the LSTM-based dual model. Compared with standard Transformer, our MAT can get 4.2 and 1.2 BLEU score improvement in Java and Python datasets respectively. MAT reaches the best BLEU scores when ρ = 0.1 on both on Java and Python datasets, which shows that the drop branch technique is important in code generation task. When ρ = 0.1, in terms of PoV, our MAT can boost the Transformer baseline by 5.8% and 6.6% points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Application to natural language understanding</head><p>We verify our proposed MAT on natural language understanding (NLU) tasks. Pre-training techniques have achieved state-of-the-art results on benchmark datasets/tasks like GLUE <ref type="bibr" target="#b28">[28]</ref>, RACE <ref type="bibr" target="#b11">[11]</ref>, etc. To use pre-training, a masked language model is first pre-trained on large amount of unlabeled data. Then the obtained model is used to initialize weights for downstream tasks. We choose RoBERTa <ref type="bibr" target="#b14">[14]</ref> as the backbone. For GLUE tasks, we focus more on accuracy. Therefore, we do not control the parameters subconsciously. We set N a = 2 for all experiments.</p><p>Following <ref type="bibr" target="#b5">[5]</ref>, we conduct experiments on MNLI-m, MPRC, QNLI abd SST-2 tasks in GLUE benchmark <ref type="bibr" target="#b28">[28]</ref>. SST-2 is a single-sentence task, which is to check whether the input movie review is a positive one or negative. The remaining tasks are two-sentence tasks. In MNLI, given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). In MRPC, the task is to check whether the two inputs are matched. In QNLI, the task is to determine whether the context sentence contains the answer to the question. We choose the 24-layer RoBERTa large 7 to initialize our MAT. We report the validation accuracy following <ref type="bibr" target="#b5">[5]</ref>. The results are reported in <ref type="table" target="#tab_11">Table 9</ref>. Compared with vanilla RoBERTa model, on the four tasks MNLI-m, MRPC, QNLI abd SST-2, we can improve them by 0.5, 1.0, 0.3 and 0.6 scores. Compared with the layer drop technique <ref type="bibr" target="#b5">[5]</ref>, we also achieve promising improvement on these four tasks. The results demonstrate that our method not only works for sequence generation tasks, but also for text classification tasks.</p><p>Drop branch also plays a central role on NLU tasks. We take the MRPC task as an example. The results are shown in <ref type="table" target="#tab_1">Table 10</ref>. MRPC achieves the best result at ρ = 0.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Exploring other variants</head><p>In this section, we first discuss whether applying multi-branch architectures to FFN layers is helpful. Then we discuss a new variant of drop branch technique. Finally we verify the importance of the multi-branch architecture, where both concatenation and averaging operations are used. We mainly conduct experiments on IWSLT tasks to verify the variants. We conduct experiments on IWSLT'14 De→En.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Multi-branch FFN layer</head><p>Algorithm: Denote the revised FFN layer as hFFN N f with N f (∈ Z + ) branches. Mathematically, given an input x from the previous layer,</p><formula xml:id="formula_8">hFFN N f (x) = x + 1 N f N f i=1 FFN(x; ω i ) I{U i ≥ ρ} 1 − ρ ,<label>(8)</label></formula><p>where ω i is the parameter of the i-th branch of the FFN layer, x is the output from the previous layer.</p><p>Experiments: We fix d as 256 and change N f . We set the attention layers as both standard multi-head attention layers (N a = 1) and multi-branch attention layers with N a = 2. We set N f d h = 2048 to ensure the number of parameters unchanged with different number of branches. Results are reported in <ref type="table" target="#tab_1">Table 11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Variants of drop branch</head><p>Algorithm: In drop branch, the attention heads within a branch are either dropped together or kept together. However, we are not sure whether dropping a complete branch is the best choice. Therefore, we design a more general way to leverage dropout. Mathematically, the j-th attention head of the i-th branch, denoted as β i,j (Q, K, V ; ρ, θ i,j ), works as follows:</p><formula xml:id="formula_9">β i,j (Q, K, V ; ρ, θ i,j ) = attn(QW i,j Q , KW i,j K , V W i,j V ) I{U i,j ≥ ρ} 1 − ρ ,<label>(9)</label></formula><p>where superscripts i and j represent the branch id and head id in a multi-head attention layer respectively,</p><formula xml:id="formula_10">U i,j ∼ unif[0, 1], θ i,j = {W i,j Q , W i,j K , W i,j V }.</formula><p>Based on Eqn.(9), we can define a more general multi-branch attentive architecture:</p><formula xml:id="formula_11">1 N a Na i=1 concat β i,1 (Q, K, V ; ρ, θ i,1 ), β i,2 (Q, K, V ; ρ, θ i,2 ), · · · , β i,M (Q, K, V ; ρ, θ i,M ) .<label>(10)</label></formula><p>Keep in mind that Eqn.(10) is associated with U i,j for any i ∈ [N a ], j ∈ [M ]. We apply several constraints to U i,j , which can lead to different ways to regularize the training:</p><formula xml:id="formula_12">1. For any i ∈ [N a ], sample U i ∼ unif[0, 1] and set U i,j = U i for any j ∈ [M ]</formula><p>. This is the drop branch as we introduced in Section 3.1. 2. Each U i,j is independently sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments:</head><p>We conduct experiments on IWSLT'14 De→En and the results are reported in <ref type="table" target="#tab_1">Table 12</ref>. Proximal initialization is leveraged. From left to right, each column represents architecture, number of parameters, BLEU scores with ρ ranging from 0.0 to 0.3. The last column marked with ∆ represents the improvement/decrease of the best results compared to those in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We can see that randomly dropping heads leads to slightly worse results than the drop branch technique. Take the network architecture with N a = 3, d h = 1024 as an example. The best BLEU  score that drop branch technique achieves is 36.22, and that obtained with randomly dropping heads is 36.07. Similarly observations can be found from other settings in <ref type="table" target="#tab_1">Table 12</ref>. Therefore, we suggest to use the drop branch technique, which requires minimal revision to the benchmark code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Ablation study on multi-branch attentions</head><p>MAT introduces multi-branch attention, in addition to the multi-head attention with the concatenation manner. <ref type="bibr" target="#b8">8</ref> It is interesting to know whether the multi-head attention is still needed given the multibranch attention. We conduct experiments with N a = 2, d = 256, d h = 1024 and vary the number of heads M in mAttn Na,M . Results are reported in <ref type="table" target="#tab_1">Table 13</ref>.</p><p>We can see that the multi-head attention is still helpful. When M = 1, i.e., removing multiple heads and using a single head, it performs worse than multiple heads (e.g., M = 4). Therefore, the multi-branch of them is the best choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and future work</head><p>In this work, we proposed a simple yet effective variant of Transformer called multi-branch attentive Transformer and leveraged two techniques, drop branch and proximal initialization, to train the model. Rich experiments on neural machine translation, code generation and natural language understanding tasks have demonstrated the effectiveness of our model.</p><p>For future work, we will apply our model to more sequence learning tasks. We will also combine our discoveries with neural architecture search, i.e., searching for better neural models for sequence learning in the search space with enriched multi-branch structures.          </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Variants of Drop Branch</head><p>We explore the variant of drop branch described in Section 7.2 on the other four IWSLT tasks. We apply all settings in <ref type="table" target="#tab_1">Table 11</ref> of the main paper to the remaining language pairs. Results are reported from <ref type="table" target="#tab_3">Table 24 to Table 27</ref>. Generally, the two types of drop branch achieve similar results.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Scripts</head><p>In this section, we summarize the scripts we used in our paper.</p><p>(1) multi-bleu.perl: https://github.com/moses-smt/mosesdecoder/blob/master/ scripts/generic/multi-bleu.perl</p><p>(2) sacreBLEU: https://github.com/mjpost/sacreBLEU</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of a block in the encoder of MAT. "LN" refers to layer normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We set N a = 4, d = 256 and d h = 1024 respectively. Both the encoder and the decoder consist of three blocks. The MAT model contains about 37M parameters. For the Transformer baseline, we set d = 512 and d h = 1024, with 55M parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>3M 34.41 35.37 34.64 25.05 3/256/2048/29.4M 33.98 35.03 35.40 35.70 4/256/1024/27.9M 33.77 34.84 35.08 35.14 4/256/2048/34.2M 33.79 34.81 35.08 35.46</figDesc><table><row><cell>Na/d/d h /Param</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>Na/d/d h /Param</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell cols="4">Standard Transformer + Drop Branch</cell><cell></cell><cell cols="3">MAT + Drop Branch</cell><cell></cell><cell></cell></row></table><note>Results on IWSLT'14 De→En with different architectures. The left and right subtables are experiments of standard transformer and MAT respectively. From left to right in each subtable, the columns represent the network architecture, number of parameters, BLEU scores with ρ ranging from 0.0 to 0.3.1/512/1024/36.7M 34.95 3.94 22.43 0.78 2/256/1024/18.4M 34.42 35.19 35.52 35.11 1/256/1024/13.7M 35.04 35.39 34.53 29.34 2/256/2048/24.7M 34.51 35.46 35.59 35.33 1/256/2048/20.0M 34.66 35.45 32.75 1.37 3/256/1024/23.1M 34.01 34.92 35.39 35.44 1/256/3072/26.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>When setting d h = 1024 and increasing N a from 2 to 3, the BLEU scores are 35.52 (with ρ = 0.2) and 35.44 (with ρ = 0.3), outperforming the standard baseline 34.95, as well as the BLEU score obtained from architecture 1/256/1024. However, it is not always good to enlarge N a . The BLEU score of MAT with N a = 4 is 35.14, which is a minor improvement over the baseline.Multi-branch attention layers also benefits from larger hidden dimension d f . When setting d f = 2048, we obtain consistent improvement compared to d f = 1024 with the same N a . Among all architectures, we find that the model 3/256/2048 achieves the best BLEU score, which is 35.70 (we also evaluate the validation perplexity of all architectures, and the model 3/256/2048 still get the lowest perplexity 4.71). Compared with the corresponding Transformer 1/256/2048 with ρ = 0, we get 1.04 improvement. We also enlarge ρ to 0.4 and 0.5, but the BLEU scores drop. Results are shown in Appendix A. 3) When setting N a = 1 and ρ B &gt; 0, the architecture still benefits from the drop branch technique. For architecture 1/256/1024, the vanilla baseline is 35.04. As we increase ρ to 0.1, we can obtain 0.35 point improvement. Wider networks with d f also benefits from this technique, which shows that drop branch is generally a useful trick for Transformer. This is consistent with the discoveries in<ref type="bibr" target="#b5">[5]</ref>. With ρ = 0.1, enlarging the hidden dimension to 2048 and 3072 lead to 35.45 and 35.37 BLEU scores, corresponding to 0.79 and 0.96 score improvements compared with that without using drop branch. But we found that when N a = 1, drop branch might lead to unstable results. For example, when ρ = 0.1, 1/512/1024 cannot lead to a reasonable result. We re-run the experiments with five random seeds but always fail. In comparison, MAT can always obtain reasonable results with drop branch.</figDesc><table /><note>(2) The drop branch is important technique for training multi-branch attention layers. Take the network architecture 3/256/1024 as an example: If we do not use drop branch technique, i.e., ρ = 0, we can only get 34.01 BLEU score, which is 0.94 point below the baseline. As we increase ρ from 0 to 0.3, the BLEU scores become better and better, demonstrating the effectiveness of drop branch.(</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell cols="5">(2, 2048) 34.99 35.50 35.85 36.12 0.79</cell></row><row><cell cols="5">(3, 1024) 35.33 35.77 35.94 35.83 0.50</cell></row><row><cell cols="5">(3, 2048) 34.83 35.63 36.08 36.22 0.52</cell></row><row><cell cols="5">(4, 1024) 35.45 35.72 36.07 36.02 0.88</cell></row><row><cell cols="5">(4, 2048) 35.06 35.61 35.81 36.09 0.63</cell></row></table><note>Results of using proximal initialization. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio ρ from 0.0 to 0.3, and the increment compared to the best results without proximal initialization. (N a , d h )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on IWSLT {Es, Fr}↔En. N a /d/d h #Params(M) Es→En / ρ En→Es / ρ #Params(M) Es→En / ρ En→Es / ρ 1/512/1024 36.8 40.37 / 0.0 38.56 / 0.0 36.8 36.10 / 0.0 35.99 / 0.0 1/256/1024 13.7 40.60 / 0.0 39.42 / 0.0 13.7 36.21 / 0.0 36.16 / 0.0 1/256/2048 20.0 40.18 / 0.0 38.78 / 0.0 20.0 36.38 / 0.0 36.73 / 0.0 The results of WMT'14 En→De are shown in Table 4. The standard baseline is 29.13 BLEU score, and the model contains 209.8M parameters. We implement an MAT with N a = 2, d h = 12288 and another with N a = 3, d h = 10240. We can see that our MAT also works for the large-scale dataset. When N a = 2, we can obtain 29.90 BLEU score. When increasing N a to 3, we can get 29.85, which is comparable with the the N a = 2 variant. Besides, for large-scale datasets, the drop branch technique is important too. ρ = 0.2 works best of all settings.</figDesc><table><row><cell>2/256/1024</cell><cell>18.4</cell><cell>41.02 / 0.1 39.56 / 0.1</cell><cell>18.4</cell><cell>36.37 / 0.2 36.57 / 0.2</cell></row><row><cell>2/256/2048</cell><cell>24.7</cell><cell>42.11 / 0.2 40.46 / 0.2</cell><cell>24.7</cell><cell>37.41 / 0.2 37.24 / 0.2</cell></row><row><cell>3/256/1024</cell><cell>23.1</cell><cell>41.39 / 0.1 40.08 / 0.2</cell><cell>23.2</cell><cell>37.44 / 0.2 37.30 / 0.1</cell></row><row><cell>3/256/2048</cell><cell>29.4</cell><cell>41.79 / 0.3 40.40 / 0.3</cell><cell>29.5</cell><cell>37.28 / 0.2 37.44 / 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on WMT'14 En→De translation.</figDesc><table><row><cell>N a /d/d h</cell><cell>#Param (M)</cell><cell>ρ</cell><cell>BLEU</cell></row><row><cell>1/1024/4096</cell><cell>209.8</cell><cell cols="2">0.0 29.08</cell></row><row><cell>1/512/10240</cell><cell>161.7</cell><cell cols="2">0.0 28.86</cell></row><row><cell>1/512/12288</cell><cell>186.9</cell><cell cols="2">0.0 28.95</cell></row><row><cell>2/512/12288</cell><cell>205.8</cell><cell cols="2">0.1 29.64</cell></row><row><cell>2/512/12288</cell><cell>205.8</cell><cell cols="2">0.2 29.90</cell></row><row><cell>2/512/12288</cell><cell>205.8</cell><cell cols="2">0.3 29.06</cell></row><row><cell>3/512/10240</cell><cell>199.5</cell><cell cols="2">0.1 29.62</cell></row><row><cell>3/512/10240</cell><cell>199.5</cell><cell cols="2">0.2 29.85</cell></row><row><cell>3/512/10240</cell><cell>199.5</cell><cell cols="2">0.3 29.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">BLEU scores of WMT'14 En→De</cell></row><row><cell>in previous work.</cell><cell></cell></row><row><cell>Algorithm</cell><cell>BLEU</cell></row><row><cell>Weighted Transformer [1]</cell><cell>28.9</cell></row><row><cell>Evolved Transformer [20]</cell><cell>29.8</cell></row><row><cell>DynamicConv [30]</cell><cell>29.7</cell></row><row><cell>Our MAT</cell><cell>29.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on WMT'19 De→Fr translation. Finally, we explore whether our proposed MAT works for larger models. We conduct experiments on WMT'19 En→De translation task. The data is downloaded from WMT'19 website 5 . We change the number of encoder layers to 12, set attention-dropout and activation-dropout as 0.1, and keep the other settings the same as WMT'14 En→De. To validate the effectiveness of MAT, we evaluate the trained models on three test sets: WMT'14   The results are shown inTable 7. Compared with the standard big transformer model of architecture 1/1024/4096, in terms of sacreBLEU, MAT with ρ = 0.2 can improve the baseline by 1.0, 1.0 and 1.1 points on WMT14, WMT18 and WMT19 test sets respectively. Specially, on WMT'14 En→De, in terms of multi-bleu, we achieve 30.8 BLEU score, setting a new record on this work under the supervised setting.</figDesc><table><row><cell>N a /d/d h</cell><cell>#Param (M)</cell><cell>ρ</cell><cell>BLEU</cell></row><row><cell>1/1024/4096</cell><cell>223.6</cell><cell cols="2">0.0 34.07</cell></row><row><cell>1/512/12288</cell><cell>193.7</cell><cell cols="2">0.0 33.90</cell></row><row><cell>2/512/12288</cell><cell>212.6</cell><cell cols="2">0.0 33.92</cell></row><row><cell>2/512/12288</cell><cell>212.6</cell><cell cols="2">0.1 34.65</cell></row><row><cell>2/512/12288</cell><cell>212.6</cell><cell cols="2">0.2 34.53</cell></row><row><cell>2/512/12288</cell><cell>212.6</cell><cell cols="2">0.3 34.49</cell></row></table><note>En→De, WMT'18 En→De and WMT'19 En→De. We use sacreBLEU to evaluate the translation quality. Furthermore, to facilitate comparison with previous work, we also use multi-bleu.perl to calculate the BLEU score for WMT'14 En→De.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results of En→De with larger models. For WMT'14, both multi-bleu (left) and sacreBLEU (right) are reported.</figDesc><table><row><cell>N a /d/d h</cell><cell>#Param (M)</cell><cell>ρ</cell><cell>WMT14</cell><cell cols="2">WMT18 WMT19</cell></row><row><cell>1/1024/4096</cell><cell>325.7</cell><cell cols="2">0.0 29.8 / 29.2</cell><cell>42.7</cell><cell>39.3</cell></row><row><cell>1/512/12288</cell><cell>288.9</cell><cell cols="2">0.0 29.5 / 29.0</cell><cell>41.3</cell><cell>37.4</cell></row><row><cell>2/512/12288</cell><cell>314.1</cell><cell cols="2">0.0 30.0 / 29.6</cell><cell>42.5</cell><cell>38.5</cell></row><row><cell>2/512/12288</cell><cell>314.1</cell><cell cols="2">0.1 29.9 / 29.4</cell><cell>43.1</cell><cell>39.5</cell></row><row><cell>2/512/12288</cell><cell>314.1</cell><cell cols="2">0.2 30.8 / 30.2</cell><cell>43.7</cell><cell>40.4</cell></row><row><cell>2/512/12288</cell><cell>314.1</cell><cell cols="2">0.3 30.1 / 29.7</cell><cell>43.8</cell><cell>40.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results on code generation. = 0.0 26.97 77.5% 16.35 65.5% Ours, ρ = 0.1 27.53 88.9% 16.66 79.9% Ours, ρ = 0.2 26.06 90.8% 15.63 84.0% Ours, ρ = 0.3 24.72 90.2% 12.85 75.3%</figDesc><table><row><cell>Algorithm</cell><cell>Java BLEU</cell><cell>PoV</cell><cell>Python BLEU PoV</cell></row><row><cell>Dual [29]</cell><cell cols="3">17.17 27.4% 12.09 51.9%</cell></row><row><cell>Transformer</cell><cell cols="3">23.30 83.1% 15.49 73.3%</cell></row><row><cell>Ours, ρ</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Validation results on various NLU tasks.</figDesc><table><row><cell>Results of RoBERTa and RoBERTa + LayerDrop</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>MRPC validation accuracy w.r.t.</figDesc><table><row><cell>ρ.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Results on IWSLT'14 De→En with multi-branch FFN layers. From left to right, the columns represent the network architecture (with the number of parameters included), BLEU scores with ρ ranging from 0.0 to 0.3.We can see that increasing the branches of FFN layer while decreasing the corresponding hidden dimension (i.e., d h ) will hurt the performance. For standard Transformer with N a = 1, as we increase N ffrom 1 to 2, 4, 8, the BLEU scores drop from 35.45 to 35.38, 34.83 and 34.38. For MAT with N a = 2, as N f grows from 1 to 8, the BLEU decreases from 35.59 to 35.55, 35.00 and 34.70. That is, constraint by the number of total parameters, we do not need to separate FFN layers into multiple branches.</figDesc><table><row><cell>N a /N f /d h /#Param</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>1/1/2048/20.0M</cell><cell cols="4">34.66 35.45 32.75 1.37</cell></row><row><cell>1/2/1024/20.0M</cell><cell cols="4">34.85 35.38 35.30 34.39</cell></row><row><cell>1/4/512/20.0M</cell><cell cols="4">34.51 34.83 34.71 34.56</cell></row><row><cell>1/8/256/20.0M</cell><cell cols="4">34.35 34.36 34.38 33.86</cell></row><row><cell>2/1/2048/24.7M</cell><cell cols="4">34.51 35.46 35.59 35.33</cell></row><row><cell>2/2/1024/24.7M</cell><cell cols="4">34.39 35.18 35.55 35.42</cell></row><row><cell>2/4/512/24.7M</cell><cell cols="4">33.86 34.90 35.00 35.07</cell></row><row><cell>2/8/256/24.7M</cell><cell cols="4">34.05 34.59 34.70 34.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Results on IWSLT'14 De→En with randomly dropping heads technique. Embedding dimension d is fixed as 256.</figDesc><table><row><cell>(Na, d h ) 0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell cols="5">(2, 2048) 34.99 35.53 35.95 35.96 −0.26</cell></row><row><cell cols="5">(3, 1024) 35.33 35.67 35.90 35.95 +0.01</cell></row><row><cell cols="5">(3, 2048) 34.83 35.45 35.76 36.07 −0.15</cell></row><row><cell cols="5">(4, 1024) 35.45 35.72 35.83 35.77 −0.24</cell></row><row><cell cols="5">(4, 2048) 35.06 35.79 35.72 36.03 −0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Results on IWSLT'14 De→En with different numbers of heads.</figDesc><table><row><cell>ρ</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell cols="4">M = 1 35.01 34.83 34.52</cell></row><row><cell cols="4">M = 2 35.11 35.45 35.15</cell></row><row><cell cols="4">M = 4 35.19 35.52 35.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Results on IWSLT'14 De→En with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with ρ ranging from 0.2 to 0.5. Results of ρ = 0, 0.1 are inTable 1 of the main text.</figDesc><table><row><cell>N a /d/d h /Param</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell cols="4">Standard Transformer + Drop Branch</cell><cell></cell></row><row><cell cols="2">1/512/1024/36.7M 22.43</cell><cell>0.78</cell><cell>1.64</cell><cell>0.00</cell></row><row><cell cols="5">1/256/1024/13.7M 34.53 29.34 14.08 12.03</cell></row><row><cell cols="2">1/256/2048/20.0M 32.75</cell><cell>1.37</cell><cell>1.48</cell><cell>9.05</cell></row><row><cell cols="3">1/256/3072/26.3M 34.64 25.05</cell><cell>1.31</cell><cell>7.84</cell></row><row><cell></cell><cell cols="2">MAT + Drop Branch</cell><cell></cell><cell></cell></row><row><cell cols="5">2/256/1024/18.4M 35.52 35.11 34.46 33.50</cell></row><row><cell cols="5">2/256/2048/24.7M 35.59 35.33 34.95 28.15</cell></row><row><cell cols="5">3/256/1024/23.1M 35.39 35.44 34.85 34.10</cell></row><row><cell cols="5">3/256/2048/29.4M 35.40 35.70 35.01 31.68</cell></row><row><cell cols="5">4/256/1024/27.9M 35.08 35.14 35.01 34.38</cell></row><row><cell cols="5">4/256/2048/34.2M 35.08 35.46 34.21 0.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Results of using proximal initialization. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio ρ from 0.2 to 0.5, and the increment compared to the best results without proximal initialization.</figDesc><table><row><cell>(N a , d h )</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>∆</cell></row><row><cell cols="6">(2, 2048) 35.85 36.12 35.62 35.03 0.79</cell></row><row><cell cols="6">(3, 1024) 35.94 35.83 35.76 35.15 0.50</cell></row><row><cell cols="6">(3, 2048) 36.08 36.22 35.87 35.58 0.52</cell></row><row><cell cols="6">(4, 1024) 36.07 36.02 35.78 35.42 0.88</cell></row><row><cell cols="6">(4, 2048) 35.81 36.09 35.80 35.57 0.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Results on IWSLT Es→En with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with ρ ranging from 0.0 to 0.3.</figDesc><table><row><cell>Na/d/d h /Param</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell cols="4">Standard Transformer + Drop Branch</cell><cell></cell></row><row><cell cols="2">1/512/1024/36.7M 39.77</cell><cell>2.19</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>1/256/1024/13.7M</cell><cell>40.60</cell><cell>40.94</cell><cell>39.46</cell><cell>26.04</cell></row><row><cell>1/256/2048/20.0M</cell><cell>40.18</cell><cell>41.30</cell><cell>39.16</cell><cell>0.54</cell></row><row><cell cols="3">MAT + Drop Branch</cell><cell></cell><cell></cell></row><row><cell>2/256/1024/18.4M</cell><cell>40.30</cell><cell>41.02</cell><cell>40.89</cell><cell>40.45</cell></row><row><cell>2/256/2048/24.7M</cell><cell>40.28</cell><cell cols="3">41.20 41.96 41.06</cell></row><row><cell>3/256/1024/23.1M</cell><cell>40.42</cell><cell>41.04</cell><cell>40.99</cell><cell>40.59</cell></row><row><cell>3/256/2048/29.4M</cell><cell>39.74</cell><cell>40.72</cell><cell>41.63</cell><cell>40.80</cell></row><row><cell>4/256/1024/27.9M</cell><cell>40.17</cell><cell>40.99</cell><cell>40.81</cell><cell>40.67</cell></row><row><cell>4/256/2048/34.2M</cell><cell>39.94</cell><cell>40.69</cell><cell>41.29</cell><cell>40.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>Results on IWSLT En→Es with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with ρ ranging from 0.0 to 0.3.</figDesc><table><row><cell>Na/d/d h /Param</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell cols="4">Standard Transformer + Drop Branch</cell><cell></cell></row><row><cell cols="2">1/512/1024/36.7M 38.56</cell><cell>1.67</cell><cell>0.01</cell><cell>0.00</cell></row><row><cell>1/256/1024/13.7M</cell><cell>39.51</cell><cell cols="2">39.58 38.42</cell><cell>23.50</cell></row><row><cell>1/256/2048/20.0M</cell><cell>38.78</cell><cell cols="2">39.84 38.22</cell><cell>0.90</cell></row><row><cell cols="3">MAT + Drop Branch</cell><cell></cell><cell></cell></row><row><cell>2/256/1024/18.4M</cell><cell>38.81</cell><cell cols="2">39.56 38.98</cell><cell>39.08</cell></row><row><cell>2/256/2048/24.7M</cell><cell>38.77</cell><cell cols="2">39.33 39.86</cell><cell>39.37</cell></row><row><cell>3/256/1024/23.1M</cell><cell>38.31</cell><cell cols="2">39.12 39.77</cell><cell>39.29</cell></row><row><cell>3/256/2048/29.4M</cell><cell>38.22</cell><cell cols="3">39.89 39.57 39.90</cell></row><row><cell>4/256/1024/27.9M</cell><cell>38.37</cell><cell cols="2">39.16 39.55</cell><cell>39.49</cell></row><row><cell>4/256/2048/34.2M</cell><cell>38.40</cell><cell cols="2">39.05 39.24</cell><cell>39.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 18 :</head><label>18</label><figDesc>Results on IWSLT Fr→En with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with ρ ranging from 0.0 to 0.3.</figDesc><table><row><cell>Na/d/d h /Param</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell cols="4">Standard Transformer + Drop Branch</cell><cell></cell></row><row><cell cols="2">1/512/1024/36.7M 36.60</cell><cell>2.09</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>1/256/1024/13.7M</cell><cell>36.21</cell><cell>36.17</cell><cell>35.15</cell><cell>3.99</cell></row><row><cell>1/256/2048/20.0M</cell><cell>36.38</cell><cell>36.42</cell><cell cols="2">35.11 14.66</cell></row><row><cell cols="3">MAT + Drop Branch</cell><cell></cell><cell></cell></row><row><cell>2/256/1024/18.4M</cell><cell>35.63</cell><cell>36.28</cell><cell cols="2">36.37 35.61</cell></row><row><cell>2/256/2048/24.7M</cell><cell>35.90</cell><cell>35.83</cell><cell cols="2">36.34 36.08</cell></row><row><cell>3/256/1024/23.1M</cell><cell>35.72</cell><cell>35.71</cell><cell cols="2">36.11 35.59</cell></row><row><cell>3/256/2048/29.4M</cell><cell>35.55</cell><cell>36.30</cell><cell cols="2">36.22 36.27</cell></row><row><cell>4/256/1024/27.9M</cell><cell>35.52</cell><cell>36.30</cell><cell cols="2">36.06 36.12</cell></row><row><cell>4/256/2048/34.2M</cell><cell>34.98</cell><cell cols="3">36.67 36.24 35.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 19 :</head><label>19</label><figDesc>Results on IWSLT En→Fr with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with ρ ranging from 0.0 to 0.3.</figDesc><table><row><cell>Na/d/d h /Param</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell cols="4">Standard Transformer + Drop Branch</cell><cell></cell></row><row><cell cols="2">1/512/1024/36.7M 36.32</cell><cell>36.07</cell><cell>0.02</cell><cell>0.05</cell></row><row><cell>1/256/1024/13.7M</cell><cell>36.16</cell><cell>36.43</cell><cell>35.61</cell><cell>0.79</cell></row><row><cell>1/256/2048/20.0M</cell><cell>36.73</cell><cell>36.80</cell><cell>35.34</cell><cell>1.59</cell></row><row><cell cols="3">MAT + Drop Branch</cell><cell></cell><cell></cell></row><row><cell>2/256/1024/18.4M</cell><cell>36.41</cell><cell>36.48</cell><cell cols="2">36.57 36.34</cell></row><row><cell>2/256/2048/24.7M</cell><cell>35.95</cell><cell cols="3">37.20 36.90 36.44</cell></row><row><cell>3/256/1024/23.1M</cell><cell>35.79</cell><cell>36.52</cell><cell cols="2">36.56 36.34</cell></row><row><cell>3/256/2048/29.4M</cell><cell>35.88</cell><cell>36.88</cell><cell cols="2">36.88 36.86</cell></row><row><cell>4/256/1024/27.9M</cell><cell>35.77</cell><cell>36.68</cell><cell cols="2">37.09 36.50</cell></row><row><cell>4/256/2048/34.2M</cell><cell>35.23</cell><cell>36.65</cell><cell cols="2">36.97 36.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 20 :</head><label>20</label><figDesc>Results of using proximal initialization on IWSLT Es→En. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio ρ from 0.0 to 0.3, and the increment compared to the best results without proximal initialization. , 2048) 40.81 41.14 42.11 41.68 0.15 (3, 1024) 40.95 41.39 41.22 41.36 0.35 (3, 2048) 40.96 41.10 41.60 41.79 0.16 (4, 1024) 41.45 41.67 41.67 41.26 0.68 (4, 2048) 40.85 41.44 41.26 42.06 0.77</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell>(2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 21 :</head><label>21</label><figDesc>Results of using proximal initialization on IWSLT En→Es. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio ρ from 0.0 to 0.3, and the increment compared to the best results without proximal initialization. , 1024) 39.47 40.06 40.08 39.84 0.31 (3, 2048) 39.34 39.61 40.08 40.40 0.50 (4, 1024) 39.36 39.99 39.81 40.00 0.45 (4, 2048) 39.75 40.16 40.12 39.91 0.81</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell cols="6">(2, 2048) 39.47 39.78 40.46 40.34 0.60</cell></row><row><cell>(3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 22 :</head><label>22</label><figDesc>Results of using proximal initialization on IWSLT Fr→En. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio ρ from 0.0 to 0.3, and the increment compared to the best results without proximal initialization. , 2048) 36.14 36.76 37.41 36.82 0.77(3, 1024)  36.73 37.03 37.44 37.10 1.33(3, 2048)  36.32 36.92 37.28 37.25 0.98 (4, 1024) 36.81 37.14 37.10 36.88 0.73 (4, 2048) 36.48 36.90 37.40 37.29 0.73</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell>(2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 23 :</head><label>23</label><figDesc>Results of using proximal initialization on IWSLT En→Fr. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio ρ from 0.0 to 0.3, and the increment compared to the best results without proximal initialization.</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 24 :</head><label>24</label><figDesc>Results on IWSLT Es→En with randomly dropping heads technique. Embedding dimension d is fixed as 256. 41.54 41.68 −0.43 (3, 1024) 40.95 41.63 41.25 41.15 +0.24 (3, 2048) 40.96 41.30 41.39 41.86 +0.07 (4, 1024) 41.45 41.41 41.04 41.29 −0.22 (4, 2048) 40.85 41.30 41.48 41.70 −0.36</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell cols="2">(2, 2048) 40.81</cell><cell>41.61</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 25 :</head><label>25</label><figDesc>Results on IWSLT En→Es with randomly dropping heads technique. Embedding dimension d is fixed as 256. , 1024) 39.47 39.99 39.96 39.41 −0.09 (3, 2048) 39.34 39.47 40.22 39.67 −0.18 (4, 1024) 39.36 39.71 39.59 39.89 −0.11 (4, 2048) 39.75 39.73 39.94 40.36 +0.20</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell cols="3">(2, 2048) 39.47 39.85</cell><cell cols="3">39.98 40.39 −0.07</cell></row><row><cell>(3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 26 :</head><label>26</label><figDesc>Results on IWSLT Fr→En with randomly dropping heads technique. Embedding dimension d is fixed as 256. , 2048) 36.14 37.10 36.85 36.71 −0.31 (3, 1024) 36.73 36.78 36.83 36.20 −0.61 (3, 2048) 36.32 36.73 36.92 37.08 −0.20 (4, 1024) 36.81 36.29 36.82 36.49 −0.32 (4, 2048) 36.48 36.39 36.89 36.99 −0.41</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell>(2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 27 :</head><label>27</label><figDesc>Results on IWSLT En→Fr with randomly dropping heads technique. Embedding dimension d is fixed as 256. , 2048) 36.73 37.18 37.48 37.15 +0.24 (3, 1024) 36.71 37.14 37.48 36.86 +0.18 (3, 2048) 36.51 36.93 37.17 36.97 −0.27 (4, 1024) 36.67 36.59 37.02 36.99 −0.22 (4, 2048) 36.70 36.81 37.80 37.37 +0.48</figDesc><table><row><cell>(N a , d h )</cell><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>∆</cell></row><row><cell>(2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/pytorch/fairseq 3 The URLs of scripts we used in this paper are summarized in Appendix C. 4 https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The data is available at http://www.statmt.org/wmt19/translation-task.html. We concatenate Europarl v9, Common Crawl corpus, News Commentary v14 and Document-split Rapid corpus.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The urls of the datasets and tools we used for code generation are summarized in Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Models from page https://github.com/pytorch/fairseq/tree/master/examples/roberta.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The multi-head attention in standard Transformer is also one kind of multi-branch attention. For simplicity, we use multi-head attention to denote the original attention layer in standard Transformer, and multi-branch attention to denote the newly introduced multi-branch attention in the multi-branch manner (both averaging and concatenation manner).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Exploring the Drop Branch in MAT</head><p>We explore more values of drop branch rate ρ, and the results are reported in <ref type="table">Table 14</ref>.</p><p>From <ref type="table">Table 14</ref>, we can see that increasing ρ to 0.4 and 0.5 will hurt the performance of MAT.</p><p>On IWSLT'14 De→En, with proximal initialization, we also explore increasing ρ to 0.4 and 0.5. The results are in <ref type="table">Table 15</ref>. Still, setting ρ larger than 0.4 will hurt the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Exploration on Other IWSLT Tasks B.1 Results of MAT</head><p>We apply the settings in <ref type="table">Table 1</ref> of the main paper to all four other IWSLT tasks. The results without proximal initialization are reported from <ref type="table">Table 16</ref> to <ref type="table">Table 19</ref>. The results with promixal initialization are shown from <ref type="table">Table 20</ref> to <ref type="table">Table 23</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The script to preprocess the IWSLT data</title>
		<ptr target="https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Weighted transformer network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02132</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing transformer depth on demand with structured dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Summarizing source code with transferred api knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2269" to="2275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">A robustly optimized bert pretraining approach. ICLR</title>
		<meeting><address><addrLine>Roberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Proximal Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Now Publishers, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations. NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<title level="m">The evolved transformer. ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Double path networks for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>COLING</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving automatic source code summarization via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering</title>
		<meeting>the 33rd ACM/IEEE International Conference on Automated Software Engineering</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Code generation as a dual task of code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6559" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Muse: Parallel multi-scale attention for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09483</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep recurrent models with fast-forward connections for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
