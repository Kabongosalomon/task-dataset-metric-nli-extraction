<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universum Prescription: Regularization Using Unlabeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<address>
									<addrLine>719 Broadway, 12th Floor</addrLine>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<address>
									<addrLine>719 Broadway, 12th Floor</addrLine>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Universum Prescription: Regularization Using Unlabeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper shows that simply prescribing "none of the above" labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter -probability of sampling from unlabeled data -is also studied empirically.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The idea of exploiting the wide abundance of unlabeled data to improve the accuracy of supervised learning tasks is a very natural one. In this paper, we study what is perhaps the simplest way to exploit unlabeled data in the context of deep learning. We assume that the unlabeled samples do not belong to any of the categories of the supervised task, and we force the classifier to produce a "none of the above" output for these samples. This is by no means a new idea, but we show empirically and theoretically that doing so has a regularization effect on supervised task and reduces the generalization gap, the expected difference between test and training errors. We study three different ways to prescribe "none of the above" outputs, dubbed uniform prescription, dustbin class, and background class and show that they improve the test error of convolutional networks trained on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b16">(Krizhevsky 2009</ref>), STL-10 <ref type="bibr" target="#b5">(Coates, Ng, and Lee 2011)</ref>, and ImageNet <ref type="bibr" target="#b29">(Russakovsky et al. 2015)</ref>. The method is theoretically justified using Radamacher complexity <ref type="bibr" target="#b0">(Bartlett and Mendelson 2003)</ref>.</p><p>Here we briefly describe our three universum prescription methods. Uniform prescription forces a discrete uniform distribution for unlabeled samples. Dustbin class simply adds an extra class and prescribe all unlabeled data to this class. Background class also adds an extra class, but it uses a constant threshold to avoid parameterization.</p><p>Our work is a direct extension to learning in the presence of universum <ref type="bibr" target="#b42">(Weston et al. 2006</ref>) ), Copyright c 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. originated from <ref type="bibr" target="#b40">(Vapnik 1998)</ref> and . The definition of universum is a set of unlabeled data that are known not to belong to any of the classes but in the same domain. We extended the idea of using universum from support vector machines to deep learning.</p><p>Most deep learning approaches utilizing unlabeled data belong to the scope of representation learning (reviewed by <ref type="bibr" target="#b2">(Bengio, Courville, and Vincent 2013)</ref> and <ref type="bibr">(Bengio and Le-Cun 2007)</ref>) and transfer learning <ref type="bibr" target="#b36">(Thrun and Pratt 1998)</ref>. They include ideas like pretraining <ref type="bibr" target="#b7">(Erhan et al. 2010</ref>) <ref type="bibr" target="#b12">(Hinton, Osindero, and Teh 2006)</ref>  ) and semisupervised training <ref type="bibr" target="#b28">(Rasmus et al. 2015</ref>) <ref type="bibr" target="#b46">(Zhao et al. 2015)</ref>. Universum prescription incoporates unlabeled data without imposing priors such as sparsity or reconstruction.</p><p>Regularization -techniques for the control of generalization gap -has been studied extensively. Most approaches implement a secondary optimization objective, such as an L 2 norm. Some other methods such as dropout <ref type="bibr" target="#b33">(Srivastava et al. 2014)</ref> cheaply simulate model averaging to control the model variance. As part of general statistical learning theory <ref type="bibr" target="#b40">(Vapnik 1995)</ref>, <ref type="bibr" target="#b40">(Vapnik 1998)</ref>, the justification for regularization is well-developed. We qualitatively justify the methods using Radamacher complexity <ref type="bibr" target="#b0">(Bartlett and Mendelson 2003)</ref>, similar to <ref type="bibr" target="#b41">(Wan et al. 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Universum Prescription</head><p>In this section we attempt to formalize the trick of prescribing "none of the above" labels -universum prescription. Consider the problem of exclusive k-way classification. In learning we hope to find a hypothesis function h ∈ H mapping to R k so that the label is determined by y = argmin i h i (x). The following assumptions are made. 1. (Loss assumption) The loss used as the optimization objective is negative log-likelihood:</p><formula xml:id="formula_0">L(h, x, y) = h y (x) + log k i=1 exp(−h i (x)) . (1)</formula><p>2. (Universum assumption) The proportion of unlabeled samples belonging to a supervised class is negligible. The loss assumption assumes that the probability of class y given an input x can be thought of as</p><formula xml:id="formula_1">Pr[Y = y|x, h] = exp(−h y (x)) k i=1 exp(−h i (x)) ,<label>(2)</label></formula><p>where (X, Y ) ∼ D and D is the distribution where labeled data are sampled. We use lowercase letters for values, uppercase letters for random variables and bold uppercase letters for distribution. The loss assumption is simply a necessary detail rather than a limitation, in the sense that one can change the type of loss and use the same principles to derive different universum learning techniques. The universum assumption implicates that labeled classes are a negligible subset. In many practical cases we only care about a small number of classes, either by problem design or due to high cost in the labeling process. At the same time, a very large amount of unlabeled data is easily obtained. Put in mathematics, assuming we draw unlabeled data from distribution U, the assumption states that</p><formula xml:id="formula_2">Pr (X,Y )∼U [X, Y ∈ {1, 2, . . . , k}] ≈ 0.<label>(3)</label></formula><p>The universum assumption is opposite to the assumptions of information regularization <ref type="bibr" target="#b6">(Corduneanu and Jaakkola 2006)</ref> and transduction learning <ref type="bibr" target="#b4">(Chapelle, Schlkopf, and Zien 2006)</ref>  <ref type="bibr" target="#b8">(Gammerman, Vovk, and Vapnik 1998)</ref>. It has similarities with (Zhang and Zhou 2010) that encourages diversity of outputs for ensemble methods. All our methods discussed below prescribe agnostic targets to the unlabeled data. During learning, we randomly present an unlabeled sample to the optimization procedure with probability p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uniform Prescription</head><p>It is known that negative log-likelihood is simply a reduced form of cross-entropy</p><formula xml:id="formula_3">L(h, x, y) = − k i=1 Q[Y = i|x] log Pr[Y = i|x, h] (4)</formula><p>in which the target probability Q[Y = y|x] = 1 and Q[Y = i|x] = 0 for i = y. Under the universum assumption, if we are presented with an unlabeled sample x, we would hope to prescribe some Q so that every class has some equally minimal probability. Q also has to satisfy k i=1 Q[Y = i|x] = 1 by the probability axioms. The only possible choice for Q is then Q[Y |x] = 1/k. The learning algorithm then uses the cross-entropy loss instead of negative log-likelihood.</p><p>It is worth noting that uniform output has the maximum entropy among all possible choices. If h is parameterized as a deep neural network, uniform output is achieved when these parameters are constantly zero. Therefore, uniform prescription may have the effect of reducing the magnitude of parameters, similar to norm-based regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dustbin Class</head><p>Another way of prescribing agnostic target is to append a "dustbin" class to the supervised task. This requires some changes to the hypothesis function h such that it outputs k + 1 targets. For deep learning models one can simply extend the last parameterized layer. All unlabeled data are prescribed to this extra "dustbin" class.</p><p>The effect of dustbin class is clearly seen in the loss function of an unlabeled sample (x, k + 1)</p><formula xml:id="formula_4">L(h, x, k + 1) = h k+1 (x) + log k+1 i=1 exp(−h i (x)) . (5)</formula><p>The second term is a "soft" maximum for all dimensions of −h. With an unlabeled sample, the algorithm attempts to introduce smoothness by minimizing probability spikes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background Class</head><p>We could further simplify dustbin class by removing parameters for class k + 1. For some given threshold constant τ , we could change the probability of a labeled sample to</p><formula xml:id="formula_5">Pr[Y = y|x, h] = exp(−h y (x)) exp(−τ ) + k i=1 exp(−h i (x)) ,<label>(6)</label></formula><p>and an unlabeled sample</p><formula xml:id="formula_6">Pr[Y = k + 1|x, h] = exp(−τ ) exp(−τ ) + k i=1 exp(−h i (x))</formula><p>.</p><p>(7) This will result in changes to the loss function of a labeled sample (x, y) as</p><formula xml:id="formula_7">L(h, x, y) = h y (x) + log exp(−τ ) + k i=1 exp(−h i (x)) ,<label>(8)</label></formula><p>and an unlabeled sample We call this method background class and τ background constant. Similar to dustbin class, the algorithm attempts to minimize the spikes of outputs, but limited to a certain extent by the inclusion of exp(−τ ) in the partition function. In our experiments τ is always set to 0.</p><formula xml:id="formula_8">L(h, x, k + 1) = τ + log exp(−τ ) + k i=1 exp(−h i (x)) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Justification</head><p>In this part, we derive a qualitative justification for universum prescription using probably approximately correct (PAC) learning <ref type="bibr" target="#b38">(Valiant 1984)</ref>. By being "qualitative", the justification is in contrast with numerical bounds such as Vapnik-Chervonenkis dimension <ref type="bibr" target="#b39">(Vapnik and Chervonenkis 1971</ref>) (VC-dim) and others. Our theory is based on Rademacher complexity <ref type="bibr" target="#b0">(Bartlett and Mendelson 2003)</ref>, similar to <ref type="bibr" target="#b41">(Wan et al. 2013)</ref> where both dropout <ref type="bibr" target="#b33">(Srivastava et al. 2014</ref>) and dropconnect <ref type="bibr" target="#b41">(Wan et al. 2013</ref>) are justified. VC-dim is an upper-bound of Rademacher complexity, suggesting that the latter is more accurate. Previous results on unlabeled data <ref type="bibr" target="#b24">(Oneto et al. 2011</ref>) <ref type="bibr" target="#b25">(Oneto et al. 2015)</ref> assume the same distribution for labeled and unlabeled data, which is impossible under the universum assumption. Definition 1 (Empirical Rademacher complexity). Let F be a family of functions mapping from X to R, and S = (x 1 , x 2 , . . . , x m ) a fixed sample of size m with elements in X . Then, the empirical Rademacher complexity of F with respect to the sample S is defined as:</p><formula xml:id="formula_9">R S (F) = E η sup f ∈F 1 m m i=1 η i f (x i ) (10) where η = (η 1 , . . . , η m ) T , with η i 's independent random variables uniformly distributed on {−1, 1}.</formula><p>Definition 2 (Rademacher complexity). Let D denote the distribution from which the samples were drawn. For any integer m ≥ 1, the Rademacher complexity of F is the expectation of the empirical Rademacher complexity over all samples of size m drawn according to D:</p><formula xml:id="formula_10">R m (F, D) = E S∼D m [R S (F )]<label>(11)</label></formula><p>Two qualitative properties of Rademacher complexity is worth noting here. First of all, Rademacher complexity is always non-negative by the convexity of supremum</p><formula xml:id="formula_11">R S (F) = E η sup f ∈F 1 m m i=1 η i f (x i ) ≥ sup f ∈F 1 m m i=1 E ηi [η i ]f (x i ) = 0.<label>(12)</label></formula><p>Secondly, if for a fixed input all functions in F output the same value, then its Rademacher complexity is 0. Assume</p><formula xml:id="formula_12">for any f ∈ F we have f (x) = f 0 (x), then R S (F) = E η sup f ∈F 1 m m i=1 η i f (x i ) = E η sup f ∈F 1 m m i=1 η i f 0 (x) = 1 m m i=1 E ηi [η i ]f 0 (x) = 0.<label>(13)</label></formula><p>Therefore, one way to minimize Rademacher complexity is to regularize functions in F such that all functions tend to have the same output for a given input. Universum prescription precisely does that -the prescribed outputs for unlabeled data are all constantly the same.</p><p>The principal PAC-learning result is a bound for functions that are finite in outputs. We use the formulation by <ref type="bibr" target="#b45">(Zhang 2013)</ref>, but anterior results exist <ref type="bibr" target="#b1">(Bartlett, Boucheron, and Lugosi 2002)</ref>  <ref type="bibr" target="#b0">(Bartlett and Mendelson 2003)</ref>  <ref type="bibr" target="#b15">(Koltchinskii 2001</ref>) <ref type="bibr" target="#b14">(Koltchinskii and Panchenko 2000)</ref>. Theorem 1 (Approximation bound with finite bound on output). For an energy function ) E(h, x, y) over hypothesis class H, input set X and output set Y, if it has lower bound 0 and upper bound M &gt; 0, then with probability at least 1 − δ, the following holds for all h ∈ H:</p><formula xml:id="formula_13">E (x,y)∼D [E(h, x, y)] ≤ 1 m (x,y)∈S E(h, x, y) + 2R m (F, D) + M log 2 δ 2m ,<label>(14)</label></formula><p>where the function family F is defined as</p><formula xml:id="formula_14">F = {E(h, x, y)|h ∈ H} .<label>(15)</label></formula><p>D is the distribution for (x, y), and S is a sample set of size m drawn indentically and independently from D.</p><p>The meaning of the theorem is two-fold. When applying the theorem to the joint problem of training using both labeled and unlabeled data, the third term on the right hand of inequality 14 is reduced by the augmentation of the extra data. The joint Rademacher complexity is written as R m (F, (1 − p)D + pU), which is reduced when we prescribe constant outputs to unlabeled data.</p><p>The second fold is that when the theorem applies to the supervised distribution D, we would hope that R n (F, D) can be bounded by R m (F, (1 − p)D + pU), where n is the number of supervised samples randomly chosen by the joint problem. Note that the number n follows a binomial distribution with mean (1 − p)m. Such a bound can be achieved in a probable and approximate sense. Theorem 2 (Rademacher complexity bound on distribution mixture). Assume we have a joint problem where p ≤ 0.5 and there are m random training samples from the joint distribution (1 − p)D + pU. With probability at least 1 − δ, the following holds</p><formula xml:id="formula_15">R n (F, D) ≤ 2 − p (1 − p) 1 − p − log(1/δ) 2m R m (F, (1 − p)D + pU),<label>(16)</label></formula><p>where n is a random number indicating the number of supervised samples in the total joint samples, and m is large enough such that</p><formula xml:id="formula_16">1 − p − log(1/δ) 2m &gt; 0.<label>(17)</label></formula><p>We present the proof of theorem 2 in the supplemental material, which utilizes Hoeffding's inequality (Hoeffding 1963) <ref type="bibr" target="#b30">(Serfling 1974)</ref>. The theorem tells us that the Rademacher complexity of the supervised problem can be bounded by that of the joint problem. The universum prescription algorithm attempts to make the Rademacher complexity of the joint problem small. Therefore, universum prescription improves generalization by incorporating unlabeled data.</p><p>However, theorem 2 has a requirement that p ≤ 0.5, otherwise the bound is not achievable. Also, the value of (2 − p)/(1 − p) 2 -the asymptotic constant factor in inequality 16 when m is large -is monitonally increasing with respect to p with a range of [2, 6] when p ≤ 0.5. These facts indicate that we need to keep p small. The following sections show that there is improvement if p is small, but training and testing errors became worse when p is large. Finally, in terms of numerical asymptotics, theorem 2 suggests that R n (F, D) ≤ O(1/ √ m), instead of the commonly known result R n (F, D) ≤ O(1/ √ n). This bounds the supervised problem with a tighter asymptotical factor because there are more joint samples than supervised samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Image Classification</head><p>In this section we test the methods on some image classification tasks. Three series of datasets -CIFAR-10/100 <ref type="bibr" target="#b16">(Krizhevsky 2009</ref>), STL-10 (Coates, Ng, and Lee 2011) and ImageNet <ref type="bibr" target="#b29">(Russakovsky et al. 2015</ref>) -are chosen due to the availability of unlabeled data. For CIFAR-10/100 and STL-10 datasets, we used a 21-layer convolutional network (Con-vNet) <ref type="bibr" target="#b17">(LeCun et al. 1989</ref><ref type="bibr" target="#b18">) (LeCun et al. 1998</ref>, in which the inputs are 32-by-32 images and all convolutional layers are 3-by-3 and fully padded. For ImageNet, the model is a 17-layer ConvNet with 64-by-64 images as inputs. These models are inspired by <ref type="bibr" target="#b32">(Simonyan and Zisserman 2014)</ref>, in which all pooling layers are max-pooling, and ReLUs <ref type="bibr" target="#b23">(Nair and Hinton 2010)</ref> are used as the non-linearity. Two dropout <ref type="bibr" target="#b33">(Srivastava et al. 2014</ref>) layers of probability 0.5 are inserted before the final two linear layers.</p><p>The algorithm used is stochastic gradient descent with momentum <ref type="bibr" target="#b26">(Polyak 1964)</ref>  <ref type="bibr" target="#b34">(Sutskever et al. 2013)</ref> 0.9 and a minibatch size of 32. The initial learning rate is 0.005 which is halved every 60,000 minibatch steps for CIFAR-10/100  and every 600,000 minibatch steps for ImageNet. The training stops at 400,000 steps for CIFAR-10/100 and STL10, and 2,500,000 steps for ImageNet. <ref type="table" target="#tab_0">Table 1</ref> and 2 summarize the configurations. The weights are initialized in the same way as <ref type="bibr" target="#b11">(He et al. 2015)</ref>. The following data augmentation steps are used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 and CIFAR-100</head><p>The samples of CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b16">(Krizhevsky 2009</ref>) are from the 80 million tiny images dataset <ref type="bibr" target="#b37">(Torralba, Fergus, and Freeman 2008)</ref>. Each dataset contains 60,000 samples, consitituting a very small portion of 80 million. This is an ideal case for our methods, in which we can use the entire 80 million images as the unlabeled data. The CIFAR-10 dataset has 10 classes, and CIFAR-100 has 20 (coarse) or 100 (fine-grained) classes. <ref type="table" target="#tab_2">Table 3</ref> and 4 contain the results. The three numbers in each tabular indicate training error, testing error and generalization gap. Bold numbers are the best ones for each case. The generalization gap is approximated by the difference between testing and training errors. All the models use unlabeled data with probability p = 0.2.</p><p>We compared other single-model results on CIFAR-10 and CIFAR-100 (fine-grained case) in table 5. It shows that our network is competitive to the state of the art. Although <ref type="bibr" target="#b10">(Graham 2014)</ref> has the best results, we believe that by applying out universum prescription methods to their model design could also improve the results further.  <ref type="bibr" target="#b9">(Goodfellow et al. 2013</ref><ref type="bibr">) 9.38 38.57 YES (Wan et al. 2013</ref> 11.10 N/A NO  15.13 42.51 NO</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STL-10</head><p>The STL-10 dataset <ref type="bibr" target="#b5">(Coates, Ng, and Lee 2011)</ref> has size 96-by-96 for each image. We downsampled them to 32-by-32 in order to use the same model. The dataset contains a very small number of training samples -5000. The accompanying unlabeled data contain 100,000 samples. There is no guarantee that these unlabeled samples do not blong to the supervised classes <ref type="bibr" target="#b5">(Coates, Ng, and Lee 2011)</ref>, therefore universum prescription failed. To verify that the extra data is the problem, an experiment using the 80 million tiny images as the unlabeled dataset is shown in table 3 and 4. In this case the improvement is observed. Due to long training times of our models, we did not perform 10-fold training in the original paper <ref type="bibr" target="#b5">(Coates, Ng, and Lee 2011)</ref>. One interesting observation is that the results on STL-10 became better with the use of 80 million tiny images instead of the original extra data. It indicates that dataset size and whether universum assumption is satisfied are affecting factors for the effectiveness of universum prescription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet</head><p>The ImageNet dataset <ref type="bibr" target="#b29">(Russakovsky et al. 2015</ref>) for classification task has in total 1,281,167 training images and 50,000 validation images. The reported testing errors are evaluated on this validation dataset. During training, we resize images to minimum dimension 64, and then feed a random 64-by-64 crop to the network. Same test-time augmentation technique as in <ref type="bibr" target="#b35">(Szegedy et al. 2015)</ref> are applied, with size variants {64, 72, 80, 88}, where each image is viewed in 144 crops.</p><p>The extra data comes from the large ImageNet 2011 release 1 , for which we only keep the classes whomself and whose children do not belong to the supervised classes. This is enabled by the super-subordinate (is-a) relation information provided with the WordNet distribution <ref type="bibr" target="#b22">(Miller 1995)</ref> 1 http://www.image-net.org/releases because all ImageNet classes are nouns of WordNet. Both top-1 and top-5 results are reported in tables 3 and 4.</p><p>In all experiments dustbin class provides best results. We believe that it is because the extra class is parameterized, which makes it adapt better on the unlabeled samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the Regularization Parameter</head><p>It is natural to ask how would the change of the probability p of sampling from unlabeled data affect the results. In this section we show the experiments. To prevent an exhaustive search on the regularization parameter from overfitting our models on the testing data, we use a different model for this section. It is described in table 6, which has 9 parameterized layers in total. The design is inspired by <ref type="bibr" target="#b31">(Sermanet et al. 2013)</ref>. For each choice of p we conducted 6 experiments combining universum prescription models and dropout. The dropout layers are two ones added in between the fully-connected layers with dropout probability 0.5. <ref type="figure" target="#fig_0">Figure 1</ref> shows the results. From <ref type="figure" target="#fig_0">figure 1</ref> we can conclude that increasing p will descrease generalization gap. However, we cannot make p too large since after a certain point the training collapses and both training and testing errors become worse. This confirms the assumptions and conclusions from theorem 2.</p><p>Comparing between CIFAR-10/100 and STL-10, one conclusion is that that the model variance is affected by the combined size of labeled and unlabeled datasets. The variance on training and testing errors are extremely small on CIFAR-10/100 datasets because the extra data we used is almost unlimited (in total 80 million), but on STL-10 the variance seems to be large with much smaller combined size of training and extra datasets. This suggests that using universum prescription with a large abundance of extra data could improve the stability of supervised learning algorithms.</p><p>Finally, the comparison between using and not using dropout does not show a difference. This suggests that the regularization effect of universum prescription alone is comparable to that of dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Outlook</head><p>This article shows that universum prescription can be used to regularize a multi-class classification problem using extra unlabeled data. Two assumptions are made. One is that loss used is negative log-likelihood and the other is negligible probability of a supervised sample existing in the unlabeled data. The loss assumption is a necessary detail rather than a limitation. The three universum prescription methods are uniform prescription, dustbin class and background class. We further provided a theoretical justification. Theorem 2 suggests that asymptotically the generalization ability of the supervised problem could be bounded by the joint problem, which has more samples due to the addition of unlabeled data. Experiments are done using CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. The effect of the regularization parameter is also studied empirically.</p><p>These experiments show that all three universum prescrition methods provide certain improvement over the generalization gap, whereas dustbin class constantly performs the best because the parameterized extra class can adapt better to the unlabeled samples. Further conclusions include that additional unlabeled data can improve the variance of models during training, and that the results are comparable to data-agnostic regularization using dropout.</p><p>In the future, we hope to apply these methods to a broader range of problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental: proof of theorem 2</head><p>This supplemental material shares the bibliography of the main paper. As an outline of our proof, we first establish a relation between R m (F, D) and R m (F, (1−p)D+pU), and then another relation between R n (F, D) and R m (F, D). The first part requires the following lemmas.</p><p>Lemma 1 (Separation of dataset on empirical Rademacher complexity). Let S be a dataset of size m. If S 1 and S 2 are two non-overlap subset of S such that |S 1 | = m−i, |S 2 | = i and S 1 ∪ S 2 = S, then the following two inequalities hold</p><formula xml:id="formula_17">R S (F) ≤ m − i mR S1 (F) + i mR S2 (F).<label>(18)</label></formula><p>Proof. Let (x j , y j ) ∈ S 1 for j = 1, 2, . . . , m − i and (x j , y j ) ∈ S 2 for i = m − j + 1, m − j + 2, . . . , m. Denote N as the discrete uniform distribution on {1, −1}. We can derive by the convexity of supremum and symmetry of N</p><formula xml:id="formula_18">R S (F) = E η∼N m   sup f ∈F 1 m m j=1 η j f (x j )   = 2 m E η∼N m   sup f ∈F   1 2 m−i j=1 η j f (x j ) + 1 2 m j=m−i+1 η j f (x j )     ≤ 2 m E η∼N m   1 2 sup f ∈F   m−i j=1 η j f (x j )   + 1 2 sup f ∈F   m j=m−i+1 η j f (x j )     = m − i m E η∼N m−i   sup f ∈F 1 m − i m−i j=1 η j f (x j )   + i m E η∼N i   sup f ∈F 1 i m j=m−i+1 η j f (x j )   = m − i mR S1 (F) + i mR S2 (F).</formula><p>Lemma 2 (Sample size inequality for Rademacher complexity). Assume 0 ≤ n ≤ m. If |S n | = n, |S m | = m and S m = S n ∪ {x n+1 , x n+2 , . . . , x m }, then</p><formula xml:id="formula_19">nR Sn (F) ≤ mR Sm (F),<label>(19)</label></formula><p>and nR n (F, D) ≤ mR m (F, D).</p><p>Proof. First of all, it is obvious that inequality 20 can be established using mathematical induction if we have mR m (F, D) ≤ (m + 1)R m+1 (F, D) for all m ≥ 0. To prove this, we first establish that if S m = {x 1 , x 2 , . . . , x m } and S m+1 = {x 1 , x 2 , . . . , x m , x m+1 } (i.e., S m+1 = S m ∪ {x m+1 }), then mR Sm (F) ≤ (m+1)R Sm+1 (F), which can also establish inequality 19. For any η m = {η 1 , η 2 , . . . , η m } and η m+1 = {η 1 , η 2 , . . . , η m , η m+1 }, that is, η m+1 = η m ∪ {η m+1 }, let f 0 = argmax f ∈F m i=1 η i f (x i ). By definition of supremum, we have</p><formula xml:id="formula_21">sup f ∈F m+1 i=1 η i f (x i ) ≥ m+1 i=1 η i f 0 (x i ) = m i=1 η i f 0 (x i ) + η m+1 f 0 (x m+1 ) = sup f ∈F m i=1</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Experiments on regularization parameter. The four rows are CIFAR-10, CIFAR-100 fine-grained, CIFAR-100 coarse and STL-10 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The 21-layer network</figDesc><table><row><cell cols="2">LAYERS DESCRIPTION</cell></row><row><cell>1-3</cell><cell>Conv 256x3x3</cell></row><row><cell>4</cell><cell>Pool 2x2</cell></row><row><cell>5-8</cell><cell>Conv 512x3x3</cell></row><row><cell>9</cell><cell>Pool 2x2</cell></row><row><cell>10-13</cell><cell>Conv 1024x3x3</cell></row><row><cell>14</cell><cell>Pool 2x2</cell></row><row><cell>15-18</cell><cell>Conv 1024x3x3</cell></row><row><cell>19</cell><cell>Pool 2x2</cell></row><row><cell>20-23</cell><cell>Conv 2048x3x3</cell></row><row><cell>24</cell><cell>Pool 2x2</cell></row><row><cell>25-26</cell><cell>Full 2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The 17-layer network</figDesc><table><row><cell cols="2">LAYERS DESCRIPTION</cell></row><row><cell>1-3</cell><cell>Conv 128x3x3</cell></row><row><cell>4</cell><cell>Pool 2x2</cell></row><row><cell>5-7</cell><cell>Conv 256x3x3</cell></row><row><cell>8</cell><cell>Pool 2x2</cell></row><row><cell>9-11</cell><cell>Conv 512x3x3</cell></row><row><cell>12</cell><cell>Pool 2x2</cell></row><row><cell>13-15</cell><cell>Conv 1024x3x3</cell></row><row><cell>16</cell><cell>Pool 2x2</cell></row><row><cell>17-19</cell><cell>Conv 2048x3x3</cell></row><row><cell>20</cell><cell>Pool 2x2</cell></row><row><cell>21-22</cell><cell>Full 4096</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Result for baseline and uniform prescription. The numbers are percentages.</figDesc><table><row><cell>DATASET</cell><cell>BASELINE</cell><cell>UNIFORM</cell></row><row><cell></cell><cell cols="2">Train Test Gap Train Test Gap</cell></row><row><cell>CIFAR-10</cell><cell cols="2">0.00 7.02 7.02 0.72 7.59 6.87</cell></row><row><cell cols="3">CIFAR-100 F. 0.09 37.58 37.49 4.91 36.23 31.32</cell></row><row><cell cols="3">CIFAR-100 C. 0.04 22.74 22.70 0.67 23.42 22.45</cell></row><row><cell>STL-10</cell><cell cols="2">0.00 31.16 31.16 2.02 36.54 34.52</cell></row><row><cell>STL-10 Tiny</cell><cell cols="2">0.00 31.16 31.16 0.62 30.15 29.47</cell></row><row><cell>ImageNet-1</cell><cell cols="2">10.19 34.39 24.20 13.84 34.61 20.77</cell></row><row><cell>ImageNet-5</cell><cell cols="2">1.62 13.68 12.06 3.02 13.70 10.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: Result for dustbin class and background class. Con-</cell></row><row><cell cols="2">tinuation of table 3</cell><cell></cell></row><row><cell>DATASET</cell><cell>DUSTBIN</cell><cell>BACKGROUND</cell></row><row><cell></cell><cell cols="2">Train Test Gap Train Test Gap</cell></row><row><cell>CIFAR-10</cell><cell cols="2">0.07 6.66 6.59 1.35 8.38 7.03</cell></row><row><cell cols="3">CIFAR-100 F. 2.52 32.84 30.32 8.56 40.57 42.01</cell></row><row><cell cols="3">CIFAR-100 C. 0.40 20.45 20.05 3.73 24.97 21.24</cell></row><row><cell>STL-10</cell><cell cols="2">3.03 36.58 33.55 14.89 38.95 24.06</cell></row><row><cell>STL-10 Tiny</cell><cell cols="2">0.00 27.96 27.96 0.11 30.38 30.27</cell></row><row><cell>ImageNet-1</cell><cell cols="2">13.80 33.67 19.87 13.43 34.69 21.26</cell></row><row><cell>ImageNet-5</cell><cell cols="2">2.83 13.35 10.52 2.74 13.84 11.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of single-model CIFAR-10 and CIFAR-100 results, in second and third columns. The fourth column indicates whether data augmentation is used for CIFAR-10. The numbers are percentages.</figDesc><table><row><cell>REF.</cell><cell>10</cell><cell>100</cell><cell>AUG.</cell></row><row><cell>(Graham 2014)</cell><cell cols="3">6.28 24.30 YES</cell></row><row><cell>(ours)</cell><cell cols="3">6.66 32.84 YES</cell></row><row><cell>(Lee et al. 2015)</cell><cell cols="3">7.97 34.57 YES</cell></row><row><cell cols="4">(Lin, Chen, and Yan 2013) 8.81 35.68 YES</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>ConvNet for the study of p</figDesc><table><row><cell cols="2">LAYERS DESCRIPTION</cell></row><row><cell>1</cell><cell>Conv 1024x5x5</cell></row><row><cell>2</cell><cell>Pool 2x2</cell></row><row><cell>3</cell><cell>Conv 1024x5x5</cell></row><row><cell>4-7</cell><cell>Conv 1024x3x3</cell></row><row><cell>8</cell><cell>Pool 2x2</cell></row><row><cell>9-11</cell><cell>Full 2048</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge NVIDIA Corporation with the donation of 2 Tesla K40 GPUs used for this research. Sainbayar Sukhbaatar offered many useful comments. Aditya Ramesh and Junbo Zhao helped cross-checking the proofs.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The third part takes advantage of the assumption that p ≤ 0.5. We know that for m/2 + 1 ≤ i ≤ m, the assumption p ≤ 0.5 implies</p><p>Therefore, using the first part, we achieve</p><p>By combining all the three parts above, we establish</p><p>The proof for theorem 3 is therefore concluded.</p><p>The relation between R n (F, D) and R m (F, D) is achieved by the following theorem. Theorem 4 (Concentration inequality of subset Rademacher complexity). Assume in solving the joint problem we obtained m idependently and identically distributed samples. Let the random number n represent the number of supervised sample obtained among these m joint samples with a proprtion probability of 1−p. Then, with probability at least 1 − δ, the following holds</p><p>for large enough m such that</p><p>Proof. Using lemma 2, we only need to prove an upper bound for m/n. Since we know that n follows a binomial distribution with mean (1 − p)m, using Hoeffding's inequality <ref type="bibr" target="#b13">(Hoeffding 1963</ref>) <ref type="bibr" target="#b30">(Serfling 1974)</ref>, we can obtain</p><p>or put differently,</p><p>The inequality is obtained by setting δ = exp(−2 2 m). The proof assumes that m is large enough such that 1 − p − log(1/δ) 2m &gt; 0.</p><p>As a result, theorem 2 can be obtained by directly combining theorem 3 and theorem 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model selection and error estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="85" to="113" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Scaling learning algorithms towards ai</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of inference with the universum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1369" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Discussion of Semi-Supervised Learning and Transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="473" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Data dependent regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Corduneanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<editor>Chapelle, O.</editor>
		<editor>Schlkopf, B.</editor>
		<editor>and Zien, A.</editor>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Semi-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why does unsupervised pretraining help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fourteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Spatially-sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno>abs/1409.6070</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rademacher processes and bounding the risk of function learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltchinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High dimensional probability II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rademacher penalties and structural risk minimization. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltchinskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1902" to="1914" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting Structured Data</title>
		<editor>Bakir, G.</editor>
		<editor>Hofman, T.</editor>
		<editor>Schölkopf, B.</editor>
		<editor>Smola, A.</editor>
		<editor>and Taskar, B.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The impact of unlabeled patterns in rademacher complexity theory for kernel classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Local rademacher complexity: Sharper risk bounds with and without unlabeled samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="115" to="125" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energybased model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>et al., J. P.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>Cortes, C.</editor>
		<editor>Lawrence, N. D.</editor>
		<editor>Lee, D. D.</editor>
		<editor>Sugiyama, M.</editor>
		<editor>and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probability inequalities for the sum in sampling without replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Serfling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="48" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th international conference on machine learning (ICML-13)</title>
		<meeting>the 30th international conference on machine learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning to Learn. Norwell</title>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the uniform convergence of relative frequencies of events to their probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Chervonenkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>New York, NY, USA; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
	<note>Estimation of Dependences Based on Empirical Data. ICML-13</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Inference with the universum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1009" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno>CoRR abs/1301.3557</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data to enhance ensemble diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="619" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pac-learning for energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Courant Institute of Mathematical Sciences, New York University</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02351</idno>
		<title level="m">Stacked what-where auto-encoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
