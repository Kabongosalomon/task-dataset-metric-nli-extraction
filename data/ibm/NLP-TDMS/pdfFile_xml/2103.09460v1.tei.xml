<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You Only Look One-level Feature</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<email>jcheng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff3">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">You Only Look One-level Feature</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramidsutilizing only one-level feature for detection. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while being 2.5× faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7× less training epochs. With an image size of 608 × 608, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In state-of-the-art two-stage detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref> and one-stage detectors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>, feature pyramids become an essential component. The most popular way to build feature pyramids is the feature pyramid networks (FPN) <ref type="bibr" target="#b21">[22]</ref>, which mainly brings two benefits: (1) multi-scale feature fusion: fusing multiple low-resolution and high-resolution feature inputs to obtain better representations; (2) divideand-conquer: detecting objects on different levels regarding * This work is done during Qiang Chen's internship at MEGVII Technology. † Corresponding author.  Here, we adopt the original RetinaNet <ref type="bibr" target="#b22">[23]</ref> as our baseline model, where C3, C4, and C5 denote output features of the backbone with a downsample rate of {8, 16, 32} and P3 to P7 represent the feature levels used for final detection. All results reported in the figure use the same backbone, ResNet-50 <ref type="bibr" target="#b13">[14]</ref>. The structure of MiMo is same as the FPN in RetinaNet <ref type="bibr" target="#b22">[23]</ref>. A detailed illustration of the structure for all encoders can be found in the <ref type="bibr">Figure 8.</ref> objects' scales. A common belief for FPN is that its success relies on the fusion of multiple level features, inducing a line of studies of designing complex fusion methods manually <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref>, or via Neural Architecture Search (NAS) algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref>. However, the belief ignores the function of the divide-and-conquer in FPN. It leads to fewer studies on how these two benefits contribute to FPN's success and may hinder new advances.</p><p>This paper studies the influence of FPN's two benefits in one-stage detectors. We design experiments by decoupling the multi-scale feature fusion and the divide-andconquer functionalities with RetinaNet <ref type="bibr" target="#b22">[23]</ref>. In detail, we consider FPN as a Multiple-in-Multiple-out (MiMo) encoder, which encodes multi-scale features from the backbone and provides feature representations for the decoder (the detection heads). We conduct controlled comparisons among Multiple-in-Multiple-out (MiMo), Single-in-Multiple-out (SiMo), Multiple-in-Single-out (MiSo), and Single-in-Single-out (SiSo) encoders in <ref type="figure" target="#fig_1">Figure 1</ref>. Surprisingly, the SiMo encoder, which only has one input feature C5 and does not perform feature fusion, can achieve comparable performance with the MiMo encoder (i.e., FPN). The performance gap is less than 1 mAP. In contrast, the performance drops dramatically (≥ 12 mAP) in MiSo and SiSo encoders. These phenomenons suggest two facts: <ref type="bibr" target="#b0">(1)</ref> the C5 feature carries sufficient context for detecting objects on various scales, which enables the SiMo encoder to achieve comparable results; <ref type="bibr" target="#b1">(2)</ref> the multi-scale feature fusion benefit is far away less critical than the divide-andconquer benefit, thus multi-scale feature fusion might not be the most significant benefit of FPN, which is also demonstrated by ExFuse <ref type="bibr" target="#b49">[50]</ref> in semantic segmentation. Thinking one step deeper, divide-and-conquer is related to the optimization problem in object detection. It divides the complex detection problem into several sub-problems by object scales, facilitating the optimization process.</p><p>The above analysis suggests that the essential factor for the success of FPN is its solution to the optimization problem in object detection. The divide-and-conquer solution is a good way. But it brings memory burdens, slows down the detectors, and make detectors' structure complex in onestage detectors like RetinaNet <ref type="bibr" target="#b22">[23]</ref>. Given that the C5 feature carries sufficient context for detection, we show a simple way to address the optimization problem.</p><p>We propose You Only Look One-level Feature (YOLOF), which only uses one single C5 feature (with a downsample rate of 32) for detection. To bridge the performance gap between the SiSo encoder and the MiMo encoder, we first design the structure of the encoder properly to extract the multi-scale contexts for objects on various scales, compensating for the lack of multiple-level features; then, we apply a uniform matching mechanism to solve the imbalance problem of positive anchors raised by the sparse anchors in the single feature.</p><p>Without bells and whistles, YOLOF achieves comparable results with its feature pyramids counterpart Reti-naNet [23] but 2.5× faster. In a single feature manner, YOLOF matches the performance of the recent proposed DETR <ref type="bibr" target="#b3">[4]</ref> while converging much faster (7×). With an image size of 608 × 608 and other techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47]</ref>, YOLOF achieve 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4 <ref type="bibr" target="#b0">[1]</ref>. In a nutshell, the contributions of this paper are:</p><p>• We show that the most significant benefits of FPN is its divide-and-conquer solution to the optimization problem in dense object detection rather than the multiscale feature fusion.</p><p>• We present YOLOF, which is a simple and efficient baseline without using FPN. In YOLOF, we propose two key components, Dilated Encoder and Uniform Matching, bridging the performance gap between the SiSo encoder and the MiMo encoder.</p><p>• Extensive experiments on COCO benchmark indicates the importance of each component. Moreover, we conduct comparisons with RetinaNet <ref type="bibr" target="#b22">[23]</ref>, DETR <ref type="bibr" target="#b3">[4]</ref> and YOLOv4 <ref type="bibr" target="#b0">[1]</ref>. We can achieve comparable results with a faster speed on GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Multiple-level feature detectors. It is a conventional technique to employ multiple features for object detection. Typical approaches to construct multiple features can be categorized into image pyramid methods and feature pyramid methods. Image pyramids based detector such as DPM <ref type="bibr" target="#b7">[8]</ref> dominates the detection in the pre-deep learning era. In CNN-based detectors, the image pyramids method also wins some researchers' <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> praise as it can achieve higher performance out of the box. However, the image pyramids method is not the only way to obtain multiple features; it is more efficient and natural to exploit feature pyramids' power in CNN models. SSD <ref type="bibr" target="#b25">[26]</ref> first utilizes multiple-scale features and performs object detection on each scale for different scales objects. FPN <ref type="bibr" target="#b21">[22]</ref> follows SSD <ref type="bibr" target="#b25">[26]</ref> and UNet <ref type="bibr" target="#b32">[33]</ref> and constructs semantic-riched feature pyramids by combining shallow features and deep features. After that, several works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37]</ref> follow FPN and focus on how to obtain better representations. FPN becomes an essential component and dominates modern detectors. It is also applied to popular one-stage detectors, such as RetinaNet <ref type="bibr" target="#b22">[23]</ref>, FCOS <ref type="bibr" target="#b37">[38]</ref>, and their variants <ref type="bibr" target="#b47">[48]</ref>. Another line of method to get feature pyramids is to use multi-branch and dilation convolution <ref type="bibr" target="#b19">[20]</ref>. Different from the above works, our method is a single-level feature detector.</p><p>Single-level feature detectors. In early times, the R-CNN series <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref> and R-FCN <ref type="bibr" target="#b5">[6]</ref> only extract RoI features on a single feature, while their performances lag behind their multiple feature counterparts <ref type="bibr" target="#b21">[22]</ref>. Also, in onestage detectors, YOLO <ref type="bibr" target="#b28">[29]</ref> and YOLOv2 <ref type="bibr" target="#b29">[30]</ref> only use the last output feature of the backbone. They can be super fast but have to bear a performance decline in detection. CornerNet <ref type="bibr" target="#b18">[19]</ref> and CenterNet <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b6">7]</ref> follow this fashion and achieve competitive results while using a single feature with a downsample rate of 4 to detect all the objects. Using a high-resolution feature map for detection brings enormous memory cost and is not friendly to deployment. Recently, DETR <ref type="bibr" target="#b3">[4]</ref> introduces the transformer <ref type="bibr" target="#b38">[39]</ref> to detection and shows that it could achieve state-of-the-art results only use a single C5 feature. Due to the totally anchor-free mechanism and transformer learning phase, DETR needs a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Encoder Decoder cat bench <ref type="figure">Figure 2</ref>. An illustration of the detection pipeline. In this paper, we format the detection pipeline into three parts: (1) the backbone;</p><p>(2) the encoder, which receives inputs from the backbone and distributes representations for detection; (3) the decoder, which performs classification and regression tasks and generate final prediction boxes. The color for the encoder is corresponding to the one in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>long training schedule for its convergence. The long training schedule characteristic is cumbersome for further improvements. Unlike these papers, we investigate the working mechanism of multiple-level detection. From the perspective of optimization, we provide an alternative solution to the widely used FPN. Moreover, YOLOF converges faster and achieves promising performance; thus, YOLOF can serve as a simple baseline for fast and accurate detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cost Analysis of MiMo Encoders</head><p>As mentioned in Section 1, the success of FPN in dense object detection is due to its solution to the optimization problem. However, the multi-level feature paradigm is inevitable to make detectors complex, brings memory burdens, and slows down the detector. In this section, we provide a quantitative study on the cost of MiMo encoders.</p><p>We design experiments based on RetinaNet <ref type="bibr" target="#b22">[23]</ref> with ResNet-50 <ref type="bibr" target="#b13">[14]</ref>. In detail, we format the pipeline for the detection task as a combination of three key parts: the backbone, the encoder, and the decoder (  <ref type="figure">Figure 3</ref>). Moreover, the detector with MiMo encoder runs much slower than the ones with SiSo encoders (13 FPS vs. 34 FPS) ( <ref type="figure">Figure 3</ref>). The slow speed is caused by detecting objects on high-resolution feature maps in the detector with MiMo encoder, such as the C3 feature (with a downsample rate of 8). Given the above drawbacks of the MiMo encoder, we aim to find an alternative way to solve the optimization problem while keeping the detector simple, accurate, and fast simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Motivated by the above purpose and the finding that the C5 feature contains enough context for detecting numerous objects, we try to replace the complex MiMo encoder with the simple SiSo encoder in this section. But this replacement is nontrivial as the performance drops extensively when applying SiSo encoders according to the results in <ref type="figure">Figure 3</ref>. Given the situation, we carefully analyze the obstacles preventing SiSo encoders from getting a comparable  <ref type="figure">Figure 3</ref>. FLOPs, accuracy, and speed comparison between the models that adopt MiMo and SiSo encoders on COCO. As the FLOPs of the decoder is affected by the encoder's outputs, we stack the FLOPs of the encoder and the decoder in the figure to better understanding the effects of encoders on the FLOPs. All models use the same backbone, ResNet-50. All FLOPs are measured with a shorter edge size 800 over the first 100 images of COCO val2017. The FPS is calculated with batch size 1 on 2080Ti from the total inference pure compute time reported in the Detec-tron2 <ref type="bibr" target="#b41">[42]</ref>. In the figure, C represents the number of channels used in the model's encoder and decoder.</p><p>performance with MiMo encoders. We find that two problems brought by SiSo encoders are responsible for the performance drop. The first problem is that the range of scales matching to the C5 feature's receptive field is limited, which impedes the detection performance for objects across various scales. The second one is the imbalance problem on positive anchors raised by sparse anchors in the single-level feature. Next, we discuss these two problems in detail and provide our solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Limited Scale Range</head><p>Recognizing objects at vastly different scales is a fundamental challenge in object detection. One feasible solution to this challenge is to leverage multiple-level features. In detectors with MiMo or SiMo encoders, they construct multiple-level features with different receptive fields (P3-P7) and detect objects on the level with receptive field matching to their scales. However, the single-level feature setting changes the game. There is only one output feature in SiSo encoders, whose receptive field is a constant. As  shown in <ref type="figure" target="#fig_5">Figure 4</ref>(a), the C5 feature's receptive field can only cover a limited scale range, resulting in poor performance if the objects' scales mismatches with the receptive field. To achieve the goal of detecting all objects with SiSo encoders, we have to find a way to generate an output feature with various receptive fields, compensating for the lack of multiple-level features. We begin with enlarging the receptive field of the C5 feature by stacking standard and dilated convolutions <ref type="bibr" target="#b44">[45]</ref>. Although the covered scale range is enlarged to some extent, it still can not cover all object scales as the enlarging process multiplies a factor greater than 1 to all originally covered scales. We illustrate the situation in <ref type="figure" target="#fig_5">Figure 4</ref>(b), where the whole scale range shifts to larger scales compare with the one in <ref type="figure" target="#fig_5">Figure 4</ref>(a). Then, we combine the original scale range and the enlarged scale range by adding the corresponding features, resulting in an output feature with multiple receptive fields covering all object scales <ref type="figure" target="#fig_5">(Figure 4</ref>(c)). The above operations can be easily achieved by constructing residual blocks <ref type="bibr" target="#b13">[14]</ref> with dilations on the middle 3 × 3 convolution layer.</p><p>Dilated Encoder: Based on the above designs, we propose our SiSo encoder in <ref type="figure">Figure 5</ref>, named as Dilated Encoder. It contains two main components: the Projector and the Residual Blocks. The projection layer first applies one 1 × 1 convolution layer to reduce the channel dimension, then add one 3 × 3 convolution layer to refine semantic contexts, which is the same as in the FPN <ref type="bibr" target="#b21">[22]</ref>. After that, we stack four successive dilated residual blocks with different dilation rates in the 3 × 3 convolution layers to generate output features with multiple receptive fields, covering all objects' scales.</p><p>Discussion: Dilated convolution <ref type="bibr" target="#b44">[45]</ref> is a common strategy to enlarge the features' receptive field in object detection. As reviewed in the Section 2, TridentNet <ref type="bibr" target="#b19">[20]</ref> use dilated convolution to generate multi-scale features. It deals with</p><formula xml:id="formula_0">1×1 3×3 1×1 3×3 1×1 ×4 C5 P5</formula><p>Projector Residual Blocks <ref type="figure">Figure 5</ref>. An illustration of the structure of Dilated Encoder. In the figure, 1 × 1 and 3 × 3 denotes 1 × 1 and 3 × 3 convolution layers and ×4 means four successive residual blocks. All convolution layers in Residual Blocks are followed by a batchnorm layer <ref type="bibr" target="#b14">[15]</ref> and a ReLU layer <ref type="bibr" target="#b26">[27]</ref>, while in Projector, we only use convolution layers and batchnorm layers <ref type="bibr" target="#b14">[15]</ref>.</p><p>the scale variation problem in object detection via multibranch structure and weight sharing mechanism, which is different from our single-level feature setting. Moreover, Dilated Encoder stack dilated residual blocks one by one without weight sharing. Although DetNet <ref type="bibr" target="#b20">[21]</ref> also successively applies dilated residual blocks, its purpose is to maintain the spatial resolution of the features and keep more details in the backbone's outputs, while ours is to generate a feature with multiple receptive fields out of the backbone. The design of Dilated Encoder enables us to detecting all objects on single-level feature instead of on multiple-level features like TridentNet <ref type="bibr" target="#b19">[20]</ref> and DetNet <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Imbalance Problem on Positive Anchors</head><p>The definition of positive anchors is crucial for the optimization problem in object detection. In anchor-based detectors, strategies to define positive are dominated by measuring the IoUs between anchors and ground-truth boxes. In RetinaNet <ref type="bibr" target="#b22">[23]</ref>, if the max IoU of the anchor and groundtruth boxes is greater than a threshold 0.5, this anchor will be set as positive. We call it Max-IoU matching.</p><p>In MiMo encoders, the anchors are pre-defined on multiple levels in a dense paved fashion, and the ground-truth boxes generate positive anchors in feature levels corresponding to their scales. Given the divide-and-conquer mechanism, Max-IoU matching enables ground-truth boxes in each scale to generate a sufficient number of positive anchors. However, when we adopt the SiSo encoder, the number of anchors diminish extensively compare to the one in the MiMo encoder, from 100k to 5k, resulting in sparse anchors <ref type="bibr" target="#b0">1</ref>   Uniform Matching: To solve this imbalance problem in positive anchors, we propose an Uniform Matching strategy: adopting the k nearest anchor as positive anchors for each ground-truth box, which makes sure that all groundtruth boxes can be matched with the same number of positive anchors uniformly regardless of their sizes ( <ref type="figure" target="#fig_7">Figure 6</ref>). Balance in positive samples makes sure that all ground-truth boxes participate in training and contribute equally. Besides, following Max-IoU matching <ref type="bibr" target="#b22">[23]</ref>, we set IoU thresholds in Uniform Matching to ignore large IoU (&gt;0.7) negative anchors and small IoU (&lt;0.15) positive anchors.</p><p>Discussion: relation to other matching methods. Applying topk in the matching process is not new. ATSS <ref type="bibr" target="#b47">[48]</ref> first select topk anchors for each ground-truth box on L feature levels, then samples positive anchors among k × L candidates by dynamic IoU thresholds. However, ATSS focuses on defining positives and negatives adaptively, while our uniform matching focuses on achieving balance on positive samples with sparse anchors. Although several previous methods achieve balance on positive samples, their matching processes are not designed for this imbalance problem. For example, YOLO <ref type="bibr" target="#b28">[29]</ref> and YOLOv2 <ref type="bibr" target="#b29">[30]</ref> match the ground-truth boxes with the best matching cell or anchor; DETR <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b35">[36]</ref> apply Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref> for matching. These matching methods can be view as top1 matching, which is a specific case of our uniform matching. More importantly, the difference between the uniform matching and the learning-to-match methods is that: the learning-to-match methods, such as FreeAnchor <ref type="bibr" target="#b48">[49]</ref> and PAA <ref type="bibr" target="#b15">[16]</ref>, adaptively separate anchors into positives and negatives according to the learning status, while uniform matching is fixed and does not evolve with training. The uniform matching is proposed to address the specific imbalance problem on positive anchors under the SiSo design. The comparison in <ref type="figure" target="#fig_7">Figure 6</ref> and the results in <ref type="table" target="#tab_6">Table 5e</ref> demonstrate the significance of the balance in positives in SiSo encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">YOLOF</head><p>Based on the solutions above, we propose a fast and straightforward framework with single-level feature, denoted as YOLOF. We format YOLOF into three parts: the backbone, the encoder, and the decoder. The sketch of YOLOF is shown in <ref type="figure">Figure 9</ref>. In this section, we give a brief introduction to the main components of YOLOF.</p><p>Backbone. In all models, we simply adopt the ResNet <ref type="bibr" target="#b13">[14]</ref> and ResNeXt <ref type="bibr" target="#b42">[43]</ref> series as our backbone. All models are pre-trained on ImageNet. The output of the backbone is the C5 feature map which has 2048 channels and with a downsample rate of 32. To make a fair comparison with other detectors, all batchnorm layers in the backbone are frozen by default.</p><p>Encoder. For the encoder ( <ref type="figure">Figure 5</ref>), we first follow FPN by adding two projection layers (one 1 × 1 and one 3 × 3 convolution) after the backbone, resulting in a feature map with 512 channels. Then, to enable the encoder's output feature to cover all objects on various scales, we propose to add residual blocks, which consist of three consecutive convolutions: the first 1 × 1 convolution apply channel reduction with a reduction rate of 4, then a 3 × 3 convolution with dilation is used to enlarge the receptive field, at last, a 1 × 1 convolution to recover the number of channels.</p><p>Decoder. For the decoder, we adopt the main design of RetinaNet, which consists of two parallel task-specific heads: the classification head and the regression head <ref type="figure">(Figure 9</ref>). We only add two minor modifications. The first one is that we follow the design of FFN in DETR <ref type="bibr" target="#b3">[4]</ref> and make the number of convolution layers in two heads different. There are four convolutions followed by batch normalization layers and ReLU layers on the regression head while only have two on the classification head. The second is that we follow Autoassign <ref type="bibr" target="#b51">[52]</ref> and add an implicit objectness prediction (without direct supervision) for each anchor on the regression head. The final classification scores for all predictions are generated by multiplying the classification output with the corresponding implicit objectness.</p><p>Other Details. As mentioned in the previous section, the pre-defined anchors in YOLOF are sparse, decreasing the match quality between anchors and ground-truth boxes. We add a random shift operation on the image to circumvent this problem. The operation shifts the image randomly with a maximum of 32 pixels in left, right, top, and bottom directions and aims to inject noises into the object's position in the image, increasing the probability of ground-truth boxes matching with high-quality anchors. Moreover, we found that a restriction on the anchors' center's shift is also helpful to the final classification when using a single-level feature. gives the results of an improved RetinaNet (with a "+"), which is RetinaNet with GIoU <ref type="bibr" target="#b31">[32]</ref>, GN <ref type="bibr" target="#b40">[41]</ref>, and implicit objectness. The last section shows the results of various YOLOF models. In the table, the model with a suffix of R101 or X101 means it use ResNet-101 <ref type="bibr" target="#b13">[14]</ref> or RetNeXt-101-64×4d <ref type="bibr" target="#b42">[43]</ref> as backbone. For those not marked with suffix, they adopt ResNet-50 <ref type="bibr" target="#b13">[14]</ref> by default. In the last two rows, we use multi-scale training and testing techniques ( † indicates multi-scale training and ‡ means multi-scale testing), whose settings follow HTC <ref type="bibr" target="#b4">[5]</ref>. More details about the settings can be found in the Appendix. In the last three columns, we show models' number of parameters (#params), GFLOPs, and inference speed. All FLOPs are measured with a shorter edge size 800 over the first 100 images of COCO val2017. Moreover, the FPS in the table is calculated with batch size 1 on 2080Ti from the total inference pure compute time reported in the Detectron2 <ref type="bibr" target="#b41">[42]</ref>.</p><p>We add a restriction that the centers' shift for all anchors should smaller than 32 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our YOLOF on the MS COCO <ref type="bibr" target="#b23">[24]</ref> benchmark and conduct comparisons with RetinaNet <ref type="bibr" target="#b22">[23]</ref> and DETR <ref type="bibr" target="#b3">[4]</ref>. Then, we provide a detailed ablation study of each component's design with quantitative results and analysis. Finally, to give insights to further research on single-level detection, we provide error analysis and show the weaknesses of YOLOF compared with DETR <ref type="bibr" target="#b3">[4]</ref>. The details are as follows.</p><p>Implementation Details. YOLOF is trained with synchronized SGD over 8 GPUs with a total of 64 images per minibatch (8 images per GPU). All models are trained with an initial learning rate of 0.12. Moreover, following DETR <ref type="bibr" target="#b3">[4]</ref>, we set a smaller learning rate for the backbone, which is 1/3 of the base learning rate. To stabilize the training at the beginning, we extend the number of warmup iterations from 500 to 1500. For training schedules, as we increase the batch size, the '1×' schedule setting in YOLOF is a total of 22.5k iterations and with base learning rate decreased by 10 in the 15k and the 20k iteration. Other schedules are adjusted according to the principles in Detectron2 <ref type="bibr" target="#b41">[42]</ref>. For model inference, we employ NMS with a threshold of 0.6 to post-process the results. For other hyperparameters, we follow the settings of RetinaNet <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with previous works</head><p>Comparison with RetinaNet: To make a fair comparison, we align RetinaNet with YOLOF by employing generalized IoU <ref type="bibr" target="#b31">[32]</ref> for the box loss, adding an implicit objectness prediction, and applying group normalization layers <ref type="bibr" target="#b40">[41]</ref> in heads (as there are only two images per GPU and both BN <ref type="bibr" target="#b14">[15]</ref> and SyncBN <ref type="bibr" target="#b45">[46]</ref> give poor results in RetinaNet 2 , we use GN <ref type="bibr" target="#b40">[41]</ref> instead of BN <ref type="bibr" target="#b14">[15]</ref> in the heads). The results are presented in <ref type="table" target="#tab_0">Table 1</ref>. All '1×' models are trained with a single scale that the shorter side is set as 800 pixels and the longer side is at most 1333 <ref type="bibr" target="#b22">[23]</ref>. In the top section, we give RetinaNet baseline results trained with De-tectron2 <ref type="bibr" target="#b41">[42]</ref>. In the middle section, we present the results of the improved RetinaNet baseline (with a "+"), whose settings are aligned with YOLOF. In the last section, we show results from multiple YOLOF models. Thanks to the single-level feature, YOLOF achieves results on par with RetinaNet+ with a 57% flops reduction (flops for each component in YOLOF are shown in <ref type="figure">Figure 3</ref>) and a 2.5× speed up. Due to the large stride (32) of the C5 feature, YOLOF has an inferior performance (−3.1) than RetinaNet+ on small objects. However, YOLOF achieves better performance on large objects (+3.3) as we add dilated residual blocks in the encoder. The comparison between RetinaNet+ and YOLOF with a ResNet-101 <ref type="bibr" target="#b13">[14]</ref> show similar evidence as well. Although YOLOF is inferior to RetinaNet+ on small objects when applying the same backbone, it can match small objects' performance with a stronger backbone ResNeXt <ref type="bibr" target="#b42">[43]</ref> while running at the same speed. Moreover, to prove that our method is compatible and complementary to current technologies in object detec-   <ref type="bibr" target="#b38">[39]</ref> to object detection. It achieves surprising results on the COCO benchmark <ref type="bibr" target="#b23">[24]</ref> and proves that by only adopting a single C5 feature, it can achieve comparable results with a multi-level feature detector (Faster R-CNN w/ FPN <ref type="bibr" target="#b21">[22]</ref>) for the first time. Given this, one might expect that layers capture global dependencies such as transformer layers <ref type="bibr" target="#b38">[39]</ref> are required to achieve promising results in single-level feature detection. However, we show that a conventional network with local convolution layers can also achieve this goal. We compare DETR with global layers and YOLOF with local convolution layers in   <ref type="figure" target="#fig_1">Figure 1</ref> and <ref type="figure">Figure 3</ref> due to the design of the decoder in YOLOF -only two convolution layers in the classification head.</p><p>Comparison with YOLOv4. YOLOv4 <ref type="bibr" target="#b0">[1]</ref> is an optimal speed and accuracy multi-level feature detector. It combines many tricks to achieve state-of-the-art results. As our purpose is to build a simple and fast baseline for single-level detectors, investigation on the bag of freebie tricks is outside of the scope of this work. Thus, we do not expect a rigidly aligned comparison on performance. To compare our YOLOF with YOLOv4, we apply the data augmentation methods as YOLOv4, adopt a three-phase training pipeline, modify the training settings accordingly, and add dilations on the last stage of the backbone (YOLOF-DC5 in <ref type="table" target="#tab_2">Table 3</ref>). More technical details about the model and the training settings are given in the Appendix. As shown in Table 3, YOLOF-DC5 can run 13% faster than YOLOv4 with a 0.8 mAP improvement on overall performance. YOLOF-DC5 achieves less competitive results on small objects than YOLOv4 (24.0 mAP vs. 26.7 mAP) while outperforms it on large objects by a large margin (+7.1 mAP). The above results indicate that single-level detectors have great potential to achieve state-of-the-art speed and accuracy simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Experiments</head><p>We run a number of ablations to analyze YOLOF. We first provide an overall analysis of the two proposed components. Then, we show the ablation experiments on detailed designs of each component. Results are shown in <ref type="table" target="#tab_4">Table 4</ref> Comparison with other matching methods. Uniform Matching achieve balance in positive anchors and get the best results among other matching methods, which is consistent with the comparison in <ref type="figure" target="#fig_7">Figure 6</ref>. Note that '*' represents that we get the best result for ATSS <ref type="bibr" target="#b47">[48]</ref> when setting topk as 15. More details can be found in the Appendix. Dilated Encoder and Uniform Matching: <ref type="table" target="#tab_4">Table 4</ref> shows that both Dilated Encoder and Uniform Matching are necessary to YOLOF and bring considerable improvements. Specifically, Dilated Encoder has a significant impact on large objects (43.8 vs. 53.2) and slightly improves the results of small and medium objects. The results indicate that the limited scale range is a severe problem in the C5 feature (Section 4.1). Our Dilated Encoder provides a simple but effective solution to this problem. On the other side, the performance of small and medium objects drops significantly (∼ 10AP ) without uniform matching, while the large objects' performance is only lightly affected. The finding is consistent with the imbalance problem on positive anchors analyzed in Section 4.2. The positive anchors are dominated by large objects, resulting in poor results on small and medium objects. Finally, when we remove both Dilated Encoder and Uniform Matching, a single-level feature detector's performance drops back to ∼ 20 mAP like the results in <ref type="figure" target="#fig_1">Figure 1</ref> and <ref type="figure">Figure 3</ref>.</p><p>Number of ResBlock: YOLOF stacks residual blocks in the SiSo encoder. The results in <ref type="table" target="#tab_6">Table 5a</ref> shows that stacking more blocks gives extensive improvements on large objects, which is due to the increment of the feature scale range. Although we observe continuous improvements with more blocks, we choose to add four residual blocks to keep YOLOF simple and neat.</p><p>Different dilations: Following the analysis in Section 4.1, to enable the C5 feature to cover large scales, we replace the standard 3 × 3 convolution layer in the residual blocks with its dilated counterpart. We show the results with different dilations in the residual blocks in <ref type="table" target="#tab_6">Table 5b</ref>. Applying dilations to residual blocks bring improvements to YOLOF, while the improvements are saturated when using too large dilations. We conjecture that the reason for this phenomenon is that dilations of 2, 4, 6, 8 are enough to match object scales in all images.</p><p>Add shortcut or not: <ref type="table" target="#tab_6">Table 5c</ref> shows that shortcuts play an essential role in Dilated Encoder. The performance of all objects will drop significantly if we remove the shortcuts in residual blocks. According to Section 4.1, shortcuts combine different scale ranges. A largely and densely paved scale range covered by the feature is the critical factor for detecting all objects in a single-level feature manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of positives:</head><p>A comparison among the number of induced positive anchors by ground-truth boxes is conducted in <ref type="table" target="#tab_6">Table 5d</ref>. Intuitively, more positive anchors can achieve better performance as the learning will be easier when given more samples. Thus, in our uniform matching manner, we empirically increase the number of positive anchors induced by each ground-truth box. As shown in Table 5d, the hyper-parameter k is very robust for the performance when k is larger than 1, which may suggest that the most important is the uniform matching manner in YOLOF. We set top4 for our uniform matching as it is the best choice according to the results. Uniform matching vs. other matchings: We compare the uniform matching with other matching strategies for YOLOF and show results in <ref type="table" target="#tab_6">Table 5e</ref>. The proposed uniform matching strategy can achieve the best results, compatible with the imbalance analysis in <ref type="figure" target="#fig_7">Figure 6</ref>. It worth noting that the Hungarian matching strategy can be roughly treated as Top1 matching <ref type="table" target="#tab_6">(Table 5d</ref>) so that they get similar performance. The difference between them is that an anchor will only match one object in Hungarian matching while the Top1 matching does not have this constraint, and the experiments show that this is not important. The original ATSS find that top9 anchors are the best choice, while we find top15 anchors are much better in the single-level feature detector. By using top15 anchors, ATSS achieves a good result of 36.5 mAP while still lags behind our uniform matching by a 1.2 mAP gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Error Analysis</head><p>We add error analysis for YOLOF in this section to provide insights for future research in single-level feature detection. We adopt the recent proposed tool TIDE <ref type="bibr" target="#b1">[2]</ref> to compare YOLOF with DETR <ref type="bibr" target="#b3">[4]</ref>. As illustrated in <ref type="figure" target="#fig_8">Figure 7</ref>, DETR has a larger error in localization than YOLOF, which may be related to its regression mechanism. DETR regresses objects in a total anchor free manner and predicts the location globally in the image, which causes difficulties in localization. In contrast, YOLOF relies on pre-defined anchors, which is responsible for higher missing error than DETR <ref type="bibr" target="#b3">[4]</ref> in the predictions. According to the analysis in Section 4.2, the anchors of YOLOF are sparsely and not flexible enough in the inference stage. Intuitively, there are situations that there are no high-quality anchors pre-defined around a ground-truth box. Thus, introducing the anchor-free mechanism into YOLOF may help alleviate this problem, and we leave it for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we identify that the success of FPN is due to its divide-and-conquer solution to the optimization problem in dense object detection. Given that FPN makes network structure complex, brings memory burdens, and slows down the detectors, we propose a simple but highly efficient method without using FPN to address the optimization problem differently, denoted as YOLOF. We prove its efficacy by making fair comparisons with RetinaNet and DETR. We hope our YOLOF can serve as a solid baseline and provide insight for designing single-level feature detectors in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: More Details</head><p>Detailed Structures of All Encoders: In <ref type="figure" target="#fig_9">Figure 8</ref>, we illustrate the detailed processes of generating outputs in encoders. The four encoders differ in the number of input features and output features. (a) The Multiple-in-Multipleout (MiMo) encoder receives three levels from the backbone and output five levels. The structure of the MiMo encoder is the same as FPN in RetinaNet <ref type="bibr" target="#b22">[23]</ref>. (b) Single-in-Multipleout (SiMo) only has one C5 feature for the input. As there are no other inputs, we remove the 1 × 1 convolution layer designed for C3 and C4. (c) Multiple-in-Single-out (MiSo) receives three input features while only generate one output feature P5. To fully utilize the context in the input features, we adopt a structure similar to PANet <ref type="bibr" target="#b24">[25]</ref> in MiSo. (d) In the Single-in-Single-out (SiSo) encoder, we remove all other convolution layers and only keep the convolution layers in the level of C5.</p><p>Network Architecture of YOLOF In <ref type="figure">Figure 9</ref>, we show a detailed network architecture of YOLOF. YOLOF detects objects on single-level feature, which is very simple. Our method consists of three components: the backbone, the encoder, and the decoder. The detailed design of these components are presented in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Time &amp; Memory:</head><p>In this section, we compare training time and training memory among YOLOF, DETR <ref type="bibr" target="#b3">[4]</ref>, and RetinaNet+ <ref type="bibr" target="#b22">[23]</ref>. As shown in <ref type="table">Table 6</ref>, due to the long training schedule, DETR needs 112.5 hours to converge on COCO with eight 2080Ti GPUs, while YOLOF and RetinaNet+ only need 4.5 hours and 9.8 hours, respectively. As for training memory, YOLOF needs less memory than RetinaNet+ and DETR, which make YOLOF be trained with larger batch size and converge faster.</p><p>More Implementation Details: The default training settings for YOLOF is a total of 64 images per mini-batch (8 images per GPU) with an initial learning rate of 0.12. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dilated Encoder CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regression Classification</head><p>Objectness <ref type="figure">Figure 9</ref>. The sketch of YOLOF, which consists of three main components: the backbone, the encoder, and the decoder. In the figure, 'C5/DC5' represents the output feature of the backbone with downsample rate of 32/16. 'Cout' means the number of channels of the feature. We set the number of channels as 512 for feature maps in the encoder and the decoder. H × W is the height and width of feature maps. Detailed Settings to Compare with YOLOv4 To match the performance of YOLOv4, we first increase the number of dilated residual blocks in the dilated encoder from 4 to 8. We adjust the dilations of these dilated residual blocks according to experimental results. We find that the dilations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> give the best result. Then following YOLOv4 <ref type="bibr" target="#b0">[1]</ref>, we adopt its data augmentations, take the CSPDarkNet-53 <ref type="bibr" target="#b39">[40]</ref> as the backbone, replace all the batch normalization layers with its synchronized counterpart, and apply LeakyReLU <ref type="bibr" target="#b43">[44]</ref> in the encoder and the decoder instead of ReLU layers. According to the results in <ref type="table">Table 9</ref>, YOLOF-DC5 gives better results than YOLOF. Thus we use YOLOF-DC5 as the baseline model in this section. After that, we set an initial learning rate of 0.04 for the whole model. To train the final model, we adopt a threephase training. At first, we training YOLOF-DC5 for a '9×' schedule; then we increase the ignore threshold for negative anchors from 0.75 to 0.8 and train a '3×' schedule based on the previous model (this phase gives a 0.5 mAP gain); at last, we train another '3×' schedule by following the recipe introduced in <ref type="bibr" target="#b46">[47]</ref>. The final result shown in <ref type="table" target="#tab_2">Table 3</ref> is produced by the SWA model, which is obtained by averaging 12 checkpoints (the SWA model gives a ∼ 1 mAP improvement).</p><formula xml:id="formula_1">× !"# × × C5/DC5 ×512× × ×512× × ×512× × ×512× × ×512× × ×2 ×4 × × × ×4 × × (a) Backbone (b) Encoder (c) Decoder × × ×</formula><p>Model areas sizes ratios AP AP 50 AP 75 APs APm AP l  <ref type="table">Table 8</ref>. An illustration of how performance changes with the variation of the hyper-parameter k in ATSS <ref type="bibr" target="#b47">[48]</ref>.  <ref type="table">Table 9</ref>. Additional results of YOLOF-DC5 with different backbones on COCO val split. * means that due to the limited memory of 2080Ti, we train with 4 images per GPU (batch size 32) for ResNet-101. Higher performance can be achieved if train with 8 images per GPU or apply SyncBN (BN layers in the encoder and decoder restrict the improvements).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Additional Experimental Results</head><p>Number of Anchors: In RetinaNet <ref type="bibr" target="#b22">[23]</ref>, anchors are generated from multiple level features (P3-P7) with areas of 32 2 to 512 2 , respectively. At each level feature, RetinaNet paves anchors with sizes {2 0 , 2 1/3 , 2 2/3 } and aspect ratios {0.5, 1, 2}. While in YOLOF, we only have a one-level feature to place anchors. To cover all objects' scales, we add anchors with areas of {32 2 , 64 2 , 128 2 , 256 2 , 512 2 }, size {1}, and aspect ratio {1} in the single feature map, resulting in 5 anchors in each position. Moreover, we investigate the influence of more anchors in YOLOF. Following RetinaNet, we generate 45 anchors in each position with different sizes ({2 0 , 2 1/3 , 2 2/3 }) and more aspect ratios ({0.5, 1, 2}). All results are shown in <ref type="table" target="#tab_8">Table 7</ref>. The results show that adding more aspect ratios does not change the performance of YOLOF, while the performance drops with more sizes. Thus, we choose to add a minimum of five anchors for YOLOF by default.</p><p>Hyper-parameter of ATSS: Here, we provide the results of using different values of k in ATSS <ref type="bibr" target="#b47">[48]</ref> in <ref type="table">Table 8</ref>. The results show that the choice of k = 9 used in the original paper is not the best choice in YOLOF. According to the results, we choose k = 15 for ATSS in this paper.</p><p>Results with Dilated C5: In this paper, we show that YOLOF performs well on the C5 feature. To boost the performance of YOLOF, we detect objects on a feature map with higher resolution than the C5 feature. Following DETR <ref type="bibr" target="#b3">[4]</ref>, we construct a backbone with dilation and without stride on its last stage. The backbone's output feature is denoted as DC5, with a downsample rate of 16. In <ref type="table">Table 9</ref>, we show the results of YOLOF-DC5 on COCO val split with ResNet-50 and ResNet-101 as the backbone. YOLOF-DC5 achieves higher performance than the original YOLOF but runs at a slower speed as the feature's resolution is larger than C5. To achieve the results, we first add a smaller anchor, resulting in 6 anchors per location ({16, 32, 64, 128, 256, 512}), then we increase the topk from 4 to 8 and change the ignore threshold for positive anchors from 0.15 to 0.1. Other parameters are the same as before.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Single-in-Single-out</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of box AP among the Multiple-in-Multipleout (MiMo), Single-in-Multiple-out (SiMo), Multiple-in-Singleout (MiSo), and Single-in-Single-out (SiSo) encoders on COCO validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2). In this view, we show the FLOPs of each component in Figure 3. Compared with SiSo encoders, the MiMo encoder brings enormous memory burdens to the encoder and the decoder(134G vs. 6G) (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>A toy example to illustrate the relation between the object scales and the scale range covered by the single feature. The axis in this figure denotes the scales. (a) indicates that the feature's receptive field can only cover a limited scale range; (b) shows that the enlarged scale ranges enable the feature to cover large objects while miss covering small ones; (c) represents that all scales can be covered the feature with multiple receptive fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. Sparse anchors raise a matching problem for detectors when applying Max-IoU matching, as shown in Figure 6. Large ground-truth boxes induce more positive anchors than small ground-truth boxes in natural, which cause an imbalance problem for positive anchors. This imbalance makes detectors pay attention to large ground-truth boxes while ignoring the small ones when training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Distribution of the generated positive anchors in various matching methods with single feature. This figure aims to show the balancedness of the generated positive anchors. The positive anchors in the Max-IoU are dominated by large ground-truth boxes, causing huge imbalance across object scales. ATSS alleviates the imbalance problem by adaptively sampling positive anchors when training. The Top1 and Ours adopt a uniform matching, generating positive anchors in a balanced manner regardless of small, medium, and large objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Error analysis for DETR-R101 and YOLOF-R101. According to TIDE [2], the figure shows the six types of errors (Cls: classification error; Loc: localization error; Both: both cls and loc error; Dupe: duplicate predictions error; Bkg: background error; Miss: missing error). The pie chart shows the relative contribution of each error, while the bar plots show their absolute contribution. FP and FN means false positive and false negative respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Detailed Structures of Multiple-in-Multiple-out (MiMo), Single-in-Multiple-out (SiMo), Multiple-in-Single-out (MiSo), and Single-in-Single-out (SiSo) encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Model FPS AP AP 50</head><label>50</label><figDesc>AP 75 AP S AP M AP L YOLOF-DC5-R50 24 39.2 58.6 42.7 22.3 43.9 50.8 YOLOF-DC5-R101 * 17 40.5 59.8 43.9 23.0 44.9 53.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Model schedule AP AP 50 AP 75 AP S AP M AP L #params GFLOPs FPS Comparison with RetinaNet on the COCO2017 validation set. The top section shows the results of RetinaNet. The middle section</figDesc><table><row><cell>RetinaNet [23]</cell><cell>1x</cell><cell>35.9</cell><cell>55.7</cell><cell>38.5</cell><cell>19.4</cell><cell>39.5</cell><cell>48.2</cell><cell>38M</cell><cell>201</cell><cell>13</cell></row><row><cell>RetinaNet-R101 [23]</cell><cell>1x</cell><cell>38.3</cell><cell>58.5</cell><cell>41.3</cell><cell>21.7</cell><cell>42.5</cell><cell>51.2</cell><cell>57M</cell><cell>266</cell><cell>11</cell></row><row><cell>RetinaNet+</cell><cell>1x</cell><cell>37.7</cell><cell>58.1</cell><cell>40.2</cell><cell>22.2</cell><cell>41.7</cell><cell>49.9</cell><cell>38M</cell><cell>201</cell><cell>13</cell></row><row><cell>RetinaNet-R101+</cell><cell>1x</cell><cell>40.0</cell><cell>60.4</cell><cell>42.7</cell><cell>23.2</cell><cell>44.1</cell><cell>53.3</cell><cell>57M</cell><cell>266</cell><cell>10</cell></row><row><cell>YOLOF</cell><cell>1x</cell><cell>37.7</cell><cell>56.9</cell><cell>40.6</cell><cell>19.1</cell><cell>42.5</cell><cell>53.2</cell><cell>44M</cell><cell>86</cell><cell>32</cell></row><row><cell>YOLOF-R101</cell><cell>1x</cell><cell>39.8</cell><cell>59.4</cell><cell>42.9</cell><cell>20.5</cell><cell>44.5</cell><cell>54.9</cell><cell>63M</cell><cell>151</cell><cell>21</cell></row><row><cell>YOLOF-X101</cell><cell>1x</cell><cell>42.2</cell><cell>62.1</cell><cell>45.7</cell><cell>23.2</cell><cell>47.0</cell><cell>57.7</cell><cell>102M</cell><cell>289</cell><cell>10</cell></row><row><cell>YOLOF-X101  †</cell><cell>3x</cell><cell>44.7</cell><cell>64.1</cell><cell>48.6</cell><cell>25.1</cell><cell>49.2</cell><cell>60.9</cell><cell>102M</cell><cell>289</cell><cell>10</cell></row><row><cell>YOLOF-X101  † ‡</cell><cell>3x</cell><cell>47.1</cell><cell>66.4</cell><cell>51.2</cell><cell>31.8</cell><cell>50.9</cell><cell>60.6</cell><cell>102M</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>ModelEpochs #params GFLOPS/FPS AP AP 50 AP 75 AP S AP M AP L Comparison with DETR on the COCO2017 validation set. We conduct comparisons with backbone ResNet-50 (without suffix) and ResNet-101 (with a suffix R101). To make fair comparison, YOLOF adopts multi-scale training (same as inTable 1) with a '6×' schedule, which is roughly 72 epochs. For the FPS of DETR, * means we follow the method in the original paper<ref type="bibr" target="#b3">[4]</ref> and re-measure it on 2080Ti.Epochs  FPS AP AP 50 AP 75 AP S AP M AP L YOLOv4 [1] 273 53 * 43.5 65.7 47.3 26.7 47.6 53.3 YOLOF-DC5 184 60 † 44.3 62.9 47.5 24.0 48.5 60.4</figDesc><table><row><cell>DETR [4]</cell><cell>500</cell><cell>41M</cell><cell>86/24  *</cell><cell>42.0</cell><cell>62.4</cell><cell>44.2</cell><cell>20.5</cell><cell>45.8</cell><cell>61.1</cell></row><row><cell cols="2">DETR-R101 [4] 500</cell><cell>60M</cell><cell>152/17  *</cell><cell>43.5</cell><cell>63.8</cell><cell>46.4</cell><cell>21.9</cell><cell>48.0</cell><cell>61.8</cell></row><row><cell>YOLOF</cell><cell>72</cell><cell>44M</cell><cell>86/32</cell><cell>41.6</cell><cell>60.5</cell><cell>45.0</cell><cell>22.4</cell><cell>46.2</cell><cell>57.6</cell></row><row><cell>YOLOF-R101</cell><cell>72</cell><cell>63M</cell><cell>151/21</cell><cell>43.7</cell><cell>62.7</cell><cell>47.4</cell><cell>24.3</cell><cell>48.3</cell><cell>58.9</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with YOLOv4 on the COCO test-dev set. Finally, with the help of multi-scale testing, we obtain our final result of 47.1 mAP and a competitive performance of 31.8 mAP on small objects.</figDesc><table><row><cell>Comparison with DETR. DETR [4] is a recent proposed</cell></row><row><cell>detector which introduces transformer</cell></row></table><note>We train YOLOF-DC5 with a '15×' schedule (184 epochs) and compare it with YOLOv4. In the table, † means that the FPS for YOLOF-DC5 is measured by following YOLOv4 [1]. It is different from the method used in Table 1, 2 in this paper. In YOLOv4 [1], the authors fuse the convolution layer and the batch normalization layer, then measure the inference time after con- verting the model to half-precision. * represents that we get the speed for YOLOv4 on 2080Ti from the official repo https: //github.com/AlexeyAB/darknet#geforce-rtx- 2080-ti.tion, we show results that training with multi-scale images and a longer schedule in the last two rows of Table 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. The results show that YOLOF</cell></row><row><cell>matches the DETR's performance, and YOLOF gets more</cell></row><row><cell>benefits from deeper networks than DETR (w/ ResNet-50</cell></row><row><cell>(−0.4) vs. w/ ResNet-101 (+0.2)). Interestingly, we find</cell></row><row><cell>that YOLOF outperforms DETR on small objects (+1.9 and</cell></row><row><cell>+2.4) while lags behind DETR on large objects (-3.5 and</cell></row><row><cell>-2.9). The finding is consistent with the local and global</cell></row></table><note>discussion above. More importantly, compared with DETR, YOLOF converge much faster (∼ 7×), making it more suit- able than DETR to serve as a simple baseline for single- level detectors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Effect of Dilated Encoder and Uniform Matching with ResNet-50. These two components improve the original single- level detector by 16.6 mAP. Note that the result of 21.1 mAP in the table is not a bug. It perform slightly worse than the detectors with SiSo encoders in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablations. We show ablation experiments for Dilation Encoder and Uniform Matching on COCO2017 val set with ResNet-50.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>While for ResNeXt-101<ref type="bibr" target="#b42">[43]</ref>, we train with 4 images per GPU (batch size 32) and set the learning rate to 0.06 following the linear rule<ref type="bibr" target="#b11">[12]</ref>. For multi-scale training, DETR<ref type="bibr" target="#b3">[4]</ref> apply random crop plus resize to simulate large image size during training. In YOLOF, we simply resize the image to large size. For multi-scale training, we follow HTC<ref type="bibr" target="#b4">[5]</ref> and adopt a strategy of random sample the image size between [400, 1400] with its largest edge no greater than 1600 pixels.</figDesc><table><row><cell>Model</cell><cell cols="2">Memory/Images Training Time</cell></row><row><cell>YOLOF</cell><cell>5.3G / 8</cell><cell>4.5h</cell></row><row><cell>RetinaNet+ [23]</cell><cell>4.9G / 2</cell><cell>9.8h</cell></row><row><cell>DETR [4]</cell><cell>7.1G / 2</cell><cell>112.5h</cell></row><row><cell cols="3">Table 6. Comparison of training memory and training time among</cell></row><row><cell cols="3">different models. All models are trained with eight 2080Ti</cell></row><row><cell cols="3">GPUs with their default settings, i.e, we train YOLOF and</cell></row><row><cell cols="3">RetinaNet+ [23] in a '1x' schedule, while train DETR [4] with</cell></row><row><cell cols="2">150 epochs on COCO2017 training set.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>56.9 40.6 19.1 42.5 53.2 YOLOF 5 1 3 37.7 57.2 40.7 19.4 42.0 52.2 YOLOF 5 3 1 35.0 52.3 37.9 15.0 40.6 52.9 YOLOF 5 3 3 35.4 52.4 38.3 14.8 41.2 52.5 Results of YOLOF with different multiple anchors per location on COCO [24] validation set. ATSS) 33.7 33.8 34.6 35.8 35.5 36.5 36.3 36.2</figDesc><table><row><cell>YOLOF 37.7 Model &amp; k 5 1 1 5 7</cell><cell>9</cell><cell>11 13 15 17 19</cell></row><row><cell>YOLOF (with</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In SiSo encoders, we simply collapse multiple anchors on multiplelevel features to single-level, e.g., we construct 5 anchors with different anchor sizes of {32, 64, 128, 256, 512} on each position of the C5 feature.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https : / / github . com / facebookresearch / detectron2/blob/master/detectron2/modeling/meta_ arch/retinanet.py#L532</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tide: A general toolbox for identifying object detection errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconfiguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="169" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cspnet: A new backbone that can enhance learning capability of cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="390" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feras</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12645</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Swa object detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">FreeAnchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Autoassign: Differentiable label assignment for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhang</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03496</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
