<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jiao</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution. The network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions. De-convolutional layers are then used to recover the image details. We propose to symmetrically link convolutional and de-convolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. First, The skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently. Second, these skip connections pass image details from convolutional layers to de-convolutional layers, which is beneficial in recovering the original image. Significantly, with the large capacity, we can handle different levels of noises using a single model. Experimental results show that our network achieves better performance than all previously reported state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of image restoration is to recover an clean image from its corrupted observation, which is known to be an ill-posed inverse problem. By accommodating different types of corruption distributions, the same mathematical model applies to problems such as image denoising and superresolution. Recently, deep neural networks (DNNs) have shown their superior performance in image processing and computer vision tasks, ranging from high-level recognition, semantic segmentation to low-level denoising, super-resolution, deblur, inpainting and recovering raw images from compressed images. Despite the progress that DNNs achieve, there still are some problems. For example, can a deeper network in general achieve better performance; can we design a single model to handle different levels of corruption.</p><p>Observing recent superior performance of DNNs on image processing tasks, we propose a convolutional neural network (CNN)-based framework for image restoration. We observe that in order to obtain good restoration performance, it is beneficial to train a very deep model. Meanwhile, we show that it is possible to achieve good performance with a single network when processing multiple different levels of corruptions due to the benefits of large-capacity networks. Specifically, the proposed framework learns end-to-end fully convolutional mappings from corrupted images to the clean ones. The network is composed of multiple layers of convolution and de-convolution operators. As deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional 29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. and de-convolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum.</p><p>Our main contributions are briefly outlined as follows: 1) A very deep network architecture, which consists of a chain of symmetric convolutional and deconvolutional layers, for image restoration is proposed in this paper. The convolutional layers act as the feature extractor which encode the primary components of image contents while eliminating the corruption. The deconvolutional layers then decode the image abstraction to recover the image content details.</p><p>2) We propose to add skip connections between corresponding convolutional and de-convolutional layers. These skip connections help to back-propagate the gradients to bottom layers and pass image details to the top layers, making training of the end-to-end mapping more easier and effective, and thus achieve performance improvement while the network going deeper.</p><p>3) Relying on the large capacity and fitting ability of our very deep network, we propose to handle different level of noises/corruption using a single model. To our knowledge, this is the first approach that achieves good accuracy for processing different levels of noises with a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Experimental results demonstrate the advantages of our network over other recent state-of-the-art methods on image denoising and super-resolution, setting new records on these topics.</p><p>Related work Extensive work has been done on image restoration in the literature. See detail reviews in a survey <ref type="bibr" target="#b20">[21]</ref>. Traditional methods such as Total variation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>, BM3D algorithm <ref type="bibr" target="#b4">[5]</ref> and dictionary learning based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b1">2]</ref> have shown very good performance on image restoration topics such as image denoising and super-resolution. Since image restoration is in general an ill-posed problem, the use of regularization <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b8">9]</ref> has been proved to be essential.</p><p>An active (and probably more promising) category for image restoration is the DNN based methods. Stacked denoising auto-encoder <ref type="bibr" target="#b28">[29]</ref> is one of the most well-known DNN models which can be used for image restoration. Xie et al. <ref type="bibr" target="#b31">[32]</ref> combined sparse coding and DNN pre-trained with denoising auto-encoder for low-level vision tasks such as image denoising and inpainting. Other neural networks based methods such as multi-layer perceptron <ref type="bibr" target="#b0">[1]</ref> and CNN <ref type="bibr" target="#b14">[15]</ref> for image denoising, as well as DNN for image or video super-resolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> and compression artifacts reduction <ref type="bibr" target="#b5">[6]</ref> have been actively studied in these years.</p><p>Burger et al. <ref type="bibr" target="#b0">[1]</ref> presented a patch-based algorithm learned with a plain multi-layer perceptron. They also concluded that with large networks, large training data, neural networks can achieve state-of-the-art image denoising performance. Jain and Seung <ref type="bibr" target="#b14">[15]</ref> proposed fully convolutional CNN for denoising. They found that CNN provide comparable or even superior performance to wavelet and Markov Random Field (MRF) methods. Cui et al. <ref type="bibr" target="#b3">[4]</ref> employed non-local self-similarity (NLSS) search on the input image in multi-scale, and then used collaborative local auto-encoder for super-resolution in a layer by layer fashion. Dong et al. <ref type="bibr" target="#b6">[7]</ref> proposed to directly learn an end-to-end mapping between the low/high-resolution images. Wang et al. <ref type="bibr" target="#b29">[30]</ref> argued that domain expertise represented by the conventional sparse coding can be combined to achieve further improved results. In general, DNN-based methods learn restoration parameters directly from data, which tends to been more effective in real-world image restoration applications. An advantage of DNN methods is that these methods are purely data driven and no assumption about the noise distributions are made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Very deep RED-Net for Image Restoration</head><p>The proposed framework mainly contains a chain of convolutional layers and symmetric deconvolutional layers, as shown in <ref type="figure">Figure 1</ref>. We term our method "RED-Net"-very deep Residual Encoder-Decoder Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>The framework is fully convolutional and deconvolutional. Rectification layers are added after each convolution and deconvolution. The convolutional layers act as feature extractor, which preserve the primary components of objects in the image and meanwhile eliminating the corruptions. The deconvolutional layers are then combined to recover the details of image contents. The output of <ref type="figure">Figure 1</ref>: The overall architecture of our proposed network. The network contains layers of symmetric convolution (encoder) and deconvolution (decoder). Skip shortcuts are connected every a few (in our experiments, two) layers from convolutional feature maps to their mirrored deconvolutional feature maps. The response from a convolutional layer is directly propagated to the corresponding mirrored deconvolutional layer, both forwardly and backwardly. the deconvolutional layers is the "clean" version of the input image. Moreover, skip connections are also added from a convolutional layer to its corresponding mirrored deconvolutional layer. The passed convolutional feature maps are summed to the deconvolutional feature maps element-wise, and passed to the next layer after rectification.</p><p>For low-level image restoration problems, we use neither pooling nor unpooling in the network as usually pooling discards useful image details that are essential for these tasks. Motivated by the VGG model <ref type="bibr" target="#b26">[27]</ref>, the kernel size for convolution and deconvolution is set to 3 × 3, which has shown excellent image recognition performance. It is worth mentioning that the size of input image can be arbitrary since our network is essentially a pixel-wise prediction. The input and output of the network are images of the same size w × h × c, where w, h and c are width, height and number of channels. In this paper, we use c = 1 although it is straightforward to apply to images with c &gt; 1. We found that using 64 feature maps for convolutional and deconvolutional layers achieves satisfactory results, although more feature maps leads to slightly better performance. Deriving from the above architecture, we propose two networks, which are 20-layer and 30-layer respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Deconvolution decoder</head><p>Architectures combining layers of convolution and deconvolution <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12]</ref> have been proposed for semantic segmentation lately. In contrast to convolutional layers, in which multiple input activations within a filter window are fused to output a single activation, deconvolutional layers associate a single input activation with multiple outputs.</p><p>One can simply replace deconvolution with convolution, which results in a architecture that is very similar to recently proposed very deep fully convolutional neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>. However, there exist essential differences between a fully convolution model and our model. In the fully convolution case, the noise is eliminated step by step, i.e., the noise level is reduced after each layer. During this process, the details of the image content may be lost. Nevertheless, in our network, convolution preserves the primary image content. Then deconvolution is used to compensate the details.</p><p>We compare the 5-layer and 10-layer fully convolutional network with our network (combining convolution and deconvolution, but without skip connection). For fully convolutional networks, we use padding and up-sample the input to make the input and output the same size. For our network, the first 5 layers are convolutional and the second 5 layers are deconvolutional. All the other parameters for training are the same, i.e., trained with SGD and learning rate of 10 −6 , noise level σ = 70. In terms of PSNR, using deconvolution works better than the fully convolutional counterpart. We see that, the fully convolutional network reduces noise layer by layer, and our network preserve primary image contents by convolution and recover some details by using deconvolution. Detailed results are in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Skip connections</head><p>An intuitive question is that, is deconvolution able to recover image details from the image abstraction only? We find that in shallow networks with only a few layers of convolution, deconvolution is able to <ref type="figure">Figure 2</ref>: An example of a building block in the proposed framework. The rectangle in solid and dotted lines denote convolution and deconvolution respectively. ⊕ denotes element-wise sum of feature maps. recover the details. However, when the network goes deeper or using operations such as max pooling, deconvolution does not work so well, possibly because too much details are already lost in the convolution. The second question is that, when our network goes deeper, does it achieve performance gain? We observe that deeper networks often suffer from gradients vanishing and become hard to train-a problem that is well addressed in the literature.</p><p>To address the above two problems, inspired by highway networks <ref type="bibr" target="#b27">[28]</ref> and deep residual networks <ref type="bibr" target="#b10">[11]</ref>, we add skip connections between two corresponding convolutional and deconvolutional layers as shown in <ref type="figure">Figure 1</ref>. A building block is shown in Figure2. There are two reasons for using such connections. First, when the network goes deeper, as mentioned above, image details can be lost, making deconvolution weaker in recovering them. However, the feature maps passed by skip connections carry much image detail, which helps deconvolution to recover a better clean image. Second, the skip connections also achieve benefits on back-propagating the gradient to bottom layer, which makes training deeper network much easier as observed in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b10">[11]</ref>. Note that our skip layer connections are very different from the ones proposed in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b10">[11]</ref>, where the only concern is on the optimization side. In our case, we want to pass information of the convolutional feature maps to the corresponding deconvolutional layers.</p><p>Instead of directly learning the mappings from input X to the output Y , we would like the network to fit the residual <ref type="bibr" target="#b10">[11]</ref> of the problem, which is denoted as F(X) = Y − X. Such a learning strategy is applied to inner blocks of the encoding-decoding network to make training more effective. Skip connections are passed every two convolutional layers to their mirrored deconvolutional layers. Other configurations are possible and our experiments show that this configuration already works very well. Using such shortcuts makes the network easier to be trained and gains restoration performance via increasing network depth.</p><p>The very deep highway networks <ref type="bibr" target="#b27">[28]</ref> are essentially feed-forward long short-term memory (LSTMs) with forget gates; and the CNN layers of deep residual network <ref type="bibr" target="#b10">[11]</ref> are feed-forward LSTMs without gates. Note that our deep residual networks are in general not in the format of standard feed-forward LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discussions</head><p>Training with symmetric skip connections As mentioned above, using skip connections mainly has two benefits: (1) passing image detail forwardly, which helps recovering clean images and (2) passing gradient backwardly, which helps finding better local minimum. We design experiments to show these observations.</p><p>We first compare two networks trained for denoising noises of σ = 70. In the first network, we use 5 layers of 3 × 3 convolution with stride 3. The input size of training data is 243 × 243, which results in a vector after 5 layers of convolution. Then deconvolution is used to recover the input. The second network uses the same settings as the first one, except for adding skip connections. The results are show in <ref type="figure" target="#fig_0">Figure 3</ref>(a). We can observe that it is hard for deconvolution to recover details from only a vector encoding the abstraction of the input, which shows that the ability on recovering image details for deconvolution is limited. However, if we use skip connections, the network can still recover the input, because details are passed to topper layers in the network.</p><p>We also train five networks to show that using skip connections help to back-propagate gradient in training to better fit the end-to-end mapping, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>  As we can see, the training loss increases when the network going deeper without shortcuts (similar phenomenon is also observed in <ref type="bibr" target="#b10">[11]</ref>), but we obtain smaller loss when using skip connections.</p><p>Comparison with deep residual networks <ref type="bibr" target="#b10">[11]</ref> One may use different types of skip connections in our network, a straightforward alternate is that in <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, the skip connections are added to divide the network into sequential blocks. A benefit of our model is that our skip connections have element-wise correspondence, which can be very important in pixel-wise prediction problems. We carry out experiments to compare the two types of skip connections. Here the block size indicates the span of the connections. The results are shown in <ref type="figure" target="#fig_0">Figure 3</ref> (c). We can observe that our connections often converge to a better optimum, demonstrating that element-wise correspondence can be important.</p><p>Dealing with different levels of noises/corruption An important question is, can we handle different levels of corruption with a single model. Almost all existing methods need to train different models for different levels of corruption and estimate the corruption level at first. We use a trained model in <ref type="bibr" target="#b0">[1]</ref>, to denoise different levels of noises with σ being 10, 30, 50 and 70. The obtained average PSNR on the 14 images are 29.95dB, 27.81dB, 18.62dB and 14.84dB, respectively. The results show that the parameters trained on a single noise level can not handle different levels of noises well. Therefore, in this paper, we aim to train a single model for recovering different levels of corruption, which are different noise levels in the task of image denoising and different scaling parameters in image super-resolution. The large capacity of the network is the key to this success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>Learning the end-to-end mapping from corrupted images to clean ones needs to estimate the weights Θ represented by the convolutional and deconvolutional kernels. This is achieved by minimizing the Euclidean loss between the outputs of the network and the clean image. In specific, given a collection of N training sample pairs X i , Y i , where X i is a corrupted image and Y i is the clean version as the groundtruth. We minimize the following Mean Squared Error(MSE):</p><formula xml:id="formula_0">L(Θ) = 1 n N n=1 F(X i ; Θ) − Y i 2 F .<label>(1)</label></formula><p>We implement and train our network using Caffe <ref type="bibr" target="#b15">[16]</ref>. In practice, we find that using Adam <ref type="bibr" target="#b16">[17]</ref> with learning rate 10 −4 for training converges faster than traditional stochastic gradient descent (SGD). The base learning rate for all layers are the same, different from <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>, in which a smaller learning rate is set for the last layer. This trick is not necessary in our network.</p><p>As general settings in the literature, we use gray-scale image for denoising and the luminance channel for super-resolution in this paper. 300 images from the Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b19">[20]</ref> are used to generate the training set. For each image, patches of size 50 × 50 are sampled as ground truth. For denoising, we add additive Gaussian noise to the patches multiple times to generate a large training set (about 0.5M). For super-resolution, we first down-sample a patch and then up-sample it to its original size, obtaining a low-resolution version as the input of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Testing</head><p>Although trained on local patches, our network can perform denoising and super-resolution on images of arbitrary size. Given a testing image, one can simply go forward through the network, which is able to obtain a better performance than existing methods. To achieve more smooth results, we propose to process a corrupted image on multiple orientations. Different from segmentation, the filter kernels in our network only eliminate the corruptions, which is not sensitive to the orientation of image contents. Therefore, we can rotate and mirror flip the kernels and perform forward multiple times, and then average the output to get a more smooth image. We see that this can lead to slightly better denoising and super-resolution performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we provide evaluation of denoising and super-resolution performance of our models against a few existing state-of-the-art methods. Denoising experiments are performed on two datasets: 14 common benchmark images <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9]</ref> and the BSD200 dataset. We test additive Gaussian noises with zero mean and standard deviation σ = 10, 30, 50 and 70 respectively. BM3D <ref type="bibr" target="#b4">[5]</ref>, NCSR <ref type="bibr" target="#b7">[8]</ref>, EPLL <ref type="bibr" target="#b33">[34]</ref>, PCLR <ref type="bibr" target="#b2">[3]</ref>, PDPD <ref type="bibr" target="#b32">[33]</ref> and WMMN <ref type="bibr" target="#b8">[9]</ref> are compared with our method. For super-resolution, we compare our network with SRCNN <ref type="bibr" target="#b6">[7]</ref>, NBSRF <ref type="bibr" target="#b24">[25]</ref>, CSCN <ref type="bibr" target="#b29">[30]</ref>, CSC <ref type="bibr" target="#b9">[10]</ref>, TSE <ref type="bibr" target="#b12">[13]</ref> and ARFL+ <ref type="bibr" target="#b25">[26]</ref> on three dataset: Set5, Set14 and BSD100. The scaling parameter are tested with 2, 3 and 4.</p><p>Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) index are calculated for evaluation. For our method, which is denoted as RED-Net, we implement three versions: RED10 contains 5 convolutional and deconvolutional layers without shortcuts, RED20 contains 10 convolutional and deconvolutional layers with shortcuts, and RED30 contains 15 convolutional and deconvolutional layers with shortcuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Denoising</head><p>Evaluation on the 14 images <ref type="table" target="#tab_0">Table 1</ref> presents the PSNR and SSIM results of σ 10, 30, 50, and 70. We can make some observations from the results. First of all, the 10 layer convolutional and deconvolutional network has already achieved better results than the state-of-the-art methods, which demonstrates that combining convolution and deconvolution for denoising works well, even without any skip connections. Moreover, when the network goes deeper, the skip connections proposed in this paper help to achieve even better denoising performance, which exceeds the existing best method WNNM <ref type="bibr" target="#b8">[9]</ref> by 0.32dB, 0.43dB, 0.49dB and 0.51dB on noise levels of σ being 10, 30, 50 and 70 respectively. While WNNM is only slightly better than the second best existing method PCLR <ref type="bibr" target="#b2">[3]</ref> by 0.01dB, 0.06dB, 0.03dB and 0.01dB respectively, which shows the large improvement of our model. Last, we can observe that the more complex the noise is, the more improvement our model achieves than other methods. Similar observations can be made on the evaluation of SSIM.  Evaluation on BSD200 For testing efficiency, we convert the images to gray-scale and resize them to smaller ones on BSD-200. Then all the methods are run on these images to get average PSNR and SSIM results of σ 10, 30, 50, and 70, as shown in <ref type="table" target="#tab_1">Table 2</ref>. For existing methods, their denoising performance does not differ much, while our model achieves 0.38dB, 0.47dB, 0.49dB and 0.42dB higher of PSNR over WNNM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image super-resolution</head><p>The evaluation on Set5 is shown in <ref type="table" target="#tab_2">Table 3</ref>. Our 10-layer network outperforms the compared methods already, and we achieve better performance with deeper networks. The 30-layer network exceeds the second best method CSCN for 0.52dB, 0.56dB and 0.47dB on scale 2, 3 and 4 respectively. The evaluation on Set14 is shown in <ref type="table" target="#tab_3">Table 4</ref>. The improvement on Set14 in not as significant as that on Set5, but we can still observe that the 30 layer network achieves higher PSNR than the second best CSCN for 0.23dB, 0.06dB and 0.1dB. The results on BSD100, as shown in <ref type="table" target="#tab_4">Table 5</ref>, is similar than that on Set5. The second best method is still CSCN, the performance of which is not as good as our 10 layer network. Our deeper network obtains much more performance gain than the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation with a single model</head><p>To construct the training set, we extract image patches with different noise levels and scaling parameters for denoising and super-resolution. Then a 30-layer network is trained for the two tasks respectively. The evaluation results are shown in <ref type="table" target="#tab_5">Table 6</ref> and <ref type="table" target="#tab_6">Table 7</ref>. Although training with different   levels of corruption, we can observe that the performance of our network only slightly degrades comparing to the case in which using separate models for denoising and super-resolution. This may be due the fact that the network has to fit much more complex mappings. Except that CSCN works slightly better on super-resolution with scales 3 and 4, our network still beats the existing methods, showing that our network works much better in image denoising and super-resolution even using only one single model to deal with complex corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper we have proposed a deep encoding and decoding framework for image restoration. Convolution and deconvolution are combined, modeling the restoration problem by extracting primary image content and recovering details. More importantly, we propose to use skip connections, which helps on recovering clean images and tackles the optimization difficulty caused by gradient vanishing, and thus obtains performance gains when the network goes deeper. Experimental results and our analysis show that our network achieves better performance than state-of-the-art methods on image denoising and super-resolution.</p><p>X.-J. Mao's contribution was made when visiting The University of Adelaide. This work was in part supported by ARC Future Fellowship (FT120100969). Correspondence should be addressed to C. Shen. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>(b). The five networks are: 10, 20 and 30 layer networks without skip connections, and 20, 30 layer networks with skip connections. Analysis on skip connections: (a) Recovering image details using deconvolution and skip connections; (b) The training loss during training; (c) Comparisons of skip connections in [11] and our model, where "Block-i-RED" is the connections in our model with block size i and "Block-i-He et al." is the connections in He et al. [11] with block size i. The PSNR at the last iteration for the curves are: 25.08, 24.59, 25.30 and 25.21.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average PSNR and SSIM results of σ 10, 30, 50, 70 for the 14 images.</figDesc><table><row><cell>PSNR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average PSNR and SSIM results of σ 10, 30, 50, 70 on 200 images from BSD.</figDesc><table><row><cell>PSNR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average PSNR and SSIM results on Set5.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SRCNN NBSRF CSCN</cell><cell>CSC</cell><cell>TSE</cell><cell cols="4">ARFL+ RED10 RED20 RED30</cell></row><row><cell>s = 2</cell><cell>36.66</cell><cell>36.76</cell><cell>37.14</cell><cell>36.62</cell><cell>36.50</cell><cell>36.89</cell><cell>37.43</cell><cell>37.62</cell><cell>37.66</cell></row><row><cell>s = 3</cell><cell>32.75</cell><cell>32.75</cell><cell>33.26</cell><cell>32.66</cell><cell>32.62</cell><cell>32.72</cell><cell>33.43</cell><cell>33.80</cell><cell>33.82</cell></row><row><cell>s = 4</cell><cell>30.49</cell><cell>30.44</cell><cell>31.04</cell><cell>30.36</cell><cell>30.33</cell><cell>30.35</cell><cell>31.12</cell><cell>31.40</cell><cell>31.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s = 2</cell><cell>0.9542</cell><cell>0.9552</cell><cell cols="3">0.9567 0.9549 0.9537</cell><cell>0.9559</cell><cell>0.9590</cell><cell>0.9597</cell><cell>0.9599</cell></row><row><cell>s = 3</cell><cell>0.9090</cell><cell>0.9104</cell><cell cols="3">0.9167 0.9098 0.9094</cell><cell>0.9094</cell><cell>0.9197</cell><cell>0.9229</cell><cell>0.9230</cell></row><row><cell>s = 4</cell><cell>0.8628</cell><cell>0.8632</cell><cell cols="3">0.8775 0.8607 0.8623</cell><cell>0.8583</cell><cell>0.8794</cell><cell>0.8847</cell><cell>0.8869</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Average PSNR and SSIM results on Set14.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SRCNN NBSRF CSCN</cell><cell>CSC</cell><cell>TSE</cell><cell cols="4">ARFL+ RED10 RED20 RED30</cell></row><row><cell>s = 2</cell><cell>32.45</cell><cell>32.45</cell><cell>32.71</cell><cell>32.31</cell><cell>32.23</cell><cell>32.52</cell><cell>32.77</cell><cell>32.87</cell><cell>32.94</cell></row><row><cell>s = 3</cell><cell>29.30</cell><cell>29.25</cell><cell>29.55</cell><cell>29.15</cell><cell>29.16</cell><cell>29.23</cell><cell>29.42</cell><cell>29.61</cell><cell>29.61</cell></row><row><cell>s = 4</cell><cell>27.50</cell><cell>27.42</cell><cell>27.76</cell><cell>27.30</cell><cell>27.40</cell><cell>27.41</cell><cell>27.58</cell><cell>27.80</cell><cell>27.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s = 2</cell><cell>0.9067</cell><cell>0.9071</cell><cell cols="3">0.9095 0.9070 0.9036</cell><cell>0.9074</cell><cell>0.9125</cell><cell>0.9138</cell><cell>0.9144</cell></row><row><cell>s = 3</cell><cell>0.8215</cell><cell>0.8212</cell><cell cols="3">0.8271 0.8208 0.8197</cell><cell>0.8201</cell><cell>0.8318</cell><cell>0.8343</cell><cell>0.8341</cell></row><row><cell>s = 4</cell><cell>0.7513</cell><cell>0.7511</cell><cell cols="3">0.7620 0.7499 0.7518</cell><cell>0.7483</cell><cell>0.7654</cell><cell>0.7697</cell><cell>0.7718</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Average PSNR and SSIM results on BSD100 for super-resolution.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SRCNN NBSRF CSCN</cell><cell>CSC</cell><cell>TSE</cell><cell cols="4">ARFL+ RED10 RED20 RED30</cell></row><row><cell>s = 2</cell><cell>31.36</cell><cell>31.30</cell><cell>31.54</cell><cell>31.27</cell><cell>31.18</cell><cell>31.35</cell><cell>31.85</cell><cell>31.95</cell><cell>31.99</cell></row><row><cell>s = 3</cell><cell>28.41</cell><cell>28.36</cell><cell>28.58</cell><cell>28.31</cell><cell>28.30</cell><cell>28.36</cell><cell>28.79</cell><cell>28.90</cell><cell>28.93</cell></row><row><cell>s = 4</cell><cell>26.90</cell><cell>26.88</cell><cell>27.11</cell><cell>26.83</cell><cell>26.85</cell><cell>26.86</cell><cell>27.25</cell><cell>27.35</cell><cell>27.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s = 2</cell><cell>0.8879</cell><cell>0.8876</cell><cell cols="3">0.8908 0.8876 0.8855</cell><cell>0.8885</cell><cell>0.8953</cell><cell>0.8969</cell><cell>0.8974</cell></row><row><cell>s = 3</cell><cell>0.7863</cell><cell>0.7856</cell><cell cols="3">0.7910 0.7853 0.7843</cell><cell>0.7851</cell><cell>0.7975</cell><cell>0.7993</cell><cell>0.7994</cell></row><row><cell>s = 4</cell><cell>0.7103</cell><cell>0.7110</cell><cell cols="3">0.7191 0.7101 0.7108</cell><cell>0.7091</cell><cell>0.7238</cell><cell>0.7268</cell><cell>0.7290</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Average PSNR and SSIM results for image denoising using a single 30-layer network. = 30 σ = 50 σ = 70 σ = 10 σ = 30 σ = 50 σ = 70</figDesc><table><row><cell></cell><cell></cell><cell cols="2">14 images</cell><cell></cell><cell></cell><cell cols="2">BSD200</cell><cell></cell></row><row><cell cols="3">σ = 10 σ PSNR 34.49 29.09</cell><cell>26.75</cell><cell>25.20</cell><cell>33.38</cell><cell>27.88</cell><cell>25.69</cell><cell>24.36</cell></row><row><cell>SSIM</cell><cell>0.9368</cell><cell>0.8414</cell><cell>0.7716</cell><cell>0.7157</cell><cell>0.9280</cell><cell>0.7980</cell><cell>0.7119</cell><cell>0.6544</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Average PSNR and SSIM results for image super-resolution using a single 30 layer network. .9595 0.9222 0.8847 0.9135 0.8334 0.7698 0.8972 0.7993 0.7276</figDesc><table><row><cell></cell><cell></cell><cell>Set5</cell><cell></cell><cell></cell><cell>Set14</cell><cell></cell><cell></cell><cell>BSD100</cell><cell></cell></row><row><cell></cell><cell>s = 2</cell><cell>s = 3</cell><cell>s = 4</cell><cell>s = 2</cell><cell>s = 3</cell><cell>s = 4</cell><cell>s = 2</cell><cell>s = 3</cell><cell>s = 4</cell></row><row><cell>PSNR</cell><cell>37.56</cell><cell>33.70</cell><cell>31.33</cell><cell>32.81</cell><cell>29.50</cell><cell>27.72</cell><cell>31.96</cell><cell>28.88</cell><cell>27.35</cell></row><row><cell cols="2">SSIM 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering-based denoising with locally learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1438" to="1451" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">External patch prior guided internal clustering for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="603" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep network cascade for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional sparse coding for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1823" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image denoising via adaptive soft-thresholding based on non-local samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A tour of modern image filtering: New insights and methods, both practical and theoretical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="128" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="489" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nonlinear total variation based noise removal algorithms. Phys. D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-11" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Naive bayes super-resolution forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning super-resolution jointly from external and internal examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4359" to="4371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Patch group based nonlocal self-similarity prior learning for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
