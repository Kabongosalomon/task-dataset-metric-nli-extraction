<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TWO-STEP SOUND SOURCE SEPARATION: TRAINING ON LEARNED LATENT TARGETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Mila-Quebec Artificial Intelligence Institute Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Mila-Quebec Artificial Intelligence Institute Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Mila-Quebec Artificial Intelligence Institute Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Mila-Quebec Artificial Intelligence Institute Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Mila-Quebec Artificial Intelligence Institute Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TWO-STEP SOUND SOURCE SEPARATION: TRAINING ON LEARNED LATENT TARGETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audio source separation</term>
					<term>signal representation</term>
					<term>cost function</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a two-step training procedure for source separation via a deep neural network. In the first step we learn a transform (and it's inverse) to a latent space where masking-based separation performance using oracles is optimal. For the second step, we train a separation module that operates on the previously learned space. In order to do so, we also make use of a scale-invariant signal to distortion ratio (SI-SDR) loss function that works in the latent space, and we prove that it lower-bounds the SI-SDR in the time domain. We run various sound separation experiments that show how this approach can obtain better performance as compared to systems that learn the transform and the separation module jointly. The proposed methodology is general enough to be applicable to a large class of neural network end-to-end separation systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Single-channel audio source separation is a fundamental problem in audio analysis, where one extracts the individual sources that constitute a mixture signal <ref type="bibr" target="#b0">[1]</ref>. Popular algorithms for source separation include independent component analysis <ref type="bibr" target="#b1">[2]</ref>, non-negative matrix factorization <ref type="bibr" target="#b2">[3]</ref> and more recently supervised <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and unsupervised <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> deep learning approaches. In many of the recent approaches, separation is performed by applying a mask on a latent representation, which is often a Fourier-based or a learned domain. Specifically, a separation module produces an estimated masked latent representation for the input sources and a decoder translates them back to the time domain.</p><p>Many approaches have used the short-time Fourier transform (STFT) as an encoder to obtain this latent representation, and conversely the inverse STFT (iSTFT) as a decoder. Using this representation, separation networks have been trained using a loss defined over various targets, such as: raw magnitude spectrogram representations <ref type="bibr" target="#b3">[4]</ref>, ideal STFT masks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and ideal affinity matrices <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>. Other works have supplemented this by additionally reconstructing the phase of the sources <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. However, the ideal STFT masks impose an upper bound on the separation performance the aforementioned criteria do not necessarily translate to optimal separation. In order to address this, recent works have proposed end-toend separation schemes where the encoder, decoder and separation modules are jointly optimized using a time-domain loss between the reconstructed sources waveforms and their clean targets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. However, a joint time-domain end-to-end training approach might Supported by NSF grant #1453104 not always yield an optimal decomposition of the input mixtures resulting to worse performance than the fixed STFT bases <ref type="bibr" target="#b16">[17]</ref>.</p><p>Some studies have reported significant benefits when performing source-separation in two stages. In <ref type="bibr" target="#b17">[18]</ref>, first the sources are separated and in a second stage the interference between the estimated sources is reduced. Similarly, an iterative scheme is proposed in <ref type="bibr" target="#b16">[17]</ref>, where the separation estimates from the first network are used as input to the final separation network. In <ref type="bibr" target="#b18">[19]</ref>, speaker separation is performed by first separating frame-level spectral components of speakers and later sequentially grouping them using a clustering network. Lately, state-of-the-art results in most natural language processing tasks have been achieved by pre-training the encoder transformation network <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this work, we propose a general two-step approach for performing source separation which can be used in any mask-based separation architecture. First we pre-train an encoder and decoder in order to learn a suitable latent representation. In the second step, we train a separation module using as loss the negative permutation invariant <ref type="bibr" target="#b20">[21]</ref> scale invariant SDR (SI-SDR) <ref type="bibr" target="#b21">[22]</ref> w.r.t. the learned latent representation. Moreover, we prove that for the case that the decoder is a transpose convolutional layer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>, SI-SDR on the latent space bounds from below time-domain SI-SDR. Our experiments show that by maximizing SI-SDR on the learned latent targets, a consistent performance improvement is achieved across multiple sound separation tasks compared to the time-domain end-toend training approach when using the exact same model architecture. The SI-SDR upper bound using the learned latent space is also significantly higher than that of STFT-domain masks. Finally, we also observe that the pre-trained encoder representations are also more sparse and structured compared to the joint training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TWO-STEP SOURCE SEPARATION</head><p>Assuming a mixture x ∈ R T that consists of N sources s1, · · · , sN ∈ R T with T samples each in the time-domain, we propose to perform source separation in two independent steps: A) We first obtain a latent representation v1, · · · , vN ∈ R K for the source signals and vx ∈ R K for the input mixture. B) Then, we train a separation module which operates on the latent representation of the mixture vx and is trained to estimate the latent representation of the clean sources vi (or their masks mi in that space).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Step 1: Learning the Latent Targets</head><p>As a first step we train an encoder E in order to obtain a latent representation for the mixture vx = E (x). We also provide the clean sources as inputs to this encoder to obtain v1, · · · , vN and apply a softmax function (across the dimension of the sources) in order to  obtain separation masks m1, · · · , mN for each source. An elementwise multiplication of these masks with the latent representation of the mixture vi = mi vx, ∀i ∈ {1, · · · , N }, can be used as an estimate for each source. The decoder module D is then trained to transform these latent representations back to time-domain using si = D (v i ) , ∀i ∈ {1, · · · , N }. In order to train the encoder and the decoder we optimize the permutation invariant <ref type="bibr" target="#b20">[21]</ref> SI-SDR <ref type="bibr" target="#b21">[22]</ref> between the clean sources s and the estimated sources s:</p><formula xml:id="formula_0">L1 = −SI-SDR(s * , s) = −10 log 10 αs * 2 / αs * − s 2<label>(1)</label></formula><p>where s * denotes the permutation of the sources that maximizes SI-SDR and the scalar α = s s * / s 2 ensures that the loss is scale invariant. A schematic representation of the aforementioned step for two sources is depicted in <ref type="figure" target="#fig_1">Fig. 1a</ref>. The objective of this step is to find a latent representation transformation, which facilitates source separation through masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Step 2: Training the Separation Module</head><p>Once the weights of the encoder and decoder modules are fixed using the training recipe described in Step 1, we can train a separation module S. Given the latent representation of an input mixture vx = E (x), S is trained to produce an estimate of the latent representationvi for each clean source vi, i.e.v = S (vx). During inference, we can use the pre-trained decoder to transform the source estimates back into the time-domainŝ = D (S (vx)). The block diagram describing the training of the separation module with a fixed encoder and decoder is shown in <ref type="figure" target="#fig_1">Fig. 1b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Training using SI-SDR on the Latent Separation Targets</head><p>In contrast to recent time-domain source-separation approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref> which train all modules E, D, and S using a variant of the loss defined in Eq. 1, we propose to use the permutation invariant SI-SDR directly on the latent representation. For simplicity of notation we assume that each source has a vector latent representation vi ∈ R K in a high dimensional space. The loss for training the separation module could then be: L2 = −SI-SDR(v * ,v). The exact same training procedure could be followed, but now we can use as targets the optimal separation targets on the latent space as opposed to the time domain signals. The premise is that if the separation module is trained on producing latent representationsv ≈ v which are close to the ideal ones (assuming ideal permutation order) then the estimates of the sources after the decoding layer would also approximate the clean sources in time-domainŝ = D (S (vx)) ≈ D (v) = s ≈ s.</p><p>The latter might not hold for any arbitrary embedding process, but in the next section we prove that SI-SDR in the latent representations lower-bounds the SI-SDR in the time-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Relation to maximization of SI-SDR on Time-Domain</head><p>We restrict ourselves to a decoder that consists of a 1-D transposed convolutional layer which is the same as the decoder selection in most of the current end-to-end source separation approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. For this part we focus on the ith target latent representation vi ∈ R K that corresponds to a source time-domain signal si ∈ R T . Because the encoder-decoder modules are trained as described in Section 2.1, the separation target produced by the auto-encoder si would be close to the clean source si, namely:</p><formula xml:id="formula_1">D (v i ) = si ≈ si<label>(2)</label></formula><p>The separation network produces an estimated latent vectorvi that corresponds to an estimated time-domain signalŝi = D (vi). Because the decoder is just a convolutional layer we can express it as a linear projection D :</p><formula xml:id="formula_2">R K → R T using the matrix P ∈ R T ×K : si = Pvi, si = Pvi, ∀i ∈ {1, · · · , N }<label>(3)</label></formula><p>Assuming the Moore-Penrose pseudo-inverse of P is well defined, we express the inverse mapping from time to the latent-space as:</p><formula xml:id="formula_3">vi = P †ŝ i, vi = P † si, ∀i ∈ {1, · · · , N }<label>(4)</label></formula><p>Proposition 1. Let y,ŷ ∈ R d and their corresponding projections through A ∈ R n×d to R n defined as Ay and Aŷ, respectively. If y = ŷ = 1 then the absolute value of their inner product on the projection space R n is bounded above from the absolute value of their inner product in R d , namely: ŷ A Ay</p><formula xml:id="formula_4">2 ≤ g (A) + ŷ y 2 ,</formula><p>where g (A) ≥ 0 and depends only on the values of A.</p><p>Proof. The inner product in the projection space can be rewritten as:</p><formula xml:id="formula_5">ŷ A Ay 2 = ŷ A A − I y +ŷ y 2 = ŷ A A − I y 2 + 2 ŷ A A − I yŷ y + ŷ y 2<label>(5)</label></formula><p>Moreover, we can bound the first term of Eq. 5 by applying Cauchy-Schwarz inequality to the inner products and using the fact that y = ŷ = 1 as shown next:</p><formula xml:id="formula_6">ŷ A A − I y ≤ ŷ · A A − I · y = A A − I<label>(6)</label></formula><p>Similarly, we use Cauchy-Schwarz inequality and inequality 6 in order to bound the second term of Eq. 5 as well:</p><formula xml:id="formula_7">ŷ A A − I yŷ y ≤ A A − I<label>(7)</label></formula><p>Then by applying inequalities 6 and 7 to Eq. 5 we get:</p><formula xml:id="formula_8">ŷ A Ay 2 ≤ A A − I 2 + 2 · A A − I + ŷ y 2<label>(8)</label></formula><p>where always g (A) = A A − I 2 + 2 · A A − I ≥ 0. Finally, we conclude that ŷ A Ay </p><p>Which means that the two optimization goals are equivalent.</p><p>Now we focus on the relationship of the maximization of SI-SDR for the ith source when it is performed directly on the latent space SI-SDR(vi,vi) and when it is performed on the time-domain using the clean source as a target SI-SDR(si,ŝi) ≈ SI-SDR( si,ŝi).</p><p>Again because all the SI-SDR measures are scale-invariant, we can assume that the separation targets and the estimates vectors have unit norms on both the time-domain and the latent space, namely v i = vi = ŝ i = si = 1. By using Proposition 1 we get:</p><formula xml:id="formula_10">v i vi 2 = ŝ i P † P † si 2 ≤ g P † + ŝ i si 2<label>(10)</label></formula><p>Thus, by using the auto-encoder property (Eq. 2) and Proposition 2 we conclude that SI-SDR(vi,vi) on the latent space lower bounds the corresponding value SI-SDR(si,ŝi) on the time-domain. The same proof holds for any encoder E and for other targets on the latent space such as the masks mi ∈ [0, 1] K . Empirically, we indeed notice that the maximization of SI-SDR(vi,vi) on the latent space leads to the maximization of SI-SDR(si,ŝi) on the time-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL FRAMEWORK</head><p>To experimentally verify our approach we perform a set of source separation experiments as described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Audio Data</head><p>We use two audio data collections. For speech sources we use 14, 823 speech utterances from Wall street journal (WSJ0) corpus <ref type="bibr" target="#b23">[24]</ref>. Training, validation and test speaker mixtures are generated by randomly selecting various speakers from the sets si tr s, si dt 05 and si et 05, respectively. For non-speech sounds we use the 2, 000 5secs audio clips which are equally balanced between 50 classes from the environmental sound classification (ESC50) data collection <ref type="bibr" target="#b24">[25]</ref>. ESC50 spans various sound categories such as: non-speech human sounds, animal sounds, natural soundscapes, interior sounds and urban noises. We split the data to train, validation and test sets with a ratio of 8 : 1 : 1, respectively. For each set, the same prior is used across classes (e.g., each class has the same number of clips). Also, the sets do not share clips which originate from the same initial source file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sound Source Separation Tasks</head><p>In order to develop a system capable of performing universal sound source separation <ref type="bibr" target="#b16">[17]</ref>, we evaluate our two-step approach under three distinct sound separation tasks. For all separation tasks, each input mixture consists of two sources which are always mixed using 4secs of their total duration. All audio clips are downsampled to 8kHz for efficient processing. We discuss the audio collection(s) that we utilize and the mixture generation process in the sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Speech Separation</head><p>We only use audio clips containing human speech from WSJ0. In accordance to other studies performing experiments on single-channel speech source separation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>, we use the publicly available WSJ0-2mix dataset <ref type="bibr" target="#b4">[5]</ref>. In total there are 20, 000, 5, 000 and 3, 000 mixtures for training, validation and testing, correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Non-Speech Separation</head><p>We use audio clips only from ESC50. In this case, the total number of the available clean sources sounds is small, and thus, we propose an augmented mixture generation process which enables the generation of much more diverse mixtures. In order to generate each mixture, we randomly select a 4sec segment from two audio files from two distinct audio classes. We mix these two segments with a random signal to noise ratio (SNR)s between −2.5 and 2.5dB. For each epoch, 20, 000 training mixtures are generated which generally are not the same with the ones generated for other epochs. For validation and test sets we fix their random seeds in order to always evaluate on the same 5, 000 and 3, 000 generated mixtures, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Mixed Separation</head><p>All four possible mixture combinations between speech and nonspeech audio are considered by using both WSJ0 and ESC50 sources. Building upon the data augmentation training idea, we also add a random variable which controls the data collection (ESC50 or WSJ0) from which a source waveform is going to be chosen. Specifically, we set an equal probability of choosing a source file from the two collections (ESC50 and WSJ0). For WSJ0 each speaker is considered a distinct sound class, thus, no mixture consists utterances from the same speaker. After the two source waveforms are chosen, we follow the mixture generation process described in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Selected Network Architectures</head><p>Based on recent state-of-the-art approaches on both speech and universal sound source separation with learnable encoder and decoder modules, we consider configurations for the encoder-decoder parts as well as the separation module which are based on a similar timedilated convolutional network (TDCN) architecture. In particular, we consider our implementations of ConvTasNet <ref type="bibr" target="#b6">[7]</ref> that we refer simply as TDCN and its improved version proposed in <ref type="bibr" target="#b16">[17]</ref> that we refer as residual-TDCN (RTDCN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Encoder-Decoder Architecture</head><p>The encoder E consists of one 1D convolutional layer and a ReLU activation on top in order to ensure a non-negative latent representation of each audio input. Following the assumptions stated in Section 2.2.2, we use a 1D transposed convolutional layer for the decoder D. Both encoder and decoder have the same number of channels (or number of bases) and their 1D kernels have a length corresponding to 2.625ms (21 samples) and a hop-size equivalent to 1.25ms (10 samples). For each task we select a different number of channels for the encoder and the decoder modules (32, 128 and 256 for speech only, mixed and non-speech only separation tasks, respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Separation Modules Architectures</head><p>Our implementation of TDCN consists of the same architecture and parameter configuration for the separation module as described in <ref type="bibr" target="#b6">[7]</ref> with an additional batch normalization layer before the final mask estimation which improved its performance over the original version on all separation tasks. Inspired by the original RTDCN separation module <ref type="bibr" target="#b16">[17]</ref>, we keep the same parameter configuration as TDCN and we additionally use a feature-wise normalization between layers instead of global normalization. We also add long-term residual connections from previous layers. Moreover, before summing the residual connections, we concatenate them, normalize them and feed them through a dense layer as the latter yields some further improvement in separation performance. (Code is available online 1 .)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Evaluation Details</head><p>In order to show the effectiveness of our proposed two-step approach, we use the same network architecture when we perform end-to-end time-domain source separation and use as a loss the negative SI-SDR between the estimated signals on the time-domain and the clean waveforms −SI-SDR(ŝ, s * ). Instead in our two-step approach, we train the encoder-decoder parts separately as described in Section 2.1. In the second step, we use the pre-trained encoders for each task and train the separation module using as loss the negative SI-SDR on the latent space targets −SI-SDR(v * ,v) or their corresponding masks −SI-SDR(m * ,m) (see Section 2.2). We train all models using the Adam optimizer <ref type="bibr" target="#b27">[28]</ref>, the batch size is equal to 4, the initial learning rate is set to 0.001 and we decrease it by a factor of 10 at the 100th epoch. We train TDCN and RTDCN separation networks for 100 epochs and 120 epochs, respectively. The encoder-decoder parts for each task are trained independently for 200 epochs (100 times faster than training the separation network). We evaluate the separation performance for all models using SI-SDR improvement (SI-SDRi) on time domain which is the difference of SI-SDR of the estimated signal and the input mixture signal <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. As the STFT oracle mask we choose the ideal ratio mask (IRM) using a Hanning window with 64ms length and 16ms hop-size <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS &amp; DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Time-Domain Separation</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, the mean separation performance of best models is reported for each task. We notice that the proposed two-step approach and training on the latent space leads to a consistent improvement over the end-to-end approach where we train the same architecture using the time-domain SI-SDR loss. This observation holds when different separation modules are used and when we test them under different separation tasks. The non-speech separation task seems the hardest one since the models have access to only a limited number of training mixtures which further underlines the importance of our proposed data-augmentation technique as described in Section 3.2.2. Our two-step approach yields an absolute SI-SDR improvement over the end-to-end baseline of up to 0.7dB, 0.5dB and 0.7dB for speech, non-speech and mixed separation tasks, respectively. Notably, this performance improvement is achieved using the exact same architecture but instead of training it end-to-end using a time-domain loss, we pre-train the auto-encoder part and use a loss on the latent representations of the sources.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Separation Targets in the Latent Space</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we see that the oracle mask obtained from the two-step approach gives a much higher upper bound of separation performance, for all tasks, compared to ideal masks on the STFT domain. This is in line with the prior work that proposed to decompose signals using learned transforms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>. In <ref type="figure" target="#fig_0">Fig. 2</ref> we can qualitatively compare the latent representations obtained from the same encoder when trained with our proposed two-step approach and with the baseline joint training of all modules. When the encoder and decoder are trained individually, a fewer number of bases are used to encode the input which leads to a sparser representation ( 1 norm is roughly 10× smaller compared to the joint training approach). Finally, the latent representations obtained from our proposed approach exhibit a spectrogram-like structure in a way that Speech is encoded using less bases than high frequency sounds like Bird Chirping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We show how by pre-learning an optimal latent space can result in better source separation performance compared to a time-domain end-to-end training approach. Our experiments show that the proposed two-step approach yields a consistent performance improvement under multiple sound separation tasks. Additionally, the obtained sound latent representations remain sparse and structured while they also enjoy a much higher upper bound of separation performance compared to STFT-domain masks. Although this approach was demonstrated on TDCN architectures, it can be easily adapted for use with any other mask-based system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 :</head><label>2</label><figDesc>Training the separation module only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Training a separation network in two independent steps. For each step, the non-trainable parts are represented with a dashed line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 ≤ g (A) + ŷ y 2 . 2 .</head><label>222</label><figDesc>Proposition Let y,ŷ ∈ R d , with unit norms, then maximizing SI-SDR(y,ŷ) w.r.t.ŷ is equivalent to maximizing ŷ y 2 w.r.t.ŷ.Proof. By assuming that there is an optimal solutionŷ :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Joint end-to-end training using time-domain SI-SDR loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>Latent representations of a 1sec mixture and its constituent sources when training the same encoder architecture: a) individually using the proposed two-step approach (top) b) jointly with the TDCN separation module using SI-SDR loss on time-domain (bottom). We sort the basis indexes w.r.t. their energy and we raise the value of each cell to 0.1 for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Source code: github.com/etzinis/two step mask learning Mean SI-SDRi (dB) of best performing models.</figDesc><table><row><cell>Separation</cell><cell>Target</cell><cell cols="3">Sound Separation Task</cell></row><row><cell>Module</cell><cell cols="4">Domain Speech Non-speech Mixed</cell></row><row><cell>TDCN</cell><cell>Time Latent</cell><cell>15.4 16.1</cell><cell>7.7 8.2</cell><cell>11.7 12.4</cell></row><row><cell>RTDCN</cell><cell>Time Latent</cell><cell>15.6 16.2</cell><cell>8.3 8.4</cell><cell>12.0 12.6</cell></row><row><cell>Oracle</cell><cell>STFT</cell><cell>13.0</cell><cell>14.8</cell><cell>14.5</cell></row><row><cell>Masks</cell><cell>Latent</cell><cell>34.1</cell><cell>39.2</cell><cell>39.5</cell></row></table><note>1</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Blind source separation based on time-frequency signal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Belouchrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moeness G Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2888" to="2897" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind source separation and independent component analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung-Min</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo-Young</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing-Letters and Reviews</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep nmf for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMIR</title>
		<meeting>ISMIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised deep clustering for source separation: Direct learning from mixtures using spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrapping single-channel source separation via unsupervised spatial clustering on stereo mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="356" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised training of a deep clustering model for multichannel blind source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hasenklever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="695" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint optimization of masks and deep recurrent neural networks for monaural source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phase reconstruction with learned time-frequency representations for single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWAENC</title>
		<meeting>IWAENC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="396" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end source separation with adaptive front-ends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Casebeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asilomar Conference on Signals, Systems, and Computers</title>
		<meeting>Asilomar Conference on Signals, Systems, and Computers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-stage single-channel audio source separation using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1783" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speakerindependent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at</title>
		<meeting><address><addrLine>Harriman, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Furcax: End-to-end monaural speech separation based on deep gated (de) convolutional neural networks with adversarial example training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoji</forename><surname>Hayakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6985" to="6989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Phasebook and friends: Leveraging discrete representations for source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Sarroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="370" to="382" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
