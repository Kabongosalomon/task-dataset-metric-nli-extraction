<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Research Institute of Robotics</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a novel approach for 3D semantic instance segmentation on point clouds. A 3D convolutional neural network called submanifold sparse convolutional network is used to generate semantic predictions and instance embeddings simultaneously. To obtain discriminative embeddings for each 3D instance, a structure-aware loss function is proposed which considers both the structure information and the embedding information. To get more consistent embeddings for each 3D instance, attention-based k nearest neighbour (KNN) is proposed to assign different weights for different neighbours. Based on the attention-based KNN, we add a graph convolutional network after the sparse convolutional network to get refined embeddings. Our network can be trained end-to-end. A simple mean-shift algorithm is utilized to cluster refined embeddings to get final instance predictions. As a result, our framework can output both the semantic prediction and the instance prediction. Experiments show that our approach outperforms all state-of-art methods on ScanNet benchmark and NYUv2 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, semantic instance segmentation is a popular topic in computer vision. As the development of 3D sensors such as RGBD cameras or LIDAR, 3D scene understanding becomes more and more important in augmented reality and autonomous driving. Compared to 2D scene understanding, 3D understanding is more challenging for the data sparsity and the expensive computation cost. However, 3D data contains geometric information which is useful for semantic understanding while 2D images do not. In this paper, we focus on 3D semantic instance segmentation.</p><p>Instance segmentation in 2D images achieves a great performance. Most approaches to 2D instance segmentation are proposal-based which first apply a object detector <ref type="bibr" target="#b7">Ren et al., 2015;</ref><ref type="bibr" target="#b7">Redmon et al., 2016;</ref><ref type="bibr">Liu et al., 2016;</ref><ref type="bibr">Lin et al., 2017;</ref><ref type="bibr" target="#b6">Lin et al., 2018]</ref> to get initial bounding boxes and then segment each bounding box binarily to get the instance mask. Such idea <ref type="bibr" target="#b4">[He et al., 2017]</ref> achieves excellent results which benefits from accurate object detection. However, these methods have some drawbacks. First, they are the combination of object detection and semantic segmentation so the training process is complex. Second, one pixel may have more than one instance labels as they may be in two overlapped bounding boxes simultaneously. The second problem can be more serious when it is the multi-class instance segmentation of clutter scenes.</p><p>An alternative idea is to generate embeddings <ref type="bibr" target="#b2">[Fathi et al., 2017;</ref><ref type="bibr" target="#b5">Kong and Fowlkes, 2018;</ref><ref type="bibr" target="#b1">De Brabandere et al., 2017]</ref> for each pixel and then apply a cluster algorithm to get the final instance result. This idea can utilize semantic segmentation networks to generate discriminative embeddings. Although such proposal-free methods cannot get as high performance as proposal-based methods in 2D images, they are simpler in the implementation and can avoid the drawbacks of proposal-based methods. Additionally, such framewrok can simultaneously segment images in the semantic level and instance level while the proposal-based method can only get the instance result. It means that objects without the instance label such as the sky or the road may be dropped by the proposal-based method. In this paper, we propose a proposalfree framework for 3D semantic instance segmentation. <ref type="figure" target="#fig_0">Figure 1</ref> shows the input and the output of our method.</p><p>Our backbone network can be arbitary 3D neural network. In this paper, we choose the submanifold sparse convolutional neural network <ref type="bibr" target="#b4">[Graham et al., 2018]</ref> to get semantic labels and generate inital embeddings for points. Within an object instance, embeddings of points in the center of the object are more likely to be similar while embeddings near the edge are more likely to be different. To get more consistent embeddings for the same instance, we considering geometric information for 3D instances and propose a structure-aware loss function for 3D instance segmentation.</p><p>In 2D images, adjacent pixels may be far away from each other. Compared to the 2D situation, adjacent points in the 3D space are more likely to be in the same instance. So k nearest neighbour (KNN) algorithm can be used to pass and aggregate information. Message passed from neighbour points cannot only enforce the consistency of embeddings but also eliminate the quantitative error caused by 3D voxel. However, a point and its neighbours are also likely to be in the different instances if the point is near the edge. If so, wrong information will be passed to the point. Considering this problem, we propose an attention-based graph convolutional neural network which automaticly aggregates useful information from neighbours.</p><p>The main contributions of this paper are as follows:</p><p>• We propose a structure-aware loss function for 3D instance segmentation which considers both the geometric information and the embedding information for each 3D instance.</p><p>• We propose an attention-based graph convolutional neural network which can automaticly choose and aggregate information from neighbours.</p><p>• We propose a novel method for 3D semantic instance segmentation. Experiments show that our proposed method outperforms all state-of-the-art methods on ScanNet benchmark <ref type="bibr" target="#b1">[Dai et al., 2017]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instance Segmentation</head><p>Instance Segmentation in 2D Images. CNN based methods have achieved excellent results on object detection <ref type="bibr" target="#b7">Ren et al., 2015;</ref><ref type="bibr" target="#b7">Redmon et al., 2016;</ref><ref type="bibr">Liu et al., 2016;</ref><ref type="bibr">Lin et al., 2017;</ref><ref type="bibr" target="#b6">Lin et al., 2018]</ref> in 2D images. As a combination of object detection and semantic segmentation, instance segmentation <ref type="bibr" target="#b4">[He et al., 2017;</ref><ref type="bibr" target="#b2">Fathi et al., 2017;</ref><ref type="bibr" target="#b5">Kong and Fowlkes, 2018;</ref><ref type="bibr" target="#b1">De Brabandere et al., 2017]</ref> becomes a hot topic in research because it can provide richer semantic information. Inspired by object detection, many approaches to instance segmentation segment the bounding box to get the instance mask. Another idea is to generate an embedding for each pixel and cluster according to the similarity between pixels. Such "proposal-free" methods can avoid some limitations of proposal-based methods. <ref type="bibr" target="#b2">[Fathi et al., 2017]</ref>   <ref type="bibr" target="#b8">[Wang et al., 2018a</ref>] is a pioneer in 3D instance segmentation. It generates an embedding for each point and proposes a double-hinge loss to supervise the embedding learning.  generates a proposal for each object by reconstructing the shape and then combine PointNet++ to get the final instance segmentation result. <ref type="bibr" target="#b4">[Hou et al., 2018]</ref> proposes a detection-based method to get the instance prediction which also fuses multi-modal inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning on Point Clouds</head><p>Deep learning on point clouds develops fast in the recent years. Voxel-based method <ref type="bibr" target="#b7">[Tchapmi et al., 2017</ref>] is a natural generalization of 2D convolution. However, the performance of voxel-based method is limited by the resolution of voxels. <ref type="bibr" target="#b7">[Riegler et al., 2017;</ref><ref type="bibr" target="#b4">Graham et al., 2018]</ref> exploit the sparsity property of 3D data and enable 3D CNN to achieve a higher resolution and efficiency. Additionally, such sparse convolutional operations can be easily combined with many great network frameworks of 2D images. PointNet <ref type="bibr" target="#b7">[Qi et al., 2017]</ref> provides a brand new direction for 3D deep learning. It directly process raw point clouds without quantitive errors.</p><p>Recently, graph <ref type="bibr">CNN [Bruna et al., 2013;</ref><ref type="bibr" target="#b5">Kipf and Welling, 2016;</ref><ref type="bibr" target="#b7">Simonovsky and Komodakis, 2017;</ref><ref type="bibr" target="#b8">Wang et al., 2018b]</ref> is popular in research as it can process unregular data. It can be viewed as the generalization of conventional convolution operation in the non-Euclidean space. Graph convolution has the strong ability to pass messages between neighbours. Point cloud is a kind of graphs which can utilize graph convolution to extract local information using graph convolution. K nearest neighbour algorithm is widely used to search neighbours for point clouds. However, it treats each neighbours equivalently and may bring some improper information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD DESCRIPTION</head><p>In this section, we first describe the whole network architecture (Section 3.1). Then we introduce our proposed structureaware loss function (Section 3.2) for supervising the learning of instance embeddings and the attention-based KNN (Section 3.3, Section 3.4) for message aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The whole network (illstruted in <ref type="figure">Figure 2</ref>) consists three main components including the submanifold convolutional network, the structure-aware loss function and the graph convolutional network. The architecture of the submanifold convolutional network is borrowed from <ref type="bibr" target="#b4">[Graham et al., 2018]</ref>. We recommend <ref type="bibr" target="#b4">[Graham et al., 2018]</ref> for more details on the network. The ouput of the submanifold convolutional network is the input to two different MLP networks. The first MLP outputs the semantic predictions for the cross entropy loss function. The second MLP outputs the instance embeddings for the structure-aware loss function. To refine the in-Figure 2: Illustration of the whole network architecture. The input is the original point coordinate with RGB attributes. The output of the submanifold network is the initial embedding for each point. Then two MLPs are followed to generate the semantic prediction and the instance embedding respectively. The number of the semantic class is C and the dimension of the instance embedding is 4 in our paper. Two GCNs are used to refine the instance embedding. The embedding generated by M LP2 and the refined embedding optimized by GCNs are both used to calculate the structure-aware loss respectively. The final training process is a multi-task learning. stance embeddings, the output of the second MLP is also inputted to a series of GCNs (Section 3.4). Finally, three loss items are added as the total loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure-aware Loss Function</head><p>After generating initial embeddings for all points, we hope that points within the same instance have similar embeddings while points from different instances are apart in the embedding space. This is a classical problem in the metric learning. However, for the 3D instance segmentation task, points withinc each instance do not only have embedding features but also have geometric relations in the 3D space. This is different from the past researches in the metric learning. We combine such structure information with embedding features to make the final results more discriminative.</p><p>First, we need to define a metric for measuring the similarity between embeddings. Euclidean distance and cosine distance are commonly used. Cosine distance is scale-invariant to the length of the embedding vector which is an advantage compared to Euclidean distance. However, if using cosine distance, all embeddings need to be lied on a hyper-sphere. One difficulty for instance segmentation is that the number of instance is not uncertain. If the number is too large, the embedding of different instances may be not discriminative enough as they are limited in a hyper-sphere. One solution is to set the dimension of the embedding very high which may cause the learning process and the post-process more difficult. Considering the above reason, we choose the Euclidean distance to measure the similarity for its simplicity.</p><p>We want to minimize the distance between embeddings within the same instance. A mean embedding can be used to describe the overall feature of a instance. For the i th instance, the loss function is formalized as follows:</p><formula xml:id="formula_0">Loss intra i = Ni j=1 1 1 + exp(−p i,j ) [s i,j − α] 2 +<label>(1)</label></formula><p>where α is a threshold for penalizing large embedding distance. N i is the point number of the i th instance. p i,j measures the spatial distance between the j th point and the geometric center µ p,i of the i th instance and s i,j measures the embedding distance between the j th point and the mean embedding µ s,i :</p><formula xml:id="formula_1">p i,j = p j − µ p,i , µ p,i = 1 N i Ni j=1 p i,j<label>(2)</label></formula><formula xml:id="formula_2">s i,j = s j − µ s,i , µ s,i = 1 N i Ni j=1 s i,j<label>(3)</label></formula><p>where p i,j and s i,j are the coordinate and the embedding of the j th point within the i th instance.</p><p>On the other hand, to make points from different instances discriminative, mean embeddings between different instances should be far away from each other. This idea is commonly used in the previous research in the metric learning:</p><formula xml:id="formula_3">Loss inter ij = [β − µ s,i − µ s,j ] 2 + (4)</formula><p>where β is a threshold for the distance between mean embeddings. It means that the loss function just penalizes small distance. If the distance is larger than the threshold, it will not contribute to the loss value as the embeddings are apart enough in the embedding space.</p><p>Our final loss function is composed of the above items:</p><formula xml:id="formula_4">Loss = 1 M M i=1 Loss intra i + 1 M (M − 1) M i=1 M j=1,j =i Loss inter ij (5)</formula><p>where M is the total number of the instance in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention-based K Nearest Neighbour</head><p>The goal of our network is to generate similar embeddings within the same instance and discriminative embeddings between different instances. To achieve this goal, k nearest neighbour (KNN) algorithm can be utilized to enhance the local consistency of embeddings. A point can aggregate information from surrounding points. However, KNN may bring some wrong information which is harmful for embeddings. For example, a point near the edge of a instance may aggregate information from another instances. So we propose an attention-based KNN for embedding aggregation.</p><p>The input embeddings of point clouds are denoted by X = {x 1 , ..., x n } ⊆ R F . x ji 1 , ..., x ji k are the k nearest neighbours of x i according to their spatial positions. We argue that our KNN use the spatial distance of points instead of the embedding distance as the metric. The standard KNN aggregation process can be formalized as follows:</p><formula xml:id="formula_5">x aggregate i = 1 k k m=1</formula><p>x ji m (6) <ref type="figure">Figure 3</ref>: Illustration of the graph convolutional neural network using attention-based KNN. The aggregator is our proposed attentionbased KNN (Section 3.3). In step one, for each input point, k nearest neighbours are searched according to the spatial coordinate. In step two, different weights are assigned to different neighbours. The output of the aggregator is the weighted average of the embeddings of k neighbours. The skip connection is used to concatenate the ouput of the aggregator and the input embedding. Finally, a fully connected layer is used to get the refined embedding.</p><p>To achieve the goal of automatic embedding selection, we utilize the attention mechanism. The operation can be formalized as follows:</p><formula xml:id="formula_6">x aggregate i = k m=1 α m x ji m<label>(7)</label></formula><p>where α m is the attention weight for each neighbour. It is related to the embedding of the neighbour and the corresponding center point. It can be calculated as follows:</p><formula xml:id="formula_7">p m = f (x i , x ji m )<label>(8)</label></formula><p>where f : R 2×F → R 1 is a trainable MLP. α m is the normalization of p m using the softmax function:</p><formula xml:id="formula_8">α m = sof tmax(p m ) = exp(p m ) k m=1 exp(p m )<label>(9)</label></formula><p>Compared to the standard KNN aggregation, attentionbased KNN can assign different weights for different neighbours. The aggregator in <ref type="figure">Figure 3</ref> illustrates the process of the attention-based KNN. In theory, the network can learn better aggregation strategy than simple average aggregation. Experiments also show its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph Convolutional Neural Network using</head><p>Attention-based KNN Normally, graph convolutional neural network consists two parts: the aggregator and the updator. The aggregator is to gather information from neighbours and the updator is to update the aggregated information by mapping embeddings into a new feature space. Here, we use our proposed attentionbased KNN as the aggregator and use a simple fully connected layer without bias as the updator (illustrated in <ref type="figure">Figure 3)</ref>. The operation is formalized as follows:</p><formula xml:id="formula_9">x update i = [x i , x aggregate i ]W<label>(10)</label></formula><p>where W ⊆ R 2F ×F is a trainable parameter of the updator. Analysis. Our proposed graph convolutional network does not need to calculate the laplacian matrix and the eigendecomposition which need huge computation cost. This is very important for graph CNN to apply to the point cloud data. Actually, our method can be viewed as one kind of spatial graph convolutional networks which do not need to compute the eigenvalue of the graph. Also, we use the KNN (complexity O(n × k)) to decribe the relation instead of using the laplacian matrix form (complexity O(n 2 )). However, the two forms are equivalent essentially. If the laplacian matrix is sparse, some sparse tricks can be used to decrease the computation cost.</p><p>The main spotlight of our graph convolutional network is that it uses the attention-based KNN as the aggregator. This is a natural and meaningful operation for point clouds. It allows the network to learn different importances for different neighbours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Datasets. We evaluate our model in two datasets providing 3D instance segmentation labels:</p><p>• ScanNet <ref type="bibr" target="#b1">[Dai et al., 2017]</ref>: This datasets contains 1613 3D indoor scans. We follow the official split of 1201 training samples, 300 validation samples and 100 testing samples (without ground truth). The dataset provides a benchmark for several tasks including 3D instance segmentation. It provides images from different views but we only use the point cloud data in our method.</p><p>• NYUv2 <ref type="bibr">[Silberman et al., 2012]</ref>: This dataset contains 1449 single RGBD images. We follow the same preprocessing method as <ref type="bibr" target="#b8">[Wang et al., 2018a]</ref> and  to get the 3D annotation of point clouds. We follow the standard split of 795 trainging samples and 654 testing samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details.</head><p>We implement the network with Pytorch1.0 and run it on a single NVIDIA GTX1080Ti. Our network can be easily trained end-to-end. We use the ADAM optimizier with constant learning rate 0.001. α and β in the structure-aware loss function is set 0.7 and 1.5 respectively. In our experiment, we use two backbone networks with different model capacities provided by <ref type="bibr">[Graham</ref>   <ref type="figure">Figure 4</ref>: Visualization of ScanNet results. The first column is the input of our model. The second column is the prediction of semantic labels. The third column is the ground truth of semantic segmentation. The forth column is the instance prediction. The fifth column is the ground truth of instance segmentation. For instance segmentation, we only visualize the 18 catagories useful for evaluation while droping other catagories. et al., 2018]. The first backbone network is a UNet-like architecture based on the submanifold sparse convolution with smaller capcity and faster speed. The second is a ResNet-like architecture with larger capcity and slower speed. We train the whole model with the UNet backbone for 50 hours until convergence. The model with the ResNet-backbone needs 150 hours and get much better results. In practice, we pretrain the backbone network first to get a pretrained semantic segmentation model. Then we train the whole model based on the pretrained model. Using pretrained model can save time when conducting multiple experiments. During the inference, mean-shift algorithm is used to cluster embeddings to get the instance prediction. The bandwidth of mean-shift is set 1.0.</p><p>Metrics. The average precison (AP) is widely used in the instance segmentation. For ScanNet dataset, the AP with an IoU threshold 0.5 (AP 0.5 ) is commonly used. For NYUv2 dataset, the AP with an IoU threshold 0.25 (AP 0.25 ) is chosen. For both of the two datasets, images and point clouds (with additional RGB attributes) are provided. Some previ-ous methods use both of the two inputs while others use a single input. In this paper, we only use the point cloud as the input. We argue that we do not use features extracting from images using image-based 2D networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Instance Segmentation on ScanNet</head><p>The ScanNet dataset provides aN online benchmark. So we first evaluate our method on the dataset. 18 categories are used in the instance segmentation task which makes it more challenging task compared to the instance segmentation on a single category. The input of our network is the coordinate and the RGB value of each point.</p><p>Our method outperforms all the state-of-arts on the Scan-Net benchmark. Among all previous methods, SGPN <ref type="bibr" target="#b8">[Wang et al., 2018a]</ref> is the most similar method with our method. Compared to SGPN, the space complexity and the computation complexity of our proposed structure-aware function are both O(n) while these of SGPN are both O(n 2 ). Also, our proposed function considers the structure information while  The UNet-backbone model outperforms almost state-ofarts except 3D-SIS which additionally uses multiple images. The ResNet-backbone model outperforms all methods by a large margin. The result shows the effectiveness of our method. Our method achieves high AP for most categories. Proposal-based methods like R-PointNet and 3D-SIS get higher results for categories such as chairs, sofa, fridges and so on. It is easy to generate bounding boxes for these categories. Mask R-CNN performs better on the picture because the feature of the picture is distinct in 2D images. Our method leverage both semantic information and instance information to generate embeddings for each point so that we can also provide the semantic prediction. Our method is more like the panoptic segmentation instead of just the instance segmentation. Additionally, our method can adapt objects with different sizes and shapes without the limitation of the bounding box. Qualitative results are showed in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instance Segmentation on NYUv2</head><p>Different from ScanNet dataset, NYUv2 dataset provides the single RGBD image instead of the whole scene. Previous methods usually use both images and point clouds as the input to increase the precision on this dataset. We only use the 3D point cloud as the input in this paper. Even though, our method outperforms all state-of-art methods on this dataset. Specially, our method achieves the highest precision for many categories. As NYUv2 dataset provides a single RGBD image with partial point clouds, categories such as boxes, monitors, garbage bins are difficult to recognize only using point clouds. It is easier to segment these categories on the image than on the point cloud. So MRCNN gets better results than our method on some of these categories. Fusing visual features from images can be also helpful. We leave the multisensor fusion as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct the ablation study on the validation set of the ScanNet (v2) dataset.</p><p>Different loss functions. To validate the effectiveness of our structure-aware loss function, we compare it with a vanilla version loss function. The vanilla version does not use the structure information which means that the importance for each points in the same instance is the same. The first two rows of <ref type="table" target="#tab_4">Table 3</ref> show the results. Using structure-aware loss function brings more than 1% gain on AP 0.5 and AP 0.25 . Also, we find that the structure-aware loss function does not contribute to the IoU of semantic segmentation. So we think that the increase of the average precision (AP) benefits from more discriminative embeddings supervised by the structureaware loss function.</p><p>Different numbers of the GCN layer. We compare different numbers of the GCN layer to explore the effectiveness of the GCN. The third to the fifth row in <ref type="table" target="#tab_4">Table 3</ref> provides the results with different layer numbers. The GCN layer can contribute to the final result. However, more layers do not mean better result. We find that using two GCN layers achieves the best result. We analyze that more layers may cause overfitting or oversmoothing. This is a common issue for the graph convolution. Also, too many layers may increase the difficulty for training.</p><p>Different backbone networks. Our proposed architecture can adapt to different semantic segmentation backbone networks. In this paper, we compare two models using the UNet backbone and the ResNet backbone respectively. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. The model using the ResNet backbone outperforms the model using the UNet backbone by a large margin. It shows that the backbone network affects our model a lot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We present a novel approach for 3D semantic instance segmentation. The structure-aware loss function considers geometric information to generate discriminative embeddings for instance segmentation. The graph convolutional neural network using attention-based KNN refines intial embeddings by automatic feature selection and aggregation. Experiments show that our approach outperforms all state-of-art methods on ScanNet benchmark and NYUv2 dataset. In the future, multi-sensor fusion can be added to our network to further increase the precision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The input of our network is point clouds with xyz coordinates and RGB attributes. The output includes two parts: semantic preditions and instance embeddings. Mean-shift algorithm is used to cluster embeddings to get the final instance predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the test set of ScanNet (v2) 3D instance segmentation benchmark. AP0.5 is reported in thetable. image point cloud mean cabinet bed chair sofa table door window bookshelf picture counter desk curtain fridge shower toilet sink bathtub other</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [He et al., 2017]</cell><cell>yes</cell><cell>no</cell><cell>5.8</cell><cell>5.3</cell><cell>0.2</cell><cell>0.2</cell><cell>10.7</cell><cell>2.0</cell><cell>4.5</cell><cell>0.6</cell><cell>0.0</cell><cell>23.8</cell><cell>0.2</cell><cell>0.0</cell><cell>2.1</cell><cell>6.5</cell><cell>0.0</cell><cell>2.0</cell><cell>1.4</cell><cell>33.3</cell><cell>2.4</cell></row><row><cell>SGPN [Wang et al., 2018a]</cell><cell>no</cell><cell>yes</cell><cell>14.3</cell><cell>6.5</cell><cell cols="4">39.0 27.5 35.1 16.8</cell><cell>8.7</cell><cell>13.8</cell><cell>16.9</cell><cell>1.4</cell><cell>2.9</cell><cell>0.0</cell><cell>6.9</cell><cell>2.7</cell><cell>0.0</cell><cell cols="2">43.8 11.2</cell><cell>20.8</cell><cell>4.3</cell></row><row><cell>MTML</cell><cell>unknown</cell><cell>unknown</cell><cell>21.1</cell><cell>2.7</cell><cell cols="5">61.4 39.0 50.0 10.5 10.0</cell><cell>0.3</cell><cell>33.7</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>11.8</cell><cell>16.7</cell><cell>14.3</cell><cell>57.0</cell><cell>4.6</cell><cell>66.7</cell><cell>2.8</cell></row><row><cell>3D-BEVIS</cell><cell>unknown</cell><cell>unknown</cell><cell>24.8</cell><cell>3.5</cell><cell cols="4">56.6 39.4 60.4 18.1</cell><cell>9.9</cell><cell>17.1</cell><cell>7.6</cell><cell>2.5</cell><cell>2.7</cell><cell>9.8</cell><cell>3.5</cell><cell>9.8</cell><cell>37.5</cell><cell cols="2">85.4 12.6</cell><cell>66.7</cell><cell>3.0</cell></row><row><cell>R-PointNet [Yi et al., 2018]</cell><cell>no</cell><cell>yes</cell><cell>30.6</cell><cell>34.8</cell><cell cols="5">40.5 58.9 39.6 27.5 28.3</cell><cell>24.5</cell><cell>31.1</cell><cell>2.8</cell><cell>5.4</cell><cell>12.6</cell><cell>6.8</cell><cell>21.9</cell><cell>21.4</cell><cell cols="2">82.1 33.1</cell><cell>50.0</cell><cell>29.0</cell></row><row><cell>3D-SIS [Hou et al., 2018]</cell><cell>yes</cell><cell>yes</cell><cell>38.2</cell><cell>24.5</cell><cell cols="5">43.2 57.7 69.9 27.1 32.0</cell><cell>23.5</cell><cell>24.5</cell><cell>7.5</cell><cell>1.3</cell><cell>3.3</cell><cell>26.3</cell><cell>42.2</cell><cell>85.7</cell><cell cols="2">88.3 11.7</cell><cell>100.0</cell><cell>24.0</cell></row><row><cell>UNet-backbone(ours)</cell><cell>no</cell><cell>yes</cell><cell>31.9</cell><cell>18.9</cell><cell cols="5">71.5 47.9 61.5 35.5 20.1</cell><cell>9.3</cell><cell>23.3</cell><cell>10.7</cell><cell>0.8</cell><cell>6.7</cell><cell>21.8</cell><cell>12.3</cell><cell>43.8</cell><cell cols="2">91.6 15.0</cell><cell>66.7</cell><cell>17.3</cell></row><row><cell>ResNet-backbone(ours)</cell><cell>no</cell><cell>yes</cell><cell>45.9</cell><cell>25.9</cell><cell cols="5">73.7 58.7 53.6 59.0 41.6</cell><cell>30.4</cell><cell>15.9</cell><cell>12.8</cell><cell>13.8</cell><cell>21.7</cell><cell>47.5</cell><cell>31.5</cell><cell>71.4</cell><cell cols="2">87.3 41.1</cell><cell>100.0</cell><cell>40.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>3D instance segmentation results on NYUv2 dataset. AP0.25 is reported in the table. Method image point cloud mean bathtub bed bookshelf box chair counter desk door dresser garbage lamp monitor night pillow sink sofa table television toilet</figDesc><table><row><cell>MRCNN</cell><cell>yes</cell><cell>no</cell><cell>29.3</cell><cell>26.3</cell><cell>54.1</cell><cell>23.4</cell><cell>3.1</cell><cell>39.3</cell><cell>34.0</cell><cell>6.2</cell><cell>17.8</cell><cell>23.7</cell><cell>23.1</cell><cell>31.1</cell><cell>35.1</cell><cell>25.4</cell><cell>26.6</cell><cell>36.4 47.1 21.0</cell><cell>23.3</cell><cell>58.8</cell></row><row><cell>MRCNN*</cell><cell>yes</cell><cell>no</cell><cell>31.5</cell><cell>24.7</cell><cell>66.3</cell><cell>20.1</cell><cell>1.4</cell><cell>44.9</cell><cell>43.9</cell><cell>6.8</cell><cell>16.6</cell><cell>29.5</cell><cell>22.1</cell><cell>29.2</cell><cell>29.3</cell><cell>36.9</cell><cell>34.6</cell><cell>37.1 48.4 26.6</cell><cell>21.9</cell><cell>58.5</cell></row><row><cell>SGPN-CNN [Wang et al., 2018a]</cell><cell>yes</cell><cell>yes</cell><cell>33.6</cell><cell>45.3</cell><cell>62.5</cell><cell>43.9</cell><cell>0.0</cell><cell>45.6</cell><cell>40.7</cell><cell cols="2">30.0 20.2</cell><cell>42.6</cell><cell>8.8</cell><cell>28.2</cell><cell>15.5</cell><cell>43.0</cell><cell>30.4</cell><cell>51.4 58.9 25.6</cell><cell>6.6</cell><cell>39.0</cell></row><row><cell>R-PointNet-CNN [Yi et al., 2018]</cell><cell>yes</cell><cell>yes</cell><cell>39.3</cell><cell>62.8</cell><cell>51.4</cell><cell>35.1</cell><cell cols="2">11.4 54.6</cell><cell>45.8</cell><cell cols="2">38.0 22.9</cell><cell>43.3</cell><cell>8.4</cell><cell>36.8</cell><cell>18.3</cell><cell>58.1</cell><cell>42.0</cell><cell>45.4 54.8 29.1</cell><cell>20.8</cell><cell>67.5</cell></row><row><cell>ResNet-backbone(ours)</cell><cell>no</cell><cell>yes</cell><cell>43.0</cell><cell>82.1</cell><cell>67.3</cell><cell>48.1</cell><cell>3.5</cell><cell>65.4</cell><cell>56.8</cell><cell cols="2">14.5 37.6</cell><cell>23.1</cell><cell>7.3</cell><cell>60.0</cell><cell>4.4</cell><cell>52.9</cell><cell>34.3</cell><cell>68.2 55.0 28.3</cell><cell>20.7</cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different network structures. Hou et al., 2018] are proposal-based methods. 3D-SIS does not only use the point cloud as the input but also uses images from multiple views. Image information also contributes to its final result. However, our UNet-backbone model and ResNet-backbone model only use the point cloud data as the input.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell cols="2">AP0.5 AP0.25</cell><cell>IoU</cell></row><row><cell>UNet+vanillaLoss</cell><cell cols="2">0.150 0.338</cell><cell>0.599</cell><cell>0.569</cell></row><row><cell>UNet+strucLoss</cell><cell cols="2">0.158 0.350</cell><cell>0.613</cell><cell>0.570</cell></row><row><cell>UNet+strucLoss+gcn×1</cell><cell cols="2">0.163 0.356</cell><cell>0.621</cell><cell>0.574</cell></row><row><cell>UNet+strucLoss+gcn×2</cell><cell cols="2">0.171 0.360</cell><cell>0.630</cell><cell>0.572</cell></row><row><cell>UNet+strucLoss+gcn×3</cell><cell cols="2">0.165 0.351</cell><cell>0.623</cell><cell>0.576</cell></row><row><cell cols="3">ResNet+strucLoss+gcn×2 0.270 0.464</cell><cell>0.672</cell><cell>0.689</cell></row><row><cell cols="5">SGPN considers each point equivalently. R-PointNet [Yi et</cell></row><row><cell>al., 2018] and 3D-SIS [</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<idno type="arXiv">arXiv:1708.02551</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Semantic instance segmentation with a discriminative loss function</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girshick ; Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07003</idno>
	</analytic>
	<monogr>
		<title level="m">Ji Hou, Angela Dai, and Matthias Nießner. 3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<editor>Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<editor>Liu et al., 2016] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
	<note>2017 International Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<idno>arXiv:1812.03320</idno>
	</analytic>
	<monogr>
		<title level="m">He Wang, Minhyuk Sung, and Leonidas Guibas. Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
