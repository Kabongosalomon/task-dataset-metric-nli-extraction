<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EXTD: Extremely Tiny Face Detector via Iterative Filter Reuse</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
							<email>youngjoon.yoo@navercorp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<email>dongyoon.han@navercorp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<email>sangdoo.yun@navercorp.com</email>
						</author>
						<title level="a" type="main">EXTD: Extremely Tiny Face Detector via Iterative Filter Reuse</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a new multi-scale face detector having an extremely tiny number of parameters (EXTD), less than 0.1 million, as well as achieving comparable performance to deep heavy detectors. While existing multiscale face detectors extract feature maps with different scales from a single backbone network, our method generates the feature maps by iteratively reusing a shared lightweight and shallow backbone network. This iterative sharing of the backbone network significantly reduces the number of parameters, and also provides the abstract image semantics captured from the higher stage of the network layers to the lower-level feature map. The proposed idea is employed by various model architectures and evaluated by extensive experiments. From the experiments from WIDER FACE dataset, we show that the proposed face detector can handle faces with various scale and conditions, and achieved comparable performance to the more massive face detectors that few hundreds and tens times heavier in model size and floating point operations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting faces in an image is considered to be one of the most practical tasks in computer vision applications, and many studies <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b29">30]</ref> are proposed from the beginning of the computer vision research. After the advent of deep neural networks, many face detection algorithms <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> applying the deep network have reported significant performance improvement to the conventional face detectors.</p><p>The state-of-the-art (SOTA) face detectors <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> for in-the-wild images employ the framework of the recent object detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref>. These methods can even handle a various scale of faces with difficult conditions such as distortion, rotation, and occlusion. Among them, the face detectors <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b57">58]</ref> using multiple feature-maps from different layer locations, which mainly stem from <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, are dominantly used since * Clova AI Research, NAVER Corp. We follow the alphabetical order except the first author.  <ref type="figure">Figure 1</ref>. Illustration of the mean average precision (mAP) regarding the parameter size (a) and Flops (b) evaluated on WIDER FACE dataset. Our method (star) shows comparable mAP to S3FD <ref type="bibr" target="#b59">[60]</ref> with a significantly smaller model. Red stars denote the proposed models with various sizes. 'S3FD+M' denotes the S3FD variation using MobileFaceNet <ref type="bibr" target="#b1">[2]</ref> as a backbone network instead of VGG-16 <ref type="bibr" target="#b41">[42]</ref>. Best viewed in wide and colored vision.</p><p>these methods can handle the faces with various scale in a single forward path. While these methods achieved impressive detection performance, they commonly share two problems. One is their large number of parameters. Since they use a large classification network such as VGG-16 <ref type="bibr" target="#b41">[42]</ref>, ResNet <ref type="bibr" target="#b10">[11]</ref>-50 or 101, and DenseNet-169 <ref type="bibr" target="#b13">[14]</ref>, the number of total parameters exceed 20 million, over 80Mb supposing 32-bit floating point for each parameter. Furthermroe, the amount of floating point operations (FLOPs) also exceeds 100G, and these make it nearly impossible to use the face detectors in CPU or mobile environment, where the most face applications run in. The second problem, from the architecture perspective, is the limited capacity of the low-level feature map in capturing object semantics. The most single-shot detector (SSD) <ref type="bibr" target="#b27">[28]</ref> variant object and face detectors struggle the problem because the low-level feature map passes shallow convolutional layers. To alleviate the problem, the variants of Feature pyramid network (FPN) architecture such as <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref> are used but requires additional parameters and memories for re-expanding the feature map.</p><p>In this paper, we propose a new multi-scale face detector with extremely tiny size (EXTD) resolving the two mentioned problems. The main discovery is that we can share the network in generating each feature-map, as shown in <ref type="figure">Figure 2</ref>. As in the figure, we design a backbone network such that reduces the size of the feature map by half, and we can get the other feature maps with recurrently passing the network. The sharing can significantly reduce the number of parameters, and this enables our model to use more layers to generate the low-level feature maps used for detecting small faces. Also, the proposed iterative architecture makes the network to observe the features from various scale of faces and from various layer locations, and hence offer abundant semantic information to the network, without adding additional parameters.</p><p>Our baseline framework follows FPN-like structures, but can also be applied to SSD-like architecture. For SSD based architecture, we adopt the setting from <ref type="bibr" target="#b59">[60]</ref>. For the FPN architectures, we refer an up-sampling strategy from <ref type="bibr" target="#b22">[23]</ref>. The backbone network is designed to have less than 0.1 million parameters with employing inverted residual blocks proposed in MobileNet-V2 <ref type="bibr" target="#b39">[40]</ref>. We note that our model does not require any extra layer commonly defined as in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">25]</ref>, and is trained from scratch. We evaluated the proposed detector and its variants on WIDER FACE <ref type="bibr" target="#b52">[53]</ref> dataset, the most widely used and similar to the in-the-wild situation.</p><p>The main contributions of this work can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose an iterative network sharing model for multi-stage face detection which can significantly reduce the parameter size, as well as provide abundant object semantic information to the lower stage feature maps.</p><p>(2) We design a lightweight backbone network for the proposed iterative feature map generation with 0.1M number of parameters, which less than 400Kb, and achieved comparable mAP to the heavy face detection methods. (3) We employ the iterative network sharing idea to the widely used detection architectures, FPN and SSD, and show the effectiveness of the proposed scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Face detectors: Face detection has been an important research topic since an initial stage of computer vision researches. Viola et al. <ref type="bibr" target="#b45">[46]</ref> proposed a face detection method using Haar features and Adaboost with decent performance, and several different approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30]</ref> followed. After deep learning has become dominant, many face detection methods applying the techniques have been published. In the early stages, various attempts were tried to employ the deep architecture to face detection, such as cascade architecture <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b56">57]</ref>, and occlusion handling <ref type="bibr" target="#b51">[52]</ref>.</p><p>Recent face detectors has been designed based on the architecture of generic object detectors including Faster-RCNN <ref type="bibr" target="#b37">[38]</ref>, R-FCN <ref type="bibr" target="#b3">[4]</ref>, SSD <ref type="bibr" target="#b27">[28]</ref>, FPN <ref type="bibr" target="#b25">[26]</ref>, and Reti-naNet <ref type="bibr" target="#b26">[27]</ref>. Face RCNN and its variants <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b55">56]</ref> apply Faster-RCNN, and <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b61">62]</ref> use R-FCN for detecting faces with meaningful performance improvements.</p><p>Also, to cope with the various scale of faces with single forward path, object detectors such as SSD, RetinaNet, and FPN are dominantly adopted since they use features from multiple layer locations for detecting objects with various scale in a single forward path. S3FD <ref type="bibr" target="#b59">[60]</ref> achieved promising performance by applying SSD with introducing multiple strategies to handle the small size of faces. FAN <ref type="bibr" target="#b47">[48]</ref> uses RetinaNet by applying anchor level attention to detect the occluded faces. After S3FD, many improved versions <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b57">58]</ref> are introduced and achieved performance gain from the previous methods. FPN based face detection methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b44">45]</ref> achieved SOTA performance by enhancing the expression capacity of the lower-level feature map used for detecting small faces.</p><p>The mentioned SOTA methods commonly use classification network such as VGG-16 <ref type="bibr" target="#b41">[42]</ref>, ResNet-50 or 101 <ref type="bibr" target="#b10">[11]</ref>, and DenseNet-169 <ref type="bibr" target="#b13">[14]</ref> as a backbone of the model. These classification networks have a large number of parameters exceeding 20 million, and the model size is over 80Mb supposing 32-bit floating point for each parameter. Some cascade methods such as <ref type="bibr" target="#b54">[55]</ref> report decent mAP with the smaller mount of model size, about 3.8Mb. However, the size is still burdensome to the devices like mobile, because users generally want their applications not to exceed few ten's of Mb. Also, the face detector should mostly be much smaller than the total size of the application because a face detector is usually an end-level function of the application.</p><p>Here, we propose a new scheme of iteratively sharing the backbone network, which can be applicable to both SSD and FPN based architectures. The method achieves comparable accuracy to the original models, and the overall model size is extremely smaller as well.</p><p>Lightweight generic object detectors: Recently, for detecting general objects in condition with a limited resource such as mobile devices, various single-stage, and two-stage lightweight detectors were proposed. For the single-stage detectors, MobileNet-SSD <ref type="bibr" target="#b12">[13]</ref>, MobileNetV2-SSDLite <ref type="bibr" target="#b39">[40]</ref>, Pelee <ref type="bibr" target="#b48">[49]</ref> and Tiny-DSOD <ref type="bibr" target="#b22">[23]</ref> were proposed. For two-stage detectors, Light-Head R-CNN <ref type="bibr" target="#b23">[24]</ref> and ThunderNet <ref type="bibr" target="#b34">[35]</ref> were proposed. The mentioned methods achieved meaningful accuracy and size trade-off, but we aim to develop a detector which has a much smaller number of parameters with introducing a new paradigm, iterative use of the backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent convolutional network:</head><p>The idea of recurrently using convolutional layers has been applied to various computer vision applications. Sharesnet <ref type="bibr" target="#b0">[1]</ref> and Iamnn <ref type="bibr" target="#b18">[19]</ref> applied recurrent residual network into classification task. Guo et al. <ref type="bibr" target="#b8">[9]</ref> reduce the parameters by sharing depthwise convolutional filters in learning multiple visual domain data. The iterative sharing is also applied to dynamic routing <ref type="bibr" target="#b15">[16]</ref>, fast inference of video <ref type="bibr" target="#b32">[33]</ref>, feature transfer <ref type="bibr" target="#b28">[29]</ref>, super-resolution <ref type="bibr" target="#b17">[18]</ref>, and recently in segmentation <ref type="bibr" target="#b19">[20]</ref>. In this paper, we introduce a method applying the concept of iterative convolutional layer sharing in the face detection task, which is the first to the best of our knowledge. : 80x80 <ref type="figure">Figure 2</ref>. The overall framework of the proposed method. The structure recurrently generates the feature maps fi (SSD version), and we upsample the feature maps with skip connection to generate the feature maps gi (FPN version). The classification and regression heads can be attached to either fi and gi.</p><formula xml:id="formula_0">E(⋅) F(⋅)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXTD</head><p>In this section, we introduce the main components of the proposed work including iterative feature map generation, the architectures of the proposed face detection models, backbone networks, and classification and regression head design. Also, implementation details for designing and training the models will be introduced. <ref type="figure">Figure 2</ref> shows the overall framework of the proposed method with two variations, SSD-like, and FPN-like frameworks. In the proposed method, we get multiple feature maps with different resolutions by recurrently passing the backbone network. Let assume that F (·) and E(·) each denotes the backbone network and the first Conv layer with stride two. Then, the iterative process is defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Iterative Feature Map Generation</head><formula xml:id="formula_1">f i = F (f i−1 ), i = 1, ..., N, f 0 = E(x).<label>(1)</label></formula><p>Here, the set {f 1 , .., f N } denote the set of feature maps, and x is the image. In FPN version, we upsample each feature map and connect the previous feature maps via skipconnection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>. The upsampling step U i (·) is conducted with bilinear upsampling followed by an upsampling block composed of separable convolution and point-wise convolution, inspired by <ref type="bibr" target="#b22">[23]</ref>. The resultant set of the feature map G = {g 1 , ..., g N } is obtained as,</p><formula xml:id="formula_2">g i+1 = U i (g i )+f N −i , i = 1, ..., N − 1, g 1 = f N .<label>(2)</label></formula><p>For the SSD-like architecture, which is the first variant, we extract feature maps f i and connect the classification and regression head to the feature maps. In FPN-like architecture, the feature maps g i from equation <ref type="formula" target="#formula_2">(2)</ref> are used. The classification and regression heads are designed by a 3x3 convolutional network and hence, both models are designed as a fully convolutional network. This enables the models to deal with various size of images. The detailed implementation of the heads is introduced in below sections.</p><p>For all the cases, we set the image x to have 640x640 resolution in training phase and use N = 6 number of feature maps. Hence, we get 160x160, 80x80, 40x40, 20x20, 10x10 and 5x5 resolution feature maps. In each location of the feature map, prior anchor candidates for the face is defined, following the same setting as S3FD <ref type="bibr" target="#b59">[60]</ref>.</p><p>One notable property of this architecture is that this method provides more abundant semantic information in lower-level feature maps compared to the face detectors adopting SSD architecture. While the existing methods commonly report the problem that the lower-level feature maps only contain limited semantic information due to their limited length of depth, our iterative architecture repeatedly shows intermediate level features and the various scale of faces to the network. We conjecture that the different features have similar semantics because the target objects in our case are faces, and the faces share homogeneous shapes regardless of their scale dissimilar to general objects. In Section 4, we show that the proposed method clearly enhances the detection accuracy for small size faces, and this can be more improved by taking the FPN architecture.</p><formula xml:id="formula_3">(a) (b) (c) (d) (e) (f)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Component Description</head><p>In the proposed model, a lightweight backbone network reducing the feature map resolution by half is used. The network is composed of inverted residual blocks followed by one 3x3 convolutional (Conv) filter with stride 2, based on <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b1">2]</ref>. The inverted residual block is composed of a set of point-wise Conv, separable Conv, and point-wise Conv. In each block, the channel width is expanded in the first point-wsie Conv and then, squeezed by the last point-wise Conv filter. The default setting of the network depth is set to 6 or 8, and the output channel width is set to 32,48 or 64, which do not largely exceed overall 0.1 million parameters. Different from MobileNet-V2 <ref type="bibr" target="#b39">[40]</ref>, PReLU <ref type="bibr" target="#b9">[10]</ref> (or leaky-ReLU) is applied and shown to be more successful than ReLU in training the proposed recurrent architecture. This phenomenon will be further discussed in Section 4.</p><p>Other than the inverted residual block, the proposed architecture also includes feature extraction block, upsampling blocks, and classification and regression heads. The detailed description of the components is introduced in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification and Regression Head Design</head><p>For detecting the faces using the generated feature maps, we use a classification head and a regression head for each feature map to classify whether each prior box contains a face, and to regress the prior box to the exact location. The classification and regression heads are both defined as single 3x3 Conv filters as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The classification head C i has two-dimensional output channel c i except C 1 that having four-dimensional channels. For C 1 , we apply Maxout <ref type="bibr" target="#b7">[8]</ref> approach to select two of the four channels for alleviating the false positive rate of the small faces, as introduced in S3FD. The regression head R i is defined to have output feature r i to have four-dimensionional channel, and each denotes width, height ratio, and center locations, adopting the dominantly used setting in RPN <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>The proposed backbone network and the classification and regression head are jointly trained by a multitask loss function from RPN composed of a classification loss l c and a regression loss l r as,</p><formula xml:id="formula_4">l({c j , r j }) = λ N cls j l c (c j , c * j ) + 1 N reg j c * j l r (r j , r * j ) (3)</formula><p>Here, j is the index of the anchor boxes, and the label c * j ∈ {0, 1} and r * j is the ground truth of the anchor box. The label c * j is set to 1 when Jaccard overlap <ref type="bibr" target="#b5">[6]</ref> between the anchor box and ground trurh box is higher than a threshold t. The denominator N cls denotes the total number of positive and negative samples. The regression loss is computed only for the positive sample and hence, the number N reg is defined by N reg = j c * j . The parameter λ is defined to balance the two losses because N cls and N reg are different from each other. The vector r * j denotes the ground truth box location and size for the face. The classification loss l c and the regression loss l r are defined as cross-entropy loss and smooth-1 loss, respectively.</p><p>The primary obstacle for the classification in the face detection task is a class imbalance problem between the face and the background, especially regarding the small faces. To alleviate the problem, we also adopt the strategies including online hard negative mining and scale compensation anchor matching introduced in S3FD. Using the hard negative mining technique, we balance the ratio of positive and negative samples N neg /N pos to 3 and the balancing parameter λ is set to 4. Also, from the scale compensation anchor matching strategy, we first pick the positive samples where the Jaccard overlap is over 0.35, and then further pick the remaining samples in sorted order from the samples that their Jaccard overlap is larger than 0.1 if the number of positive samples is insufficient.</p><p>For Data augmentation, we follow the conventional augmentation setting from S3FD. The augmentation includes color distortions <ref type="bibr" target="#b11">[12]</ref>, random crop, horizontal flip, and vertical flip. The proposed method is implemented with PyTorch <ref type="bibr" target="#b33">[34]</ref> and NAVER Smart Machine Learning (NSML) <ref type="bibr" target="#b16">[17]</ref> system. Please refer Appendix A to see the detailed training and optimization settings for training the proposed network. Code will be available at https:// github.com/clovaai.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we quantitatively and qualitatively analyze the proposed method with various ablations. For the quantitative analysis, we compare the detection performance of the proposed method and the SOTA face detection algorithms. Qualitatively, we show that our method can successfully detect faces in various conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Datasets: we tested the proposed method and ablations of the method with WIDER FACE <ref type="bibr" target="#b52">[53]</ref> dataset, which is most recent and is similar to in-the-wild face detection situation. The images in the dataset are divided into Easy, Medium, and Hard cases which are roughly categorized by different scales: large, medium, and small, of faces. The Hard case includes all the images of the dataset, and the Easy and Medium cases both are the subsets of the Hard case. The dataset has total 32,203 images with 393,703 labeled faces and is split into training (40%), validation (20%) and testing (40%) set. We trained the detectors with the training set and evaluated them with validation and test sets.</p><p>Comparison: Since our method followed the training and implementation details such as anchor design, data augmentation, and feature-map resolution design equivalent to S3FD <ref type="bibr" target="#b59">[60]</ref>, which has become one of the baseline methods in face detection field, we mostly evaluated the performance by comparing the S3FD model and its SOTA variations <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21]</ref>. The other techniques based on the S3FD model such as Pyramid anchor <ref type="bibr" target="#b43">[44]</ref>, Feature enhancement module, Improved anchor matching, and Progressive anchor loss <ref type="bibr" target="#b20">[21]</ref> would be able to be adapted to the proposed model without revising the proposed structure. Also, we used the MobileFaceNet <ref type="bibr" target="#b1">[2]</ref>, the face variant of the MobileNet-V2 <ref type="bibr" target="#b39">[40]</ref>, to the S3FD model instead of VGG-16 to see the effectiveness of the proposed method compared to the case of using the lightweight backbone network.</p><p>Variations: We applied the proposed recurrent scheme mainly into the FPN-based structure. For the model, we designed three variations which have a different number of parameters, lighter one having 0.063M parameters with 32 channels for each feature maps, intermediate one having 0.1M parameters with 48 channels, and the heavier one with 64 channels and 0.16M parameters when designed as FPN. See Appendix B for the detailed configuration of the backbone networks for each case.</p><p>Also, we tested different activation functions: ReLU, PReLU, and Leaky-ReLU for each model. The negative slope of the Leaky-ReLU is set to 0.25, which is identical to the initial negative slope of the PReLU. In the following section, we will term each variation by a combination of abbreviations; EXTD-model-channel-activation. For example, the term EXTD-FPN-32-PReLU denotes the proposed model combined with FPN, with feature channel width 32 and with activation function PReLU.</p><p>As an ablation, we also applied the proposed recurrent backbone into SSD-like structure as well. The ablation was trained and tested with the same conditions to the FPNbased version and abbreviated as SSD. Same as FPN case, for example, the term EXTD-SSD-32-PReLU denotes the proposed model combined with SSD, with feature channel width 32 and with activation function PReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Analysis</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we list the quantitative evaluation results of face detection in WIDER FACE dataset and the comparison to the SOTA face detectors. The table shows the mAP of the models on Easy, Medium, Hard cases for both validation and test sets of the dataset. Also, the table includes model information such as their backbone networks, number of parameters, and total number of adder arithmetics (Madds). In <ref type="figure" target="#fig_4">Figure 4</ref>, the precision recall curve for the proposed and the other methods are presented. <ref type="figure">Figure 5</ref> shows the examples of the face detection results from images with various conditions. In <ref type="figure">Figure 6</ref>, we evaluate the latency of the models in terms of the resolution of images, which measured via a machine with CPU i7 core and NVIDIA TITAN-X. For a fair comparison, all the inference processes of the models are implemented by PyTorch 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to the Existing Methods:</head><p>The results in <ref type="table" target="#tab_1">Table 1</ref> shows that some variations of the proposed method achieved comparable performance to the baseline model S3FD. Among lighter models and intermediate models, EXTD-FPN-32-PReLU and EXTD-FPN-48-PReLU each got a mAP score 3.4% and 1.2% lower than S3FD in WIDER Face hard validation set. When compared to S3FD trained scratch, EXTD-FPN-64-PReLU achieved even performances. For the heavier version, we found that our FPN variant achieved nearly the same accuracy, only 0.3% in WIDER FACE hard validation set and 0.8% in test set to S3FD in spite of the huge model size and memory usage gaps. It is meaningful in that the proposed detectors:   138 times lighter in model size and are 28.3, 19.2, and 11 times lighter in Madds. When compared to SOTA face detectors such as Pyra-midBox <ref type="bibr" target="#b43">[44]</ref> and DSFD <ref type="bibr" target="#b20">[21]</ref>, our best model EXTD-FPN-64-PReLU achieved lower results. The margin between PyramidBox and the proposed model on WIDER FACE hard case was 3.4%. Considering that PyramidBox inherits from S3FD and our model follows the equivalent training and detection setting to S3FD, our model would have a possibility to further increase the detection performance by adding the schemes proposed in PyramidBox. The mAP gap to DSFD, which is tremendously heavier, is about 5.0%, but it would be safe to suggest that the proposed method offers more decent trade-off in that DSFD uses about 2860 times more parameters than the proposed method. This is also meaningful result in that our method did not use any kind of pre-training of the backbone network using the other dataset such as ImageNet <ref type="bibr" target="#b4">[5]</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> shows the ROC curves of the proposed EXTD-FPN-64-PReLU and the other methods. From the graphs, we can see that our method is included in the SOTA group of the detectors using heavyweight pre-trained backbone networks.</p><p>When it comes to our SSD-based variations, they got lower mAP results than FPN-based variants. However, when compared with the S3FD version trained with Mo-bileFaceNet backbone network, the proposed SSD variants achieved comparable or better detection performance. It is a meaningful result in that the proposed variations have smaller feature map width, S3FD-MobileFaceNet holds feature map size of <ref type="bibr">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128]</ref>, and use the smaller number of layer blocks; inverted residual blocks same as MobileFaceNet, repeatedly. This shows that the proposed itertative scheme efficiently reduces the number of parameters without loss of accuracy.</p><p>Also, from the graph in <ref type="figure">Figure 6</ref>, we showed that our EXTD achieved faster inference speed to the S3FD, which is considered as real-time face detector, in a wide range of an input image resolution. This shows that the proposed face detector can safely alter S3FD without losing accuracy and with consuming much smaller capacity, as well as maintaining the inference speed. It is interesting to note that the inference was much slow when using MobileFaceNet instead of VGG-16. It would mainly be due to that Mobile-FaceNet version should pass more filters (48) than VGG-16 version <ref type="bibr" target="#b23">(24)</ref>, and the inference times of the filters including pooling, depth-wise, point-wise and ordinary convolutional filters are not that different in Pytorch implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection performance regarding the Face Scale:</head><p>One notable characteristic of the proposed method captured from the evaluation is that our detector obtained better performance when dealing with a small size of faces. From the table, we can see that our method achieved higher performance in WIDER FACE hard dataset than other cases. Since the Easy and Medium cases are subsets of the Hard dataset, this means that the proposed method is especially fitted to capture small sized faces. This tendency is commonly observed for different variations, for the different model architecture, and for the different channel widths. This supports the proposition suggested in Section 3.1 that the proposed recurrent structure strengthens the feature map, especially for the lower-level feature maps, and hence enhance the detection performance of the small faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Variation Analysis</head><p>The evaluation on the variations of the proposed EXTD is summarized in <ref type="table">Table 2</ref>. The table mainly consists of three blocks in rows. Each first, second, and third block lists the evaluation results from the smaller version (32 channels), intermediate version <ref type="bibr">(48 channel)</ref>, and the heavier version (64 channel) with applying different activation functions.  <ref type="table">Table 2</ref>. Variation study on WIDER FACE validation dataset. The models with boldface denotes the representative models for each block.</p><p>The value in the parentheses shows the margin between the best model in the block (written in boldface).</p><p>Effect of the Model Architecture: From the table, we can find two common observations among the proposed variations. First, for all the different channel width, FPN based architecture achieved better detection performance compared to SSD based architecture, especially for detecting small faces. The idea of expanding the number of layers for reaching the largest sized feature-map, for detecting the smallest size of objects, is a common strategy for SSD variant methods. This approach assumes that typical SSD structure passes too small number of layers and hence, the resultant feature-map could not import much information useful for the detection task. In the face detection task, this assumption seems to be correct in that the FPN based models notably achieved superior detection performance on small faces compared to SSD based models for all the cases.</p><p>Second, for both SSD based and FPN based model, channel width was another key factor for performance enhancement. As the channel width increased by 32 to 64, we can see that the detection accuracy significantly enhanced for all the cases; Easy, Medium, and Hard. Considering that we used a smaller number of layers for 48 and 64 channel cases than the case with 32 channel, this shows that having enough size of channel width is critical for embedding sufficient information to the feature map for detecting faces.</p><p>Effect of the Activation functions: From the evaluation, we found that the choice of the activation function is another factor governing the detection performance of the proposed method. In all the cases including FPN based and SSD based structures, PReLU was the most effective choice when it comes to mAP, but the gap between Leaky-ReLU was not that significant for the FPN variants. When tested with SSD based architecture, PReLU outperformed Leaky-ReLU with larger margin than those using FPN structure.</p><p>It is worth noting that ReLU occurred notable performance decreases especially when the channel width was small for both SSD and FPN cases. When the channel width was set to 32, mAP for all the three cases were lower than 10% to 20% compared to those using other activation functions. The decreases were alleviated as the channel width increased. When the channel width was 48, the gap was about 2.2%, and in the channel width 64 case, the margin was about 1.2%. From the results, we conjecture that the nature of ReLU that set all the negative values to zero occurs information loss in the proposed iterative process since it makes the feature map too sparse, and this information loss would be much critical when the channel width is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new face detector which significantly reduces the model sizes as well as maintaining the detection accuracy. By re-using backbone network layers recurrently, we reduced the vast amount of the network parameters and also obtained comparable performance to recent deep face detection methods using heavy backbone networks. We showed that our methods achieved very close mAP to the baseline S3FD only with hundreds time smaller parameters and with using tens time smaller Madd without using pre-training. We expect that our method can be further improved by applying recent techniques of the SOTA detectors which integrated to S3FD.   </p><formula xml:id="formula_5">I-Residual type (a) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b) (b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Implementation detail</head><p>For training the proposed architecture, a stochastic gradient descent optimizer (SGD) with learning rate 1e −3 , with 0.9 momentum, 0.0005 weight decay, and batch size 16 is used. The training is conducted from scratch, and the network weights were initialized with He-method <ref type="bibr" target="#b9">[10]</ref>. The maximum iteration number is basically set to 240K, and we drop the learning rate to 1e −4 and 1e −5 at 120K and 180K iterations. Also, we test the architecture with twice larger iterations 480K as well. In this case, the learning rate is dropped at 240K and 360K iterations. Similar to the other networks using depth-wise separable networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23]</ref>, further performance improvements were observed when training the network with larger iteration. Appendix B. Detailed Architecture Information <ref type="figure" target="#fig_7">Figure 7</ref> shows the detailed structures of the backbone network for the variation having channel sizes 32, 48, and 64. The layers in 'blue', 'green', and 'red' boxes in the figure each denotes the version of the proposed detectors having channel width to 32, 48, and 64. Each model has parameter size 0.063M, 0.10M, and 0.16M respectively, when designed as FPN structure. The term 'I-Residual' denotes the inverted residual block (a) and (b), where the configuration of the block is introduced in <ref type="figure" target="#fig_1">Figure 3</ref> of the paper. The heavier versions which have 0.10M, and 0.16M model parameters are designed to have less number of parameters to reduce the parameter when compared to the lightest version. The results in the paper show that the width of the channels for each layer is more critical than the depth of the layers for the detection performance in the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Implementation of S3FD with MobileFaceNet Backbone</head><p>In the paper, we implemented the S3FD variation where the backbone network was set to MobileFaceNet instead of VGG-16. The backbone network consists of 14 inverted residual blocks followed by 3x3 convolutional filter which has output channel width 64 and stride two. The lowest-level inverted residual block is defined as in I-Residual (a), and the others are defined as I-Residual (b). The detailed setting of each blocks are described in <ref type="table" target="#tab_3">Table 3</ref>. We added a classification and regression head at the bottom of layers 6, 7, and 14. After layer 14, three extra layers defined by 3x3 convolutional filter with output channel width 128 are attached. This extra layer setting is equivalent to original S3FD, and the resolutions of the feature maps are <ref type="bibr">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128]</ref> with total parameter number 1.2 million. The MobileFaceNet backbone itself is a reduced version of MobileNet-V2, and we only used the part of the MobileFaceNet layers. However, we can still see that the backbone network requires a large number of parameters which makes challenging to be embedded in smaller devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Detailed configuration of the components. The terms s, p, g, cin, and cout denote the stride, padding, group, input channel width, and output channel width. Figures (a) and (b) each shows the initial and remaining inverted residual blocks. In (c) and (d), upsampling block and the Feature extraction block are presented. Figures (e) and (f) each denotes the classification and regression head. For the activation function, PReLU or Leaky-ReLU are used for (a) and (b), and ReLU is used for the others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3. The figures in (a) and (b) each shows the inverted residual block architecture. Residual skip-connection is applied when the input and output channel width are equivalent, and at the same time, the stride is set to one. The upsampling block in (c) consists of bilinear upsample layer followed by depth-wise and point-wise Conv blocks. Fea-ture extraction block (d) is defined by a 3x3 Conv network followed by batch normalization and the activation function. The classification (e) and regression (f) heads are also defined by a 3x3 Conv network. The implementation of the head is described in Section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Faceness-WIDER -0.714 Two-stage CNN -0.681 DSFD -0.966 ISRN -0.968 Multitask Cascade CNN -0.849 VIM-FD -0.965 SSH -0.931 Face R-CNN -0.938 CMS-RCNN -0.899 MSCNN -0.917 Zhu et al. -0.949 ScaleFace -Faceness-WIDER -0.635 Two-stage CNN -0.619 DSFD -0.957 ISRN -0.959 Multitask Cascade CNN -0.826 VIM-FD -0.954 SSH -0.921 Face R-CNN -0.922 CMS-RCNN -0.875 MSCNN -0.904 Zhu et al. -0.934 ScaleFace -Faceness-WIDER -0.346 Two-stage CNN -0.324 DSFD -0.905 ISRN -0.909 Multitask Cascade CNN -0.598 VIM-FD -0.904 SSH -0.846 Face R-CNN -0.831 CMS-RCNN -0.624 MSCNN -0.803 Zhu et al. -0.861 ScaleFace -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>ROC curves on WIDER FACE dataset. Best viewed in wide vision. The curves from our method are illustrated by 'black'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Illustration of the face detection results. The illustration includes vulnerable cases such as scale, illumination, face print, occlusion, pose, color, and paintings. EXTD-FPN-64-PReLU version was used to detect the images. Best viewed in wide and colored vision. Evaluation time given image resolutions (averaged 1000 trials each). The horizontal axis denotes the size of an image and the vertical axis shows the frame per second (FPS). The model with the higher value means that it has faster inference speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Backbone architectures for the recursive feature generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison to recent state-of-the-art face detectors on WIDER FACE dataset. '*' denotes results reported in the original papers. For the proposed model with highest validation mAP, we list the mAPs from validation set and that from test set at the left-side and right-side of the slash in fifth to seventh columns. The other cases, mAPs from the validation set are listed.</figDesc><table><row><cell>lighter, intermediate, and heavier versions, are about 343, 220, and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Structure of MobileFaceNet backbone attached in S3FD. Three extra layers are attached to further reduce the feature map size.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We are grateful to Clova AI members with valuable discussions, and to Jung-Woo Ha for proofreading the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08782</idno>
		<title level="m">Sharesnet: reducing residual network parameter number by sharing weights</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Selective refinement network for high performance face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02693</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Depthwise convolution is all you need for learning multiple visual domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00927</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face detection with the faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="650" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Polykovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04380</idno>
		<title level="m">Reset: Learning recurrent dynamic routing in resnet-like neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Iamnn: Iterative and adaptive mobile neural network for efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhoedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02216</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10220</idno>
		<title level="m">Dsfd: dual shot face detector</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical learning of multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tiny-dsod: Lightweight object detection for resource-restricted usages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11013</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lighthead r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
	</analytic>
	<monogr>
		<title level="m">defense of two-stage object detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient Recurrent Residual Networks Improved by Feature Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Delft University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint haar-like features for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1619" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssh: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4875" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent residual module for fast inference in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1536" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11752</idno>
		<title level="m">Thundernet: Towards real-time generic object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fishnet: A versatile backbone for image, region, and pixel level prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="754" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramidbox: A contextassisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="797" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning better features for face detection with feature fusion and segmentation supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08557</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01061</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Face r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Face attention network: an effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07246</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pelee: A real-time object detection system on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1963" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Detecting faces using region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05256</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international joint conference on biometrics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02863</idno>
		<title level="m">Face detection through scale-friendly deep convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Anchor cascade for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2490" to="2501" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Face detection using improved faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02142</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Improved selective refinement network for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06651</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchor&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5127" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05413</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
