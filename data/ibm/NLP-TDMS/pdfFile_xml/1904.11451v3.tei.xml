<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Holistic Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gall</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<addrLine>5 Sensifai</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<addrLine>3 KIT</addrLine>
									<settlement>Karlsruhe</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Scale Holistic Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition -focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a largescale "Holistic Video Understanding Dataset" (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios. We demonstrate the generalisation capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called "Holistic Appearance and Temporal Network" (HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications. https://holistic-video-understanding.github.io/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video understanding is a comprehensive problem that encompasses the recognition of multiple semantic aspects that include: a scene or an environment, objects, actions, events, attributes, and concepts. Even if considerable progress is made in video recognition, it is still rather limited to action recognition -this is due to the fact that there is no established video benchmark that integrates joint  recognition of multiple semantic aspects in the dynamic scene. While Convolutional Networks(ConvNets) have caused several sub-fields of computer vision to leap forward, one of the expected drawbacks of training the ConvNets for video understanding with a single label per task is insufficiency to describe the content of a video. This issue primarily impedes the ConvNets to learn a generic feature representation towards challenging holistic video analysis. To this end, one can easily overcome this issue by recasting the video understanding problem as multi-task classification, where multiple labels are assigned to a video from multiple semantic aspects. Furthermore, it is possible to learn a generic feature representation for video analysis and understanding. This is in line with image classification ConvNets trained on ImageNet that facilitated the learning of generic feature representation for several vision tasks. Thus, training ConvNets on a multiple semantic aspects dataset can be directly applied for holistic recognition and understanding of concepts in video data, which makes it very useful to describe the content of a video.</p><p>To address the above drawbacks, this work presents the "Holistic Video Understanding Dataset" (HVU). HVU is organized hierarchically in a semantic taxonomy that aims at providing a multi-label and multi-task large-scale video benchmark with a comprehensive list of tasks and annotations for video analysis and understanding. HVU dataset consists of 476k, 31k and 65k samples in train, validation and test set, and is a sufficiently large dataset, which means that the scale of dataset approaches that of image datasets. HVU contains approx. 572k videos in total, with ∼7.5M annotations for training set, ∼600K for validation set, and ∼1.3M for test set spanning over 3142 labels. A full spectrum encompasses the recognition of multiple semantic aspects defined on them including 248 categories for scenes, 1678 for objects, 739 for actions, 69 for events, 117 for attributes and 291 for concepts, which naturally captures the long tail distribution of visual concepts in the real world problems. All these tasks are supported by rich annotations with an average of 2112 annotations per label. The HVU action categories builds on action recognition datasets <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b44">47,</ref><ref type="bibr" target="#b61">64]</ref> and further extend them by incorporating labels of scene, objects, events, attributes, and concepts in a video. The above thorough annotations enable developments of strong algorithms for a holistic video understanding to describe the content of a video. <ref type="table" target="#tab_1">Table 1</ref> shows the dataset statistics. In order to show the importance of holistic representation learning, we demonstrate the influence of HVU on three challenging tasks: video classification, video captioning and video clustering. Motivated by holistic representation learning, for the task of video classification, we introduce a new spatio-temporal architecture called "Holistic Appearance and Temporal Network" (HATNet) that focuses on the multi-label and multi-task learning for jointly solving multiple spatiotemporal problems simultaneously. HATNet fuses 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues, leading to a robust spatio-temporal representation. Our HATNet is evaluated on challenging video classification datasets, namely HMDB51, UCF101 and Kinetics. We experimentally show that our HATNet achieves outstanding results. Furthermore, we show the positive effect of training models using more semantic concepts on transfer learning. In particular, we show that pre-training the model on HVU with more semantic concepts improves the fine-tuning results on other datasets and tasks compared to pre-training on single semantic category datasets such as, Kinetics. This shows the richness of our dataset as well as the importance of multi-task learning. Furthermore, our experiments on video captioning and video clustering demonstrates the generalisation capability of HVU on other tasks by showing promising results in comparison to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video Recognition with ConvNets: As to prior hand-engineered <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b36">39,</ref><ref type="bibr" target="#b52">55,</ref><ref type="bibr" target="#b58">61]</ref> and low-level temporal structure <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b55">58]</ref> descriptor learning there is a vast literature and is beyond the scope of this paper.</p><p>Recently ConvNets-based action recognition <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b43">46,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b56">59]</ref> has taken a leap to exploit the appearance and the temporal information. These methods operate on 2D (individual image-level) <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b11">14,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b46">49,</ref><ref type="bibr" target="#b56">59,</ref><ref type="bibr" target="#b60">63]</ref> or 3D (video-clips or snippets of K frames) <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b50">53]</ref>. The filters and pooling kernels for these architectures are 3D (x, y, time) i.e. 3D convolutions (s × s × d) <ref type="bibr" target="#b60">[63]</ref> where d is the kernel's temporal depth and s is the kernel's spatial size. These 3D ConvNets are intuitively effective because such 3D convolution can be used to directly extract spatio-temporal features from raw videos. Carreiraet al. proposed inception <ref type="bibr" target="#b22">[25]</ref>  based 3D CNNs, which they referred to as I3D <ref type="bibr" target="#b3">[6]</ref>. More recently, some works introduced temporal transition layer that models variable temporal convolution kernel depths over shorter and longer temporal ranges, namely T3D <ref type="bibr" target="#b8">[11]</ref>. Further Dibaet al. <ref type="bibr" target="#b7">[10]</ref> propose spatio-temporal channel correlation that models correlations between channels of a 3D ConvNets wrt. both spatial and temporal dimensions. In contrast to these prior works, our work differs substantially in scope and technical approach. We propose an architecture, HATNet, that exploits both 2D ConvNets and 3D ConvNets to learn an effective spatio-temporal feature representation. Finally, it is worth noting the self-supervised ConvNet training works from unlabeled sources <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b41">44]</ref>, such as Fernandoet al. <ref type="bibr" target="#b14">[17]</ref> and Mishraet al. <ref type="bibr" target="#b30">[33]</ref> generate training data by shuffling the video frames; Sharmaet al. <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b37">40,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b40">43]</ref> mines labels using a distance matrix or clustering based on similarity although for video face clustering; Weiet al. <ref type="bibr" target="#b57">[60]</ref> predict the ordering task; Nget al. <ref type="bibr" target="#b31">[34]</ref> estimates optical flow while recognizing actions; Dibaet al. <ref type="bibr" target="#b10">[13]</ref> predicts short term future frames while recognizing actions. Self-supervised and unsupervised representation learning is beyond the scope of this paper.</p><p>The closest work to ours is by Rayet al. <ref type="bibr" target="#b33">[36]</ref>. Rayet al. concatenate pre-trained deep features, learned independently for the different tasks, scenes, object and actions aiming to the recognition, in contrast our HATNet is trained end-to-end for multi-task and multi-label recognition in videos. Video Classification Datasets: Over the last decade, several video classification datasets <ref type="bibr" target="#b1">[4,</ref><ref type="bibr" target="#b2">5,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b44">47]</ref> have been made publicly available with a focus on action recognition, as summarized in <ref type="table" target="#tab_2">Table 2</ref>. We briefly review some of the most influential action datasets available. The HMDB51 <ref type="bibr" target="#b26">[29]</ref> and UCF101 <ref type="bibr" target="#b44">[47]</ref> has been very important in the field of action recognition. However, they are simply not large enough for training deep ConvNets from scratch. Recently, some large action recognition datasets were introduced, such as ActivityNet <ref type="bibr" target="#b2">[5]</ref> and Kinetics <ref type="bibr" target="#b24">[27]</ref>. ActivityNet contains 849 hours of videos, including 28,000 action instances. Kinetics-600 contains 500k videos spanning 600 human action classes with more than 400 examples for each class. The current experimental strategy is   to first pre-train models on these large-scale video datasets <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b24">27]</ref> from scratch and then fine-tune them on small-scale datasets <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b44">47]</ref> to analyze their transfer behavior. Recently, a few other action datasets have been introduced with more samples, temporal duration and the diversity of category taxonomy, they are HACS <ref type="bibr" target="#b61">[64]</ref>, AVA <ref type="bibr" target="#b20">[23]</ref>, Charades <ref type="bibr" target="#b42">[45]</ref> and Something-Something <ref type="bibr" target="#b19">[22]</ref>. Sports-1M <ref type="bibr" target="#b23">[26]</ref> and YouTube-8M <ref type="bibr" target="#b0">[3]</ref> are the video datasets with million-scale samples. They consist quite longer videos rather than the other datasets and their annotations are provided in video-level and not temporally stamped. YouTube-8M labels are machine-generated without any human verification in the loop and Sports-1M is just focused on sport activities.</p><p>A similar spirit of HVU is observed in SOA dataset <ref type="bibr" target="#b33">[36]</ref>. SOA aims to recognize visual concepts, such as scenes, objects and actions. In contrast, HVU has several orders of magnitude more semantic labels(6 times larger than SOA) and not just limited to scenes, objects, actions only, but also including events, attributes, and concepts. Our HVU dataset can help the computer vision community and bring more attention to holistic video understanding as a comprehensive, multi-faceted problem. Noticeably, the SOA paper was published in 2018, however the dataset is not released while our dataset is ready to become publicly available.</p><p>Motivated by efforts in large-scale benchmarks for object recognition in static images, i.e. the Large Scale Visual Recognition Challenge (ILSVRC) to learn a generic feature representation is now a back-bone to support several related vision tasks. We are driven by the same spirit towards learning a generic feature representation at the video level for holistic video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HVU Dataset</head><p>The HVU dataset is organized hierarchically in a semantic taxonomy of holistic video understanding. Almost all real-wold conditioned video datasets are targeting human action recognition. However, a video is not only about an action which provides a human-centric description of the video. By focusing on humancentric descriptions, we ignore the information about scene, objects, events and also attributes of the scenes or objects available in the video. While SOA <ref type="bibr" target="#b33">[36]</ref> has categories of scenes, objects, and actions, to our knowledge it is not publicly available. Furthermore, HVU has more categories as it is shown in <ref type="table" target="#tab_2">Table 2</ref>. One of the important research questions which is not addressed well in recent works on action recognition, is leveraging the other contextual information in a video. The HVU dataset makes it possible to assess the effect of learning and knowledge transfer among different tasks, such as enabling transfer learning of object recognition in videos to action recognition and vice-versa. In summary, HVU can help the vision community and bring more interesting solutions to holistic video understanding. Our dataset focuses on the recognition of scenes, objects, actions, attributes, events, and concepts in user generated videos. Scene, object, action and event categories definition is the same and standard as in other image and datasets. For attribute labels, we target attributes describing scenes, actions, objects or events. The concept category refers to any noun and label which present a grouping definition or related higher level in the taxonomy tree for labels of other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HVU Statistics</head><p>HVU consists of 572k videos. The number of video-clips for train, validation, and test set are 481k, 31k and 65k respectively. The dataset consists of trimmed video clips. In practice, the duration of the videos are different with a maximum of 10 seconds length. HVU has 6 main categories: scene, object, action, event, attribute, and concept. In total, there are 3142 labels with approx. 7.5M annotations for the training, validation and test set. On average, there are ∼2112 annotations per label. We depict the distribution of categories with respect to the number of annotations, labels, and annotations per label in <ref type="figure" target="#fig_2">Fig. 2</ref>. We can observe that the object category has the highest quota of labels and annotations, which is due to the abundance of objects in video. Despite having the highest quota of the labels and annotations, the object category does not have the highest annotations per label ratio. However, the average number of ∼2112 annotations per label is a reasonable amount of training data for each label. The scene category does not have a large amount of labels and annotations which is due to two reasons: the trimmed videos of the dataset and the short duration of the videos. This distribution is somewhat the same for the action category. The dataset statistics for each category are shown in <ref type="table" target="#tab_1">Table 1</ref> for the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collection and Annotation</head><p>Building a large-scale video understanding dataset is a time-consuming task. In practice, there are two main tasks which are usually most time consuming for creating a large-scale video dataset: (a) data collection and (b) data annotation. Recent popular datasets, such as ActivityNet, Kinetics, and YouTube-8M are collected from Internet sources like YouTube. For the annotation of these datasets, usually a semi-automatic crowdsourcing strategy is used, in which a human manually verifies the crawled videos from the web. We adopt a similar strategy with difference in the technical approach to reduce the cost of data collection and annotation. Since, we are interested in the user generated videos, thanks to the taxonomy diversity of YouTube-8M <ref type="bibr" target="#b0">[3]</ref>, Kinetics-600 <ref type="bibr" target="#b24">[27]</ref> and HACS <ref type="bibr" target="#b61">[64]</ref>, we use these datasets as main source of the HVU. By using these datasets as the source, we also do not have to deal with copyright or privacy issues so we can publicly release the dataset. Moreover, this ensures that none of the test videos of existing datasets is part of the training set of HVU. Note that, all of the aforementioned datasets are action recognition datasets.</p><p>Manually annotating a large number of videos with multiple semantic categories (i.e thousands of concepts and tags) has two major shortcomings, (a) manual annotations are error-prone because a human cannot be attentive to every detail occurring in the video that leads to mislabeling and are difficult to eradicate; (b) large scale video annotation in specific is a very time consuming task due to the amount and temporal duration of the videos. To overcome these issues, we employ a two-stage framework for the HVU annotation. In the first stage, we utilize the Google Vision API [1] and Sensifai Video Tagging API [2] to get rough annotations of the videos. The APIs predict 30 tags per video. We keep the probability threshold of the APIs relative low (∼ 30%) as a guarantee to avoid false rejects of tags in the video. The tags were chosen from a dictionary with almost 8K words. This process resulted in almost 18 million tags for the whole dataset. In the second stage, we apply human verification to remove any possible mislabeled noisy tags and also add possible missing tags missed by the APIs from some recommended tags of similar videos. The human annotation step resulted in 9 million tags for the whole dataset with ∼3500 different tags.</p><p>We provide more detailed statistics and discusion regarding the annotation process in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Taxonomy</head><p>Based on the predicted tags from the Google and the Sensifai APIs, we found that the number of obtained tags is approximately ∼8K before cleaning. The services can recognize videos with tags spanning over categories of scenes, objects, events, attributes, concepts, logos, emotions, and actions. As mentioned earlier, we remove tags with imbalanced distribution and finally, refine the tags to get the final taxonomy by using the WordNet <ref type="bibr" target="#b29">[32]</ref> ontology. The refinement and pruning process aims to preserve the true distribution of labels. Finally, we  ask the human annotators to classify the tags into 6 main semantic categories, which are scenes, objects, actions, events, attributes and concepts. In fact, each video can be assigned to multiple semantic categories. Almost 100K of the videos have all of the semantic categories. In comparison to SOA, almost half of HVU videos have labels for scene, object and action together. <ref type="figure" target="#fig_3">Figure 3</ref> shows the percentage of the different subsets of the main categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Holistic Appearance and Temporal Network</head><p>We first briefly discuss state-of-the-art 3D ConvNets for video classification and then propose our new proposed "Holistic Appearance and Temporal Network" (HATNet) for multi-task and multi-label video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">3D-ConvNets Baselines</head><p>3D ConvNets are designed to handle temporal cues available in video clips and are shown to be efficient performance-wise for video classification. 3D ConvNets exploit both spatial and temporal information in one pipeline. In this work, we chose 3D-ResNet <ref type="bibr" target="#b48">[51]</ref> and STCnet <ref type="bibr" target="#b7">[10]</ref> as our 3D CNNs baseline which have competitive results on Kinetics and UCF101. To measure the performance on the multi-label HVU dataset, we use mean average precision (mAP) over all labels. We also report the individual performance on each category separately. The comparison between all of the methods can be found in <ref type="table" target="#tab_5">Table 3</ref>. These networks are trained with binary cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Task Learning 3D-ConvNets</head><p>Another approach which is studied in this work to tackle the HVU dataset is to have the problem solved with multi-task learning or a joint training method. As we know the HVU dataset consists of high-level categories like objects, scenes, events, attributes, and concepts, so each of these categories can be dealt like separate tasks. In our experiments, we have defined six tasks, scene, object, action, event, attribute, and concept classification. So our multi-task learning network is trained with six objective functions, that is with multi-label classification for each task. The trained network is a 3D-ConvNet which has separate Conv layers as separate heads for each of the tasks at the end of the network.</p><p>For each head we use the binary cross entropy loss since it is a multi-label classification for each of the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">2D/3D HATNet</head><p>Our "Holistic Appearance and Temporal Network" (HATNet) is a spatio-temporal neural network, which extracts temporal and appearance information in a novel way to maximize engagement of the two sources of information and also the efficiency of video recognition. The motivation of proposing this method is deeply rooted in a need of handling different levels of concepts in holistic video recognition. Since we are dealing with still objects, dynamic scenes, different attributes and also different human activities, we need a deep neural network that is able to focus on different levels of semantic information. We propose a flexible method to use a 2D pre-trained model on a large image dataset like ImageNet and a 3D pre-trained model on video datasets like Kinetics to fasten the process of training but the model can be trained from scratch as it is shown in our experiments as well. The proposed HATNet is capable of learning a hierarchy of spatio-temporal feature representation using appearance and temporal neural modules.</p><p>Appearance Neural Module. In HATNet design, we use 2D ConvNets with 2D Convolutional (2DConv) blocks to extract static cues of individual frames in a video-clip. Since we aim to recognize objects, scenes and attributes alongside of actions, it is necessary to have this module in the network which can handle these concepts better. Specifically, we use 2DConv to capture the spatial structure in the frame.</p><p>Temporal Neural Module. In HATNet architecture, the 3D Convolutions (3DConv) module handles temporal cues dealing with interaction in a batch of frames. 3DConv aims to capture the relative temporal information between frames. It is crucial to have 3D convolutions in the network to learn relational motion cues for efficiently understanding dynamic scenes and human activities. We use ResNet18/50 for both the 3D and 2D modules, so that they have the same spatial kernel sizes, and thus we can combine the output of the appearance and temporal branches at any intermediate stage of the network. <ref type="figure" target="#fig_4">Figure 4</ref> shows how we combine the 2DConv and 3DConv branches and use merge and reduction blocks to fuse feature maps at the intermediate stages of HATNet. Intuitively, combining the appearance and temporal features are complementary for video understanding and this fusion step aims to compress them into a more compact and robust representation. In the experiment section, we discuss in more detail about the HATNet design and how we apply merge and reduction modules between 2D and 3D neural modules. Supported by our extensive experiments, we show that HATNet complements the holistic video recognition, including understanding the dynamic and static aspects of a scene and also human action recognition. In our experiments, we have also performed tests on HATNet based multi-task learning similar to 3D-ConvNets   based multi-task learning discussed in Section 4.2. HATNet has some similarity to the SlowFast <ref type="bibr" target="#b12">[15]</ref> network but there are major differences. SlowFast uses two 3D-CNN networks for a slow and a fast branch. HATNet has one 3D-CNN branch to handle motion and dynamic information and one 2D-CNN to handle static information and appearance. HATNet also has skip connections with M&amp;R blocks between 3D and 2D convolutional blocks to exploit more information. 2D/3D HATNet Design. The HATNet includes two branches: first is the 3D-Conv blocks with merging and reduction block and second branch is 2D-Conv blocks. After each 2D/3D blocks, we merge the feature maps from each block and perform a channel reduction by applying a 1×1×1 convolution. Given the feature maps of the first block of both 2DConv and 3DConv, that have 64 channels each. We first concatenate these maps, resulting in 128 channels, and then apply 1 × 1 × 1 convolution with 64 kernels for channel reduction, resulting in an output with 64 channels. The merging and reduction is done in the 3D and 2D branches, and continues independently until the last merging with two branches.</p><p>We employ 3D-ResNet and STCnet <ref type="bibr" target="#b7">[10]</ref> with ResNet18/50 as the HATNet backbone in our experiments. The STCnet is a model of 3D networks with spatiotemporal channel correlation modules which improves 3D networks performance significantly. We also had to make a small change to the 2D branch and remove pooling layers right after the first 2D Conv to maintain a similar feature map size between the 2D and 3D branches since we use 112×112 as input-size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we demonstrate the importance of HVU on three different tasks: video classification, video captioning and video clustering. First, we introduce   the implementation details and then show the results of each mentioned method on multi-label video recognition. Following, we compare the transfer learning ability of HVU against Kinetics. Next, as an additional experiment, we show the importance of having more categories of tags such as scenes and objects for video classification. Finally, we show the generalisation capability of HVU for video captioning and clustering tasks. For each task, we test and compare our method with the state-of-the-art on benchmark datasets. For all experiments, we use RGB frames as input to the ConvNet. For training, we use 16 or 32 frames long video clips as single input. We use PyTorch framework for implementation and all the networks are trained on a machine with 8 V100 NVIDIA GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">HVU Results</head><p>In <ref type="table" target="#tab_5">Table 3</ref>, we report the overall performance of different simpler or multitask learning baselines and HATNet on the HVU validation set. The reported performance is mean average precision on all of the labels/tags. HATNet that exploits both appearance and temporal information in the same pipeline achieves the best performance, since recognizing objects, scenes and attributes need an appearance module which other baselines do not have. With HATNet, we show that combining the 3D (temporal) and 2D (appearance) convolutional blocks one can learn a more robust reasoning ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-Task Learning on HVU</head><p>Since the HVU is a multi-task classification dataset, it is interesting to compare the performance of different deep neural networks in the multi-task learning paradigm as well. For this, we have used the same architecture as in the previous experiment, but with different last layer of convolutions to observe multi-task learning performance. We have targeted six tasks: scene, object, action, event, attribute, and concept classification. In <ref type="table" target="#tab_6">Table 4</ref>, we have compared standard training without multi-task learning heads versus multi-task learning networks. The simple baseline multi-task learning methods achieve higher performance on individual tasks as expected, in comparison to standard networks learning for all categories as a single task. Therefore this initial result on a real-world multi-task video dataset motivates the investigation of more efficient multi-task learning methods for video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transfer Learning: HVU vs Kinetics</head><p>Here, we study the ability of transfer learning with the HVU dataset. We compare the results of pre-training 3D-ResNet18 using Kinetics versus using HVU and then fine-tuning on UCF101, HMDB51 and Kinetics. Obviously, there is a large benefit from pre-training of deep 3D-ConvNets and then fine-tune them on smaller datasets (i.e. HVU, Kinetics ⇒ UCF101 and HMDB51). As it can be observed in <ref type="table" target="#tab_8">Table 5</ref>, models pre-trained on our HVU dataset performed notably better than models pre-trained on the Kinetics dataset. Moreover, pre-training on HVU can improve the results on Kinetics also.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Benefit of Multiple Semantic Categories</head><p>Here, we study the effect of training models with multiple semantic categories, in comparison to using only a single semantic category, such as Kinetics which covers only action category. In particular, we designed an experiment by having the model trained in multiple steps by adding different categories of tags one by one. Specifically, we first train 3D-ResNet18 with action tags of HVU, following in second step we add tags from object category and in the last step we add tags from the scene category. For performance evaluation, we consider action category of HVU. In the first step the gained performance was 43.6% accuracy and after second step it was improved to 44.5% and finally in the last step it raised to 45.6%. The results show that adding high-level categories to the training, boosts the performance for action recognition in each step. As it was also shown in <ref type="table" target="#tab_6">Table 4</ref>, training all the categories together yields 47.5% for the action category which is ∼4% gain over action as single category for training. Thus we can</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Pre-Training Dataset BLEU@4 SA(VGG+C3D) <ref type="bibr">[</ref>  <ref type="table">Table 7</ref>: Captioning performance comparisons of <ref type="bibr" target="#b51">[54]</ref> with different models and pre-training datasets. M denotes the motion features from optical flow extracted as in the original paper.</p><p>infer from this that an effective feature representation can be learned by adding additional categories, and also acquire knowledge for an in-depth understanding of the video in holistic sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison on UCF, HMDB, Kinetics</head><p>In <ref type="table" target="#tab_9">Table 6</ref>, we compare the HATNet performance with the state-of-the-art on UCF101, HMDB51 and Kinetics. For our baselines and HATNet, we employ pre-training in two separate setups: one with HVU and another with Kinetics, and then fine-tune on the target datasets. For UCF101 and HMDB51, we report the average accuracy over all three splits. We have used ResNet18/50 as backbone model for all of our networks with 16 and 32 input-frames. HATNet pre-trained on HVU with 32 frames input achieved superior performance on all three datasets with standard network backbones. Note that on Kinetics, HAT-Net even with ResNet18 as a backbone ConvNet performs almost comparable to SlowFast which is trained by dual 3D-ResNet50. In <ref type="table" target="#tab_9">Table 6</ref>, however while SlowFast has better performance using dual 3D-ResNet101 architecture, HAT-Net obtains comparable results with much smaller backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Video Captioning</head><p>We present a second task that showcases the effectiveness of our HVU dataset, we evaluate the effectiveness of HVU for video captioning task. We conduct experiments on a large-scale video captioning dataset, namely MSR-VTT <ref type="bibr" target="#b59">[62]</ref>. We follow the standard training/testing splits and protocols provided originally in <ref type="bibr" target="#b59">[62]</ref>. For video captioning, the performance is measured using the BLEU metric.  For comparison, we considered two models of 3D-ResNet50, pre-trained on (i) Kinetics and (ii) HVU. <ref type="table">Table 7</ref> shows that the model trained on HVU obtained better gains in comparison to Kinetics. This shows HVU helps to learn more generic video representation to achieve better performance in other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Video Clustering</head><p>With this experiment, we evaluate the effectiveness of generic features learned using HVU when compared to Kinetics. Dataset: We conduct experiments on ActivityNet-100 <ref type="bibr" target="#b2">[5]</ref> dataset. For this experiment we provide results when considering 20 action categories with 1500 test videos. We have selected ActivityNet dataset to make sure there are no same videos in HVU and Kinetics training set. For clustering, the performance is measured using clustering accuracy <ref type="bibr" target="#b38">[41]</ref>. Method and Results: We extract features using 3D-ResNet50 and HATNet pre-trained on Kinetics-600 and HVU for the test videos and then cluster them with KMeans clustering algorithm with the given number of action categories. <ref type="table" target="#tab_12">Table 8</ref> clearly shows that the features learned using HVU is far more effective compared to features learned using Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work presents the "Holistic Video Understanding Dataset" (HVU), a largescale multi-task, multi-label video benchmark dataset with comprehensive tasks and annotations. It contains 572k videos in total with 9M annotations, which is richly labeled over 3142 labels encompassing scenes, objects, actions, events, attributes and concepts categories. Through our experiments, we show that the HVU can play a key role in learning a generic video representation via demonstration on three real-world tasks: video classification, video captioning and video clustering. Furthermore, we present a novel network architecture, HATNet, that combines 2D and 3D ConvNets in order to learn a robust spatio-temporal feature representation via multi-task and multi-label learning in an end-to-end manner. We believe that our work will inspire new research ideas for holistic video understanding. For the future plan, we are going to expand the dataset to 1 million videos with similar rich semantic labels and also provide annotations for other important tasks like activity and object detection and video captioning.</p><p>Acknowledgements: This work was supported by DBOF PhD scholarship &amp; GC4 Flemish AI project, and the ERC Starting Grant ARCA (677650). We also would like to thank Sensifai for giving us access to the Video Tagging API for dataset preparation.   dataset covers a diverse set of tags with clean annotations. Using machine generated tags in the first step helps us to cover larger number of tags than a human can remember and label it in a reasonable time.</p><p>To make sure that we have a balanced distribution of samples per tag, we consider a minimum number of 50 samples.</p><p>To provide more details regarding the HVU human annotation process, we report the statistics of the different stages of the annotation process. <ref type="table" target="#tab_1">Table 1</ref> shows the statistics of the machine generated annotations of training set. Note, that the labels and categories are result of the initial human annotation process over the validation set of the dataset. The category with the highest number of labels and annotations is the object category. Concept is the category with the lowest number of labels. To have a better understanding of the statistics of the annotations we depict the distribution of categories with respect to the number of annotations, labels, and annotations per label in <ref type="figure" target="#fig_1">Figure 1</ref>. We can observe that the object category has the highest quota of labels and annotations, which is due to the abundance of objects in video. Despite having the highest quota of the labels and annotations, the object category does not have the highest annotations per label ratio. <ref type="figure" target="#fig_2">Figure 2</ref> shows the percentage of the different subsets of the main categories. There are 50 different sets of videos based on assigned semantic categories. About 36% of the videos have all of the categories.   <ref type="table" target="#tab_2">Table 2</ref>: Performance comparison between machine generated and humanverified tags of HVU. This evaluation shows how human annotation process is crucial to have a more efficient dataset. The CNN model which is used for this experiment is 3D-ResNet18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Effect of Human Annotation</head><p>To present the impact of human annotation process, we have evaluated both versions of the HVU with machine-generated tags and human-annotated tags. We have trained two 3D-ResNet18 for each set and the comparison came in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 HVU Samples</head><p>We present some samples of videos and their corresponding tags in <ref type="figure" target="#fig_3">Fig 3</ref> and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Effect of Additional Categories on Kinetics</head><p>One of our arguments in our paper is about how more semantic categories like object, scene, etc can lead to learn effective video representation. We have shown results on the HVU dataset in the paper. Here, we provided the similar experiment for the Kinetics-600 as a subset of our HVU. We have compared performance of a 3D-ResNet18 trained on Kinetics videos with its action labels versus</p><p>Training Labels Action Recognition Performance Action 65.6 Action + HVU 68.8 <ref type="table" target="#tab_5">Table 3</ref>: Evaluation of training Kinetics with HVU labels. trained on full HVU labels for the same videos. For the evaluation, we have measured the performance on Kinetics action labels. It can be seen in <ref type="table" target="#tab_5">Table 3</ref> that having more semantic labels in the training for Kinetics, improves the action classification performance. It is due to the fact that HVU can bring more capabilities to the deep models for learning new visual features for understanding videos.</p><p>forest,musician,flutist,music,musical_instrument,brass_inst rument,wind_instrument,flautist,recreation,musical_instrum ent_accessory,plant,playing_flute,tree string_instrument,musician,man,,guitarist,plucked_string_i nstruments,music,tapping_guitar,bass,musical_instrument _accessory,performance,string_instrument_accessory,elec tric_guitar,sitting,monochrome_photography,musical_instru ment,guitar_accessory,resonator sport_venue,shoe,outdoor_shoe,joint,foot,ball,grass,knee, human_leg,fun,football_player,ball_game,green,footwear,f ootball,player,sports_equipment,juggling_soccer_ball,socc er,plant,soccer_ball,sports,play opening_bottle_not_wine_,joint,muscle,service,finger,distill ed_beverage,fun,taste,standing,arm,t_shirt,glass,alcohol,d rink,hand,bottle,photograph,cooking smile,nose,textile,cheek,thigh,mouth,girl,diaper,finger,baby _products,human_leg,fun,playing_xylophone,infant,toy,faci al_expression,skin,child,hand,sitting,human_hair_color,day time,play,toddler coast,watercourse,plant,wetland,terrain,floodplain,marsh,w ading_through_mud,boulder,tree,water,natural_resources,r iver,rock,waterway,outcrop,shore,creek </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Ali Diba ,</head><label>Diba</label><figDesc>Mohsen Fayyaz and Vivek Sharma contributed equally to this work and listed in alphabetical order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Holistic Video Understanding Dataset: A multi-label and multi-task fully annotated dataset and HATNet as a new deep ConvNet for video classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Left: Average number of samples per label in each of main categories. Middle: Number of labels for each main category. Right: Number of samples per main category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Coverage of different subsets of the 6 main semantic categories in videos. 16.6% of the videos have annotations of all categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>HATNet: A new 2D/3D deep neural network with 2DConv, 3DConv blocks and merge and reduction (M&amp;R) block to fuse 2D and 3D feature maps in intermediate stages of the network. HATNet combines the appearance and temporal cues with the overall goal to compress them into a more compact representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 :</head><label>1</label><figDesc>Left: Average number of samples per label in each of main categories. Middle: Number of labels for each main category. Right: Number of samples per main category. All statistics are for the machine generated tags of HVU training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 :</head><label>2</label><figDesc>Coverage of different subsets of the 6 main semantic categories in videos.<ref type="bibr" target="#b13">16</ref>.4% of the videos have annotations of all categories. All statistics are for the machine generated tags of HVU training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 :</head><label>3</label><figDesc>Video frame samples from HVU with corresponding tags of different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the HVU training set for different categories. The category with the highest number of labels and annotations is the object category.</figDesc><table><row><cell>Dataset</cell><cell cols="8">Scene Object Action Event Attribute Concept #Videos Year</cell></row><row><cell>HMDB51 [29]</cell><cell>-</cell><cell>-</cell><cell>51</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7K</cell><cell>'11</cell></row><row><cell>UCF101 [47]</cell><cell>-</cell><cell>-</cell><cell>101</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13K</cell><cell>'12</cell></row><row><cell>ActivityNet [5]</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>20K</cell><cell>'15</cell></row><row><cell>AVA [23]</cell><cell>-</cell><cell>-</cell><cell>80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.6K</cell><cell>'18</cell></row><row><cell>Something-Something [22]</cell><cell>-</cell><cell>-</cell><cell>174</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>108K</cell><cell>'17</cell></row><row><cell>HACS [64]</cell><cell>-</cell><cell>-</cell><cell>200</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>140K</cell><cell>'19</cell></row><row><cell>Kinetics [27]</cell><cell>-</cell><cell>-</cell><cell>600</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>500K</cell><cell>'17</cell></row><row><cell>EPIC-KITCHEN [9]</cell><cell>-</cell><cell>323</cell><cell>149</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>39.6K</cell><cell>'18</cell></row><row><cell>SOA [36]</cell><cell>49</cell><cell>356</cell><cell>148</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>562K</cell><cell>'18</cell></row><row><cell>HVU (Ours)</cell><cell>248</cell><cell>1678</cell><cell>739</cell><cell>69</cell><cell>117</cell><cell>291</cell><cell>572K</cell><cell>'20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the HVU dataset with other publicly available video recognition datasets in terms of #labels per category. Note that SOA is not publicly available.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>MAP (%) performance of different architecture on the HVU dataset. The backbone ConvNet for all models is ResNet18.</figDesc><table><row><cell>Model</cell><cell cols="7">Scene Object Action Event Attribute Concept Overall</cell></row><row><cell>3D-ResNet (Standard)</cell><cell>50.6</cell><cell>28.6</cell><cell>48.2</cell><cell>35.9</cell><cell>29</cell><cell>22.5</cell><cell>35.8</cell></row><row><cell>HATNet (Standard)</cell><cell>55.8</cell><cell>34.2</cell><cell>51.8</cell><cell>38.5</cell><cell>33.6</cell><cell>26.1</cell><cell>40</cell></row><row><cell cols="2">3D-ResNet (Multi-Task) 51.7</cell><cell>29.6</cell><cell>48.9</cell><cell>36.6</cell><cell>31.1</cell><cell>24.1</cell><cell>37</cell></row><row><cell>HATNet (Multi-Task)</cell><cell cols="2">57.2 35.1</cell><cell cols="2">53.5 39.8</cell><cell>34.9</cell><cell cols="2">27.3 41.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Multi-task learning performance (mAP (%) comparison of 3D-ResNet18 and HATNet, when trained on HVU with all categories in the multi-task pipeline. The backbone ConvNet for all models is ResNet18.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance (mAP (%)) comparison of HVU and Kinetics datasets for transfer learning generalization ability when evaluated on different action recognition dataset. The trained model for all of the datasets is 3D-ResNet18.</figDesc><table><row><cell>Method</cell><cell cols="6">Pre-Trained Dataset CNN Backbone UCF101 HMDB51 Kinetics-400 Kinetics-600</cell></row><row><cell>Two Stream (spatial stream) [46]</cell><cell>Imagenet</cell><cell>VGG-M</cell><cell>73</cell><cell>40.5</cell><cell>-</cell><cell></cell></row><row><cell>RGB-I3D [6]</cell><cell>Imagenet</cell><cell>Inception v1</cell><cell>84.5</cell><cell>49.8</cell><cell>-</cell><cell></cell></row><row><cell>C3D [50]</cell><cell>Sport1M</cell><cell>VGG11</cell><cell>82.3</cell><cell>51.6</cell><cell>-</cell><cell></cell></row><row><cell>TSN [59]</cell><cell>Imagenet,Kinetics</cell><cell>Inception v3</cell><cell>93.2</cell><cell>-</cell><cell>72.5</cell><cell></cell></row><row><cell>RGB-I3D [6]</cell><cell>Imagenet,Kinetics</cell><cell>Inception v1</cell><cell>95.6</cell><cell>74.8</cell><cell>72.1</cell><cell></cell></row><row><cell>3D ResNext 101 (16 frames) [24]</cell><cell>Kinetics</cell><cell>ResNext101</cell><cell>90.7</cell><cell>63.8</cell><cell>65.1</cell><cell></cell></row><row><cell>STC-ResNext 101 (64 frames) [10]</cell><cell>Kinetics</cell><cell>ResNext101</cell><cell>96.5</cell><cell>74.9</cell><cell>68.7</cell><cell></cell></row><row><cell>ARTNet [57]</cell><cell>Kinetics</cell><cell>ResNet18</cell><cell>93.5</cell><cell>67.6</cell><cell>69.2</cell><cell></cell></row><row><cell>R(2+1)D [53]</cell><cell>Kinetics</cell><cell>ResNet50</cell><cell>96.8</cell><cell>74.5</cell><cell>72</cell><cell></cell></row><row><cell>ir-CSN-101 [52]</cell><cell>Kinetics</cell><cell>ResNet101</cell><cell>-</cell><cell>-</cell><cell>76.7</cell><cell></cell></row><row><cell>DynamoNet [13]</cell><cell>Kinetics</cell><cell>ResNet101</cell><cell>-</cell><cell>-</cell><cell>76.8</cell><cell></cell></row><row><cell>SlowFast 4×16 [15]</cell><cell>Kinetics</cell><cell>ResNet50</cell><cell>-</cell><cell>-</cell><cell>75.6</cell><cell>78.8</cell></row><row><cell>SlowFast 16×8* [15]</cell><cell>Kinetics</cell><cell>ResNet101</cell><cell>-</cell><cell>-</cell><cell>78.9*</cell><cell>81.1</cell></row><row><cell>HATNet (32 frames)</cell><cell>Kinetics</cell><cell>ResNet50</cell><cell>96.8</cell><cell>74.8</cell><cell>77.2</cell><cell>80.2</cell></row><row><cell>HATNet (32 frames)</cell><cell>HVU</cell><cell>ResNet18</cell><cell>96.9</cell><cell>74.5</cell><cell>74.2</cell><cell>77.4</cell></row><row><cell>HATNet (16 frames)</cell><cell>HVU</cell><cell>ResNet50</cell><cell>96.5</cell><cell>73.4</cell><cell>76.3</cell><cell>79.4</cell></row><row><cell>HATNet (32 frames)</cell><cell>HVU</cell><cell>ResNet50</cell><cell>97.8</cell><cell>76.5</cell><cell>79.3</cell><cell>81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>State-of-the-art performance comparison on UCF101, HMDB51 test sets and Kinetics validation set. The results on UCF101 and HMDB51 are average mAP over three splits, and for Kinetics(400,600) is Top-1 mAP on validation set. For a fair comparison, here we report the performance of methods which utilize only RGB frames as input.</figDesc><table /><note>*SlowFast uses multiple branches of 3D-ResNet with bigger backbones.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Method and Results: Most of the state-of-the-art video captioning methods use models pre-trained on Kinetics or other video recognition datasets. With this experiment, we intend to show another generalisation capability of HVU dataset where we evaluate the performance of pre-trained models trained on HVU against Kinetics. For our experiment, we use the Controllable Gated Network<ref type="bibr" target="#b51">[54]</ref> method, which is to the best of our knowledge the state-of-the-art for captioning task.</figDesc><table><row><cell>Model</cell><cell cols="2">Pre-Training Dataset Clustering Accuracy (%)</cell></row><row><cell>3D-ResNet50</cell><cell>Kinetics</cell><cell>50.3</cell></row><row><cell>3D-ResNet50</cell><cell>HVU</cell><cell>53.5</cell></row><row><cell>HATNet</cell><cell>HVU</cell><cell>54.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Video clustering performance: evaluation based on extracted features from networks pre-trained on Kinetics and HVU datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 1 :</head><label>1</label><figDesc>Statistics of machine generated tags of HVU training set for different categories. The category with the highest number of labels and annotations is the object category.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">KU Leuven, 2 University of Bonn, 3 KIT, Karlsruhe, 4 ETH Zürich, 5 Sensifai {firstname.lastname}@kuleuven.be, {lastname}@iai.uni-bonn.de, {firstname.lastname}@kit.edu, Balamanohar@gmail.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">arXiv:1904.11451v3 [cs.CV] 15 Dec 2020</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix: This document provides supplementary material as mentioned in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HVU Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Human Annotation Details</head><p>The row machine generated annotations consist almost 8K labels. The initial stage of human verification on validation set resulted in 4378 labels. And the final stage of complete human verification/modification process ended up in 3142 labels. In human annotation process, 80 new labels are added by human annotators.</p><p>In specific for the HVU human verification task, we employed three different teams <ref type="figure">(</ref> To make sure both Team-B and Team-C have a clear understanding of the tags and the corresponding videos, we ask them to use the provided tags definition from Team-A. For the aforementioned four tasks, Team-B goes through all the videos and provides the first round of clean annotations. Followed by this, Team-C reviews the annotations from Team-B to guarantee an accurate and cleaner version of annotations. Finally, Team-A reviews the suggestions provided from tasks (c) and (d) and apply them to the dataset. The verification process takes ∼100 seconds on average per video clip for a trained worker. It took about 8500 person-hours to firstly clean the machine-generated tags and remove errors and secondly add any possible missing labels from the dictionary. By incorporating the machine generated tags and human annotation, the HVU  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Motion guided spatial attention for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maria Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Temporal 3d convnets using temporal transition layer. In: CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep temporal linear encoding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dynamonet: Dynamic action and motion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Slowfast networks for video recognition. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distinit: Learning video representations without a single labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ava: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsza Lek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hmdb51: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing in Science and Engineering</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sibnet: Sibling convolutional encoder for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scenes-objects-actions: A multi-task, multi-label video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-supervised face-grouping on graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Roethlingshoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ACMMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple and effective technique for face clustering in tv series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop on Brave New Motion Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised learning of face representations for video face clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video face clustering with self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Behavior, and Identity Science</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Clustering based contrastive learning for improving face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep multimodal feature encoding for video ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop on Holistic Video Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00182</idno>
		<title level="m">Deep fishernet for object classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Controllable video captioning with pos sequence guidance based on gated fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">M3: Multimodal memory modelling for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Video action detection with relational dynamicposelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">) mopping_floor,wood_stain,sleeve,design,man,wood,wood _flooring,gentleman,standing,swab,facial_hair,shirt,outerw ear,tartan,flooring,laminate_flooring,floor,dress_shirt,plaid, angle individual_sports,indoor_games_and_sports,joint,games,c ombat_sport,weapon_combat_sports,leisure,net,fun,recrea tion,martial_arts,epee,striking_combat_sports,fencing,com petition,contact_sport,fencing_sport_,flooring,fencing_wea pon,floor,sports,play italian_food,food,pizza,making_pizza,appetizer,cuisine,piz za_cheese,prosciutto,vegetable,darkness,sicilian_pizza,re cipe,rectangle,european_food,flatbread charcoal,campfire,shovel,smoke,outdoor_grill,fire,animal_s ource_foods,barbecue_grill,grilling,winter,fun,ice,meat,coo king_on_campfire,roasting,grass,barbequing hand,multimedia,server,electronics,electronic_device,finge r,computer_hardware,technology,assembling_computer,co mputer_case,arm,magenta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09374</idno>
	</analytic>
	<monogr>
		<title level="m">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>text bee_keeping,human,grass,backyard,outdoor_structure,wo od,tree,forest,human_behavior,beekeeper,leaf,garden,bee, yard,apiary,t_shirt,plant,beehive,male</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
