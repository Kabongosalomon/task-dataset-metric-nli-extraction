<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmentation-driven 6D Object Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
							<email>yinlin.hu@epfl.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Hugonot</surname></persName>
							<email>joachim.hugonot@epfl.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<email>pascal.fua@epfl.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>EPFL, Switzerland</roleName><forename type="first">Mathieu</forename><surname>Salzmann Cvlab</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
						</author>
						<title level="a" type="main">Segmentation-driven 6D Object Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The most recent trend in estimating the 6D pose of rigid objects has been to train deep networks to either directly regress the pose from the image or to predict the 2D locations of 3D keypoints, from which the pose can be obtained using a PnP algorithm. In both cases, the object is treated as a global entity, and a single pose estimate is computed. As a consequence, the resulting techniques can be vulnerable to large occlusions.</p><p>In this paper, we introduce a segmentation-driven 6D pose estimation framework where each visible part of the objects contributes a local pose prediction in the form of 2D keypoint locations. We then use a predicted measure of confidence to combine these pose candidates into a robust set of 3D-to-2D correspondences, from which a reliable pose estimate can be obtained. We outperform the state-ofthe-art on the challenging Occluded-LINEMOD and YCB-Video datasets, which is evidence that our approach deals well with multiple poorly-textured objects occluding each other. Furthermore, it relies on a simple enough architecture to achieve real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head><p>Result</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-based 6D object pose estimation is crucial in many real-world applications, such as augmented reality or robot manipulation. Traditionally, it has been handled by establishing correspondences between the object's known 3D model and 2D pixel locations, followed by using the Perspective-n-Point (PnP) algorithm to compute the 6 pose parameters <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref>. While very robust when the object is well textured, this approach can fail when it is featureless or when the scene is cluttered with multiple objects occluding each other.</p><p>Recent work has therefore focused on overcoming these difficulties, typically using deep networks to either regress directly from image to 6D pose <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref> or to detect keypoints associated to the object <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, which can then be used to perform PnP. In both cases, however, the object is still treated as a global entity, which makes the algorithm vulnerable to large occlusions. <ref type="figure" target="#fig_0">Fig. 1</ref> depicts a such a case:</p><p>The bounding box of an occluded drill overlaps other objects that provide irrelevant information to the pose estimator and thereby degrade its performance. Because this happens often, many of these recent methods require an additional post-processing step to refine the pose <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this paper, we show that more robust pose estimates can be obtained by combining multiple local predictions instead of a single global one. To this end, we introduce a segmentation-driven 6D pose estimation network in which each visible object patch contributes a pose estimate for the object it belongs to in the form of the predicted 2D projections of predefined 3D keypoints. Using confidence values also predicted by our network, we then combine the most reliable 2D projections for each 3D keypoint, which yields a robust set of 3D-to-2D correspondences. We then use a RANSAC-based PnP strategy to infer a single reliable pose per object.</p><p>Reasoning in terms of local patches not only makes our approach robust to occlusions, but also yields a rough segmentation of each object in the scene. In other words, unlike other methods that divorce object detection from pose estimation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref>, we perform both jointly while still relying on a simple enough architecture for real-time performance.</p><p>In short, our contribution is a simple but effective segmentation-driven network that produces accurate 6D object pose estimates without the need for post-processing, even when there are multiple poorly-textured objects occluding each other. It combines segmentation and ensemble learning in an effective and efficient architecture. We will show that it outperforms the state-of-the-art methods on standard benchmarks, such as the OccludedLINEMOD and YCB-Video datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this paper, we focus on 6D object pose estimation from RGB images, without access to a depth map, unlike in RGBD-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28]</ref>. The classical approach to performing this task involves extracting local features from the input image, matching them with those of the model, and then running a PnP algorithm on the resulting 3D-to-2D correspondences. Over the years, much effort has been invested in designing local feature descriptors that are invariant to various transformations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32]</ref>, so that they can be matched more robustly <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref>. In parallel, increasingly effective PnP methods have been developed to handle noise and mismatches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8]</ref>. As a consequence, when dealing with well-textured objects, feature-based pose estimation is now fast and robust, even in the presence of mild occlusions. However, it typically struggles with heavily-occluded and poorly-textured objects.</p><p>In the past, textureless objects have often been handled by template-matching <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Image edges then become the dominant information source <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, and researchers have developed strategies based on different distances, such as the Hausdorff <ref type="bibr" target="#b14">[15]</ref> and the Chamfer <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13]</ref> ones, to match the 3D model against the input image. While effective for poorly-textured objects, these techniques often fail in the presence of mild occlusions and cluttered background.</p><p>As in many computer vision areas, the modern take on 6D object pose estimation involves deep neural networks. Two main trends have emerged: Either regressing from the image directly to the 6D pose <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref> or predicting 2D keypoint locations in the image <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, from which the pose can be obtained via PnP. Both approaches treat the object as a global entity and produce a single pose estimate. This makes them vulnerable to occlusions because, when considering object bounding boxes as they all do, signal coming from other objects or from the background will contaminate the prediction. While, in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref>, this is addressed by seg-menting the object of interest, the resulting algorithms still provide a single, global pose estimate, that can be unreliable, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref> and demonstrated in the results section. As a consequence, these methods typically invoke an additional pose refinement step <ref type="bibr" target="#b22">[23]</ref>.</p><p>To the best of our knowledge, the work of <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b30">[31]</ref> constitute the only recent attempts at going beyond a global prediction. While the method in <ref type="bibr" target="#b15">[16]</ref> also relies on segmentation via a state-of-the-art semantic segmentation network, its use of regression to 3D object coordinates, which reside in a very large space, yields disappointing performance. By contrast, the technique in <ref type="bibr" target="#b30">[31]</ref> predicts multiple keypoint location heatmaps from local patches and assembles them to form an input to a PnP algorithm. The employed patches, however, remain relatively large, thus still potentially containing irrelevant information. Furthermore, at runtime, this approach relies on a computationally-expensive slidingwindow strategy that is ill-adapted to real-time processing. Here, we propose to achieve robustness by combining multiple local pose predictions in an ensemble manner and in real time, without post-processing. In the results section, we will show that this outperforms the state-of-the-art approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Note that human pose estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b32">33]</ref> is also related to global 6D object pose prediction techniques. By targeting non-rigid objects, however, these methods require the more global information extracted from larger receptive fields and are inevitably more sensitive to occlusions. By contrast, dealing with rigid objects allows us to rely on local predictions that can be robustly combined, and local visible object parts can provide reliable predictions for all keypoints. We show that assembling these local predictions yields robust pose estimates, even when observing multiple objects that occlude each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Given an input RGB image, our goal is to simultaneously detect objects and estimate their 6D pose, in terms of 3 rotations and 3 translations. We assume the objects to be rigid and their 3D model to be available. As in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref>, we design a CNN architecture to regress the 2D projections of some predefined 3D points, such as the 8 corners of the objects' bounding boxes. However, unlike these methods whose predictions are global for each object and therefore affected by occlusions, we make individual image patches predict both to which object they belong and where the 2D projections are. We then combine the predictions of all patches assigned to the same object for robust PnP-based pose estimation. <ref type="figure">Fig. 2</ref> depicts the corresponding workflow. In the remainder of this section, we first introduce our two-stream network architecture. We then describe each stream individually and finally our inference strategy. <ref type="figure">Figure 2</ref>: Overall workflow of our method. Our architecture has two streams: One for object segmentation and the other to regress 2D keypoint locations. These two streams share a common encoder, but the decoders are separate. Each one produces a tensor of a spatial resolution that defines an S ×S grid over the image. The segmentation stream predicts the label of the object observed at each grid location. The regression stream predicts the 2D keypoint locations for that object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>In essence, we aim to jointly perform segmentation by assigning image patches to objects and 2D coordinate regression of keypoints belonging to these objects, as shown in <ref type="figure">Fig. 3</ref>. To this end, we design the two-stream architecture depicted by <ref type="figure">Fig. 2</ref>, with one stream for each task. It has an encoder-decoder structure, with a common encoder for both streams and two separate decoders.</p><p>For the encoder, we use the Darknet-53 architecture of YOLOv3 <ref type="bibr" target="#b36">[37]</ref> that has proven highly effective and efficient for objection detection. For the decoders, we designed networks that output 3D tensors of spatial resolution S × S and feature dimensions D seg and D reg , respectively. This amounts to superposing an S × S grid on the image and computing a feature vector of dimension D seg or D reg per grid element. The spatial resolution of that grid controls the size of the image patches that vote for the object label and specific keypoint projections. A high resolution yields fine segmentation masks and many votes. However, it comes at a higher computational cost, which may be unnecessary for our purposes. Therefore, instead of matching the 5 downsampling layers of the Darknet-53 encoder with 5 upsampling layers, we only use 2 such layers, with a standard stride of 2. The same architecture, albeit with a different output feature size, is used for both decoder streams.</p><p>To train our model end-to-end, we define a loss function</p><formula xml:id="formula_0">L = L seg + L reg ,<label>(1)</label></formula><p>which combines a segmentation and a regression term that we use to score the output of each stream. We now turn to their individual descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segmentation Stream</head><p>The role of the segmentation stream is to assign a label to each cell of the virtual S × S grid superposed on the image, as shown in <ref type="figure">Fig. 3(a)</ref>. More precisely, given K object classes, this translates into outputting a vector of dimension D seg = K + 1 at each spatial location, with an additional dimension to account for the background.</p><p>During training, we have access to both the 3D object models and their ground-truth pose. We can therefore generate the ground-truth semantic labels by projecting the 3D models in the images while taking into account the depth of each object to handle occlusions. In practice, the images typically contain many more background regions than object ones. Therefore, we take the loss L seg of Eq. 1 to be the Focal Loss of <ref type="bibr" target="#b23">[24]</ref>, a dynamically weighted version of the cross-entropy. Furthermore, we rely on the median frequency balancing technique of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1]</ref> to weigh the different samples. We do this according to the pixel-wise class frequencies rather than the global class frequencies to account for the fact that objects have different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Regression Stream</head><p>The purpose of the regression stream is to predict the 2D projections of predefined 3D keypoints associated to the 3D object models. Following standard practice <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36]</ref>, we typically take these keypoints to be the 8 corners of the model bounding boxes.</p><p>Recall that the output of the regression stream is a 3D tensor of size S × S × D reg . Let N be the number of 3D keypoints per object whose projection we want to predict. When using bounding box corners, N = 8. We take D reg (a) Object segmentation.</p><p>(b) Keypoint 2D locations. <ref type="figure">Figure 3</ref>: Outputs of our two-stream network. (a) The segmentation stream assigns a label to each cell of the virtual grid superposed on the image. (b) In the regression stream, each grid cell predicts the 2D keypoint locations of the object it belongs to. Here, we take the 8 bounding box corners to be our keypoints.</p><p>to be 3N to represent at each spatial location the N pairs of 2D projection values along with a confidence value for each.</p><p>In practice, we do not predict directly the keypoints' 2D coordinates. Instead, for each one, we predict an offset vector with respect to the center of the corresponding grid cell, as illustrated by <ref type="figure">Fig. 3(b)</ref>. That is, let c be the 2D location of a grid cell center. For the i th keypoint, we seek to predict an offset h i (c), such that the resulting location c + h i (c) is close to the ground-truth 2D location g i . During training, this is expressed by the residual</p><formula xml:id="formula_1">∆ i (c) = c + h i (c) − g i ,<label>(2)</label></formula><p>and by defining the loss function</p><formula xml:id="formula_2">L pos = c∈M N i=1 ∆ i (c) 1 ,<label>(3)</label></formula><p>where M is the foreground segmentation mask, and · 1 denotes the L 1 loss function, which is less sensitive to outliers than the L 2 loss. Only accounting for the keypoints that fall within the segmentation mask M focuses the computation on image regions that truly belong to objects. As mentioned above, the regression stream also outputs a confidence value s i (c) for each predicted keypoint, which is obtained via a sigmoid function on the network output. These confidence values should reflect the proximity of the predicted 2D projections to the ground truth. To encourage this, we define a second loss term</p><formula xml:id="formula_3">L conf = c∈M N i=1 s i (c) − exp(−τ ∆ i (c) 2 ) 1 ,<label>(4)</label></formula><p>where τ is a modulating factor. We then take the regression loss term of Eq. 1 to be where β and γ modulate the influence of the two terms. Note that because the two terms in Eq. 5 focus on the regions that are within the segmentation mask M , their gradients are also backpropagated to these regions only. As in the segmentation stream, to account for pixel-wise class imbalance, we weigh the regression loss term for different objects according to the pixel-wise class frequencies in the training set.</p><formula xml:id="formula_4">L reg = βL pos + γL conf ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference Strategy</head><p>At test time, given a query image, our network returns, for each foreground cell in the S × S grid of Section 3.1, an object class and a set of predicted 2D locations for the projection of the N 3D keypoints. As we perform class-based segmentation instead of instance-based segmentation, there might be ambiguities if two objects of the same class are present in the scene. To avoid that, we leverage the fact that the predicted 2D keypoint locations tend to cluster according to the objects they correspond and use a simple pixel distance threshold to identify such clusters.</p><p>For each cluster, that is, for each object, we then exploit the confidence scores predicted by the network to establish 2D-to-3D correspondences between the image and the object's 3D model. The simplest way of doing so would  <ref type="table">Table 1</ref>: Comparison with the state of the art on Occluded-LINEMOD. We compare our results with those of PoseCNN <ref type="bibr" target="#b44">[45]</ref>, BB8 <ref type="bibr" target="#b34">[35]</ref>, Tekin <ref type="bibr" target="#b38">[39]</ref>, iPose <ref type="bibr" target="#b15">[16]</ref>, and Heatmaps <ref type="bibr" target="#b30">[31]</ref>. The results missing from the original papers are denoted as "-".   <ref type="table">Table 3</ref>: Accuracy (REP-5px) of different fusion strategies on Occluded-LINEMOD. We compare a No-Fusion (NF) scheme with one that relies on the Highest-Confidence predictions, and with strategies relying on performing RANSAC on the n most confident predictions (B-n). Oracle consists of choosing the best 2D location using the ground-truth one, and is reported to indicate the potential for improvement of our approach. In the bottom row, we also report the average runtime of these different strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PoseCNN BB8 Tekin Heatmaps iPose Ours</head><p>be to use RANSAC on all the predictions. This, however, would significantly slow down our approach. Instead, we rely on the n most confident 2D predictions for each 3D keypoint. In practice, we found n = 10 to yield a good balance between speed and accuracy. Given these filtered 2D-to-3D correspondences, we obtain the 6D pose of each object using the RANSAC-based version of the EPnP algorithm of <ref type="bibr" target="#b19">[20]</ref>. <ref type="figure" target="#fig_1">Fig. 4</ref> illustrates this procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now evaluate our segmentation-driven multi-object 6D pose estimation method on the challenging Occluded-LINEMOD <ref type="bibr" target="#b17">[18]</ref> and YCB-Video <ref type="bibr" target="#b44">[45]</ref> datasets, which, unlike LINEMOD <ref type="bibr" target="#b11">[12]</ref>, contain 6D pose annotations for each object appearing in all images.</p><p>Metrics. We report the commonly-used 2D reprojection (REP) error <ref type="bibr" target="#b2">[3]</ref>. It encodes the average distance between the 2D reprojection of the 3D model points obtained using the predicted pose and those obtained with the groundtruth one. Furthermore, we also report the pose error in 3D space <ref type="bibr" target="#b11">[12]</ref>, which corresponds to the average distance between the 3D points transformed using the predicted pose and those obtained with the ground-truth one. As in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>, we will refer to it as ADD. Since many objects in the datasets are symmetric, we use the symmetric version of these two metrics and report their REP-5px and ADD-0.1d values. They assume the predicted pose to be correct if the REP is below a 5 pixel threshold and the ADD below 10% of the model diameter, respectively. Below, we denote the objects that are considered to be symmetric by a * superscript.</p><p>Implementation Details. As in <ref type="bibr" target="#b36">[37]</ref>, we scale the input image to a 608 × 608 resolution for both training and testing. Furthermore, when regressing the 2D reprojections, we normalize the horizontal and vertical positions to the range [0, 10]. We use the same normalization procedure when estimating the confidences. We train the network for 300 epochs on Occluded-LINEMOD and 30 epochs on YCB-Video. In both cases, the initial learning rate is set to 1e-3, and is divided by 10 after 50%, 75%, and 90% of the total number of epochs. We use SGD as our optimizer with a momentum of 0.9 and a weight decay of 5e-4. Each training batch contains 8 images, and we have employed the usual data augmentation techniques, such as random luminance, Gaus-ADD-0.1d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REP-5px</head><p>Mask R-CNN CPM <ref type="bibr" target="#b44">[45]</ref> Ours Mask R-CNN CPM <ref type="bibr" target="#b44">[45]</ref>   <ref type="table">Table 4</ref>: Comparison with human pose estimation methods on Occluded-LINEMOD. We modified two state-of-the-art human pose estimation methods, Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> and CPM [44], to output bounding box corner locations. While both Mask R-CNN and CPM perform slightly better than other global-inference methods, our local approach yields much more accurate predictions. sian noise, translation and scaling. We have also used the random erasing technique of <ref type="bibr" target="#b46">[47]</ref> for better occlusion handling. Our source code is publicly available at https://github.com/cvlab-epfl/segmentation-driven-pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on Occluded-LINEMOD</head><p>The Occluded-LINEMOD dataset <ref type="bibr" target="#b17">[18]</ref> was compiled by annotating the pose of all the objects in a subset of the raw LINEMOD dataset <ref type="bibr" target="#b11">[12]</ref>. This subset depicts 8 different objects in 1214 images. Although depth information is also provided, we only exploit the RGB images. The Occluded-LINEMOD images, as the LINEMOD ones, depict a central object surrounded by non-central ones. The standard protocol consists of only evaluating on the non-central objects.</p><p>To create training data for our model, we follow the same procedure as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>. We use the mask inferred from the ground-truth pose to segment the central object in each image, since, as mentioned above, it will not be used for evaluation. We then generate synthetic images by inpainting between 3 and 8 objects on random PASCAL VOC images <ref type="bibr" target="#b6">[7]</ref>. These objects are placed at random locations, orientations, and scales. This procedure still enables us to recover the occlusion state of each object and generate the corresponding segmentation mask. By using the central objects from any of the raw LINEMOD images, provided that it is one of the 8 objects used in Occluded-LINEMOD, we generated 20k training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparing against the State of the Art</head><p>We compare our method with the state-of-the-art ones of <ref type="bibr" target="#b44">[45]</ref> (PoseCNN), <ref type="bibr" target="#b34">[35]</ref> (BB8), and <ref type="bibr" target="#b38">[39]</ref> (Tekin), which all produce a single global pose estimate. Furthermore, we also report the results of the recent work of <ref type="bibr" target="#b15">[16]</ref> (iPose), and <ref type="bibr" target="#b30">[31]</ref> (Heatmaps), which combines the predictions of multiple, relatively large patches, but relies on an expensive slidingwindow strategy. Note that <ref type="bibr" target="#b30">[31]</ref> also provides results obtained with the Feature Mapping technique <ref type="bibr" target="#b35">[36]</ref>. However, most methods, including ours, do not use this technique, and for a fair comparison, we therefore report the results of all methods, including that of <ref type="bibr" target="#b30">[31]</ref>, without it.</p><p>We report our results in <ref type="table">Table 1</ref> and provide the runtimes of the methods in <ref type="table" target="#tab_1">Table 2</ref>. Our method outperforms the global inference ones <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref> by a large margin. It also outperforms Heatmaps, albeit by a smaller one. Further-more, thanks to our simple architecture and one-shot inference strategy, our method runs more than 5 times faster than Heatmaps. Our approach takes 30ms per-image for segmentation and 2D reprojection estimation, and 3-4ms per object for fusion. With 5 objects per image on average, this yields a runtime of about 50ms. <ref type="figure" target="#fig_3">Fig. 5</ref> depicts some of our results. Note their accuracy even in the presence of large occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison of Different Fusion Strategies</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, not all local predictions of the 2D keypoint locations are accurate. Therefore, the fusion strategy based on the predicted confidence values that we described in Section 3.4 is important to select the right ones. Here, we evaluate its impact on the final pose estimate. To this end, we report the results obtained by taking the 2D location with highest confidence (HC) for each 3D keypoint and those obtained with different values n in our n most-confident selection strategy. We refer to this as B-n for a particular value n. Note that we then use RANSAC on the selected 2D-to-3D correspondences.</p><p>In <ref type="table">Table 3</ref>, we compare the results of these different strategies with a fusion-free method that always uses the 2D reprojections predicted by the center grid, which we refer to as No-Fusion (NF). These results evidence that all fusion schemes outperform the No-Fusion one. We also report the Oracle results obtained by selecting the best predicted 2D location for each 3D keypoint using the ground truth 2D reprojections. This indicates that our approach could further benefit from improving the confidence predictions or designing a better fusion scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison with Human Pose Methods</head><p>Our method enables us to infer keypoints' locations of rigid objects from local visible object regions and does not require the more global information extracted from larger receptive fields that are more sensitive to occlusions. To further back up this claim, we compare our approach to two state-of-the-art human pose estimation methods, Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> and Convolutional Pose Machines (CPM) <ref type="bibr">[44]</ref>, which target non-rigid objects, i.e. human bodies. By contrast, dealing with rigid objects allows us to rely on local predictions that can be robustly combined. Specifically, we modified the publicly available code of Mask R-CNN    and CPM to output 8 bounding box 2D corners instead of human keypoints and trained these methods on Occluded-LINEMOD. As shown in <ref type="table">Table 4</ref>, while both Mask R-CNN and CPM perform slightly better than other globalinference methods, our local approach yields much more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on YCB-Video</head><p>We also evaluate our method on the recent and more challenging YCB-Video dataset <ref type="bibr" target="#b44">[45]</ref>. It comprises 21 objects taken from the YCB dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>, which are of diverse sizes and with different degrees of texture. This dataset contains about 130K real images from 92 video sequences, with an additional 80K synthetically rendered images that only contain foreground objects. It provides the pose annotations of all the objects, as well as the corresponding segmentation masks. The test images depict a great diversity in illumination, noise, and occlusions, which makes this dataset extremely challenging. As before, while depth information is available, we only use the color images. Here, we generate complete synthetic images from the 80K synthetic foreground ones by using the same random background procedure as in Section 4.1. As before, we report results without feature mapping, because neither PoseCNN nor our approach use them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparing against the State of the Art</head><p>Fewer methods have reported results on this newer dataset. In <ref type="table" target="#tab_4">Table 5</ref>, we contrast our method with the two baselines that have. Our method clearly outperforms both PoseCNN <ref type="bibr" target="#b44">[45]</ref> and Heatmaps <ref type="bibr" target="#b30">[31]</ref>. Furthermore, recall that our approach runs more than 5 times faster than either of them.</p><p>In <ref type="figure" target="#fig_4">Fig. 6</ref>, we compare qualitative results of PoseCNN and ours. While our pose estimates are not as accurate on this dataset as on Occluded-LINEMOD, they are still much better than those of PoseCNN. Again, this demonstrates the benefits of reasoning about local object parts instead of globally, particularly in the presence of large occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>Although our method performs well in most cases, it still can handle neither the most extreme occlusions nor tiny objects. In such cases, the grid we rely on becomes to rough a representation. This, however, could be addressed by using a finer grid, or, to limit the computational burden, a grid that is adaptively subdivided to better handle each image region. Furthermore, as shown in <ref type="table">Table 3</ref>, we do not yet match the performance of an oracle that chooses the best predicted 2D location for each 3D keypoint. This suggests that there is room to improve the quality of the predicted confidence score, as well as the fusion procedure itself. This will be the topic of our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a segmentation-driven approach to 6D object pose estimation, which jointly detects multiple objects and estimates their pose. By combining multiple local pose estimates in a robust fashion, our approach produces accurate results without the need for a refinement step, even in the presence of large occlusions. Our experiments on two challenging datasets have shown that our approach outperforms the state of the art, and, as opposed to the best competitors, predicts the pose of multiple objects in real time. In the future, we will investigate the use of other backbone architectures for the encoder and devise a better fusion strategy to select the best predictions before performing PnP. We will also seek to incorporate the PnP step of our approach into the network, so as to have a complete, end-to-end learning framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Global pose estimation vs our segmentation-driven approach. (a) The drill's bounding box overlaps another occluding it. (b) As a result, the globally-estimated pose [45] is wrong. (c) In our approach, only image patches labeled as corresponding to the drill contribute to the pose estimate. (d) It is now correct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Combining pose candidates. (a) Grid cells predicted to belong to the cup are overlaid on the image. (b) Each one predicts 2D locations for the corresponding keypoints, shown as green dots. (c) For each 3D keypoint, the n = 10 2D locations about which the network is most confident are selected. (d) Running a RANSACbased PnP on these yields an accurate pose estimate, as evidenced by the correctly drawn outline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Occluded-LINEMOD results. In each column, we show, from top to bottom: the foreground segmentation mask, all 2D reprojection candidates, the selected 2D reprojections, and the final pose results. Our method generates accurate pose estimates, even in the presence of large occlusions. Furthermore, it can process multiple objects in real time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison to PoseCNN [45] on YCB-Video. (Top) PoseCNN and (Bottom) Our method. This demonstrates the benefits of reasoning about local object parts instead of globally, particularly in the presence of large occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Runtime comparisons on Occluded-LINEMOD. All methods run on a modern Nvidia GPU.</figDesc><table><row><cell></cell><cell>NF</cell><cell cols="4">HC B-2 B-10 Oracle</cell></row><row><cell>Ape</cell><cell cols="4">37.8 58.2 58.2 59.1</cell><cell>84.0</cell></row><row><cell>Can</cell><cell cols="4">53.4 58.7 58.5 59.8</cell><cell>89.0</cell></row><row><cell>Cat</cell><cell cols="4">42.6 46.1 47.4 46.9</cell><cell>60.6</cell></row><row><cell>Driller</cell><cell cols="4">52.5 56.8 59.4 59.0</cell><cell>90.3</cell></row><row><cell>Duck</cell><cell cols="4">40.4 42.8 42.4 42.6</cell><cell>55.6</cell></row><row><cell cols="5">Eggbox  *  12.8 11.2 12.1 11.9</cell><cell>10.9</cell></row><row><cell>Glue  *</cell><cell cols="4">14.7 15.8 15.1 16.5</cell><cell>41.0</cell></row><row><cell cols="5">Holepun. 58.4 62.2 63.1 63.6</cell><cell>89.3</cell></row><row><cell>Average</cell><cell cols="4">39.1 44.0 44.5 44.9</cell><cell>65.1</cell></row><row><cell>FPS</cell><cell>26</cell><cell>26</cell><cell>25</cell><cell>22</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Ours [45] [31] Ours</figDesc><table><row><cell></cell><cell></cell><cell>ADD-0.1d</cell><cell></cell><cell></cell><cell>REP-5px</cell><cell></cell></row><row><cell cols="3">[45] [31] master chef can 3.6 32.9</cell><cell>33.0</cell><cell>0.1</cell><cell>9.9</cell><cell>21.0</cell></row><row><cell>cracker box</cell><cell cols="2">25.1 62.6</cell><cell>44.6</cell><cell cols="2">0.1 24.5</cell><cell>12.0</cell></row><row><cell>sugar box</cell><cell cols="2">40.3 44.5</cell><cell>75.6</cell><cell cols="2">7.1 47.0</cell><cell>56.3</cell></row><row><cell>tomato soup can</cell><cell cols="2">25.5 31.1</cell><cell>40.8</cell><cell cols="2">5.2 41.5</cell><cell>46.2</cell></row><row><cell>mustard bottle</cell><cell cols="2">61.9 42.0</cell><cell>70.6</cell><cell cols="2">6.4 42.3</cell><cell>70.3</cell></row><row><cell>tuna fish can</cell><cell>11.4</cell><cell>6.8</cell><cell>18.1</cell><cell>3.0</cell><cell>7.1</cell><cell>39.3</cell></row><row><cell>pudding box</cell><cell cols="2">14.5 58.4</cell><cell>12.2</cell><cell cols="2">5.1 43.9</cell><cell>17.3</cell></row><row><cell>gelatin box</cell><cell cols="2">12.1 42.5</cell><cell cols="3">59.4 15.8 62.1</cell><cell>83.6</cell></row><row><cell>potted meat can</cell><cell cols="2">18.9 37.6</cell><cell cols="3">33.3 23.1 38.5</cell><cell>60.7</cell></row><row><cell>banana</cell><cell cols="2">30.3 16.8</cell><cell>16.6</cell><cell>0.3</cell><cell>8.2</cell><cell>22.4</cell></row><row><cell>pitcher base</cell><cell cols="2">15.6 57.2</cell><cell>90.0</cell><cell cols="2">0 15.9</cell><cell>33.5</cell></row><row><cell>bleach cleanser</cell><cell cols="2">21.2 65.3</cell><cell>70.9</cell><cell cols="2">1.2 12.1</cell><cell>43.3</cell></row><row><cell>bowl  *</cell><cell cols="2">12.1 25.6</cell><cell>30.5</cell><cell cols="2">4.4 16.0</cell><cell>13.3</cell></row><row><cell>mug</cell><cell cols="2">5.2 11.6</cell><cell>40.7</cell><cell cols="2">0.8 20.3</cell><cell>38.1</cell></row><row><cell>power drill</cell><cell cols="2">29.9 46.1</cell><cell>63.5</cell><cell cols="2">3.3 40.9</cell><cell>43.3</cell></row><row><cell>wood block  *</cell><cell cols="2">10.7 34.3</cell><cell>27.7</cell><cell>0</cell><cell>2.5</cell><cell>2.5</cell></row><row><cell>scissors</cell><cell>2.2</cell><cell>0</cell><cell>17.1</cell><cell>0</cell><cell>0</cell><cell>8.8</cell></row><row><cell>large marker</cell><cell>3.4</cell><cell>3.2</cell><cell>4.8</cell><cell>1.4</cell><cell>0</cell><cell>13.6</cell></row><row><cell>large clamp  *</cell><cell cols="2">28.5 10.8</cell><cell>25.6</cell><cell>0.3</cell><cell>0</cell><cell>7.6</cell></row><row><cell cols="3">extra large clamp  *  19.6 29.6</cell><cell>8.8</cell><cell>0.6</cell><cell>0</cell><cell>0.6</cell></row><row><cell>foam brick  *</cell><cell cols="2">54.5 51.7</cell><cell>34.7</cell><cell cols="2">0 52.4</cell><cell>13.5</cell></row><row><cell>Average</cell><cell cols="2">21.3 33.6</cell><cell>39.0</cell><cell cols="2">3.7 23.1</cell><cell>30.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state of the art on YCB-Video. We compare our results with those of PoseCNN<ref type="bibr" target="#b44">[45]</ref> and Heatmaps<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported in part by the Swiss Innovation Agency Innosuisse. We would like to thank Markus Oberweger and Yi Li for clarifying details about their papers, and Zheng Dang for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<title level="m">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning 6D Object Pose Estimation Using 3D Object Coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uncertainty-Driven 6D Pose Estimation of Objects and Scenes from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Yale-CMU-Berkeley dataset for robotic manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The YCB Object and Model Set: Towards Common Benchmarks for Manipulation Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Robotics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very Fast Solution to the PnP Problem with Algebraic Outlier Rejection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D Pose Estimation and 3D Model Retrieval for Objects in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient Response Maps for Real-Time Detection of Textureless Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoißer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model Based Training, Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoißer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Car Make and Model Recognition Using 3D Curve Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient Coarseto-Fine Patch Match for Large Displacement Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparing Images Using the Hausdorff Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">A</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Klanderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rucklidge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="850" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSD-6D: Making Rgb-Based 3D Detection and 6D Pose Estimation Great Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Analysis-By-Synthesis for 6D Pose Estimation in RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Monocular Model-Based 3D Tracking of Rigid Objects: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Now Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">EPnP: An Accurate O(n) Solution to the PnP Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deformable Registration Using Edge-preserving Scale Space for Adaptive Image-guided Radiation Therapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengwang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuying</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Applied Clinical Medical Physics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Robust O(n) Solution to the Perspective-N-Point Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1444" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepIM: Deep Iterative Matching for 6D Poseestimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast Directional Chamfer Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fitting Parameterized Three-Dimensional Models to Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="1991-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Global Hypothesis Generation for 6D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable Nearest Neighbor Algorithms for High Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LF-Net: Learning Local Features from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Konstantinos G. Derpanis, and Kostas Daniilidis. 6-DoF Object Pose from Semantic Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects Without Using Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>BB8: A Scalable</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">YOLOv3: An Incremental Improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3D Object Modeling and Recognition Using Local Affine-Invariant Image Descriptors and Multi-View Spatial Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Rothganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-Time Seamless Single Shot 6D Object Pose Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DAISY: An Efficient Dense Descriptor Applied to Wide Baseline Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="815" to="830" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Image Descriptors with the Boosting-Trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Mario</forename><surname>Tomasz Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Viewpoints and Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pose Tracking from Natural Features on Mobile Phones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Reitmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Mulloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional Pose Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting the PnP Problem: A Fast, General and Optimal Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeki</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalle</forename><surname>Aström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Random Erasing Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
