<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 COUPLED OSCILLATORY RECURRENT NEURAL NET- WORK (CORNN): AN ACCURATE AND (GRADIENT) STABLE ARCHITECTURE FOR LEARNING LONG TIME DEPENDENCIES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Konstantin</forename><surname>Rusch</surname></persName>
							<email>trusch@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Seminar for Applied Mathematics (SAM) Department of Mathematics</orgName>
								<orgName type="institution">ETH Zürich Zürich</orgName>
								<address>
									<postCode>8092</postCode>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
							<email>smishra@ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Seminar for Applied Mathematics (SAM) Department of Mathematics ETH Zürich Zürich</orgName>
								<address>
									<postCode>8092</postCode>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 COUPLED OSCILLATORY RECURRENT NEURAL NET- WORK (CORNN): AN ACCURATE AND (GRADIENT) STABLE ARCHITECTURE FOR LEARNING LONG TIME DEPENDENCIES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recurrent neural networks (RNNs) have achieved tremendous success in a variety of tasks involving sequential (time series) inputs and outputs, ranging from speech recognition to computer vision and natural language processing, among others. However, it is well known that training RNNs to process inputs over long time scales (input sequences) is notoriously hard on account of the so-called exploding and vanishing gradient problem (EVGP) <ref type="bibr">(Pascanu et al., 2013)</ref>, which stems from the fact that the well-established BPTT algorithm for training RNNs requires computing products of gradients (Jacobians) of the underlying hidden states over very long time scales. Consequently, the overall gradient can grow (to infinity) or decay (to zero) exponentially fast with respect to the number of recurrent interactions.</p><p>A variety of approaches have been suggested to mitigate the exploding and vanishing gradient problem. These include adding gating mechanisms to the RNN in order to control the flow of information in the network, leading to architectures such as long short-term memory (LSTM) <ref type="bibr">(Hochreiter &amp; Schmidhuber, 1997)</ref> and gated recurring units (GRU) <ref type="bibr" target="#b8">(Cho et al., 2014)</ref>, that can overcome the vanishing gradient problem on account of the underlying additive structure. However, the gradients might still explode and learning very long term dependencies remains a challenge <ref type="bibr">(Li et al., 2018)</ref>. Another popular approach for handling the EVGP is to constrain the structure of underlying recurrent weight matrices by requiring them to be orthogonal (unitary), leading to the so-called orthogonal RNNs <ref type="bibr">(Henaff et al., 2016;</ref><ref type="bibr" target="#b1">Arjovsky et al., 2016;</ref><ref type="bibr" target="#b24">Wisdom et al., 2016;</ref><ref type="bibr">Kerg et al., 2019)</ref> and references therein. By construction, the resulting Jacobians have eigen-and singular-spectra with unit norm, alleviating the EVGP. However as pointed out by <ref type="bibr">Kerg et al. (2019)</ref>, imposing such constraints on the recurrent matrices may lead to a significant loss of expressivity of the RNN resulting in inadequate performance on realistic tasks.</p><p>In this article, we adopt a different approach, based on observation that coupled networks of controlled non-linear forced and damped oscillators, that arise in many physical, engineering and biological Published as a conference paper at ICLR 2021 systems, such as networks of biological neurons, do seem to ensure expressive representations while constraining the dynamics of state variables and their gradients. This motivates us to propose a novel architecture for RNNs, based on time-discretizations of second-order systems of non-linear ordinary differential equations (ODEs) (1) that model coupled oscillators. Under verifiable hypotheses, we are able to rigorously prove precise bounds on the hidden states of these RNNs and their gradients, enabling a possible solution of the exploding and vanishing gradient problem, while demonstrating through benchmark numerical experiments, that the resulting system still retains sufficient expressivity, i.e. ability to process complex inputs, with a competitive performance, with respect to the state of the art, on a variety of sequential learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED RNN</head><p>Our proposed RNN is based on the following second-order system of ODEs, y = σ (Wy + Wy + Vu + b) − γy − y .</p><p>(1)</p><p>Here, t ∈ [0, 1] is the (continuous) time variable, u = u(t) ∈ R d is the time-dependent input signal, y = y(t) ∈ R m is the hidden state of the RNN with W, W ∈ R m×m , V ∈ R m×d are weight matrices, b ∈ R m is the bias vector and 0 &lt; γ, are parameters, representing oscillation frequency and the amount of damping (friction) in the system, respectively. σ : R → R is the activation function, set to σ(u) = tanh(u) here. By introducing the so-called velocity variable z = y (t) ∈ R m , we rewrite (1) as the first-order system:</p><formula xml:id="formula_0">y = z, z = σ (Wy + Wz + Vu + b) − γy − z.<label>(2)</label></formula><p>We fix a timestep 0 &lt; ∆t &lt; 1 and define our proposed RNN hidden states at time t n = n∆t ∈ [0, 1] (while omitting the affine output state) as the following IMEX (implicit-explicit) discretization of the first order system (2):</p><p>y n = y n−1 + ∆tz n , z n = z n−1 + ∆tσ (Wy n−1 + Wz n−1 + Vu n + b) − ∆tγy n−1 − ∆t zn,</p><p>with eithern = n orn = n − 1. Note that the only difference in the two versions of the RNN (3) lies in the implicit (n = n) or explicit (n = n − 1) treatment of the damping term − z in (2), whereas both versions retain the implicit treatment of the first equation in (2).</p><p>Motivation and background. To see that the underlying ODE (2) models a coupled network of controlled forced and damped nonlinear oscillators, we start with the single neuron (scalar) case by setting d = m = 1 in (1) and assume an identity activation function σ(x) = x. Setting W = W = V = b = = 0 leads to the simple ODE, y + γy = 0, which exactly models simple harmonic motion with frequency γ, for instance that of a mass attached to a spring <ref type="bibr" target="#b14">(Guckenheimer &amp; Holmes, 1990)</ref>. Letting &gt; 0 in (1) adds damping or friction to the system <ref type="bibr" target="#b14">(Guckenheimer &amp; Holmes, 1990)</ref>. Then, by introducing non-zero V in (1), we drive the system with a driving force proportional to the input signal u(t). The parameters V, b modulate the effect of the driving force, W controls the frequency of oscillations and W the amount of damping in the system. Finally, the tanh activation mediates a non-linear response in the oscillator. In the coupled network (2) with m &gt; 1, each neuron updates its hidden state based on the input signal as well as information from other neurons. The diagonal entries of W (and the scalar hyperparameter γ) control the frequency whereas the diagonal entries of W (and the hyperparameter ) determine the amount of damping for each neuron, respectively, whereas the non-diagonal entries of these matrices modulate interactions between neurons. Hence, given this behavior of the underlying ODE (2), we term the RNN (3) as a coupled oscillatory Recurrent Neural Network (coRNN). sees the emergence of non-trivial non-oscillatory hidden states from oscillatory inputs. In practice, a network of a large number of neurons is used and can lead to extremely rich global dynamics. Hence, we argue that the ability of a network of (forced, driven) oscillators to access a very rich set of output states may lead to high expressivity of the system, allowing it to approximate outputs from complicated sequential inputs.</p><p>Oscillator networks are ubiquitous in nature and in engineering systems <ref type="bibr" target="#b14">(Guckenheimer &amp; Holmes, 1990;</ref><ref type="bibr" target="#b21">Strogatz, 2015)</ref> with canonical examples being pendulums (classical mechanics), business cycles (economics), heartbeat (biology) for single oscillators and electrical circuits for networks of oscillators. Our motivating examples arise in neurobiology, where individual biological neurons can be viewed as oscillators with periodic spiking and firing of the action potential. Moreover, functional circuits of the brain, such as cortical columns and prefrontal-striatal-hippocampal circuits, are being increasingly interpreted by networks of oscillatory neurons, see <ref type="bibr" target="#b20">Stiefel &amp; Ermentrout (2016)</ref> for an overview. Following well-established paths in machine learning, such as for convolutional neural networks <ref type="bibr">(LeCun et al., 2015)</ref>, our focus here is to abstract the essence of functional brain circuits being networks of oscillators and design an RNN based on much simpler mechanistic systems, such as those modeled by (2), while ignoring the complicated biological details of neural function.</p><p>Related work. There is an increasing trend of basing RNN architectures on ODEs and dynamical systems. These approaches can roughly be classified into two branches, namely RNNs based on discretized ODEs and continuous-time RNNs. Examples of continuous-time approaches include neural ODEs <ref type="bibr" target="#b6">(Chen et al., 2018)</ref> with ODE-RNNs <ref type="bibr" target="#b18">(Rubanova et al., 2019)</ref> as its recurrent extension as well as E (2017) and references therein, to name just a few. We focus, however, in this article on an ODE-inspired discrete-time RNN, as the proposed coRNN is derived from a discretization of the ODE (1). A good example for a discrete-time ODE-based RNNs is the so-called anti-symmetric RNN of <ref type="bibr" target="#b4">Chang et al. (2019)</ref>, where the RNN architecture is based on a stable ODE resulting from a skew-symmetric hidden weight matrix, thus constraining the stable (gradient) dynamics of the network. This approach has much in common with previously mentioned unitary/orthogonal/nonnormal RNNs in constraining the structure of the hidden-to-hidden layer weight matrices. However, adding such strong constraints might reduce expressivity of the resulting RNN and might lead to inadequate performance on complex tasks. In contrast to these approaches, our proposed coRNN does not explicitly constrain the weight matrices but relies on the dynamics of the underlying ODE (and the IMEX discretization (3)), to provide gradient stability. Moreover, no gating mechanisms as in LSTMs/GRUs are used in the current version of coRNN. There is also an increasing interest in designing hybrid methods, which use a discretization of an ODE (in particular a Hamiltonian system) in order to learn the continuous representation of the data, see for instance <ref type="bibr" target="#b12">Greydanus et al. (2019)</ref>; <ref type="bibr" target="#b7">Chen et al. (2020)</ref>. Overall, our approach here differs from these papers in our use of networks of oscillators to build the RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RIGOROUS ANALYSIS OF THE PROPOSED RNN</head><p>An attractive feature of the underlying ODE system (2) lies in the fact that the resulting hidden states (and their gradients) are bounded (see SM §D for precise statements and proofs). Hence, one can expect that a suitable discretization of the ODE (2) that preserves these bounds will not have exploding gradients. We claim that one such structure preserving discretization is given by the IMEX discretization that results in the RNN (3) and proceed to derive bounds on this RNN below.</p><p>Following standard practice we set y(0) = z(0) = 0 and purely for the simplicity of exposition, we set the control parameters, = γ = 1 andn = n in (3) leading to, y n = y n−1 + ∆tz n , z n = zn−1 1+∆t + ∆t 1+∆t σ(A n−1 ) − ∆t 1+∆t y n−1 , A n−1 := Wy n−1 + Wz n−1 + Vu n + b.</p><p>Analogous results and proofs for the case wheren = n − 1 and for general values of , γ are provided in SM §F.</p><p>Bounds on the hidden states. As with the underlying ODE (2), the hidden states of the RNN (3) are bounded, i.e. Proposition 3.1 Let y n , z n be the hidden states of the RNN (4) for 1 ≤ n ≤ N , then the hidden states satisfy the following (energy) bounds:</p><formula xml:id="formula_3">y n y n + z n z n ≤ nm∆t = mt n ≤ m.<label>(5)</label></formula><p>The proof of the energy bound (5) is provided in SM §E.1 and a straightforward variant of the proof (see SM §E.2) yields an estimate on the sensitivity of the hidden states to changing inputs. As with the underlying ODE (see SM §D) , this bound rules out chaotic behavior of hidden states.</p><p>Bounds on hidden state gradients. We train the RNN (3) to minimize the loss function,</p><formula xml:id="formula_4">E := 1 N N n=1 E n , E n = 1 2 y n −ȳ n 2 2 ,<label>(6)</label></formula><p>withȳ being the underlying ground truth (training data). During training, we compute gradients of the loss function (6) with respect to the weights and biases Θ = [W, W, V, b], i.e.</p><formula xml:id="formula_5">∂E ∂θ = 1 N N n=1 ∂E n ∂θ , ∀ θ ∈ Θ.<label>(7)</label></formula><p>Proposition 3.2 Let y n , z n be the hidden states generated by the RNN (4). We assume that the time step ∆t &lt;&lt; 1 can be chosen such that,</p><formula xml:id="formula_6">max ∆t(1 + W ∞ ) 1 + ∆t , ∆t W ∞ 1 + ∆t = η ≤ ∆t r , 1 2 ≤ r ≤ 1.<label>(8)</label></formula><p>Denoting δ = 1 1+∆t , the gradient of the loss function E (6) with respect to any parameter θ ∈ Θ is bounded as,</p><formula xml:id="formula_7">∂E ∂θ ≤ 3 2 m +Ȳ √ m ,<label>(9)</label></formula><p>withȲ = max 1≤n≤N ȳ n ∞ be a bound on the underlying training data.</p><p>Sketch of the proof. Denoting X n = [y n , z n ], we can apply the chain rule repeatedly (for instance as in <ref type="bibr">Pascanu et al. (2013)</ref>) to obtain,</p><formula xml:id="formula_8">∂E n ∂θ = 1≤k≤n ∂E n ∂X n ∂X n ∂X k ∂ + X k ∂θ ∂E (k) n ∂θ .<label>(10)</label></formula><p>Here, the notation ∂ + X k ∂θ refers to taking the partial derivative of X k with respect to the parameter θ, while keeping the other arguments constant. This quantity can be readily calculated from the structure of the RNN (4) and is presented in the detailed proof provided in SM §E.3. From (6), we can directly compute that ∂En ∂Xn = [y n −ȳ n , 0] . Repeated application of the chain rule and a direct calculation with (4) yields,</p><formula xml:id="formula_9">∂X n ∂X k = k&lt;i≤n ∂X i ∂X i−1 , ∂X i ∂X i−1 = I + ∆tB i−1 ∆tC i−1 B i−1 C i−1 ,<label>(11)</label></formula><p>where I is the identity matrix and</p><formula xml:id="formula_10">B i−1 = δ∆t (diag(σ (A i−1 ))W − I) , C i−1 = δ (I + ∆t diag(σ (A i−1 ))W) .<label>(12)</label></formula><p>It is straightforward to calculate using the assumption (8) that B i−1 ∞ &lt; η and C i−1 ∞ ≤ η + δ.</p><p>Using the definitions of matrix norms and (8), we obtain:</p><formula xml:id="formula_11">∂X i ∂X i−1 ∞ ≤ max (1 + ∆t( B i−1 ∞ + C i−1 ∞ ), B i−1 ∞ + C i−1 ∞ ) ≤ max (1 + ∆t(δ + 2η), δ + 2η) ≤ 1 + 3∆t r .<label>(13)</label></formula><p>Therefore, using (11), we have</p><formula xml:id="formula_12">∂X n ∂X k ∞ ≤ k&lt;i≤n ∂X i ∂X i−1 ∞ ≤ (1 + 3∆t r ) n−k ≈ 1 + 3(n − k)∆t r .<label>(14)</label></formula><p>Note that we have used an expansion around 1 and neglected terms of O(∆t 2r ) as ∆t &lt;&lt; 1. We remark that the bound (13) is the crux of our argument about gradient control as we see from the structure of the RNN that the recurrent matrices have close to unit norm. The detailed proof is presented in SM §E.3. As the entire gradient of the loss function <ref type="formula" target="#formula_4">(6)</ref>, with respect to the weights and biases of the network, is bounded above in (9), the exploding gradient problem is mitigated for this RNN.</p><p>On the vanishing gradient problem. The vanishing gradient problem <ref type="bibr">(Pascanu et al., 2013)</ref> arises</p><formula xml:id="formula_13">if ∂E (k)</formula><p>n ∂θ , defined in (10), → 0 exponentially fast in k, for k &lt;&lt; n (long-term dependencies). In that case, the RNN does not have long-term memory, as the contribution of the k-th hidden state to error at time step t n is infinitesimally small. We already see from (14) that ∂Xn ∂X k ∞ ≈ 1 (independently of k). Thus, we should not expect the products in (10) to decay fast. In fact, we will provide a much more precise characterization of this gradient. To this end, we introduce the following order-notation,</p><formula xml:id="formula_14">β = O(α), for α, β ∈ R + if there exists constants C, C such that Cα ≤ β ≤ Cα. M = O(α), for M ∈ R d1×d2 , α ∈ R + if there exists constant C such that M ≤ Cα.<label>(15)</label></formula><p>For simplicity of notation, we will also setȳ n = u n ≡ 0, for all n, b = 0 and r = 1 in (8) and we will only consider θ = W i,j for some 1 ≤ i, j ≤ m in the following proposition.</p><p>Proposition 3.3 Let y n be the hidden states generated by the RNN (4). Under the assumption that y i n = O( √ t n ), for all 1 ≤ i ≤ m and (8), the gradient for long-term dependencies satisfies,</p><formula xml:id="formula_15">∂E (k) n ∂θ = O ĉδ∆t 3 2 + O ĉδ(1 + δ)∆t 5 2 + O(∆t 3 ),ĉ = sech 2 √ k∆t(1 + ∆t) , k &lt;&lt; n.<label>(16)</label></formula><p>This precise bound (16) on the gradient shows that although the gradient can be small, i.e O(∆t 3 2 ), it is in fact independent of k, ensuring that long-term dependencies contribute to gradients at much later steps and mitigating the vanishing gradient problem. The detailed proof is presented in SM §E.5.</p><p>Summarizing, we see that the RNN (3) indeed satisfied similar bounds to the underlying ODE (2) that resulted in upper bounds on the hidden states and its gradients. However, the lower bound on the gradient (16) is due to the specific choice of this discretization and does not appear to have a continuous analogue, making the specific choice of discretization of (2) crucial for mitigating the vanishing gradient problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We present results on a variety of learning tasks with coRNN (3) withn = n − 1, as this version resulted in marginally better performance than the version withn = n. Details of the training procedure for each experiment can be found in SM §B. We wish to clarify here that we use a straightforward hyperparameter tuning protocol based on a validation set and do not use additional performance enhancing tools, such as dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref>, gradient clipping <ref type="bibr">(Pascanu et al., 2013)</ref> or batch normalization <ref type="bibr">(Ioffe &amp; Szegedy, 2015)</ref>, which might further improve the performance of coRNNs.</p><p>Adding problem. We start with the well-known adding problem <ref type="bibr">(Hochreiter &amp; Schmidhuber, 1997)</ref>, proposed to test the ability of an RNN to learn (very) long-term dependencies. The input is a two-dimensional sequence of length T , with the first dimension consisting of random numbers drawn from U([0, 1]) and with two non-zero entries (both set to 1) in the second dimension, chosen at random locations, but one each in both halves of the sequence. The output is the sum of two numbers of the first dimension at positions, corresponding to the two 1 entries in the second dimension. We compare the proposed coRNN to three recently proposed RNNs, which were explicitly designed to learn LTDs, namely the FastRNN (Kusupati et al., 2018), the antisymmetric (anti.sym.) RNN <ref type="bibr" target="#b4">(Chang et al., 2019)</ref> and the expRNN <ref type="bibr">(Lezcano-Casado &amp; Martínez-Rubio, 2019)</ref>, and to a plain vanilla tanh RNN, with the goal of beating the baseline mean square error (MSE) of 0.167 (which stems from the variance of the baseline output 1). All methods have 128 hidden units (dimensionality of the hidden state y) and the same training protocol is used in all cases. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the results for different lengths T of the input sequences. We can see that while the tanh RNN is not able to beat the baseline for any sequence length, the other methods successfully learn the adding task for T = 500. However, in this case, coRNN converges significantly faster and reaches a lower test MSE than other tested methods. When setting the length to the much more challenging case of T = 2000, we see that only coRNN and the expRNN beat the baseline. However, the expRNN fails to reach a desired test MSE of 0.01 within training time. In order to further demonstrate the superiority of coRNN over recently proposed RNN architectures for learning LTDs, we consider the adding problem for T = 5000 and observe that coRNN converges very quickly even in this case, while expRNN fails to consistently beat the baseline. We thus conclude that the coRNN mitigates the vanishing/exploding gradient problem even for very long sequences.  <ref type="bibr">, 1998)</ref> digit one pixel at a time leading to a classification task with a sequence length of T = 784. In permuted sequential MNIST (psMNIST), a fixed random permutation is applied in order to increase the time-delay between interdependent pixels and to make the problem harder. In <ref type="table" target="#tab_0">Table 1</ref>, we compare the test accuracy for coRNN on sMNIST and psMNIST with recently published best case results for other recurrent models, which were explicitly designed to solve long-term dependencies together with baselines corresponding to gated and unitary RNNs. To the best of our knowledge the proposed coRNN outperforms all single-layer recurrent architectures, published in the literature, for both the sMNIST and psMNIST. Moreover in <ref type="figure">Fig. 2</ref>, we present the performance (with respect to number of epochs) of different RNN architectures for psMNIST with the same fixed random permutation and the same number of hidden units, i.e. 128. As seen from this figure, coRNN clearly outperforms the other architectures, some of which were explicitly designed to learn LTDs, handily for this permutation. Noise padded CIFAR-10. Another challenging test problem for learning LTDs is the recently proposed noise padded CIFAR-10 experiment by <ref type="bibr" target="#b4">Chang et al. (2019)</ref>, in which CIFAR-10 data points <ref type="bibr">(Krizhevsky et al., 2009</ref>) are fed to the RNN row-wise and flattened along the channels resulting in sequences of length 32. To test the long term memory, entries of uniform random numbers are added such that the resulting sequences have a length of 1000, i.e. the last 968 entries of each sequence are only noise to distract the network. <ref type="table" target="#tab_1">Table 2</ref> shows the result for coRNN together with other recently published best case results. We observe that coRNN readily outperforms other RNN architectures on this benchmark, while requiring only 128 hidden units. Human activity recognition. This experiment is based on the human activity recognition data set provided by <ref type="bibr" target="#b0">Anguita et al. (2012)</ref>. The data set is a collection of tracked human activities, which were measured by an accelerometer and gyroscope on a Samsung Galaxy S3 smartphone. Six activities were binarized to obtain two merged classes {Sitting, Laying, Walking_Upstairs} and {Standing, Walking, Walking_Downstairs}, leading to the HAR-2 data set, which was first proposed in <ref type="bibr">Kusupati et al. (2018)</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the result for coRNN together with other very recently published best case results on the same data set. We can see that coRNN readily outperforms all other methods. We also ran this experiment on a tiny coRNN with very few parameters, i.e. only 1k. We can see that even in this case, the tiny coRNN beats all baselines. We thus conclude that coRNN can efficiently be used on resource-constrained IoT micro-controllers.</p><p>IMDB sentiment analysis. The IMDB data set (Maas et al., 2011) is a collection of 50k movie reviews, where 25k reviews are used for training (with 7.5k of these reviews used for validating) and 25k reviews are used for testing. The aim of this binary sentiment classification task is to decide whether a movie review is positive or negative. We follow the standard procedure by initializing the word embedding with pretrained 100d GloVe <ref type="bibr" target="#b17">(Pennington et al., 2014)</ref> vectors and restrict the dictionary to 25k words. <ref type="table" target="#tab_3">Table 4</ref> shows the results for coRNN and other recently published models, which are trained similarly and have the same number of hidden units, i.e. 128. We can see that coRNN compares favorable with gated baselines (which are known to perform very well on this task), while at the same time requiring significantly less parameters.  <ref type="bibr" target="#b2">(Campos et al., 2018)</ref> 86.6% 128 220k GRU <ref type="bibr" target="#b2">(Campos et al., 2018)</ref> 86.2% 128 164k Skip GRU <ref type="bibr" target="#b2">(Campos et al., 2018)</ref> 86.6% 128 164k ReLU GRU <ref type="bibr" target="#b9">(Dey &amp; Salemt, 2017)</ref>  Further experimental results. To shed further light on the performance of coRNN, we consider the following issues. First, the theory suggested that coRNN mitigates the exploding/vanishing gradient problem as long as the assumptions <ref type="formula" target="#formula_6">(8)</ref>   <ref type="figure">Fig. 3</ref>, where we show that <ref type="formula" target="#formula_6">(8)</ref> holds for all LTD tasks during training. Thus, the presented theory applies and one can expect control over hidden state gradients with coRNN. Next, we recall that the frequency parameter γ and damping parameter play a role for coRNNs (see SM §F for the theoretical dependence and <ref type="table" target="#tab_10">Table 8</ref> for best performing values of , γ for each numerical experiment within the range considered in <ref type="table" target="#tab_9">Table 7</ref>). How sensitive is the performance of coRNN to the choice of these 2 parameters? To investigate this dependence, we focus on the noise padded CIFAR-10 experiment and show the results of an ablation study in <ref type="figure" target="#fig_1">Fig. 4</ref>, where the test accuracy for different coRNNs based on a two dimensional hyperparameter grid ( , γ) ∈ [0.8, 1.8] × [5.7, 17, 7] (i.e., sufficiently large intervals around the best performing values of , γ from <ref type="table" target="#tab_10">Table 8</ref>) is plotted. We observe from the figure that although there are reductions in test accuracy for non-optimal values of ( , γ), there is no large variation and the performance is rather robust with respect to these hyperparameters. Finally, note that we follow standard practice and present best reported results with coRNN as well as other competing RNNs in order to compare the relative performance. However, it is natural to investigate the dependence of these best results on the random initial (before training) values of the weight matrices. To this end, in <ref type="table" target="#tab_6">Table 5</ref> of SM, we report the mean and standard deviation (over 10 retrainings) of the test accuracy with coRNN on various learning tasks and find that the mean value is comparable to the best reported value, with low standard deviations. This indicates further robustness of the performance of coRNNs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Inspired by many models in physics, biology and engineering, we proposed a novel RNN architecture (3) based on a model (1) of a network of controlled forced and damped oscillators. For this RNN, we rigorously showed that under verifiable hypotheses on the time step and weight matrices, the hidden states are bounded (5) and obtained precise bounds on the gradients (Jacobians) of the hidden states, (9) and <ref type="formula" target="#formula_4">(16)</ref>. Thus by design, this architecture can mitigate the exploding and vanishing gradient problem (EVGP) for RNNs. We present a series of numerical experiments that include sequential image classification, activity recognition and sentiment analysis, to demonstrate that the proposed coRNN keeps hidden states and their gradients under control, while retaining sufficient expressivity to perform complex tasks. Thus, we provide a novel and promising strategy for designing RNN architectures that are motivated by the functioning of natural systems, have rigorous bounds on hidden state gradients and are robust, accurate, straightforward to train and cheap to evaluate.</p><p>This work can be extended in different directions. For instance in this article, we have mainly focused on the learning of tasks with long-term dependencies and observed that coRNNs are comparable in performance to the best published results in the literature. Given that coRNNs are built with networks of oscillators, it is natural to expect that they will perform very well on tasks with oscillatory inputs/outputs, such as the time series analysis of high-resolution biomedical data, for instance EEG (electroencephalography) and EMG (electromyography) data and seismic activity data from geoscience. This will be pursued in a follow-up article. Similarly, applications of coRNN to language modeling will be covered in future work.</p><p>However, it is essential to point out that coRNNs might not be suitable for every learning task involving sequential inputs/outputs. As a concrete example, we consider the problem of predicting time series corresponding to a chaotic dynamical system. We recall that by construction, the underlying ODE (2) (and the discretization <ref type="formula" target="#formula_1">(3)</ref>) do not allow for super-linear (in time) separation of trajectories for nearby inputs. Thus, we cannot expect that coRNNs will be effective at predicting chaotic time series and it is indeed investigated and demonstrated for a Lorenz-96 ODE in SM §A, where we observe that the coRNN is outperformed by LSTMs in the chaotic regime.</p><p>Our main theoretical focus in this paper was to demonstrate the possible mitigation of the exploding and vanishing gradient problem. On the other hand, we only provided some heuristics and numerical evidence on why the proposed RNN still has sufficient expressivity. A priori, it is natural to think that the proposed RNN architecture might introduce a strong bias towards oscillatory functions. However, as we argue in SM §C, the proposed coRNN can be significantly more expressive, as the damping, forcing and coupling of several oscillators modulates nonlinear response to yield a very rich and diverse set of output states. This is also evidenced by the ability of coRNNs to deal with many tasks in our numerical experiments, which do not have an explicit oscillatory structure. This sets the stage for a rigorous investigation of universality of the proposed coRNN architecture, as in the case of echo state networks in <ref type="bibr" target="#b13">Grigoryeva &amp; Ortega (2018)</ref>. A possible approach would be to leverage the ability of the proposed RNN to convert general inputs into a rich set of superpositions of harmonics (oscillatory wave forms). Moreover, the proposed RNN was based on the simplest model of coupled oscillators (1). Much more detailed models of oscillators are available, particularly those that arise in the modeling of biological neurons, <ref type="bibr" target="#b20">Stiefel &amp; Ermentrout (2016)</ref> and references therein. An interesting variant of our proposed RNN would be to base the RNN architecture on these more elaborate models, resulting in analogues of the spiking neurons model of Maass <ref type="formula" target="#formula_0">(2001)</ref>  According to proposition E.1, coRNN does not exhibit chaotic behavior by design. While this property is highly desirable for learning long-term dependencies (a slight perturbation of the input should not result in an unbounded perturbation of the prediction), it impairs the performance on tasks, where the network has to learn actual chaotic dynamics. To test this numerically, we consider the following version of the Lorenz 96 system: <ref type="bibr">(Lorenz, 1996)</ref>:</p><formula xml:id="formula_16">x j = (x i+1 − x i−2 )x i−1 − x i + F,<label>(17)</label></formula><p>where x j ∈ R for all j = 1, . . . , 5 and F is an external force controlling the level of chaos in the system. <ref type="figure" target="#fig_3">Fig. 5</ref> shows a trajectory of the system <ref type="formula" target="#formula_5">(17)</ref>   force of F = 0.9 as well as a trajectory for a large external force of F = 8. We can see that while for F = 0.9 the system does not exhibit chaotic behavior, the dynamics for F = 8 is already highly chaotic.</p><p>Our task consists of predicting the 25-th next state of a trajectory of the system (17). We provide 128 trajectories of length 2000 for each of the training, validation and test sets. The trajectories are generated by numerically solving the system (17) and evaluating it at 2000 equidistantly distributed discrete time points with distance 0.01. The initial value for each trajectory is chosen uniform at random on [F − 1/2, F + 1/2] 5 around the equilibrium point (F, . . . , F ) of the system (17).</p><p>Since LSTMs are known to be able to produce chaotic dynamics, even in the autonomous (zero-entry) case <ref type="bibr">(Laurent &amp; von Brecht, 2017)</ref>, we expect them to perform significantly better than coRNN if the underlying system exhibits strong chaotic behavior. <ref type="table" target="#tab_8">Table 6</ref> shows the normalized root mean square error (NRMSE) (RMSE divided by the root mean square of the target trajectory) on the test set for coRNN and LSTM. We can see that indeed for the non-chaotic case of using an external force of F = 0.9 LSTM and coRNN perform similarly. However, when the dynamics get chaotic (in this case using an external force of F = 8), the LSTM clearly outperforms coRNN. , where n in denotes the input dimension of each affine transformation. Instead of treating the parameters ∆t, γ and as fixed hyperparameters, we can also treat them as trainable network parameters by constraining ∆t to [0, 1] by using a sigmoidal activation function and , γ &gt; 0 by the use of ReLU for instance. However, in this case no major difference in performance is obtained. The hyperparameters are optimized with a random search algorithm, where the results of the best performing coRNN (based on the validation set) are reported. The ranges of the hyperparameters for the random search algorithm are provided in <ref type="table" target="#tab_9">Table 7</ref>. <ref type="table" target="#tab_10">Table 8</ref> shows the rounded hyperparameters of the best performing coRNN architecture resulting from the random search algorithm for each learning task. We used 100 training epochs for sMNIST, psMNIST and noise padded CIFAR-10 with additional 20 epochs in which the learning rate was reduced by a factor of 10. Additionally, we used 100 epochs for the IMDB task and 250 epochs for the HAR-2 task.  Adding (T = 5000) 2 × 10 −2 50 1.6 × 10 −2 94.5 9.5 sMNIST (n hid = 128) 3.5 × 10 −3 120 5.3 × 10 −2 1.7 4 sMNIST (n hid = 256) 2.1 × 10 −3 120 4.2 × 10 −2 2.7 4.7 psMNIST (n hid = 128) 3.7 × 10 −3 120 8.3 × 10 −2 1.3 × 10 −1 4.1 psMNIST (n hid = 256) 5.4 × 10 −3 120 7.6 × 10 −2 4 × 10 −1 8.0 Noise padded CIFAR-10 7.5 × 10 −3 100 3.4 × 10 −2 1.3 12.7 HAR-2 1.7 × 10 −2 64 10 −1 2 × 10 −1 6.4 IMDB 6.0 × 10 −4 64 5.4 × 10 −2 4.9 4.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C HEURISTICS OF NETWORK FUNCTION</head><p>At the level of a single neuron, the dynamics of the RNN is relatively straightforward. We start with the scalar case, i.e. m = d = 1 and illustrate different hidden states y as a function of time, for different input signals, in <ref type="figure" target="#fig_4">Fig. 6</ref>. In this figure, we consider two different input signals, one oscillatory signal given by u(t) = cos(4t) and another is a combination of step functions. First, we plot the solution y(t) of (1), with the parameters V, b, W, W, = 0 and γ = 1. This simply corresponds to the case of a simple harmonic oscillator (SHO) and the solution is described by a sine wave with the natural frequency of the oscillator. Next, we introduce forcing by the input signal by setting V = 1 and the activation function is the identity σ(x) = x, leading to a forced damped oscillator (FDO). As seen from <ref type="figure" target="#fig_4">Fig. 6</ref>, in the case of an oscillatory signal, this leads to a very minor change over the SHO, whereas for the step function, the change is only in the amplitude of the wave. Next, we add damping by setting = 0.25 and see that the resulting forced damped oscillator (FDO), merely damps the amplitude of the waves, without changing their frequency. Then, we consider the case of controlled oscillator (CFDO) by setting W = −2, V = 2, b = 0.25, W = 0.75. As seen from <ref type="figure" target="#fig_4">Fig. 6</ref>, this leads to a significant change in the wave form in both cases. For the oscillatory input, the output is now a superposition of many different forms, with different amplitudes and frequencies (phases) whereas for the step function input, the phase is shifted. Already, we can see that for a linear controlled oscillator, the output can be very complicated with the superposition of different waves. This holds true when the activation function is set to σ(x) = tanh(x) (which is our proposed coRNN). For both inputs, the output is a modulated version of the one generated by CFDO, expressed as a superposition of waves. On the other hand, we also plot the solution with a Duffing type oscillator (DUFF) by setting the activation function as,</p><formula xml:id="formula_17">σ(x) = x − x 3 3 .<label>(18)</label></formula><p>In this case, the solution is very different from the CFDO and coRNN solutions and is heavily damped (either in the output or its derivative). On the other hand, given the chaotic nature of the dynamical system in this case, a slight change in the parameters led to the output blowing up. Thus, a bounded nonlinearity seems essential in this context.</p><p>Coupling neurons together further accentuates this generation of superpositions of different waveforms, as seen even with the simplest case of a network with two neurons, shown in <ref type="figure" target="#fig_4">Fig. 6</ref> (Bottom row). For this figure, we consider two neurons, i.e m = 2 and two different network topologies.</p><p>For the first, we only allow the first neuron to influence the second one and not vice versa. This is enforced with the weight matrices,</p><formula xml:id="formula_18">W = −2 0 3 −2 , W = 0.75 0 −1 0.75 .</formula><p>We also set V = [2, 2] , b = [0.25, 0.25] . Note that in this case (we name as ORD (for ordered connections)), the output of the first neuron should be exactly the same as in the uncoupled (UC) case, whereas there is a distinct change in the output of the second neuron and we see that the first neuron has modulated a sharp change in the resulting output wave form. It is well illustrated by the emergence of an approximation to the step function (Bottom Right of <ref type="figure" target="#fig_4">Fig. 6</ref>), even though the input signal is oscillatory.</p><p>Next, we consider the case of fully connected (FC) neurons by setting the weight matrices as,</p><formula xml:id="formula_19">W = −2 1 3 −2 , W = 0.75 0.3 −1 0.75 .</formula><p>The resulting outputs for the first neuron are now slightly different from the uncoupled case. On the the other hand, the approximation of step function output for the second neuron is further accentuated.</p><p>Even these simple examples illustrate the functioning of a network of controlled oscillators well. The input signal is converted into a superposition of waves with different frequencies and amplitudes, with these quantities being controlled by the weights and biases in (1). Thus, very complicated outputs can be generated by modulating the number, frequencies and amplitudes of the waves. In practice, a network of a large number of neurons is used and can lead to extremely rich global dynamics, along the lines of emergence of synchronization or bistable heterogeneous behavior seen in systems of idealized oscillators and explained by their mean field limit, see H. <ref type="bibr" target="#b15">Sakaguchi &amp; Kuramoto (1987)</ref>; <ref type="bibr" target="#b23">Winfree (1967)</ref>; <ref type="bibr" target="#b22">Strogatz (2001)</ref>. Thus, we argue that the ability of the network of (forced, driven) oscillators to access a very rich set of output states can lead to high expressivity of the system. The training process selects the weights that modulate frequencies, phases and amplitudes of individual neurons and their interaction to guide the system to its target output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D BOUNDS ON THE DYNAMICS OF THE ORDINARY DIFFERENTIAL EQUATION</head><p>In this section, we present bounds that show how the continuous time dynamics of the ordinary differential equation <ref type="formula" target="#formula_0">(2)</ref>, modeling non-linear damped and forced networks of oscillators, is constrained. We start with the following estimate on the energy of the solutions of the system (2). Proposition D.1 Let y(t), z(t) be the solutions of the ODE system (2) at any time t ∈ [0, T ] and assume that the damping parameter ≥ 1 2 and the initial data for (2) is given by, y(0) = z(0) ≡ 0.</p><p>Then, the solutions are bounded as,</p><formula xml:id="formula_21">y(t) y(t) ≤ mt γ , z(t) z(t) ≤ mt, ∀t ∈ (0, T ].<label>(19)</label></formula><p>To prove this proposition, we multiply the first equation in (2) with y(t) and the second equation in (2) with 1 γ z(t) to obtain, d dt</p><formula xml:id="formula_22">y(t) y(t) 2 + z(t) z(t) 2γ = z(t) σ(A(t)) γ − γ z(t) z(t),<label>(20)</label></formula><p>with A(t) = Wy(t) + Wz(t) + Vu(t) + b. Using the elementary Cauchy's inequality repeatedly in (20) results in, d dt</p><formula xml:id="formula_23">y(t) y(t) 2 + z(t) z(t) 2γ ≤ σ(A) σ(A) 2γ + 1 γ 1 2 − z z ≤ m 2γ</formula><p>(as |σ| ≤ 1 and ≥ 1 2 ).</p><p>Integrating the above inequality over the time interval [0, t] and using the fact that the initial data are y(0) = z(0) ≡ 0, we obtain the bounds (19).</p><p>The above proposition and estimate <ref type="bibr">(19)</ref> clearly demonstrate that the dynamics of the network of coupled non-linear oscillators (1) is bounded. The fact that the nonlinear activation function σ = tanh is uniformly bounded in its arguments played a crucial role in deriving the energy bound <ref type="bibr">(19)</ref>. A straightforward adaptation of this argument leads to the following proposition about the sensitivity of the system to inputs, Proposition D.2 Let y(t), z(t) be the solutions of the ODE system (2) with respect to the input signal u(t). Letȳ(t),z(t) be the solutions of the ODE system (2), but with respect to the input signalū(t). Assume that the damping parameter ≥ 1 2 and the initial data are given by,</p><formula xml:id="formula_24">y(0) = z(0) =ȳ(0) =z(0) ≡ 0.</formula><p>Then we have the following bound,</p><formula xml:id="formula_25">(y(t) −ȳ(t)) (y(t) −ȳ(t)) ≤ 4mt γ , (z(t) −z(t)) (z(t) −z(t)) ≤ 4mt, ∀t ∈ (0, T ].<label>(21)</label></formula><p>Thus from the bound <ref type="formula" target="#formula_0">(21)</ref>, there can be atmost linear separation (in time) with respect to the trajectories of the ODE (2) for different input signals. Hence, chaotic behavior, which is characterized by the (super-)exponential separation of trajectories is ruled out by the structure of the ODE system (2). Note that this property of the ODE system was primarily a result of the uniform boundedness of the activation function σ. Using a different activation function such as ReLU might enable to obtain an exponential separation of trajectories that is a prerequisite for a chaotic dynamical system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 GRADIENT DYNAMICS FOR THE ODE SYSTEM (2)</head><p>Let θ denote the i, j-th entry of the Weight matrices W, W, V or the i-th entry of the bias vector b.</p><p>We are interested in finding out how the gradients of the hidden state y (and the auxiliary hidden state z) with respect to parameter θ, vary with time. Note that these gradients are precisely the objects of interest in the training of an RNN, based on a discretization of the ODE system (2). To this end, we differentiate (2) with respect to the parameter θ and denote</p><formula xml:id="formula_26">y θ (t) = ∂y ∂θ (t), z θ (t) = ∂z ∂θ (t),</formula><p>to obtain,</p><formula xml:id="formula_27">y θ = z θ , z θ = diag(σ (A)) [Wy θ + Wz θ ] + Z i,j m,m (A)ρ − γy θ − z θ .<label>(22)</label></formula><p>As introduced before, Z i,j m,m (A) ∈ R m×m is a matrix with all elements are zero except for the (i, j)-th entry which is set to σ (A(t)) i , i.e. the i-th entry of σ (A), and we have,</p><formula xml:id="formula_28">ρ = y,m = m, if θ = W i,j , ρ = z,m = m, if θ = W i,j , ρ = u,m = d, if θ = V i,j , ρ = 1,m = 1, if θ = b i .</formula><p>We see from <ref type="formula" target="#formula_0">(22)</ref> that the ODEs governing the gradients with respect to the parameter θ also represent a system of oscillators but with additional coupling and forcing terms, proportional to the hidden states y, z or input signal u. As we have already proved with estimate (19) that the hidden states are always bounded and the input signal is assumed to be bounded, it is natural to expect that the gradients of the states with respect to θ are also bounded. We make this statement explicit in the following proposition, which for simplicity of exposition, we consider the case of θ = W i,j , as the other values of θ are very similar in their behavior. Proposition D.3 Let θ = W i,j and y, z be the solutions of the ODE system (2). Assume that the weights and the damping parameter satisfy,</p><formula xml:id="formula_29">W ∞ + W ∞ ≤ ,</formula><p>then we have the following bounds on the gradients,</p><formula xml:id="formula_30">y θ (t) y θ (t) + 1 γ z θ (t) z θ (t) ≤ y θ (0) y θ (0) + 1 γ z θ (0) z θ (0) e Ct + mt 2 2γ 2 , t ∈ (0, T ], C = max W 1 γ , 1 + W 1 .<label>(23)</label></formula><p>The proof of this proposition follows exactly along the same lines as the proof of proposition D.1 and we skip the details, while noting the crucial role played by the energy bound <ref type="bibr">(19)</ref>.</p><p>We remark that the bound (23) indicates that as long as the initial gradients with respect to θ are bounded and the weights are controlled by the damping parameter, the hidden state gradients remain bounded in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E SUPPLEMENT TO THE RIGOROUS ANALYSIS OF CORNN</head><p>In this section, we supplement the section on the rigorous analysis of the proposed RNN (4). We start with E.1 PROOF OF PROPOSITION 3.1</p><p>We multiply (y n−1 , z n ) to (3) and use the elementary identities,</p><formula xml:id="formula_31">a (a − b) = a a 2 − b b 2 + 1 2 (a − b) (a − b), b (a − b) = a a 2 − b b 2 − 1 2 (a − b) (a − b),</formula><p>to obtain the following, y n y n + z n z n 2 = y n−1 y n−1 + z n−1 z n−1 2 + (y n − y n−1 ) (y n − y n−1 ) 2</p><formula xml:id="formula_32">− (z n − z n−1 ) (z n − z n−1 ) 2 + ∆tz n σ(A n−1 ) − ∆tz n z n ≤ y n−1 y n−1 + z n−1 z n−1 2 + ∆t (1/2 + ∆t/2 − 1) z n z n + ∆t 2 σ (A n−1 )σ(A n−1 )</formula><p>≤ y n−1 y n−1 + z n−1 z n−1 2 + m∆t 2 as σ 2 ≤ 1 and &gt; ∆t &lt;&lt; 1.</p><p>Iterating the above inequality n times leads to the energy bound, y n y n + z n z n ≤ y 0 y 0 + z 0 z 0 + nm∆t = mt n ,</p><p>as y 0 = z 0 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 SENSITIVITY TO INPUTS</head><p>Next, we examine how changes in the input signal u affect the dynamics. We have the following proposition:</p><p>Proposition E.1 Let y n , z n be the hidden states of the trained RNN (4) with respect to the input u = {u n } N n=1 and let y n , z n be the hidden states of the same RNN (4), but with respect to the input u = {u n } N n=1 , then the differences in the hidden states are bounded by,</p><formula xml:id="formula_34">(y n − y n ) (y n − y n ) + (z n − z n ) (z n − z n ) ≤ 4mt n .<label>(25)</label></formula><p>The proof of this proposition is completely analogous to the proof of proposition 3.1, we subtract y n = y n−1 + ∆tz n , z n = zn−1 1+∆t + ∆t 1+∆t σ(A n−1 ) − ∆t 1+∆t y n−1 , A n−1 := Wy n−1 + Wz n−1 + Vu n + b.</p><p>(26) from (4) and multiply (y n − y n ) , (z n − z n ) to the difference. The estimate (25) follows identically to the proof of (5) (presented above) by realizing that σ(A n−1 ) − σ(A n−1 ) ≤ 2.</p><p>Note that the bound (25) ensures that the hidden states can only separate linearly in time for changes in the input. Thus, chaotic behavior, such as for Duffing type oscillators, characterized by at least exponential separation of trajectories, is ruled out for this proposed RNN, showing that it is stable with respect to changes in the input. This is largely on account of the fact that the activation function σ in <ref type="formula" target="#formula_1">(3)</ref> is globally bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 PROOF OF PROPOSITION 3.2</head><p>From <ref type="formula" target="#formula_4">(6)</ref>, we readily calculate that,</p><formula xml:id="formula_35">∂E n ∂X n = [y n −ȳ n , 0] .<label>(27)</label></formula><p>Similarly from <ref type="formula" target="#formula_1">(3)</ref>, we calculate,</p><formula xml:id="formula_36">∂ + X k ∂θ =                          ∆t 2 1+∆t Z i,j m,m (A k−1 )y k−1 , ∆t 1+∆t Z i,j m,m (A k−1 )y k−1 if θ = (i, j)−th entry of W, ∆t 2 1+∆t Z i,j m,m (A k−1 )z k−1 , ∆t 1+∆t Z i,j m,m (A k−1 )z k−1 if θ = (i, j)−th entry of W, ∆t 2 1+∆t Z i,j m,d (A k−1 )u k , ∆t 1+∆t Z i,j m,d (A k−1 )u k if θ = (i, j)−th entry of V, ∆t 2 1+∆t Z i,1 m,1 (A k−1 ) , ∆t 1+∆t Z i,1 m,1 (A k−1 ) if θ = i−th entry of b,<label>(28)</label></formula><p>where Z i,j m,m (A k−1 ) ∈ R m×m is a matrix with all elements are zero except for the (i, j)-th entry which is set to σ (A k−1 ) i , i.e. the i-th entry of σ (A k−1 ). We easily see that Z i,j m,m (A k−1 ) ∞ ≤ 1 for all i, j, m,m and all choices of A k−1 . Now, using definitions of matrix and vector norms and applying <ref type="formula" target="#formula_2">(14)</ref> in <ref type="formula" target="#formula_8">(10)</ref>, together with <ref type="formula" target="#formula_0">(27)</ref> and <ref type="formula" target="#formula_0">(28)</ref>, we obtain the following estimate on the norm:</p><formula xml:id="formula_37">∂E (k) n ∂θ ≤        ( y n ∞ + ȳ n ∞ )(1 + 3(n − k)∆t r )δ∆t y k−1 ∞ , if θ is entry of W, ( y n ∞ + ȳ n ∞ )(1 + 3(n − k)∆t r )δ∆t z k−1 ∞ , if θ is entry of W, ( y n ∞ + ȳ n ∞ )(1 + 3(n − k)∆t r )δ∆t u k ∞ , if θ is entry of V, ( y n ∞ + ȳ n ∞ )(1 + 3(n − k)∆t r )δ∆t, if θ is entry of b.<label>(29)</label></formula><p>We will estimate the above term, just for the case of θ is an entry of W, the rest of the terms are very similar to estimate.</p><p>For simplicity of notation, we let k − 1 ≈ k and aim to estimate the term,</p><formula xml:id="formula_38">∂E (k) n ∂θ ≤ y n ∞ y k ∞ (1 + 3(n − k)∆t r )δ∆t + ȳ n ∞ y k ∞ (1 + 3(n − k)∆t r )δ∆t ≤ m √ nk∆t(1 + 3(n − k)∆t r )δ∆t + ȳ n ∞ √ mk √ ∆t(1 + 3(n − k)∆t r )δ∆t (by (5)) ≤ m √ nkδ∆t 2 + 3m √ nk(n − k)δ∆t r+2 + ȳ n ∞ √ mk √ ∆t(1 + 3(n − k)∆t r )δ∆t.</formula><p>(30) To further analyze the above estimate, we recall that n∆t = t n ≤ 1 and consider two different regimes. Let us start by considering short-term dependencies by letting k ≈ n, i.e n − k = c with constant c ∼ O(1), independent of n, k. In this case, a straightforward application of the above assumptions in the bound (30) yields, Next, we consider long-term dependencies by setting k &lt;&lt; n and estimating,</p><formula xml:id="formula_39">∂E (k) n ∂θ ≤ m √ nkδ∆t 2 + 3m √ nk(n − k)δ∆t r+2 + ȳ n ∞ √ mδ∆t 3 2 + 3 ȳ n ∞ √ mnδ∆t r+ 3 2 ≤ m √ t n δ∆t 3 2 + 3mt 3 2 n δ∆t r+ 1 2 + ȳ n ∞ √ mδ∆t 3 2 + 3 ȳ n ∞ √ mt n δ∆t r+ 1 2 ≤ mδ∆t 3 2 + 3mδ∆t r+ 1 2 + ȳ n ∞ √ mδ∆t 3 2 + 3 ȳ n ∞ √ mδ∆t r+ 1 2 (as t n &lt; 1) ≤ 3mδ∆t r+ 1 2 + 3 ȳ n ∞ √ mδ∆t r+ 1 2</formula><p>(as r ≤ 1 and ∆t &lt;&lt; 1).</p><p>(32) Thus, in all cases, we have that,</p><formula xml:id="formula_40">∂E (k) n ∂θ ≤ 3δ∆t m + √ m ȳ n ∞ (as r ≥ 1/2).<label>(33)</label></formula><p>Applying the above estimate in (10) allows us to bound the gradient by,</p><formula xml:id="formula_41">∂E n ∂θ ≤ 1≤k≤n ∂E (k) n ∂θ ≤ 3δt n m + √ m ȳ n ∞ .<label>(34)</label></formula><p>Therefore, the gradient of the loss function <ref type="formula" target="#formula_4">(6)</ref> can be bounded as,</p><formula xml:id="formula_42">∂E ∂θ ≤ 1 N N n=1 ∂E n ∂θ ≤ 3δ m∆t N N n=1 n + √ m∆t N N n=1 ȳ n ∞ n ≤ 3δ m∆t N N n=1 n + √ mȲ ∆t N N n=1 n ≤ 3 2 δ(N + 1)∆t m +Ȳ √ m ≤ 3 2 δ(t N + ∆t) m +Ȳ √ m ≤ 3 2 δ(1 + ∆t) m +Ȳ √ m (as t N = 1) ≤ 3 2 m +Ȳ √ m ,<label>(35)</label></formula><p>which is the desired estimate (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 ON THE ASSUMPTION (8) AND TRAINING</head><p>Note that all the estimates were based on the fact that we were able to choose a time step ∆t in (3) that enforces the condition (8). For any fixed weights W, W, we can indeed choose such a value of to satisfy (8). However, we train the RNN to find the weights that minimize the loss function (6). Can we find a hyperparameter ∆t such that (8) is satisfied at every step of the stochastic gradient descent method for training?</p><p>To investigate this issue, we consider a simple gradient descent method of the form:</p><formula xml:id="formula_43">θ +1 = θ − ζ ∂E ∂θ (θ ).<label>(36)</label></formula><p>Note that ζ is the constant (non-adapted) learning rate. We assume for simplicity that θ 0 = 0 (other choices lead to the addition of a constant). Then, a straightforward estimate on the weight is given by,</p><formula xml:id="formula_44">|θ +1 | ≤ |θ | + ζ ∂E ∂θ (θ ) ≤ |θ | + ζ 3 2 m +Ȳ √ m (by (35)) ≤ |θ 0 | + ζ 3 2 m +Ȳ √ m = ζ 3 2 m +Ȳ √ m .<label>(37)</label></formula><p>In order to calculate the minimum number of steps L in the gradient descent method (36) such that the condition <ref type="formula" target="#formula_6">(8)</ref> is satisfied, we set = L in (37) and applying it to the condition (8) leads to the straightforward estimate,</p><formula xml:id="formula_45">L ≥ 1 ζ 3 2 m +Ȳ √ m m∆t 1−r δ .<label>(38)</label></formula><p>Note that the parameter δ &lt; 1, while in general, the learning rate ζ &lt;&lt; 1. Thus, as long as r ≤ 1, we see that the assumption (8) holds for a large number of steps of the gradient descent method. We remark that the above estimate (38) is a large underestimate on L. In the experiments presented in this article, we are able to take a very large number of training steps, while the gradients remain within a range (see <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 PROOF OF PROPOSITION 3.3</head><p>We start with the following decomposition of the recurrent matrices:</p><formula xml:id="formula_46">∂X i ∂X i−1 = M i−1 + ∆tM i−1 , M i−1 := I ∆tC i−1 B i−1 C i−1 ,M i−1 := B i−1 0 0 0 ,</formula><p>with B, C defined in (12). By the assumption (8), one can readily check that M i−1 ∞ ≤ ∆t, for all k ≤ i ≤ n − 1.</p><p>We will use an induction argument to show the following representation formula for the product of Jacobians, We start by the outermost product and calculate, ∂X n ∂X n−1 ∂X n−1 ∂X n−2 = M n−1 + ∆tM n−1 M n−2 + ∆tM n−2 = M n−1 M n−2 + ∆t(M n−1 M n−2 + M n−1Mn−2 ) + O(∆t 2 ).</p><p>By direct multiplication, we obtain, M n−1 M n−2 = I ∆t (C n−2 + C n−1 C n−2 ) B n−1 + C n−1 B n−2 C n−1 C n−2 + ∆t C n−1 B n−2 0 0 B n−1 C n−2 .</p><p>Using the definitions in <ref type="formula" target="#formula_0">(12)</ref> and <ref type="formula" target="#formula_6">(8)</ref>, we can easily see that C n−1 B n−2 0 0 B n−1 C n−2 = O(∆t).</p><p>Similarly, it is easy to show thatM n−1 M n−2 , M n−1Mn−2 ∼ O(∆t).</p><p>Plugging all the above estimates yields, ∂X n ∂X n−1 ∂X n−1 ∂X n−2 = I ∆t (C n−2 + C n−1 C n−2 ) B n−1 + C n−1 B n−2 C n−1 C n−2 + O(∆t 2 ), which is exactly the form of the leading term (39).</p><p>Iterating the above calculations (n − k) times and realizing that (n − k)∆t 2 ≈ n∆t 2 = t n ∆t yields the formula (39).</p><p>Recall that we have set θ = W i,j , for some 1 ≤ i, j ≤ m in proposition 3.3. Directly calculating with (27), (28) and the representation formula (39) yields the formula, ∂E (k) n ∂θ = y n ∆t 2 δZ i,j m,m (A k−1 )y k−1 + y n ∆t 2 δC * Z i,j m,m (A k−1 )y k−1 + O(∆t 3 ),</p><p>with matrix C * defined as,</p><formula xml:id="formula_48">C * := n−1 j=k k i=j C i ,</formula><p>and Z i,j m,m (A k−1 ) ∈ R m×m is a matrix with all elements are zero except for the (i, j)-th entry which is set to σ (a i k−1 ), i.e. the i-th entry of σ (A k−1 ). Note that the formula (40) can be explicitly written as,</p><formula xml:id="formula_49">∂E (k) n ∂θ = δ∆t 2 σ (a i k−1 )y i n y j k−1 + δ∆t 2 σ (a i k−1 ) m =1 C * i y n y j k−1 + O(∆t 3 ),<label>(41)</label></formula><p>with y j n denoting the j-th element of vector y n , and</p><formula xml:id="formula_50">a i k−1 := m =1 W i y k−1 + m =1 W i z k−1 .<label>(42)</label></formula><p>By the assumption <ref type="formula" target="#formula_6">(8)</ref>, we can readily see that W ∞ , W ∞ ≤ 1 + ∆t.</p><p>Therefore by the fact that σ = sech 2 , the assumption y i k = O( √ t k ) and <ref type="formula" target="#formula_0">(42)</ref>, we obtain,</p><formula xml:id="formula_51">c = sech 2 ( √ k∆t(1 + ∆t) ≤ σ (a k−1 i ) ≤ 1.<label>(43)</label></formula><p>Using <ref type="formula" target="#formula_1">(43)</ref> in <ref type="formula" target="#formula_2">(41)</ref>, we obtain,</p><formula xml:id="formula_52">δ∆t 2 σ (a i k−1 )y i n y j k−1 = O ĉδ∆t 5 2 .<label>(44)</label></formula><p>Using the definition of C i , we can expand the product in C * and neglect terms of order O(∆t 4 ), to obtain k i=j C i = (O(1) + O((j − k + 1)δ∆t 2 ))I.</p><p>Summing over j and using the fact that k &lt;&lt; n, we obtain that C * = (O(n) + O(δ∆t 0 ))I.</p><p>Plugging <ref type="formula" target="#formula_2">(45)</ref> and <ref type="formula" target="#formula_1">(43)</ref>  </p><p>Combining <ref type="formula" target="#formula_2">(44)</ref> and <ref type="formula" target="#formula_2">(46)</ref> yields the desired estimate (16).</p><p>Remark. A careful examination of the above proof reveals that the constants hidden in the prefactors of the leading term O ĉδ∆t 3 2 of (16) stem from the formula (46). Here, we have used the assumption that y i k = O( √ t k ). Note that this assumption implicitly assumes that the energy bound (5) is equidistributed among all the elements of the vector y k and results in the obfuscation of the constants in the leading term of (16). Given that the energy bound (5) is too coarse to allow for precise upper and lower bounds on each individual element of the hidden state vector y k , we do not see any other way of, in general, determining the distribution of energy among individual entries of the hidden state vector. Thus, assuming equidistribution seems reasonable. On the other hand, in practice, one has access to all the terms in formula (46) for each numerical experiment and if one is interested, then one can directly evaluate the precise bound on the leading term of the formula (16). F RIGOROUS ESTIMATES FOR THE RNN (3) WITHn = n − 1 AND GENERAL VALUES OF , γ</p><p>In this section, we will provide rigorous estimates, similar to that of propositions 3.1, E.1 and 3.2 for the version of coRNN (3) that results by settingn = n − 1 in (3) leading to, y n = y n−1 + ∆tz n , z n = z n−1 + ∆tσ (Wy n−1 + Wz n−1 + Vu n + b) − ∆tγy n−1 − ∆t z n−1 .</p><p>Note that (47) can be equivalently written as, y n = y n−1 + ∆tz n , z n = (1 − ∆t) z n−1 + ∆tσ (Wy n−1 + Wz n−1 + Vu n + b) − ∆tγy n−1 .</p><p>We will also consider the case of non-unit values of the control parameters γ and below.</p><p>Bounds on Hidden states. We start the following bound on the hidden states of (47), Proposition F.1 Let the damping parameter &gt; 1 2 and the time step ∆t in the RNN (47) satisfy the following condition,</p><formula xml:id="formula_57">∆t &lt; 2 − 1 γ + 2 .<label>(49)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Results of the adding problem for coRNN, expRNN, FastRNN, anti.sym. RNN and tanh RNN based on three different sequence lengths T , i.e. T = 500, T = 2000 and T = 5000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>during training for all LTD experiments (mean and standard deviation of 10 different runs for each task). Ablation study on the hyperparameters , γ in (3) using the noise padded CIFAR-10 experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>plotted on the x 1 x 2 -plane for a small external</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Exemplary (x 1 , x 2 )-trajectories of the Lorenz 96 system (17) for different forces F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of the hidden state y of coRNN (3) with a scalar input signal u (Top, Middle, Left) with one neuron with state y (Top and Middle, Right) and two neurons with states y 1 (Bottom left), and y 2 (Bottom right), corresponding to scalar input signal, shown in Top Left. Legend is SHO (simple harmonic oscillator), FHO (forced oscillator), FDO (forced and damped oscillator), CFDO (controlled forced and damped oscillator), DUFF (Duffing type) UC (Uncoupled), Ord (ordered coupling) and FC (fully coupled). Legend explained in the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>into (41) leads to,δ∆t 2 σ (a i k−1 ) m =1 C * i y n y j k−1 = O ĉδ∆t 3 2 + O ĉδ 2 ∆t 5 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracies on sMNIST and psMNIST (we provide our own psMNIST result for the FastGRNN, as no official result for this task has been published so far).</figDesc><table><row><cell>Model</cell><cell cols="4">sMNIST psMNIST # units # params</cell></row><row><cell>uRNN (Arjovsky et al., 2016)</cell><cell>95.1%</cell><cell>91.4%</cell><cell>512</cell><cell>9k</cell></row><row><cell>LSTM (Helfrich et al., 2018)</cell><cell>98.9%</cell><cell>92.9%</cell><cell>256</cell><cell>270k</cell></row><row><cell>GRU (Chang et al., 2017)</cell><cell>99.1%</cell><cell>94.1%</cell><cell>256</cell><cell>200k</cell></row><row><cell cols="2">anti.sym. RNN (Chang et al., 2019) 98.0%</cell><cell>95.8%</cell><cell>128</cell><cell>10k</cell></row><row><cell>DTRIV∞ (Casado, 2019)</cell><cell>99.0%</cell><cell>96.8%</cell><cell>512</cell><cell>137k</cell></row><row><cell>FastGRNN (Kusupati et al., 2018)</cell><cell>98.7%</cell><cell>94.8%</cell><cell>128</cell><cell>18k</cell></row><row><cell>coRNN (128 units)</cell><cell>99.3%</cell><cell>96.6%</cell><cell>128</cell><cell>34k</cell></row><row><cell>coRNN (256 units)</cell><cell>99.4%</cell><cell>97.3%</cell><cell>256</cell><cell>134k</cell></row></table><note>Sequential (permuted) MNIST. Sequential MNIST (sMNIST) (Le et al., 2015) is a benchmark for RNNs, in which the model is required to classify an MNIST (LeCun et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test accuracies on noise padded CIFAR-10.</figDesc><table><row><cell>Model</cell><cell cols="3">test accuracy # units # params</cell></row><row><cell>LSTM (Kag et al., 2020)</cell><cell>11.6%</cell><cell>128</cell><cell>64k</cell></row><row><cell>Incremental RNN (Kag et al., 2020)</cell><cell>54.5%</cell><cell>128</cell><cell>12k</cell></row><row><cell>FastRNN (Kag et al., 2020)</cell><cell>45.8%</cell><cell>128</cell><cell>16k</cell></row><row><cell>anti.sym. RNN (Chang et al., 2019)</cell><cell>48.3%</cell><cell>256</cell><cell>36k</cell></row><row><cell cols="2">Gated anti.sym. RNN (Chang et al., 2019) 54.7%</cell><cell>256</cell><cell>37k</cell></row><row><cell>Lipschitz RNN (Erichson et al., 2020)</cell><cell>55.2%</cell><cell>256</cell><cell>134k</cell></row><row><cell>coRNN</cell><cell>59.0%</cell><cell>128</cell><cell>46k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test accuracies on HAR-2.</figDesc><table><row><cell>Model</cell><cell cols="3">test accuracy # units # params</cell></row><row><cell>GRU (Kusupati et al., 2018)</cell><cell>93.6%</cell><cell>75</cell><cell>19k</cell></row><row><cell>LSTM (Kag et al., 2020)</cell><cell>93.7%</cell><cell>64</cell><cell>16k</cell></row><row><cell>FastRNN (Kusupati et al., 2018)</cell><cell>94.5%</cell><cell>80</cell><cell>7k</cell></row><row><cell>FastGRNN (Kusupati et al., 2018)</cell><cell>95.6%</cell><cell>80</cell><cell>7k</cell></row><row><cell>anti.sym. RNN (Kag et al., 2020)</cell><cell>93.2%</cell><cell>120</cell><cell>8k</cell></row><row><cell cols="2">incremental RNN (Kag et al., 2020) 96.3%</cell><cell>64</cell><cell>4k</cell></row><row><cell>coRNN</cell><cell>97.2%</cell><cell>64</cell><cell>9k</cell></row><row><cell>tiny coRNN</cell><cell>96.5%</cell><cell>20</cell><cell>1k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test accuracies on IMDB.</figDesc><table><row><cell>Model</cell><cell cols="3">test accuracy # units # params</cell></row><row><cell>LSTM (Campos et al., 2018)</cell><cell>86.8%</cell><cell>128</cell><cell>220k</cell></row><row><cell>Skip LSTM</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>on the time step ∆t and weight matrices W, W hold. Clearly one can choose a suitable ∆t to enforce (8) before training, but do these assumptions remain valid during training? In SM §E.4, we argue, based on worst-case estimates, that the assumptions will remain valid for possibly a large number of training steps. More pertinently, we can verify experimentally that (8) holds during training. This is demonstrated in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Distributional information (mean and standard deviation) on the results for each classification experiment presented in the paper based on 10 re-trainings of the best performing coRNN using random initialization of the trainable parameters.</figDesc><table><row><cell>Experiment</cell><cell>Mean</cell><cell>Standard deviation</cell></row><row><cell>sMNIST (256 units)</cell><cell cols="2">99.17% 0.07%</cell></row><row><cell>psMNIST (256 units)</cell><cell cols="2">96.10% 1.20%</cell></row><row><cell cols="3">Noise padded CIFAR-10 58.56% 0.35%</cell></row><row><cell>HAR-2 (64 units)</cell><cell cols="2">96.01% 0.53%</cell></row><row><cell>IMDB</cell><cell cols="2">86.65% 0.31%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>for RNNs. Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory tasks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 2034-2042, 2016.</figDesc><table><row><cell>Supplementary Material for:</cell></row><row><cell>Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable</cell></row><row><cell>architecture for learning long time dependencies</cell></row><row><cell>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): A CHAOTIC TIME-SERIES PREDICTION.</cell></row><row><cell>1735-1780, 1997.</cell></row><row><cell>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by</cell></row><row><cell>reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine</cell></row><row><cell>Learning, ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 448-456.</cell></row><row><cell>JMLR.org, 2015.</cell></row><row><cell>Anil Kag, Ziming Zhang, and Venkatesh Saligrama. Rnns incrementally evolving on an equilibrium</cell></row><row><cell>manifold: A panacea for vanishing and exploding gradients? In 8th International Conference on</cell></row><row><cell>Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.</cell></row><row><cell>Jun 2019.</cell></row><row><cell>PMLR.</cell></row><row><cell>Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network</cell></row><row><cell>(indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE Conference on Computer</cell></row><row><cell>Vision and Pattern Recognition, pp. 5457-5466, 2018.</cell></row><row><cell>Edward N Lorenz. Predictability: A problem partly solved. In Proc. Seminar on Predictability,</cell></row><row><cell>volume 1, 1996.</cell></row><row><cell>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher</cell></row><row><cell>Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting</cell></row><row><cell>of the Association for Computational Linguistics: Human Language Technologies, volume 1, pp.</cell></row><row><cell>142-150. Association for Computational Linguistics, 2011.</cell></row></table><note>Giancarlo Kerg, Kyle Goyette, Maximilian Puelma Touzel, Gauthier Gidel, Eugene Vorontsov, Yoshua Bengio, and Guillaume Lajoie. Non-normal recurrent neural network (nnrnn): learning long time dependencies while improving expressivity with transient dynamics. In Advances in Neural Information Processing Systems, pp. 13591-13601, 2019. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma. Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In Advances in Neural Information Processing Systems, pp. 9017-9028, 2018. Thomas Laurent and James von Brecht. A recurrent neural network without chaos. In 5th Interna- tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. Quoc V Le, Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent networks of rectified linear units. arXiv preprint arXiv:1504.00941, 2015.Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436-444, 2015. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. Mario Lezcano-Casado and David Martínez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. volume 97 of Proceedings of Machine Learning Research, pp. 3794-3803, Long Beach, California, USA, 09-15W. Maass. Fast sigmoidal networks via spiking neurons. Neural Computation, 9:279-304, 2001. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on International Conference on Machine Learning, volume 28 of ICML'13, pp. III-1310-III-1318. JMLR.org, 2013.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Test NRMSE on the Lorenz 96 system (17) for coRNN and LSTM.The IMDB task was conducted on an NVIDIA GeForce GTX 1080 Ti GPU, while all other experiments were run on a Intel Xeon E3-1585Lv5 CPU. The weights and biases of coRNN are randomly initialized according to U(− 1</figDesc><table><row><cell>Model</cell><cell>F = 0.9</cell><cell>F = 8</cell><cell cols="2"># units # params</cell></row><row><cell cols="4">LSTM coRNN 2.0 × 10 −2 9.8 × 10 −2 64 2.0 × 10 −2 6.8 × 10 −2 44</cell><cell>9k 9k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Setting for the hyperparameter optimization of coRNN. Intervals denote ranges of the corresponding hyperparameter for the grid search algorithm, while fixed numbers mean that no hyperparameter optimization was done in this case. Noise padded CIFAR-10 [10 −4 , 10 −1 ] 100</figDesc><table><row><cell>task</cell><cell cols="2">learning rate batch size ∆t</cell><cell>γ</cell></row><row><cell>Adding sMNIST (n hid = 128)</cell><cell cols="3">50 [10 [10 −2 , 10 −1 ] [1, 100] [10 −2 , 10 −1 ] [1, 100] 2 × 10 −2</cell><cell>[1, 100] [1, 100]</cell></row><row><cell>HAR-2</cell><cell>[10 −4 , 10 −1 ] 64</cell><cell cols="2">[10 −2 , 10 −1 ] [10 −1 , 10] [10 −1 , 10]</cell></row><row><cell>IMDB</cell><cell>[10 −4 , 10 −1 ] 64</cell><cell cols="2">[10 −2 , 10 −1 ] [10 −1 , 10] [10 −1 , 10]</cell></row></table><note>−4 , 10 −1 ] 120 [10 −2 , 10 −1 ] [10 −1 , 10] [10 −1 , 10] sMNIST (n hid = 256) [10 −4 , 10 −1 ] 120 [10 −2 , 10 −1 ] [10 −1 , 10] [10 −1 , 10] psMNIST (n hid = 128) [10 −4 , 10 −1 ] 120 [10 −2 , 10 −1 ] [10 −1 , 10] [10 −1 , 10] psMNIST (n hid = 256) [10 −4 , 10 −1 ] 120 [10 −2 , 10 −1 ] [10 −1 , 10] [10 −1 , 10]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Rounded hyperparameters of the best performing coRNN architecture.</figDesc><table><row><cell>task</cell><cell>learning rate batch size ∆t</cell><cell>γ</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂E</head><p>≤ mt n δ∆t + mct n δ∆t r+1 + ȳ n ∞ √ m √ t n δ∆t + ȳ n ∞ √ m √ t n cδ∆t r+1 ≤ t n mδ∆t + ȳ n ∞ √ m √ t n δ∆t (for ∆t &lt;&lt; 1 as r ≥ 1/2) ≤ mδ∆t + ȳ n ∞ √ mδ∆t.</p><p>Let y n , z n be the hidden states of the RNN (47) for 1 ≤ n ≤ N , then the hidden states satisfy the following (energy) bounds:</p><p>We set A n−1 = Wy n−1 + Wz n−1 + Vu n−1 + b and as in the proof of proposition 3.1, we multiply (y n−1 , 1 γ z n ) to (47) and use elementary identities and rearrange terms to obtain, y n y n 2 + z n z n 2γ = y n−1 y n−1 2 + z n−1 z n−1 2γ + (y n − y n−1 ) (y n − y n−1 ) 2</p><p>We use a rescaled version of the well-known Cauchy's inequality</p><p>for a constant c &gt; 0 to be determined, to rewrite the above identity as, y n y n 2 + z n z n 2γ ≤ y n−1 y n−1 2 + z n−1 z n−1 2γ + (y n − y n−1 ) (y n − y n−1 ) 2</p><p>Using the first equation in (47), the above inequality reduces to,</p><p>As long as,</p><p>we can easily check that,</p><p>Iterating the above bound till n = 0 and using the zero initial data yields the desired (50) as long as we find a c such that the condition <ref type="formula">(51)</ref> is satisfied. To do so, we equalize the two terms on the right hand side of (51) to obtain,</p><p>From the assumption (49) and the fact that &gt; 1 2 , we see that such a c &gt; 0 always exists for any value of γ &gt; 0 and <ref type="formula">(51)</ref> is satisfied, which completes the proof.</p><p>We remark that the same bound on the hidden states is obtained for both versions of coRNN, i.e. (3) withn = n and (47). However, the difference lies in the constraint on the time step ∆t. In contrast to (49), a careful examination of the proof of proposition 3.1 reveals that the condition on the time step for the stability of (3) withn = n is given by,</p><p>and is clearly less stringent than the condition (51) for the stability of (47). For instance, in the prototypical case of γ = = 1, the stability of (3) withn = n is ensured for any ∆t &lt; 1. On the other hand, the stability of (47) is ensured as long as ∆t &lt; 1 2 . However, it is essential to recall that these conditions are only sufficient to ensure stability and are by no means necessary. Thus in practice, the coRNN version (47) is found to be stable in the same range of time steps as the version (3) withn = n.</p><p>On the exploding and vanishing gradient problems for coRNN (47) Next, we have the following upper bound on the hidden state gradients for the version (47) of coRNN, Proposition F.2 Let y n , z n be the hidden states generated by the RNN (47). We assume that the damping parameter &gt; 1 2 and the time step ∆t can be chosen such that in addition to (51) it also satisfies,</p><p>and with the constantC independent of the other parameters of the RNN (47). Then the gradient of the loss function E (6) with respect to any parameter θ ∈ Θ is bounded as,</p><p>with the constantC, defined in (53) andȲ = max 1≤n≤N ȳ n ∞ be a bound on the underlying training data Published as a conference paper at ICLR 2021</p><p>The proof of this proposition is completely analogous to the proof of proposition 3.2 and we omit the details here.</p><p>Note that the bound (54) enforces that hidden state gradients cannot explode for version <ref type="formula">(47)</ref>  </p><p>The proof is a repetition of the steps of the proof of proposition 3.3, with suitable modifications for the structure of the RNN and non-unit , γ and we omit the tedious calculations here. Note that (55) rules out the vanishing gradient problem for the coRNN version (47).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge L Reyes-Ortiz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Ambient Assisted Living</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skip RNN: learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giró-I-Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trivializations for gradient-based optimization on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casado</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9154" to="9164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Antisymmetricrnn: A dynamical system view on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Symplectic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gate-variants of gated recurrent unit (gru) neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salemt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1597" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>N Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12070</idno>
		<title level="m">Lipschitz recurrent neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hamiltonian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misko</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15379" to="15389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Echo state networks are universal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyudmila</forename><surname>Grigoryeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Pablo</forename><surname>Ortega</surname></persName>
		</author>
		<idno>0893-6080</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="495" to="508" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nonlinear oscillations, dynamical systems, and bifurcations of vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guckenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local and global self-entrainment in oscillator lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shinomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuramoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress of Theoretical Physics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1005" to="1010" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Orthogonal recurrent neural networks with scaled cayley transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Helfrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1969" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5320" to="5330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neurons as oscillators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Stiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Ermentrout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="2950" to="2960" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Nonlinear Dynamics and Chaos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strogatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Westview, Boulder CO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Biological rhythms and the behavior of populations of coupled oscillators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Winfree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical Biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="15" to="42" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
