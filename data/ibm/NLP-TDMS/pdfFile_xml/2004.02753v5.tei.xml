<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally Coherent Embeddings for Self-Supervised Video Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Knights</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Autonomous Systems</orgName>
								<address>
									<postCode>Data61 CSIRO, 4069</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Autonomous Systems</orgName>
								<address>
									<postCode>Data61 CSIRO, 4069</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ward</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Autonomous Systems</orgName>
								<address>
									<postCode>Data61 CSIRO, 4069</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Vanderkop</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Autonomous Systems</orgName>
								<address>
									<postCode>Data61 CSIRO, 4069</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Mackenzie-Ross</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Autonomous Systems</orgName>
								<address>
									<postCode>Data61 CSIRO, 4069</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Moghadam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics and Autonomous Systems</orgName>
								<address>
									<postCode>Data61 CSIRO, 4069</postCode>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally Coherent Embeddings for Self-Supervised Video Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Random Initialisation (b) TCE Epoch 25 (c) TCE Epoch 50 Fig. 1: TCE: Temporally Coherent Embeddings of frames from a single video visualised using t-SNE, pre-trained on Kinetics400.</p><p>Abstract-This paper presents TCE: Temporally Coherent Embeddings for self-supervised video representation learning. The proposed method exploits inherent structure of unlabeled video data to explicitly enforce temporal coherency in the embedding space, rather than indirectly learning it through ranking or predictive proxy tasks. In the same way that high-level visual information in the world changes smoothly, we believe that nearby frames in learned representations will benefit from demonstrating similar properties. Using this assumption, we train our TCE model to encode videos such that adjacent frames exist close to each other and videos are separated from one another. Using TCE we learn robust representations from large quantities of unlabeled video data. We thoroughly analyse and evaluate our self-supervised learned TCE models on a downstream task of video action recognition using multiple challenging benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN backbone and only RGB stream inputs, TCE pre-trained representations outperform all previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code and pre-trained models for this paper can be downloaded at: https://github.com/csiro-robotics/TCE</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Self-Supervised Learning (SSL) is a new and promising paradigm. In SSL, a model is trained on unlabeled data and supervised with a learning signal constructed from inherent structure in the training samples. Robust representations can be learned from enormous amounts of unlabeled data using SSL. These methods are often pre-trained on large unlabeled datasets with specific upstream (i.e., proxy or pretext) tasks and then fine-tuned to adapt to specific downstream tasks.</p><p>Video data is an appealing datatype as many image level proxy tasks are applicable and the inherent temporal signal provides an additional source of supervision. Leveraging the temporal aspect of video data, many methods attempt to predict future frames <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. However, these methods suffer from wasted model capacity attempting to reconstruct information such as texture or lighting changes which are not transferable to many downstream tasks, e.g. action recognition. Other methods have avoided the need for pixel level reconstruction by predicting latent representations of future frames <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Following the belief that spatiotemporal convolutions in 3D-CNNs are vital for video processing <ref type="bibr" target="#b6">[7]</ref>, many of the best performing methods for learning from video data rely on 3D-CNN architectures and hence, require large amounts of data and computing resources for training. Ranking methods avoid reconstruction entirely by instead solving proxy tasks aimed at recovering the temporal frame ordering <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Inspired by recent advances in self-supervised learning in the image domain <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> we introduce Temporally Coherent Embeddings (TCE), a spatiotemporal approach to self-supervised learning. Our approach is driven by a novel objective function designed to learn a temporally coherent embedding space from unlabeled videos as seen in <ref type="figure">Figure 1</ref>. Our temporal coherency objective enables a network to learn a temporally coherent embedding space where adjacent frames exist close to each other along a smooth trajectory and distinct arXiv:2004.02753v5 [cs.CV] 17 Nov 2020 videos are embedded with separation between them. By explicitly optimising for temporal coherency in the embedding space using a 2D-CNN, we avoid the challenges of pixel level reconstruction losses and the computationally expensive 3D-CNN architectures. We argue that equivalent performance can be achieved using 2D-CNNs and an adequately defined loss function. This results in faster training, lower resource requirements and faster inference for our method. We also believe that our temporal coherency objective will better generalise and subsequently translate to downstream tasks when compared to other upstream proxy tasks, such as frame ranking. By encouraging the formation of a structured, intuitive and temporally coherent learned representation we demonstrate strong results on the downstream task of action recognition when compared to other methods. We summarise our contributions in this paper as follows:</p><p>• We propose a novel temporal coherency objective for selfsupervised video representation learning. This objective exploits the structure of video data to explicitly enforce temporal coherency in the embedding, rather than indirectly learning it through ranking or predictive tasks. • We quantitatively evaluate the effectiveness of our temporal coherency objective at encoding spatiotemporal information both by comparing RGB and stack-of-differences inputs and through the exploration of a higher order objective • We demonstrate that, contrary to the current direction of the field, our 2D-CNNs can learn robust spatiotemporal embeddings for downstream action recognition tasks without relying on data and compute hungry 3D-CNNs. • We formulate a novel approach for semi-hard mining of negative samples during self-supervised learning on large datasets and apply this method when training on Kinetics400. • TCE outperforms all previous state-of-the-art self-supervised action recognition methods that have been pretrained on UCF101. For methods pre-trained on Kinet-ics400, our approach also outperforms all but one of the competing methods on UCF101 and achieves state-ofthe-art for HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Our approach relates to contrastive SSL methods which build representations by modelling the differences and similarities between two or more inputs. Here the supervisory signal is obtained by contrastively ranking similar (positive) samples with respect to negative examples. The key differences are that the loss is computed at a feature level and negative examples are required to contrast against. Common image examples include contrasting between different view points <ref type="bibr" target="#b15">[16]</ref>, data modalities <ref type="bibr" target="#b16">[17]</ref>. and colour spaces <ref type="bibr" target="#b16">[17]</ref>.</p><p>Other work, more closely related to ours, learns from the temporal dynamics of video data. Many approaches attempt to generate future video frames <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> but suffer from the aforementioned pixel level reconstruction loss limitations <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b18">[19]</ref> use geometric information such as optical flow and left-right disparity to pre-train the network backbone. Others classified video sequences as having the correct frame ordering <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> or sorting the frames. Of the sorting methods, those employing 3D-CNNs <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b4">[5]</ref> require significantly more computational resources but outperform their 2D-CNN counterparts <ref type="bibr" target="#b10">[11]</ref>. Many recent works leverage 3D-CNNs which are pre-trained on the much larger Kinetics400 dataset. Contrary to prior 3D-CNN works, TCE demonstrates that competitive spatiotemporal features can be learned using 2D-CNNs when optimising for temporal coherency in the embedding space.</p><p>Avoiding computationally expensive 3D-CNNs, complex frame sorting and pixel level loss limitations, we explicitly optimise for a representation which enforces cosine similarity between frames by contrastively clustering the embeddings of neighbouring frames. This results in a temporally coherent embedding space ( <ref type="figure" target="#fig_0">Fig 2)</ref> where different videos are separated and individual frames are attracted. Temporal coherency was explored by <ref type="bibr" target="#b19">[20]</ref> as an auxiliary signal for learning from videos. However, their formulation only utilises a triplet loss and operates in a semi-supervised setting, whereas we use a combination of contrastive clustering with many negatives and a semi-hard negative mining curriculum to extend our approach to learning in a totally self-supervised manner. This approach is motivated by recent results which achieve performance improvements proportional to the number of negatives used <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Other works have also explored coherency in terms of frame correspondences. <ref type="bibr" target="#b21">[22]</ref> solve a pairwise sequence matching problem using triplet loss and bounding box annotations for pose estimation in video. Wang et al. <ref type="bibr" target="#b22">[23]</ref> uses a triplet loss formulation to learn patchwise frame correspondences. This work, to the best of our knowledge, is the first to utilise temporal coherency as the only supervisory signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We propose a simple, yet effective framework to learn a temporally coherent embedding space from unlabeled videos. The mathematical formulation of our method, TCE: Temporally Coherent Embeddings for self-supervised video representation learning, is explained in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal Coherency Training</head><p>The goal of our proposed method, TCE, is to explicitly enforce coherency in the embedding space by encouraging similarity in the embeddings of temporally adjacent frames from a video without any labels.</p><p>To develop representations which are coherent in time, we seek to learn an embedding function f (.) which transforms a video frame x i t from pixel-space into a lower dimensional embedding space. We adopt the shorthand notation f (x i t ) := f i t for the transformed frame. We define temporal coherency as minimisation of the temporal derivatives of the representations in embedding space. First-order temporal coherency in the embedding space is thus achieved when ∂f t /∂t ≈ 0. To the first order, temporal coherency is achieved by maximising a similarity function s(f i t+1 , f i t ) between two temporally adjacent frames in the same video, as ∂f t /∂t ∝ f t+1 − f t . For this purpose, we use the cosine similarity function. A trivial solution to this optimisation goal is apparent: an embedding function which simply maps all inputs to the same point in the embedding space. To avoid this trivial solution, we define a loss formulation which enforces high cosine similarity between the embeddings of neighbouring frames while enforcing low cosine similarity against a set of negative frame embeddings.</p><p>Consider a video dataset where each video</p><formula xml:id="formula_0">V i contains T i frames x i 1 , x i 2 , ..., x i T i .</formula><p>We consider a pair of temporally neighbouring frames from one video as positive examples, and consider all frames from other videos to be negative examples.</p><p>We sample these negative examples to form a set N containing N frames. We adopt a standard cross-entropy loss in Equation 1 which is minimised when s(f t , f t+1 ) is large and s(f t , f n ) is small for all x n ∈ N .</p><formula xml:id="formula_1">L 1 st (x i t+1 , x i t , N ) = −E log e s(ft,ft+1) e s1(ft,ft+1) + N e s(ft,fn)<label>(1)</label></formula><p>Minimising this loss function is analogous to training a classifier to correctly select the positive example from all negative examples in N . This first order coherency objective will encourage neighbouring video frames to cluster in representation space because it penalises large distances between frames in the embedding space. Additional temporal structure in the embeddings can also be captured through higher order coherency. In Section V-C we explore the use of a secondorder temporal coherency objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Leveraging Multiple Negative Examples with NCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For large numbers of negative examples, calculating the normalization factor for the full softmax distribution in Equation 1</head><p>can prove computationally intractable <ref type="bibr" target="#b23">[24]</ref>. Noise Contrastive Estimation (NCE) <ref type="bibr" target="#b24">[25]</ref> is a computationally efficient means of estimating unnormalised statistical models and performing logistic regression to discriminate between observed data and a noise distribution. In this case, discriminating between the positive and negative examples.</p><p>The NCE based approximation of the optimization goal of the model can be adapted from Equation 1 as</p><formula xml:id="formula_2">L N CE = −E x,xp [log P (C|x; x p )] + N . E xn∈N log P (C|x; x n ) (2)</formula><p>where P (C|x; x n ) = 1 − P (C|x; x n ) is the probability of correctly classifying a sample from a uniform noise distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Annealing as a Semi-Hard Mining Curriculum</head><p>While increasing the number of negative examples can lead to improved performance on a number of tasks, the vanishing gradient problem is still a concern when training on larger datasets that contain a broad range of visually distinct videos. In these training environments, randomly sampling negative frames will result in a reduced rate of learning once the majority of negatives have been address by the loss function.</p><p>Drawing both from annealing methods and semi-hard mining approaches in the supervised domain <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> we formulate a novel approach for negative selection during selfsupervised learning on large datasets. Our goal is to define a fine-grained online training curriculum that mines increasingly more challenging negatives in order to continuously produce useful training gradients for self-supervised learning. As such, we define hyperspherical boundaries that are centred on each embedded video frame and have decaying radii given by:</p><formula xml:id="formula_3">r (t) = r 0 + (r E − r 0 ) 1 − e − 5t E (3)</formula><p>with t a fractional representation of the current progress through training epochs, r 0 the initial radius at epoch 0 and r E the radius at final epoch E. When selecting negative examples using these boundaries we begin by selecting the closest negatives that are embedded outside of a particular hypersphere. If during the early epochs of training there are fewer mined negatives than the amount required for training, then the remainder are selected with random sampling inside the hypersphere. This approach also addresses the potential drawback of sampling negatives from highly similar videos in the dataset, as frames from said videos would not be considered until very late in the training process if at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In this section we describe the specific experimental parameters that we have applied in generating the results we present in Section V below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Augmentation</head><p>In this paper, our pre-training and evaluation is focused on video action recognition datasets UCF101 <ref type="bibr" target="#b27">[28]</ref>, HMDB51 <ref type="bibr" target="#b28">[29]</ref> and Kinetics400 <ref type="bibr" target="#b29">[30]</ref>. UCF101 contains 13K videos split between 101 action classes, and is used for pre-training several of our ablation studies. HMDB51 contains 7K videos split between 51 action classes and Kinetics400 contains 306K videos split between 400 action classes. During pre-training we randomly crop a 224×224 window from each video frame as an input to a network. We augment these inputs by applying random crop, random horizontal flip, random grey and color jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>We use 2D ResNet-18 <ref type="bibr" target="#b30">[31]</ref> as the backbone for our network architecture unless otherwise stated for our experiments. The final convolutional layer of the network is flattened and passed through a single fully-connected layer to produce a 128dimensional feature vector for the network output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-Supervised Pre-training</head><p>For our UCF101 experiments, we initialise our networks with random weights and train for a total of E = 9 epochs on 4 Tesla-V100 GPUs. Here one epoch consists of every frame in UCF101 being used as an anchor example. For our Kinetics400 experiments we instead train for E = 50 epochs, however for these epochs we are only using a single randomly selected anchor from each pre-training video. All pre-training is performed using a stochastic gradient descent optimiser and a batch size of 100. We set an initial learning rate of 0.03 and then reduce it by a factor of 10 after 5 epochs for UCF101 and 25 epochs for Kinetics400. In addition to the data augmentation described above, we follow <ref type="bibr" target="#b31">[32]</ref> in pre-training with a rotation auxiliary task to predict a rotation angle from the set [0 • , 90 • , 180 • , 270 • ] to avoid trivial solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Negative Selection</head><p>All of our experiments are run using N = 8192 negative samples for optimising our first order coherency objective. We maintain a memory bank to store the embedded representation of each frame in the dataset for UFC101 and for the most recent anchor in each video for Kinetics400. This allows us to efficiently retrieve noisy samples without re-computing their embeddings. As such, the memory bank is dynamically updated with the new anchor embeddings on every forward pass of the network. Due to our use of cosine similarity, our semi-hard mining limits from Equation 3 are set to r 0 = −1 and r E = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation on Action Recognition</head><p>After self-supervised pre-training, we evaluate our models by their performance on the downstream tasks of action recognition on UCF101 and HMDB51. In general we report top-1 accuracy for the downstream task on the first testing split of the dataset. However, for our comparison with stateof-the-art in <ref type="table" target="#tab_6">Table V</ref> we report the average accuracy over three splits. We replace the final fully-connected layer of our pretrained networks with a new fully-connected layer that has a dimensionality equal to the number of classes in the evaluation dataset. The network is then fine-tuned using 4 Tesla-V100 GPUs and stochastic gradient descent for 600 epochs, with a learning rate of 0.05 decayed by a factor of 0.1 at 375 epochs. For networks pre-trained on Kinetics400 we fine-tune for an additional 300 epochs, decaying the learning rate again at 600 epochs, and for our HMDB51 results we use a dropout of 0.9 for the fully connected layer. During fine-tuning, each video is divided into three equal portions and a frame is randomly sampled from each for passing through the network. The average output feature of these three frames is then used as the final prediction. Then during evaluation, nineteen evenly spaced frames are sampled from each video and the softmax averaged output features from these frames is then used to determine the network's prediction. Our reported results are taken from the highest scoring epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND ANALYSIS</head><p>In this section we evaluate and analyse the performance of our proposed method in order to better understand what role temporal consistency plays in both forming embeddings and for downstream action recognition tasks 1 . In Section V-A we demonstrate the importance of using an auxiliary task to avoid learning trivial solutions during self-supervised learning. Then Section V-B evaluates the effectiveness of our method in encoding the spatiotemporal context of a scene from single input frames. Section V-C explores the effects of using a higher order loss function both in terms of forming a temporally coherent embedding space and for downstream action recognition. In Section V-D we demonstrate the advantages and challenges of pre-training with larger and more diverse datasets. Then in Section V-E we compare the downstream   performance of our method with competing state-of-the-art methods. Finally in Section V-F we present low dimensional mappings of several learned embedding spaces in order to visually inspect the structure given by our temporal coherency objective.</p><p>A. Impact of Auxiliary <ref type="table" target="#tab_1">Task   Table I</ref> details the performance achieved by fine-tuned networks on an action recognition task when starting from different baselines. Our self-supervised pre-training shows a dramatic improvement of 21% over random initialization. Furthermore, the addition of the auxiliary task to the selfsupervised pre-training provides further improvement of 5.9% to the performance on the downstream task. This demonstrates the susceptibility for our self-supervised learning method to learn trivial solutions such as optical flow during the pretraining, reducing the transferability of the leaned embeddings to other downstream tasks. By adding the rotation sub-task, we restrict the embedding space from being able to learn these trivial solutions and see an additional boost to downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Input</head><p>Several previous works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> have leveraged the advantages of using a stack-of-differences input in order to capture the temporal dynamics of the video during finetuning and classification. In <ref type="table" target="#tab_1">Table II</ref> we compare our baseline method to one using the stack-of-differences approach for the downstream action recognition task. For our method, stackof-differences yields a 2.3% performance improvement. This is substantially smaller than the roughly 10% improvement seen for other methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. This demonstrates that our coherency objective has already encoded these temporal dynamics into the embedding space, and so the application of stack-of-differences inputs has a reduced benefit for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Temporal Coherency and Higher Order Objectives</head><p>An appropriate measure is required for us to quantitatively evaluate the effectiveness of our methodology at forming temporally coherent embeddings of video frame sequences. As such we apply two methods for measuring how a discretely sampled curve deviates from a linear trajectory. For a video V containing a sequence of T frame embeddings {x 1 , ..., x T } ∈ X the Total Absolute Curvature (TAC) is given by</p><formula xml:id="formula_4">T AC = T −1 i=2 arccos (x i − x i−1 ).(x i+1 − x i ) ||x i − x i−1 || ||x i+1 − x i ||<label>(4)</label></formula><p>and we also define the Maximum Absolute Curvature (MAC) to be</p><formula xml:id="formula_5">M AC = max i=2,...,T −1 arccos (x i − x i−1 ).(x i+1 − x i ) ||x i − x i−1 || ||x i+1 − x i || .<label>(5)</label></formula><p>Here we also explore extending our coherency objective to second-order, so that the optimization goal is changed from clustering temporally adjacent frame embeddings to also clustering the differences between those embeddings. Clustering the differences between embeddings should ensure that the trajectory of video embeddings does not significantly vary over short time periods. Our second-order temporal coherency objective is given as</p><formula xml:id="formula_6">L 2 nd = −E log e s2(ft,ft+1,ft+2) e s2(ft,ft+1,ft+2) + N2 e s2(ft,ft+1,fn)<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">s 2 (f i , f i+1 , f i+2 ) = s(f i+1 −f i , f i+2 −f i+1 )</formula><p>. Importantly, the negative examples used in calculating the secondorder cross-entropy loss are sampled within the same video as the positives. This is because we believe that sampling negative examples from other videos does not provide sufficiently difficult negative examples to learn from. <ref type="figure" target="#fig_1">Figure 3</ref> presents TAC and MAC measures across selfsupervised pre-training runs using our proposed loss functions. Each data point is an average of the TACs or MACs computed on 375 videos sampled evenly across all classes of the UCF101 test set. We report results for the first order loss and a combined loss using the first and second order losses in a 5 : 1 weight ratio. The TAC plot shows that the addition of a second order loss acts to continuously reduce the curvature of the video trajectories. As such, the combined higher order loss is more effective at improving the temporal coherency of the video embeddings. However, both sets of MAC results decrease throughout the self-supervised pre-training. This indicates that the least temporally coherent frames within each video are still being moved towards a consistent trajectory, even when only the first order loss is applied.</p><p>Lastly in <ref type="table" target="#tab_1">Table III</ref> we evaluate these trained embedding spaces on the downstream action recognition task. Here we see that the addition of a higher order loss results in minor decreased performance on the downstream task. From this we conclude that while our proposed loss functions are successfully forming embedding spaces with increasing temporal coherency, this property is not a perfect proxy for the downstream task being evaluated. And as such, it is possible to push the temporal coherency constraints too far in the context of action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method UCF101</head><p>First Order Only 68.2 First + Second Order 66.88  In <ref type="table" target="#tab_1">Table IV</ref> we demonstrates the scalability of TCE to larger datasets with results on Kinetics400. However a naive transition to the large dataset results in reduced performance on the downstream action recognition task. Then with the addition of our semi-hard mining method from Section III-C, we are instead able to achieve an improvement in performance. This result emphasises the importance of adaptive negative sampling when scaling to larger datasets, and suggests that the vanishing gradient problem is relevant to our temporal coherence loss. Lastly, we achieve a significant performance improvement when we move from pre-training on 2D ResNet-18 to 2D ResNet-50. This result reconfirms the findings of <ref type="bibr" target="#b32">[33]</ref> in that higher-capacity networks are more suited for learning from large datasets. However, it also demonstrates similar performance is achievable based on 2D-CNN SSL solutions on spatiotemporal datasets without relying on the additional representation capacity of 3D convolutions. This gives our approach several inherent advantages in terms of pre-training time, computational resources required and inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to State-of-the-Art</head><p>In <ref type="table" target="#tab_6">Table V</ref> we compare the results of our pre-training method against other state-of-the-art results. Unless noted otherwise the results in this table are averaged over all three train-test splits of the UCF101 dataset, unlike our previous experiments which are performed only on the first split. When considering only self-supervised pre-training on UCF101, TCE outperforms all other methods including several that utilise the additional capacity and have the additional inference times associated with 3D-CNN backbones. Then in the context of self-supervised pre-training on Kinetics400, TCE outperforms all other methods except for DPC <ref type="bibr" target="#b4">[5]</ref> when evaluating on UCF101 and our method sets a new state-of-the-art performance for HMDB51. It is worth noting that the results from DPC are generated using a much higher capacity network and took six weeks to train, while our method is implemented on a smaller capacity network and was trained for only 72 hours. In addition to increased pre-training time, using larger networks also typically result in longer inference times. The same comparisons can be made to a lesser degree between our Kinetics400 results using different backbones. Based on our strong results, we argue that contrary to the current direction of the field, 2D-CNNs are still capable of learning sufficiently complex spatiotemporal embeddings for many downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visual Inspection of Temporal Coherency</head><p>In this section we qualitatively evaluate the effectiveness of TCE in creating a temporally coherent embedding space. We visualise our embedding spaces by using t-SNE to learn a nonlinear dimension reduction of frame embeddings from a single video. This video was selected from the Bowling class of the UCF101 validation set due to the visually distinct actions of back swinging, releasing the ball, following the ball and the pins falling. Once reduced to two dimensions, we plot the resulting values such that each point represents a single video frame. We apply a color map to show the temporal order of frames within the embedded video. <ref type="figure">Figure 1</ref>, seen at the top of this paper, visualises the evolution of our embedding space over the course of unsupervised training. This figure reinforces that our training is continuously increasing the temporal coherency of the embedding space across epochs; from negligible coherency for random weights, to partial coherency at 25 epochs and then very strong coherency at 50 epochs. This result supports our quantitative analysis in Section V-C that demonstrated that we are continuously increasing the temporal coherency through a decreasing maximum absolute curvature.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref> we compare visualisations of our embedding space against an ImageNet pre-trained checkpoint and the DPC context representation <ref type="bibr" target="#b4">[5]</ref>. For DPC, five consecutive video   blocks were sampled and then extracted as feature maps. These feature maps were then aggregated into context representation and finally pooled into vectors. From visual inspection of the three methods, we see that our method is the only one to form a coherent path with no major discontinuities. This suggests that our method is generating embeddings with greater temporal coherency than both ImageNet and DPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we have presented TCE: Temporally Coherent Embeddings, an effective 2D-CNN approach for selfsupervised representation learning of spatiotemporal features from videos. We train our model in a self-supervised manner by leveraging the temporal information embedded in video data and enforcing coherency in the embedding space. Our first order temporal coherency objective encourages the relative attraction and separation of an anchor frame, an adjacent frame and N negative frames sampled from other videos. Addition-ally, we explored measures for evaluating the effectiveness of our coherency objective and its extension to a second order objective that can further increase the temporal coherence of our learned embeddings.</p><p>We evaluated our learned representations on a downstream action recognition task using multiple challenging benchmarks (Kinetics400, UCF101, HMDB51). With these we have empirically demonstrated the importance of auxiliary tasks for avoiding the learning of trivial solutions and the importance of semi-hard mining for effective negative selection when learning from larger datasets using our temporal coherency objective. In combination these components resulted in our method outperforming all previous state-of-the-art 2D-CNN and 3D-CNN self-supervised learning for action recognition methods that have been pre-trained on UCF101, by at least 3.3% with UCF101 and 2.2% with HMDB51. For methods pre-trained on Kinetics400 we present the only competitive approach using a 2D-CNN. Finally, our method outperforms all but one of the competing methods on on UCF101 and sets a new state-of-the-art performance for HMDB51. This gives our method the added advantages of shorter training and inference times with less computational hardware, while also demonstrating that 2D-CNNs can still effectively encode spatiotemporal information.</p><p>Overall, our analysis has demonstrated that our methodology can deliver competitive generalization results without the complexity of 3D-CNN network architecture. As such, we conclude that explicitly enforcing temporal coherency between nearby frame embeddings is a powerful learning selfsupervised training signal. In applying our learned embeddings to the downstream action task, we are able to leverage smaller computational training costs and are faster at inference time. Interestingly, we have also found that there could be such a thing as too much temporal coherency in the context of this particular downstream task. As such, in future work we plan to investigate the application of pre-trained TCE for downstream tasks that require a higher level of temporal understanding, such as action anticipation or phase classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of TCE: Temporally Coherent Embeddings for self-supervised video representation learning. At each step relative attraction and separation is achieved by computing our temporal coherency objective using an anchor frame, an adjacent frame and N negative frames sampled from other videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>A comparison of TAC and MAC scores during selfsupervised pre-training with and without a second order loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>A comparison between frame embeddings generated with (a) TCE; (b) ImageNet Pre-Training; and (c) DPC<ref type="bibr" target="#b4">[5]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>A comparison of the performance of our approach with and without the rotation auxiliary loss.</figDesc><table><row><cell cols="2">Network Train Set Input</cell><cell>UCF101</cell></row><row><cell>2D R-18 UCF101</cell><cell>RGB</cell><cell>68.2</cell></row><row><cell>2D R-18 UCF101</cell><cell>Stack-of-Differences</cell><cell>70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell>: A comparison of TCE performance when trained</cell></row><row><cell>with RGB streams or stack-of-differences inputs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>A comparison of TCE performance when trained with and without enforcing a second order loss.</figDesc><table><row><cell cols="2">D. Benefits of Large Datasets</cell></row><row><cell>Network Train Set</cell><cell>Semi-Hard Mining UCF101</cell></row><row><cell>2D R-18 UCF101</cell><cell>68.2</cell></row><row><cell>2D R-18 Kinetics400</cell><cell>65.34</cell></row><row><cell>2D R-18 Kinetics400</cell><cell>68.75</cell></row><row><cell>2D R-50 Kinetics400</cell><cell>72.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc></figDesc><table /><note>A comparison of TCE performance when pre- trained on a larger dataset with semi-hard mining.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Top-1 accuracy for action recognition on UCF101 and HMDB51 datasets. Rows are ordered by UCF101 performance. For fair comparison, we exclude methods which use additional modalities such as optical flow or audio as network inputs.</figDesc><table /><note>+ Results reported on train/test split 1 of UCF101.† Modified Network Architecture</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Supplementary materials at https://csiro-robotics.github.io/TCE-Webpage</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Skip-clip: Selfsupervised spatiotemporal representation learning by future clip order ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12770</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selfsupervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5589" to="5597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Slow and steady feature analysis: higher order temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3852" to="3861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised video understanding by reconciliation of posture similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4394" to="4404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning via video rotation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4006" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
