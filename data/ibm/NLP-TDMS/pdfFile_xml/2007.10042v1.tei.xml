<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Local Spatial Propagation Network for Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsun</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungdon</forename><surname>Joo</surname></persName>
							<email>kjoo@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Hikvision Research America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kuei</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Hikvision Research America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<email>iskweon77@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Local Spatial Propagation Network for Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depth completion</term>
					<term>Non-local</term>
					<term>Spatial propagation network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a robust and efficient end-to-end non-local spatial propagation network for depth completion. The proposed network takes RGB and sparse depth images as inputs and estimates non-local neighbors and their affinities of each pixel, as well as an initial depth map with pixel-wise confidences. The initial depth prediction is then iteratively refined by its confidence and non-local spatial propagation procedure based on the predicted non-local neighbors and corresponding affinities. Unlike previous algorithms that utilize fixedlocal neighbors, the proposed algorithm effectively avoids irrelevant local neighbors and concentrates on relevant non-local neighbors during propagation. In addition, we introduce a learnable affinity normalization to better learn the affinity combinations compared to conventional methods. The proposed algorithm is inherently robust to the mixed-depth problem on depth boundaries, which is one of the major issues for existing depth estimation/completion algorithms. Experimental results on indoor and outdoor datasets demonstrate that the proposed algorithm is superior to conventional algorithms in terms of depth completion accuracy and robustness to the mixed-depth problem. Our implementation is publicly available on the project page. 4</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Depth estimation has become an important problem in recent years with the rapid growth of computer vision applications, such as augmented reality, unmanned aerial vehicle control, autonomous driving, and motion planning. To obtain a reliable depth prediction, information from various sensors is utilized, e.g., RGB cameras, radar, LiDAR, and ultrasonic sensors [2,3]. Depth sensors, such as LiDAR sensors, produce accurate depth measurements with high frequency. However, the density of the acquired depth is often sparse due to hardware limitations, such as the number of scanning channels. To overcome such <ref type="bibr">(a)</ref> (b) (c) (d) (e) <ref type="figure">Fig. 1</ref>. Example of the depth completion on the NYU Depth V2 dataset <ref type="bibr" target="#b25">[29]</ref>.</p><p>(a) RGB image and a few samples of the estimated non-local neighbors. Depth completion results by (b) direct regression <ref type="bibr" target="#b17">[21]</ref>, (c) local propagation <ref type="bibr" target="#b5">[9]</ref>, and (d) non-local propagation (ours), respectively, and (e) the ground truth.</p><p>limitations, there have been a lot of works to estimate dense depth information based on the given sparse depth values, called depth completion.</p><p>Early methods for depth completion <ref type="bibr" target="#b26">[30,</ref><ref type="bibr" target="#b6">10]</ref> rely only on sparse measurement. Therefore, their predictions suffer from unwanted artifacts, such as blurry and mixed-depth values (i.e., mixed-depth problem). Because RGB images show subtle changes of color and texture, recent methods use RGB images as the guidance to predict accurate dense depth maps. Direct depth completion algorithms <ref type="bibr" target="#b26">[30,</ref><ref type="bibr" target="#b17">21]</ref> take RGB or RGB-D images and directly infer a dense depth using a deep convolutional neural network (CNN). These direct algorithms have shown superior performance compared to conventional ones; however, they still generate blurry depth maps near depth boundaries. Soon after, this phenomenon is alleviated by recent affinity-based spatial propagation methods <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b28">32]</ref>. By learning affinities for local neighbors and iteratively refining depth predictions, the final dense depth becomes more accurate. Nonetheless, previous propagation networks <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b5">9]</ref> have an explicit limitation that they have a fixed-local neighborhood configuration for propagation. Fixedlocal neighbors often have irrelevant information that should not be mixed with reference information, especially on depth boundaries. Hence, they still suffer from the mixed-depth problem in the depth completion task (see <ref type="figure">Fig. 1(c)</ref>).</p><p>To tackle the problem, we propose a Non-Local Spatial Propagation Network (NLSPN) that predicts non-local neighbors for each pixel (i.e., where the information should come from) and then aggregates relevant information using the spatially-varying affinities (i.e., how much information should be propagated), which are also predicted from the network. By relaxing the fixed-local neighborhood configuration, the proposed network can avoid irrelevant local neighbors affiliated with other adjacent objects. Therefore, our method is inherently robust to the mixed-depth problem. In addition, based on our analysis of conventional affinity normalization schemes, we propose a learnable affinity normalization method that has a larger representation capability of affinity combinations. It enables more accurate affinity estimation and thus improves the propagation among non-local neighbors. To further improve robustness to outliers from input and inaccurate initial prediction, we predict the confidence of the initial dense depth simultaneously, and it is incorporated into the affinity normalization to minimize the propagation of unreliable depth values. Experimental results on the indoor <ref type="bibr" target="#b25">[29]</ref> and outdoor <ref type="bibr" target="#b26">[30]</ref> datasets demonstrate that our method achieves superior depth completion performance compared with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Depth Estimation and Completion The objective of depth estimation is to generate dense depth predictions based on various input information, such as a single RGB image, multi-view images, sparse LiDAR measurements, and so on. Conventional depth estimation algorithms often utilize information from a single modality. Eigen et al. <ref type="bibr" target="#b7">[11]</ref> used a multi-scale neural network to predict depth from a single image. In the method introduced by Zbontar and LeCun <ref type="bibr" target="#b31">[35]</ref>, the deep features of image patches are extracted from stereo rectified images, and then the disparity is determined by searching for the most similar patch along the epipolar line. Depth estimation with accurate but sparse depth information (i.e., depth completion) has been intensively explored as well. Uhrig et al. <ref type="bibr" target="#b26">[30]</ref> proposed sparsity invariant CNNs to predict a dense depth map given a sparse depth image from a LiDAR sensor. Ma and Sertac <ref type="bibr" target="#b17">[21]</ref> introduced a method to construct a 4D volume by concatenating RGB and sparse depth images and then feed it into an encoder-decoder CNN for the final prediction. Chen et al. <ref type="bibr" target="#b3">[7]</ref> adopted a fusion of 2D convolution and 3D continuous convolution to effectively consider the geometric configuration of 3D points. Spatial Propagation Network Although direct depth completion algorithms have demonstrated decent performance, sparse-to-dense propagation with accurate guidance from different modalities (e.g., an RGB image) is a more effective way to obtain dense prediction from sparse inputs <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b13">17,</ref><ref type="bibr" target="#b18">22]</ref>. Liu et al. <ref type="bibr" target="#b15">[19]</ref> proposed a spatial propagation network (SPN) to learn local affinities. The SPN learns task-specific affinity values from large-scale data, and it can be applied to a variety of high-level vision tasks, including depth completion and semantic segmentation. However, the individual three-way connection in four-direction is adopted for spatial propagation, which is not suitable for considering all local neighbors simultaneously. This limitation was overcome by Cheng et al. <ref type="bibr" target="#b5">[9]</ref>, who proposed a convolutional spatial propagation network (CSPN) to predict affinity values for local neighbors and update all the pixels simultaneously with their local context for efficiency. However, both the SPN and the CSPN rely on fixed-local neighbors, which could be from irrelevant objects. Therefore, the propagation based on those neighbors would result in mixed-depth values, and the iterative propagation procedure used in their architectures would increase the impact. Moreover, the fixed neighborhood patterns restrict the usage of relevant but wide-range (i.e., non-local) context within the image. Non-Local Network The importance of non-local information has been widely explored in various vision tasks <ref type="bibr" target="#b1">[5,</ref><ref type="bibr" target="#b27">31,</ref><ref type="bibr" target="#b30">34,</ref><ref type="bibr" target="#b24">28]</ref>. Recently, a non-local block in deep neural networks was proposed by Wang et al. <ref type="bibr" target="#b27">[31]</ref>. It consists of pairwise affinity calculation and feature-processing modules. The authors demonstrated the effectiveness of non-local blocks by embedding them into existing deep networks for video classification and image recognition. These methods showed significant improvement over local methods. Our Work Unlike previous algorithms <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b5">9,</ref><ref type="bibr" target="#b28">32]</ref>, our network is trained to predict non-local neighbors with corresponding affinities. In addition, our learnable affinity normalization algorithm searches for the optimal affinity space, which has The encoder-decoder network is built upon the residual network <ref type="bibr" target="#b9">[13]</ref>. Given RGB and sparse depth images, an initial dense depth and its confidence, non-local neighbors, and corresponding affinities are predicted from the network. Then non-local spatial propagation is conducted iteratively with the confidence-incorporated learnable affinity normalization.</p><p>not been explored in conventional algorithms <ref type="bibr" target="#b2">[6,</ref><ref type="bibr" target="#b15">19,</ref><ref type="bibr" target="#b5">9]</ref>. Furthermore, we incorporate the confidence of the initial dense depth prediction (which will be refined by propagation procedure) into affinity normalization to minimize the propagation of unconfident depth values. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of our algorithm. Each component will be described in subsequent sections in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-Local Spatial Propagation</head><p>The goal of spatial propagation is to estimate missing values and refine less confident values by propagating neighbor observations with corresponding affinities (i.e., similarities). Spatial propagation has been utilized as one of the key modules in various computer vision applications <ref type="bibr" target="#b20">[24,</ref><ref type="bibr" target="#b13">17,</ref><ref type="bibr" target="#b12">16]</ref>. In particular, spatial propagation is suitable for the depth completion task <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b5">9,</ref><ref type="bibr" target="#b28">32]</ref>, and its superior performance compared to direct regression algorithms has been demonstrated <ref type="bibr" target="#b26">[30,</ref><ref type="bibr" target="#b17">21]</ref>. In this section, we first briefly review the local SPNs and their limitations, and then describe the proposed non-local SPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Spatial Propagation Network</head><p>Let X = (x m,n ) ∈ R M ×N denote a 2D map to be updated by spatial propagation, where x m,n denotes the pixel value at (m, n). The propagation of x m,n at the step t with its local neighbors, denoted by N m,n , is defined as follows:</p><formula xml:id="formula_0">x t m,n =w c m,n x t−1 m,n + (i,j)∈Nm,n w i,j m,n x t−1 i,j ,<label>(1)</label></formula><p>where (m, n) and (i, j) are the coordinates of reference and neighbor pixels, respectively; w c m,n represents the affinity of the reference pixel; and w i,j m,n indicates the affinity between the pixels at (m, n) and (i, j). The first term in the righthand side represents the propagation of the reference pixel, while the second term stands for the propagation of its neighbors weighted by the corresponding (a) SPN <ref type="bibr" target="#b15">[19]</ref> (b) CSPN <ref type="bibr" target="#b5">[9]</ref> (c) Ours affinities. The affinity of the reference pixel w c m,n (i.e., how much the original value will be preserved) is obtained as</p><formula xml:id="formula_1">w c m,n = 1 − (i,j)∈Nm,n w i,j m,n .</formula><p>(2)</p><p>Spatial Propagation Network The original SPN <ref type="bibr" target="#b15">[19]</ref> is formulated on the configuration of three-way local connections, where each pixel is linked to three adjacent pixels from the previous row or column (see <ref type="figure">Fig. 3(a)</ref>). For instance, the local neighbors of the pixel at (m, n) for top-to-bottom propagation (i.e., vertical) in the SPN, denoted by N S m,n , are defined as follows:</p><formula xml:id="formula_2">N S m,n = {x m+p,n+q | p = −1, q ∈ {−1, 0, 1}} .<label>(3)</label></formula><p>The local neighbors for other directions (i.e., bottom-to-top, left-to-right and right-to-left) can be defined in similar ways. <ref type="figure">Figure 3</ref>(a) shows several examples of N S for other directions. Note that the SPN updates rows or columns in X sequentially. Thus, a natural limitation of the three-way connection is that it does not explore information from all the directions simultaneously. Convolutional Spatial Propagation Network To consider all the possible propagation directions together, the original SPN propagates in four directions individually. Then it utilizes max-pooling to integrate those predictions <ref type="bibr" target="#b15">[19]</ref>. The CSPN <ref type="bibr" target="#b5">[9]</ref> addresses the inefficiency issue by simplifying separate propagations via convolution operation at each propagation step. For the CSPN with a 3×3 local window size, the local neighbors N CS m,n are defined as follows:</p><formula xml:id="formula_3">N CS m,n = {x m+p,n+q | p ∈ {−1, 0, 1} , q ∈ {−1, 0, 1} , (p, q) = (0, 0)}.<label>(4)</label></formula><p>Figure 3(b) shows some examples of N CS . For more details of each network (the SPN and the CSPN), please refer to earlier works <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b5">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-Local Spatial Propagation Network</head><p>The SPN and the CSPN are effective in propagating information from more confident areas into less confident ones with data-dependent affinities. However, their potential improvement is inherently limited by the fixed-local neighborhood configuration ( <ref type="figure">Fig. 3(e)</ref>). The fixed-local neighborhood configuration ignores object/depth distribution within the local area; thus, it often results in mixed-depth values of foreground and background objects after propagation. Although affinities predicted from the network can alleviate the depth mixing between irrelevant pixels to a certain degree, they can hardly avoid incorrect predictions and hold up the use of appropriate neighbors beyond the local area. To resolve the above issues, we introduce a deep neural network that estimates the neighbors of each pixel beyond the local region (i.e., non-local) based on color and depth information within a wide area. The non-local neighbors N NL m,n are defined as follows:</p><formula xml:id="formula_4">N NL m,n = {x m+p,n+q | (p, q) ∈ f φ (I, D, m, n), p, q ∈ R},<label>(5)</label></formula><p>where I and D are the RGB and sparse depth images, respectively, and f φ (·) is the non-local neighbor prediction network that estimates K neighbors for each pixel, under the learnable parameters φ. We adopt an encoder-decoder CNN architecture for f φ (·), which will be described in Sec. 5.1. It should be noted that p and q are real numbers in Eq. <ref type="formula" target="#formula_4">(5)</ref>; thus, the non-local neighbors can be defined to sub-pixel accuracy, as illustrated in <ref type="figure">Fig. 3(c)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Confidence-Incorporated Affinity Learning</head><p>Affinity learning is one of the key components in SPNs, which enables accurate and stable propagation. Conventional affinity-based algorithms utilize color statistics or hand-crafted features <ref type="bibr" target="#b13">[17,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b12">16]</ref>. Recent affinity learning methods <ref type="bibr" target="#b14">[18,</ref><ref type="bibr" target="#b15">19,</ref><ref type="bibr" target="#b5">9]</ref> adopt deep neural networks to predict affinities and show substantial performance improvement. In these methods, affinity normalization plays an important role to stabilize the propagation process.</p><p>In this section, we analyze the conventional normalization approach and its limitation, and then propose a normalization approach in a learnable way. Moreover, we incorporate the confidence of the initial prediction during normalization to suppress negative effects from unreliable depth values during propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Affinity Normalization</head><p>The purpose of affinity normalization is to ensure stability during propagation. For stability, the norm of the temporal Jacobian of x, ∂x t /∂x t−1 should be equal to or less than one <ref type="bibr" target="#b15">[19]</ref>. Under the spatial propagation formulation in Eq. (1), this condition would be satisfied if (i,j)∈Nm,n |w i,j m,n | ≤ 1, ∀m, n. To enforce the condition, previous works <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b5">9]</ref> normalize affinities by the absolute-sum (dubbed Abs−Sum) as follows:</p><formula xml:id="formula_5">(a) Abs−Sum (b) Abs−Sum * (c) Tanh−C (d) Tanh−γ−Abs−Sum * (e) Norm. Prob.</formula><formula xml:id="formula_6">w i,j m,n =ŵ i,j m,n / (i,j)∈Nm,n |ŵ i,j m,n |,<label>(6)</label></formula><p>whereŵ denotes the raw affinity before normalization. Although the stability condition is satisfied by Abs−Sum, it has a problem in that the viable combinations of normalized affinities are biased to a narrow high-dimensional space. Without loss of generality, we first analyze the biased affinity problem using a toy example of the 2-neighbor case and then present solutions to the issue. In the 2-neighbor case, we denote affinities of the two neighbors as w 1 and w 2 with a slight abuse of notation. We assume that the unnormalized affinities are sampled from the standard normal distribution, N(0, 1) for simplicity.</p><p>For the Abs−Sum, the normalized affinities lie on the lines satisfying |w 1 | + |w 2 | = 1 (referred to as A 1 ), as shown in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. This limits the usage of potentially advantageous affinity configuration within the area |w 1 | + |w 2 | &lt; 1 (referred to as A 2 ). To fully explore the affinity configuration |w 1 | + |w 2 | ≤ 1, a simple remedy is to apply Eq. (6) only when i |w i | &gt; 1 (noted as Abs−Sum * ). <ref type="figure" target="#fig_3">Figure 4</ref>(b) shows the affinity distribution of our simple remedy. However, the affinities normalized by Abs−Sum * still have a high chance to fall on A 1 . Indeed, with the increasing number of neighbors K, the affinities are more likely to lie on A 1 . (e.g., the normalization probability is 0.985 when K = 4). <ref type="figure" target="#fig_3">Figure 4</ref>(e) (blue bars) shows the probability of affinities falling on A 1 with various K values.</p><p>One way to reduce the bias is to limit the range of raw affinities <ref type="bibr" target="#b16">[20]</ref>, for example, to [−1/C, 1/C] using the hyperbolic tangent function (tanh(·)) with a normalization factor C. We refer to this normalization procedure as Tanh−C, which is defined as follows:</p><formula xml:id="formula_7">w i,j m,n = tanh(ŵ i,j m,n )/C, C ≥ K,<label>(7)</label></formula><p>where the condition C ≥ K enforces the normalized affinities to guarantee (i,j)∈Nm,n |w i,j m,n | ≤ 1; therefore, this condition ensures stability. <ref type="figure" target="#fig_3">Figure 4</ref>  optimal value of C in Tanh−C may vary depending on the training task, e.g., the number of neighbors, the activation functions, and the dataset.</p><p>To determine the optimal value for the task, we propose to learn the normalization factor together with non-local affinities, and apply the normalization only when (i,j)∈Nm,n |w i,j m,n | &gt; 1. The affinity of the proposed normalization, referred to as Tanh−γ−Abs−Sum * , is defined as follows:</p><formula xml:id="formula_8">w i,j m,n = tanh(ŵ i,j m,n )/γ, γ min ≤ γ ≤ γ max ,<label>(8)</label></formula><p>where γ denotes the learnable normalization parameter, and γ min and γ max are the minimum and maximum values that can be empirically set. <ref type="figure" target="#fig_3">Figure 4</ref>  <ref type="formula">)</ref>). The probability of affinities falling on the boundary with respect to the number of neighbors with γ = K/2 is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>(e) (yellow bars). Compared to Abs−Sum * , Tanh−γ−Abs−Sum * still has a chance to avoid normalization, and it allows us to explore more diverse affinities with a larger number of neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Confidence-Incorporated Affinity Normalization</head><p>In the existing propagation frameworks <ref type="bibr" target="#b13">[17,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b12">16,</ref><ref type="bibr" target="#b14">18,</ref><ref type="bibr" target="#b15">19,</ref><ref type="bibr" target="#b5">9]</ref>, the affinity depicts the correlation between pixels and provides guidance for propagation based on similarity. In this case, each pixel in the map is treated equally without consideration of its reliability. However, in the depth completion task, different pixels should be weighted based on their reliability. For example, information from unreliable pixels (e.g., noisy pixels and pixels on depth boundaries) should not be propagated into neighbors regardless of their affinity to the neighboring pixels. The recent work DepthNormal <ref type="bibr" target="#b28">[32]</ref> addresses this problem with confidence prediction. It utilizes confidence as a mask for the weighted summation of input and prediction for seed point preservation. However, it does not fully prevent the propagation of incorrect depth values because weighted summation is conducted before each propagation separately. In this work, we consider the confidence map of pixels and combine it with affinity normalization. That is, we predict not only the initial dense depth but also its confidence, and then the confidence is incorporated into affinity normalization to reduce disturbances from unreliable depths during propagation. The affinity of the confidence-incorporated Tanh−γ−Abs−Sum * is defined as follows:</p><formula xml:id="formula_9">w i,j m,n = c i,j · tanh(ŵ i,j m,n )/γ,<label>(9)</label></formula><p>where c i,j ∈ [0, 1] denotes the confidence of the pixel at (i, j). <ref type="figure" target="#fig_5">Figure 5(d)</ref> shows an example of a confidence-agnostic depth estimation result. Some noisy input depth points generate unreliable depth values with low confidences (see <ref type="figure" target="#fig_5">Fig. 5(c)</ref>). Without using confidence, the noisy and less confident pixels would harm their neighbor pixels during propagation and lead to unpleasing artifacts (see <ref type="figure" target="#fig_5">Fig. 5(d)</ref>). After the incorporation of confidence into normalization, our algorithm can successfully eliminate the impact of unconfident pixels and generate more accurate depth estimation, as shown in <ref type="figure" target="#fig_5">Fig. 5(e</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Depth Completion Network</head><p>In this section, we describe network architecture and loss functions for network training. The proposed NLSPN mainly consists of two parts: (1) an encoderdecoder architecture for the initial depth map, a confidence map and non-local neighbors prediction with their raw affinities, and (2) a non-local spatial propagation layer with a learnable affinity normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network Architecture</head><p>The encoder-decoder part of the proposed network is built upon residual networks <ref type="bibr" target="#b9">[13]</ref>, and it extracts high-level features from RGB and sparse depth images. Additionally, we adopt the encoder-decoder feature connection strategy <ref type="bibr" target="#b22">[26,</ref><ref type="bibr" target="#b5">9]</ref> to simultaneously utilize low-level and high-level features.</p><p>In <ref type="figure" target="#fig_0">Fig. 2</ref>, we provide an overview of our algorithm. Features from the encoderdecoder network are shared for the initial dense depth, confidence, non-local neighbor, and raw affinity estimation. Then non-local spatial propagation is conducted in an iterative manner. As described in Sec. 3.2, non-local neighbors can have fractional coordinates. To better incorporate fractional coordinates into training, differentiable sampling <ref type="bibr" target="#b11">[15,</ref><ref type="bibr" target="#b32">36]</ref> is adopted during propagation. We note that our non-local propagation can be efficiently calculated by deformable convolutions <ref type="bibr" target="#b32">[36]</ref>. Therefore, each propagation requires a simple forward step of deformable convolution with our affinity normalization. Please refer to the supplementary material for the detailed network configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss Function</head><p>For accurate prediction of the dense depth map, we train our network with 1 or 2 loss as a reconstruction loss with the ground truth depth as follows:</p><formula xml:id="formula_10">L recon (D gt , D pred ) = 1 |V| v∈V d gt v − d pred v ρ ,<label>(10)</label></formula><p>where D gt is the ground truth depth; D pred is the prediction from our algorithm; and d v , V, and |V| denote the depth values at pixel index v, valid pixels of D gt , and the number of valid pixels, respectively. Here, ρ is set to 1 for 1 loss and 2 for 2 loss. Note that we do not have any supervision on the confidence because there is no ground truth; therefore, it is indirectly trained based on L recon .</p><p>(a) RGB (b) Depth (c) S2D <ref type="bibr" target="#b17">[21]</ref> (d) CSPN <ref type="bibr" target="#b5">[9]</ref> (e) Ours (f) GT <ref type="figure">Fig. 6</ref>. Depth completion results on the NYUv2 dataset <ref type="bibr" target="#b25">[29]</ref>. Note that sparse depth images are dilated for visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>In this section, we first describe implementation details and the training environment. After that, quantitative and qualitative comparisons to previous algorithms on indoor and outdoor datasets are presented. We also present ablation studies to verify the effectiveness of each component of the proposed algorithm. The proposed method was implemented using PyTorch <ref type="bibr" target="#b19">[23]</ref> with NVIDIA Apex [1] and trained with a machine equipped with Intel Xeon E5-2620 and 4 NVIDIA GTX 1080 Ti GPUs. For all our experiments, we adopted an ADAM optimizer with β 1 = 0.9, β 2 = 0.999, and the initial learning rate of 0.001. The network training took about 1 and 3 days on the NYU Depth V2 <ref type="bibr" target="#b25">[29]</ref> and KITTI Depth Completion <ref type="bibr" target="#b26">[30]</ref> datasets, respectively. We adopted the ResNet34 <ref type="bibr" target="#b9">[13]</ref> as our encoder-decoder baseline network. The number of non-local neighbors was set to 8 for a fair comparison to other algorithms using 3×3 local neighbors. The number of propagation steps was set to 18 empirically. Other training details will be described for each dataset individually. For the quantitative evaluation, we utilized the following commonly used metrics <ref type="bibr" target="#b25">[29,</ref><ref type="bibr" target="#b17">21,</ref><ref type="bibr" target="#b5">9]</ref>:</p><p>-RMSE (mm) :</p><formula xml:id="formula_11">1 |V| v∈V d gt v − d pred v 2 -MAE (mm) : 1 |V| v∈V d gt v − d pred v -iRMSE (1/km) : 1 |V| v∈V 1/d gt v − 1/d pred v 2 -iMAE (1/km) : 1 |V| v∈V 1/d gt v − 1/d pred v -REL : 1 |V| v∈V (d gt v − d pred v )/d gt v</formula><p>δτ : Percentage of pixels satisfying</p><formula xml:id="formula_12">max d gt v d pred v , d pred v d gt v &lt; τ 6.1 NYU Depth V2</formula><p>The NYU Depth V2 dataset <ref type="bibr" target="#b25">[29]</ref> (NYUv2) consists of RGB and depth images of 464 indoor scenes captured by a Kinect sensor. For the training data, we utilized a subset of ∼50K images from the official training split. Each image was downsized to 320×240, and then 304×228 center-cropping was applied. We trained the model for 25 epochs with 1 loss, and the learning rate decayed by 0.2 every 5 epochs after the first 10 epochs. We set the batch size to 24. The official test split of 654 images was used for evaluation and comparisons. In <ref type="figure">Fig. 6</ref>, we present some depth completion results obtained for the NYUv2 dataset. As in previous works <ref type="bibr" target="#b17">[21,</ref><ref type="bibr" target="#b5">9]</ref>, 500 depth pixels were randomly sampled from a dense depth image and used as the input along with the corresponding RGB image. For comparison, we provide results from the Sparse-to-Dense  (S2D) <ref type="bibr" target="#b17">[21]</ref> and the CSPN <ref type="bibr" target="#b5">[9]</ref>. The S2D <ref type="figure">(Fig. 6(c)</ref>) generates blurry depth images, as it is a direct regression algorithm. Compared to the S2D, the CSPN and our method generate depth maps with substantially improved accuracy thanks to the iterative spatial propagation procedure. However, the CSPN suffers from mixeddepth problems, especially on tiny or thin structures. In contrast, our method well preserves tiny structures and depth boundaries using non-local propagation. <ref type="table">Table 1</ref> shows the quantitative evaluation of the NYUv2 dataset. The proposed algorithm achieves the best result and outperforms other methods by a large margin (RMSE 0.020m). Compared to geometry-agnostic methods <ref type="bibr" target="#b17">[21,</ref><ref type="bibr" target="#b15">19,</ref><ref type="bibr" target="#b5">9]</ref>, geometry-aware ones <ref type="bibr" target="#b10">[14,</ref><ref type="bibr" target="#b4">8,</ref><ref type="bibr" target="#b21">25,</ref><ref type="bibr" target="#b28">32]</ref> show better performance in general. The proposed algorithm can be also viewed as a geometry-aware algorithm because it implicitly explores geometrically relevant neighbors for propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">KITTI Depth Completion</head><p>The KITTI Depth Completion (KITTI DC) dataset <ref type="bibr" target="#b26">[30]</ref> consists of over 90K RGB and LiDAR pairs. We ignored regions without LiDAR projection (i.e., top 100 pixels) and center-cropped 1216 × 240 patches for training. The proposed network was trained for 25 epochs with both 1 and 2 losses to balance RMSE and MAE, and the initial learning rate decayed by 0.4 every 5 epochs after the first 10 epochs. We used a batch size of 25 for the training. <ref type="table" target="#tab_1">Table 2</ref> shows the quantitative evaluation of the KITTI DC dataset. Similar to the results obtained for the NYUv2, geometry-aware algorithms <ref type="bibr" target="#b28">[32,</ref><ref type="bibr" target="#b21">25,</ref><ref type="bibr" target="#b3">7,</ref><ref type="bibr" target="#b4">8]</ref> perform better in general compared to geometry-agnostic methods <ref type="bibr" target="#b17">[21,</ref><ref type="bibr" target="#b5">9]</ref>. Since LiDAR sensor noise (i.e., mixed foreground and background points as shown in <ref type="figure" target="#fig_5">Fig. 5)</ref> is inevitable, the predicted confidence is highly beneficial to eliminate the impact of the noise. DepthNormal <ref type="bibr" target="#b28">[32]</ref> utilizes confidence values as a mask for weighted summation during refinement. However, its confidence mask does not totally prevent incorrect values from propagating into neighboring pixels. On the contrary, the proposed confidence-incorporated affinity normalization effectively restricts the propagation of erroneous values during propagation. We note that  <ref type="bibr" target="#b26">[30]</ref>. (a) RGB, (b) Sparse depth, (c) CSPN <ref type="bibr" target="#b5">[9]</ref>, (d) DepthNormal <ref type="bibr" target="#b28">[32]</ref>, (e) DeepLiDAR <ref type="bibr" target="#b21">[25]</ref>, (f) FuseNet <ref type="bibr" target="#b3">[7]</ref>, (g) CSPN++ <ref type="bibr" target="#b4">[8]</ref>, (h) Ours. Note that sparse depth images are dilated for visualization.</p><p>the proposed method outperformed all the peer-reviewed methods in the KITTI online leaderboard when we submitted the paper. <ref type="figure" target="#fig_7">Figure 7</ref> shows some examples of predicted dense depth with highlighted challenging areas. Those areas usually contain small structures near depth boundaries, which can be easily affected by the mixed-depth problem. Compared to the other methods (Figs. 7(c)-(g)), our algorithm ( <ref type="figure" target="#fig_7">Fig. 7(h)</ref>) handles those challenging areas better with the help of non-local neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies</head><p>We conducted ablation studies to verify the role of each component of our network, including non-local propagation, affinity normalization, and the confidenceincorporated propagation. For all the experiments, we used a set of 10K images sampled from the KITTI DC training dataset for training and evaluated the performance on the full validation dataset. The network was trained for 20 epochs with center-cropped patches of 912×228 for fast training, and the batch size was set to 12. Other settings were set the same as those mentioned in Sec. 6.2. Non-Local Neighbors <ref type="figure" target="#fig_8">Figure 8</ref> visualizes some examples of non-local neighbors predicted by our algorithm. Compared to fixed-local neighbors, our predicted non-local neighbors have higher flexibility in the selection of neighbor pixels. In particular, non-local neighbors are selected from chromatically and geometrically relevant locations near the depth boundaries (e.g., same objects or planes). Moreover, we collected the statistics of the depth variance of neigh-   boring pixels to show the relevance of the selected neighbors. On the KITTI DC validation set, the average depth variances for fixed-local and non-local neighbor configurations were 22.7mm and 11.6mm, respectively. The small variance of the non-local neighbor configuration demonstrates that the proposed method is able to select more relevant neighbors for propagation.</p><p>The quantitative results obtained for the network with fixed-local N CS and that with non-local neighbors N NL are shown in Tab. 3. These networks were also tested with two normalization techniques: (1) with Abs−Sum (Tab. 3(b) and (g)), and (2) with Tanh−γ−Abs−Sum * (Tab. 3(d) and (m)). The proposed method with non-local neighbors consistently outperformed that with fixed-local neighbors, demonstrating the superiority of the non-local framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Affinity Normalization and Confidence Incorporation</head><p>To validate the proposed affinity normalization algorithm, we compare it with three different affinity normalization methods (cf., Sec. 4). Table 3(g)-(i), and (m) assessed the performance using the same network but different affinity normalization methods. The model with Abs−Sum does not perform well due to the limited range of affinity combinations, as shown in <ref type="figure" target="#fig_3">Fig. 4(a)</ref>. When relaxing the normalization condition while maintaining the stability condition (Abs−Sum * ), the performance was improved thanks to the wider area of feasible affinity space and better affinity distribution ( <ref type="figure" target="#fig_3">Fig. 4(b)</ref>). Tanh−C strengthens the stability condition without explicit normalization. However, as shown in <ref type="figure" target="#fig_3">Fig. 4(c)</ref>, the resulting affinity values reside in a smaller affinity space (i.e., in a K-dimensional hypercube with edge size 2/K); therefore, it achieved a slightly worse performance compared to Abs−Sum * . The proposed Tanh−γ−Abs−Sum * was able to alleviate this limitation with a learnable normalization parameter γ. The learned γ compromises between Abs−Sum * and Tanh−C, and can boost the performance. Note that the final γ values (initialized with γ = K = 8) trained on the NYUv2 (Sec. 6.1) and the KITTI DC (Sec. 6.2) datasets were 5.2 and 6.3, respectively. This observation indicates that the optimal γ varies based on the training environment.  <ref type="table">Table 4</ref>. Comparison of the number of network parameters. Note that only methods with publicly available implementations <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b29">33,</ref><ref type="bibr" target="#b8">12,</ref><ref type="bibr" target="#b17">21,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b21">25]</ref> are included.</p><p>We also compared the performance of the network with and without confidence, to verify the importance of confidence incorporation. In addition, we tested two alternative confidence-aware networks (1) by generating a binary mask from confidence with a threshold of 0.5 and (2) with the weighted summation approach of DepthNormal <ref type="bibr" target="#b28">[32]</ref>, and applying each method during the propagation to eliminate the effect of outliers. The comparison results are shown in Tab. 3(j)-(m). The proposed confidence-incorporated affinity normalization (Tab. 3(m)) outperforms the others due to its capability of suppressing propagation from unreliable pixels. The mask-based (Tab. 3(k)) and weighted summation (Tab. 3(l)) approaches show worse performance compared to that of ours, indicating that the hard-thresholding and weighted summation approaches are not optimal for encouraging propagation from relevant pixels but suppressing that from irrelevant pixels. Note that the proposed confidence-incorporated approach is effective for both the network with N NL and that with N CS (Tab. 3(a)-(d)). These results demonstrate the effectiveness of our confidence incorporation. Further Analysis To verify the importance of learned affinities, we further evaluated the proposed method with conventional affinities calculated based on the Euclidean distance between color intensities. As shown in Tab. 3(e) and (m), the network using learned affinities performed much better than the network using the hand-crafted one. In addition, we provide the number of network parameters of the compared methods in Tab. 4. The proposed method achieved superior performance with a relatively small number of network parameters. Please refer to the supplementary material for additional experimental results, visualizations, and ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed an end-to-end trainable non-local spatial propagation network for depth completion. The proposed method gives high flexibility in selecting neighbors for propagation, which is beneficial for accurate propagation, and it eases the affinity learning problem. Unlike previous algorithms (i.e., fixed-local propagation), the proposed non-local spatial propagation efficiently excludes irrelevant neighbors and enforces the propagation to focus on a synergy between relevant ones. In addition, the proposed confidence-incorporated learnable affinity normalization encourages more affinity combinations and minimizes harmful effects from incorrect depth values during propagation. Our experimental results demonstrated the superiority of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the proposed algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(d) RGB/Depth (e) Fixed-local (f) Non-local Fig. 3. Visual comparison of SPNs. (a)-(c) Examples of neighbor configurations of the (a) SPN [19], (b) CSPN [9], and (c) NLSPN (ours), where purple and light purple pixels denote reference and neighboring pixels, respectively. Compared to the others, our neighbor configuration is highly flexible, and can be fractional. (d)-(f) Comparison of fixed-local and non-local configurations for various situations. The fixed-local configuration (e) cannot utilize relevant information beyond the fixed-local region. In contrast, the non-local configuration (f) avoids this problem effectively by predicting and utilizing relevant neighbors at various distances without limitation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3(f) shows some examples of appropriate and desired non-local neighbors near depth boundaries. In the fixed-local setup, affinity learning learns how to encourage the influence of the related pixels and suppress that of unrelated ones simultaneously. On the contrary, affinity learning with the non-local setup concentrates on relevant neighbors, and this facilitates the learning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of affinity normalization schemes. (a)-(d) Affinity distribution after various normalization schemes for the 2-neighbor case. Color bar is shown on the left. (e) Probabilities of normalization with different strategies for each number of neighbors. Please refer to the text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(c) shows the affinity distribution of Tanh−C when C = 2. With a sacrifice of boundary values, Tanh−C enables a more balanced affinity distribution. Moreover, the LOW HIGH (a) RGB (b) Depth (c) Confidence (d) Without Conf. (e) With Conf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Example of propagation with and without confidence incorporation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(d) shows an example of Tanh−γ−Abs−Sum * when γ = 1.25. Here, Tanh−γ−Abs−Sum * can be viewed as a mixture of Abs−Sum * and Tanh−C (see Figs. 4(b) and (c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Depth completion results on the KITTI DC dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Examples of nonlocal neighbors predicted by our network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. Quantitative evalua-</cell></row><row><cell>tion on the KITTI DC test</cell></row><row><cell>dataset [30]. The results from other</cell></row><row><cell>methods are obtained from the KITTI</cell></row><row><cell>online evaluation site.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation on the KITTI DC validation set<ref type="bibr" target="#b26">[30]</ref> with various configurations. Please refer to the text for details.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/zzangjinsun/NLSPN ECCV20 arXiv:2007.10042v1 [cs.CV] 20 Jul 2020</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was partially supported by the National Information Society Agency for construction of training data for artificial intelligence (2100-2131-305-107-19).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Computer Vision (ECCV)</title>
		<meeting>of European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of IEEE Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>of AAAI Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Computer Vision (ECCV)</title>
		<meeting>of European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep convolutional compressed sensing for lidar depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Asian Conf. on Computer Vision (ACCV)</title>
		<meeting>of Asian Conf. on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Confidence propagation through cnns for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. and Mach. Intell. (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth coefficients for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A closed form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning recursive filters for low-level vision via a hybrid neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Computer Vision (ECCV)</title>
		<meeting>of European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)</title>
		<meeting>of IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A unified approach of multi-scale deep and hand-crafted features for defocus estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. and Mach. Intell. (TPAMI)</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<meeting>of Int&apos;l Conf. on Medical Image Computing and Computer Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust reference-based super-resolution with similarity-aware deformable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Computer Vision (ECCV)</title>
		<meeting>of European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant CNNs. In: Int&apos;l Conf. on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of IEEE Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive support-weight approach for correspondence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Anal. and Mach. Intell</title>
		<imprint>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
