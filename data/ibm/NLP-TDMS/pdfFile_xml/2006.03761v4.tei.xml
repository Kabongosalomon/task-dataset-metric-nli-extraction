<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRNet: Gridding Residual Network for Dense Point Cloud Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRNet: Gridding Residual Network for Dense Point Cloud Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Point cloud completion</term>
					<term>gridding</term>
					<term>cubic feature sampling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2[0000−0001−9596−5179] , Hongxun Yao 1[0000−0003−3298−2574] , Shangchen Zhou 3[0000−0001−8201−8877] , Jiageng Mao 4[0000−0003−2571−8767] , Shengping Zhang 1,5[0000−0001−5200−3420] , and Wenxiu Sun 2[0000−0001−5026−8820]</p><p>Abstract. Estimating the complete 3D point cloud from an incomplete one is a key problem in many vision and robotics applications. Mainstream methods (e.g., PCN and TopNet) use Multi-layer Perceptrons (MLPs) to directly process point clouds, which may cause the loss of details because the structural and context of point clouds are not fully considered. To solve this problem, we introduce 3D grids as intermediate representations to regularize unordered point clouds and propose a novel Gridding Residual Network (GRNet) for point cloud completion. In particular, we devise two novel differentiable layers, named Gridding and Gridding Reverse, to convert between point clouds and 3D grids without losing structural information. We also present the differentiable Cubic Feature Sampling layer to extract features of neighboring points, which preserves context information. In addition, we design a new loss function, namely Gridding Loss, to calculate the L1 distance between the 3D grids of the predicted and ground truth point clouds, which is helpful to recover details. Experimental results indicate that the proposed GRNet performs favorably against state-of-the-art methods on the ShapeNet, Completion3D, and KITTI benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of 3D acquisition technologies, 3D sensors (e.g., <ref type="bibr">Li-DARs)</ref> are becoming increasingly available and affordable. As a commonly used format, point clouds are the preferred representation for describing the 3D shape of an object. Complete 3D shapes are required in many applications, including semantic segmentation and SLAM <ref type="bibr" target="#b0">[1]</ref>. However, due to limited sensor resolution and occlusion, highly sparse and incomplete point clouds can be acquired, which causes loss in geometric and semantic information. Consequently, recovering the complete point clouds from partial observations, named point cloud completion, is very important for practical applications. In the recent few years, convolutional neural networks (CNNs) have been applied to 2D images and 3D voxels. Since the convolution can not be directly applied to point clouds due to their irregularity and unorderedness, most of the existing methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> voxelize the point cloud into binary voxels, where 3D convolutional neural networks can be applied. However, the voxelization operation leads to an irreversible loss of geometric information. Other approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> use the Multi-Layer Perceptrons (MLPs) to process point clouds directly. However, these approaches use max pooling to aggregate information across points in a global or hierarchical manner, which do not fully consider the connectivity across points and the context of neighboring points. More recently, several attempts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> have been made to incorporate graph convolutional networks (GCN) <ref type="bibr" target="#b13">[14]</ref> to build local graphs in the neighborhood of each point in the point cloud. However, constructing the graph relies on the K-nearest neighbor (KNN) algorithm, which is sensitive to the point cloud density <ref type="bibr" target="#b14">[15]</ref>.</p><p>Several attempts in point cloud segmentation have been made to capture spatial relationships in point clouds through more general convolution operations. SPLATNet <ref type="bibr" target="#b15">[16]</ref> and InterpConv <ref type="bibr" target="#b16">[17]</ref> perform convolution on high-dimensional lattices and 3D cubes interpolated from neighboring points, respectively. However, both of them are based on a strong assumption that the 3D coordinates of the output points are the same as the input points and thus can not be used for 3D point completion.</p><p>To address the issues mentioned above, we introduce 3D grids as intermediate representations to regularize unordered point clouds, which explicitly preserves the structural and context of point clouds. Consequently, we propose a novel Gridding Residual Network (GRNet) for point cloud completion, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Besides 3D CNN and MLP, we devise three differentiable layers: Gridding, Gridding Reverse, and Cubic Feature Sampling. In Gridding, for each point of the point cloud, eight vertices of the 3D grid cell that the point lies in are first weighted using an interpolation function that explicitly measures the geometric relations of the point cloud. Then, a 3D convolutional neural network <ref type="table">(3D CNN)</ref> with skip connections is adopted to learn context-aware and spatially-aware features, which allows the network to complete missing parts of the incomplete point cloud. Next, Gridding Reverse converts the output 3D grid to a coarse point cloud by replacing each 3D grid cell with a new point whose coordinate is the weighted sum of the eight vertices of the 3D grid cell. The following Cubic Feature Sampling extracts features for each point in the coarse point cloud by concatenating the features of the corresponding eight vertices of the 3D grid cell that the point lies in. The coarse point cloud and the features are forwarded to an MLP to obtain the final completed point cloud.</p><p>Existing methods adopt Chamfer Distance in PSGN <ref type="bibr" target="#b17">[18]</ref> as the loss function to train the neural networks. This loss function penalizes the prediction deviating from the ground-truth. However, there is no guarantee that the predicted point clouds follow the geometric layout of objects, and the networks tend to output a mean shape that minimizes the distance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Some recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> attempt to solve the unorderness while preserving fine-grained details by projecting the 3D point cloud to an image, which is then supervised by the corresponding ground truth masks. However, the projection requires extrinsic camera parameters, which are challenging to estimate in most scenarios <ref type="bibr" target="#b23">[24]</ref>. To solve the unorderedness of point clouds, we propose Gridding Loss, which calculates the L1 distance between the generated points and ground truth by representing them in regular 3D grids with the proposed Gridding layer.</p><p>The contributions can be summarized as follows:</p><p>- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>According to the network architecture used in point cloud completion and reconstruction, existing networks can be roughly categorized into MLP-based, graphbased, and convolution-based networks. MLP-based Networks. Pioneered by PointNet <ref type="bibr" target="#b24">[25]</ref>, several works use MLP for point cloud processing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> and reconstruction <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> because of its simplicity and strong representation ability. These methods model each point independently using several Multi-layer Perceptrons and then aggregate a global feature using a symmetric function (e.g., Max Pooling). However, the geometric relationships among 3D points are not fully considered. PointNet++ <ref type="bibr" target="#b27">[28]</ref> and TopNet <ref type="bibr" target="#b10">[11]</ref> incorporate a hierarchical architecture to consider the geometric structure. To relief the structure loss caused by MLP, AtlasNet <ref type="bibr" target="#b28">[29]</ref> and MSN <ref type="bibr" target="#b29">[30]</ref> recover the complete point cloud of an object by estimating a collection of parametric surface elements.</p><p>Graph-based Networks. By considering each point in a point cloud as a vertex of a graph, graph-based networks generate directed edges for the graph based on the neighbors of each point. In these methods, convolution is usually operated on spatial neighbors, and pooling is used to produce a new coarse graph by aggregating information from each point's neighbors. Compared with MLP-based methods, graph-based networks take local geometric structures into account.</p><p>In DGCNN <ref type="bibr" target="#b11">[12]</ref>, a graph is constructed in the feature space and dynamically updated after each layer of the network. Further, LDGCNN <ref type="bibr" target="#b30">[31]</ref> removes the transformation network and link the hierarchical features from different layers in DGCNN to improve its performance and reduce the model size. Inspired by DGCNN, Hassani and Haley <ref type="bibr" target="#b31">[32]</ref> introduce the multi-scale graph-based network to learn point and shape features for self-supervised classification and reconstruction. DCG <ref type="bibr" target="#b12">[13]</ref> also follows DGCNN to encode additional local connection into a feature vector and progressively evolves from coarse to fine point clouds.</p><p>Convolution-based Networks. Early works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref> usually apply 3D convolutional neural networks (CNNs) build upon the volumetric representation of 3D point clouds. However, converting point clouds into 3D volumes introduces a quantization effect that discards some details of the data <ref type="bibr" target="#b33">[34]</ref> and is not suitable for representing fine-grained information. To the best of our knowledge, no work directly applies CNNs on irregular point clouds for shape completion. In point cloud understanding, several works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> develop CNNs operating on discrete 3D grids that are transformed from point clouds. Hua et al. <ref type="bibr" target="#b34">[35]</ref> define convolutional kernels on regular 3D grids, where the points are assigned with the same weights when falling into the same grid. PointCNN <ref type="bibr" target="#b37">[38]</ref> achieves permutation invariance through a χ-conv transformation. Besides CNNs on discrete space, several methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> define convolutional kernels on continuous space. Thomas et al. <ref type="bibr" target="#b14">[15]</ref> propose both rigid and deformable kernel point convolution (KPConv) operators for 3D point clouds using a set of learnable kernel points. Compared with graph-based networks, convolution-based networks are more efficient and robust to point cloud density <ref type="bibr" target="#b16">[17]</ref>.</p><p>3 Gridding Residual Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The proposed GRNet aims to recover the complete point cloud from an incomplete one in a coarse-to-fine fashion. It consists of five components, including Gridding (Section 3.2), 3D Convolutional Neural Network (Section 3.3), Gridding Reverse (Section 3.4), Cubic Feature Sampling (Section 3.5), and Multi-layer Perceptron (Section 3.6), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Given an incomplete point cloud P as input, Gridding is first used to obtain a 3D grid G =&lt; V, W &gt;, where V and W are the vertex set and value set of G, respectively. Then, W is fed to a 3D CNN, whose output is W . Next, Gridding Reverse produces a coarse point cloud P c from the 3D grid G =&lt; V, W &gt;. Subsequently, Cubic Feature Sampling generates features F c for the coarse point cloud P c . Finally, MLP takes the coarse point cloud P c and the corresponding features F c as input to produce the final completed point cloud P f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gridding</head><p>2D and 3D convolutions have been developed to process regularly arranged data such as images and voxel grids. However, it is challenging to directly apply standard 2D and 3D convolutions to unordered and irregular point clouds. Several methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33]</ref> convert point clouds into 3D voxels and then apply 3D convolutions to them. However, the voxelization process leads to an irreversible loss of geometric information. Recent methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> adopt Multi-layer Perceptrons (MLPs) to directly operate on point clouds and aggregate information across points with max pooling. However, MLP-based methods may lose local context information because the connectivity and layouts of points are not fully considered. Recent studies also indicate that simply applying MLPs to point clouds cannot always work in practice <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>In this paper, we introduce 3D grids as intermediate representations to regularize point clouds and further propose a differentiable Gridding layer, which converts an unordered and irregular point cloud</p><formula xml:id="formula_0">P = {p i } n i=1 into a regular 3D grid G =&lt; V, W &gt; while preserving spatial layouts of the point cloud, where p i ∈ R 3 , V = {v i } N 3 i=1 , W = {w i } N 3 i=1 , v i ∈ {(− N 2 , − N 2 , − N 2 ), . . . , ( N 2 − 1, N 2 − 1, N 2 − 1)}, w i ∈ R,</formula><p>n is the number of points in P , and N is the resolution of the 3D grid G. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b), we define a cell as a cubic consisting of eight vertices.</p><formula xml:id="formula_1">For each vertex v i = (x v i , y v i , z v i )</formula><p>of the 3D grid cell G, we define the neighboring points N (v i ) as points that lie in the adjacent 8 cells of this vertex. The point p = (x, y, z) ∈ N (v i ) is defined as a neighboring point of vertex v i by satisfying p ∈ P ,</p><formula xml:id="formula_2">x v i − 1 &lt; x &lt; x v i + 1, y v i − 1 &lt; y &lt; y v i + 1, and z v i − 1 &lt; z &lt; z v i + 1, respectively.</formula><p>In standard voxelization, value w i at the vertex v i is computed as</p><formula xml:id="formula_3">w i = 0 ∀p ∈ N (v i ) 1 ∃p ∈ N (v i )<label>(1)</label></formula><p>However, this voxelization process introduces a quantization effect that discards some details of an object. In addition, voxelization is not differentiable and thus can not be applied to point cloud reconstruction. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (b), given a vertex v i and its neighboring points p ∈ N (v i ), the proposed Gridding layer computes the corresponding value w i of this vertex v i as</p><formula xml:id="formula_4">w i = p∈N (vi) w(v i , p) |N (v i )| (2) where |N (v i )| is the number of neighboring points of v i . Specially, we define w i = 0 if |N (v i )| = 0. The interpolation function w(v i , p) is defined as w(v i , p) = (1 − |x v i − x|)(1 − |y v i − y|)(1 − |z v i − z|) (3) 4 ! ×32 4 ! ×64 4 ! ×128 4 ! ×256 4 ! ×128 4 ! ×64 4 ! ×32 4 ! ×1 256×4 ! 2 ! 2 ! 2 ! 2 ! 2048×1792 16384×3 2048×3 1×64 ! Fig. 2. The network architecture of GRNet.</formula><p>denotes the sum operation. Tile creates a new tensor of size 16384 × 3 by replicating the "Coarse Point Cloud" 8 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">3D Convolutional Neural Network</head><p>The 3D Convolutional Neural Network (3D CNN) with skip connections aims to complete the missing parts of the incomplete point cloud. It follows the idea of a 3D encoder-decoder with U-net connections <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Given W as input, the 3D CNN can be formulated as</p><formula xml:id="formula_5">W = 3DCNN(W ) (4) where W = {w i } N 3 i=1</formula><p>and w i ∈ R. As shown in <ref type="figure">Figure 2</ref>, the encoder of the 3D CNN has four 3D convolutional layers, each of which has a bank of 4 3 filters with padding of 2, followed by batch normalization, leaky ReLU activation, and a max pooling layer with a kernel size of 2 3 . The numbers of output channels of convolutional layers are 32, 64, 128, 256, respectively. The encoder is finally followed by two fully connected layers with dimensions of 2048 and 16384. The decoder consists of four transposed convolutional layers, each of which has a bank of 4 3 filters with padding of 2 and stride of 1, followed by a batch normalization layer and a ReLU activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Gridding Reverse</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (c), we propose Gridding Reverse to generate the coarse</p><formula xml:id="formula_6">point cloud P c = {p c i } m i=1 from the 3D grid G =&lt; V, W &gt;, where p c i ∈ R 3</formula><p>and m is the number of points in the coarse point cloud P c . Let Θ i = {θ i j } 8 j=1 be the index set of vertices of the i−th 3D grid cell. Gridding Reverse generates one point coordinate p c i for this grid cell by a weighted combination of eight vertices coordinates {v θ |θ ∈ Θ i } and the corresponding values {w θ |θ ∈ Θ i } in this cell, which is computed as</p><formula xml:id="formula_7">p c i = θ∈Θ i w θ v θ θ∈Θ i w θ<label>(5)</label></formula><p>Specially, we ignore the point p c i for this cell if θ∈Θ i w θ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cubic Feature Sampling</head><p>MLP-based methods (e.g., PCN) are unable to take the context of neighboring points into account due to no local spatial connectivity across points. These methods use max-pooling to aggregate information globally, which may lose local context information.</p><p>To overcome this issue, we present Cubic Feature Sampling to aggregate fea-</p><formula xml:id="formula_8">tures F c = {f c } m i=1</formula><p>for the coarse point cloud P c , which is helpful for the following MLP to recover the details of point clouds, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>  </p><formula xml:id="formula_9">(d). Let F = {f v 1 , f v 2 , . . . , f v t 3 } be the feature map of 3D CNN, where f v i ∈ R c</formula><formula xml:id="formula_10">f c i = [f v θ i 1 , f v θ i 2 , . . . , f v θ i 8 ] (6) where [·] is the concatenation operation. {f v θ i j } 8</formula><p>j=1 denotes the features of eight vertices of the i-th 3D gird cell where p c i lies in. In GRNet, Cubic Feature Sampling extracts the point features from feature maps generated by the first three transposed convolutional layers in 3D CNN. To reduce the redundancy of these features and generate a fixed number of points, we randomly sample 2, 048 points from the coarse point cloud P c . Consequently, it produces a feature map of size 2048 × 1792.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Multi-layer Perceptron</head><p>The Multi-layer Perceptron (MLP) is used to recover the details from the coarse point cloud by learning residual offsets between the coordinates of points in the coarse and final completed point cloud. It takes the coarse point cloud P c and the corresponding features F c as input, and outputs the final completed point</p><formula xml:id="formula_11">cloud P f = {p f i } k i=1 as P f = MLP(F c ) + Tile(P c , r)<label>(7)</label></formula><p>where p f i ∈ R 3 and k is the number of points in the final completed point cloud P f . Tile creates a new tensor of size rm × 3 by replicating P c r times.</p><p>In GRNet, r is set to 8. The MLP consists of four fully connected layers with dimensions of 1792, 448, 112, and 24, respectively. The output of MLP is reshaped to 16384 × 3, which corresponds to the offsets of the coordinates of 16, 384 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Gridding Loss</head><p>Existing methods adopt Chamfer Distance <ref type="bibr" target="#b17">[18]</ref> as the loss function to train the neural networks. This loss function penalizes the prediction deviating from the ground-truth. However, it can not guarantee that the predicted points follow the geometric layout of the object. Therefore the networks tend to output a mean shape that minimizes the distance, which causes the loss of the object's details <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Due to the unorderedness of point clouds, it is difficult to directly apply binary cross-entropy like voxels or L1/L2 loss like images. With the proposed Gridding, we can convert unordered point clouds into regular 3D grids ( <ref type="figure" target="#fig_0">Figure  1 (e)</ref>). Therefore, we design a new loss function based on Gridding, namely Gridding Loss, which is defined as the L1 distance between value sets of the two 3D grids. Let G pred =&lt; V pred , W pred &gt; and G gt =&lt; V gt , W gt &gt; be the 3D grids obtained by Gridding the predicted and ground truth point clouds, respectively, where W pred ∈ R N 3 G , W gt ∈ R N 3 G , and N G is the resolution of the two 3D grids. The Gridding Loss can be defined as</p><formula xml:id="formula_12">L Gridding (W pred , W gt ) = 1 N 3 G ||W pred − W gt ||<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>ShapeNet. The ShapeNet dataset <ref type="bibr" target="#b45">[46]</ref> for point cloud completion is derived from PCN <ref type="bibr" target="#b8">[9]</ref>, which consists of 30,974 3D models from 8 categories. The ground truth point clouds containing 16,384 points are uniformly sampled on mesh surfaces. The partial point clouds are generated by back-projecting 2.5D depth maps into 3D. For a fair comparison, we use the same train/val/test splits as PCN.</p><p>Completion3D. The Completion3D benchmark <ref type="bibr" target="#b10">[11]</ref> is composed of 28,974 and 800 samples for training and validation, respectively. Different from the ShapeNet dataset generated by PCN, there are only 2,048 points in the ground truth point clouds.</p><p>KITTI. The KITTI dataset <ref type="bibr" target="#b46">[47]</ref> is composed of a sequence of real-world Velodyne LiDAR scans, also derived from PCN <ref type="bibr" target="#b8">[9]</ref>. For each frame, the car objects are extracted according to the 3D bounding boxes, which results in 2,401 partial point clouds. The partial point clouds in KITTI are highly sparse and do not have complete point clouds as ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><formula xml:id="formula_13">Let T = {(x i , y i , z i )} n T i=1 be the ground truth and R = {(x i , y i , z i )} n R i=1</formula><p>be a reconstructed point set being evaluated, where n T and n R are the numbers of points of T and R, respectively. In our experiments, we use both Chamfer Distance and F-Score as quantitative evaluation metrics. Chamfer Distance. Follow PSGN <ref type="bibr" target="#b17">[18]</ref> and TopNet <ref type="bibr" target="#b10">[11]</ref>, the distance between T and R are defined as</p><formula xml:id="formula_14">CD = 1 n T t∈T min r∈R ||t − r|| 2 2 + 1 n R r∈R min t∈T ||t − r|| 2 2<label>(9)</label></formula><p>F-Score. As pointed out in <ref type="bibr" target="#b47">[48]</ref>, Chamfer Distance may sometimes be misleading. As suggested in <ref type="bibr" target="#b47">[48]</ref>, we take F-Score as an extra metric to evaluate the performance of point completion results, which can be defined as following where P (d) and R(d) denote the precision and recall for a distance threshold d, respectively.</p><formula xml:id="formula_15">F-Score(d) = 2P (d)R(d) P (d) + R(d)<label>(10)</label></formula><formula xml:id="formula_16">P (d) = 1 n R r∈R min t∈T ||t − r|| &lt; d<label>(11)</label></formula><formula xml:id="formula_17">R(d) = 1 n T t∈T min r∈R ||t − r|| &lt; d<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We implement our network using PyTorch <ref type="bibr" target="#b48">[49]</ref> and CUDA 1 . All models are optimized with an Adam optimizer <ref type="bibr" target="#b49">[50]</ref> with β 1 = 0.9 and β 2 = 0.999. We train the network with a batch size of 32 on two NVIDIA TITAN Xp GPUs. The initial learning rate is set to 1e − 4 and decayed by 2 after 50 epochs. The optimization is set to stop after 150 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Shape Completion on ShapeNet</head><p>To compare the performance of GRNet with other state-of-the-art methods, we conduct experiments on the ShapeNet dataset. AtlasNet <ref type="bibr" target="#b28">[29]</ref> generates a point cloud with a set of parametric surface elements. To compare with other methods fairly, we sample 16,384 points from the generated primitive surface elements. PCN <ref type="bibr" target="#b8">[9]</ref> completes the partial point cloud with a stacked version of PointNet <ref type="bibr" target="#b24">[25]</ref>, which directly outputs the coordinates of 16,384 points. FoldingNet <ref type="bibr" target="#b50">[51]</ref> is a baseline method adopted in PCN <ref type="bibr" target="#b8">[9]</ref>, which deforms a 128 × 128 2D grid into 3D point cloud. TopNet <ref type="bibr" target="#b10">[11]</ref> incorporates a decoder following a hierarchical rooted tree structure to consider the topology of point clouds. Due to the scalable architecture of TopNet, it can easily generate 16,384 points by setting the number of nodes and the size of feature embedding. A very recent method MSN <ref type="bibr" target="#b29">[30]</ref> generates dense point cloud containing <ref type="bibr" target="#b7">8,</ref><ref type="bibr">192</ref> points in a coarse-to-fine fashion. To generate 16,384 points, we combine the generated points of 2 times forward propagation.</p><p>Quantitative results in <ref type="table" target="#tab_1">Tables 2 and 1</ref> indicate that GRNet outperforms all competitive methods in terms of Chamfer Distance and F-Score@1%. <ref type="figure" target="#fig_1">Figure 3</ref> shows the qualitative results for point completion on ShapeNet, which indicates that the proposed method recovers better details of objects (e.g., chairs and lamps) than the other methods. <ref type="table">Table 3</ref>. Point completion results on Completion3D compared using Chamfer Distance (CD) with L2 norm. Note that the CD is computed on 2,048 points and multiplied by 10 4 . The best results are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Airplane Cabinet Car Chair Lamp Sofa </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Shape Completion on Completion3D</head><p>Using the model with the lowest Chamfer Distance (CD) on the validation set, we recover the complete point clouds for 1,184 objects in the Completion3D testing set. Then, random subsampling is applied to the generated point clouds to obtain 2,048 points for benchmark evaluation. According to the online leaderboard 2 , as shown in <ref type="table">Table 3</ref>, the overall CD for the proposed GRNet is 10.64, which remarkably outperforms state-of-the-art methods and ranks first on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Shape Completion on KITTI</head><p>To evaluate the performance of the proposed method on real-world LiDAR scans, we test GRNet on the KITTI dataset for completing sparse point clouds of cars. Unlike ShapeNet generated by back-projected from 2.5D images, point clouds from LiDAR scans can be highly sparse, which are much sparser than those in ShapeNet.</p><p>We fine-tuned all competitive methods on ShapeNetCars (the cars from ShapeNet) except PCN that directly uses released output for evaluation. During testing, each point cloud is transformed into the bounding box's coordinates and transformed back to the world frame after completion. The models trained specifically on cars are able to incorporate prior knowledge of the object class.</p><p>Since there are no complete ground truth point clouds for KITTI, we use Consistency and Uniformity to evaluate the performance of all competitive methods. Consistency in PCN <ref type="bibr" target="#b8">[9]</ref> is the average CD between the output of the same car instance in n f consecutive frames. Let R j ti be the output for the j-th car instance at time t i . The Consistency for the j-th car can be calculated as Following PU-GAN <ref type="bibr" target="#b51">[52]</ref>, we adopt Uniformity to evaluate the distribution uniformity of the completed point clouds, which can be formulated as</p><formula xml:id="formula_18">Consistency = 1 n f − 1 n f i=2 CD(R j ti−1 , R j ti )<label>(13)</label></formula><formula xml:id="formula_19">Uniformity(p) = 1 M M i=1 U imbalance (S i )U clutter (S i )<label>(14)</label></formula><p>where S i (i = 1, 2, . . . , M ) is a point subset cropped from a patch of the output R using the farthest sampling and ball query of radius √ p. The term U imbalance and U clutter account for the global and local distribution uniformity, respectively.</p><formula xml:id="formula_20">U imbalance (S i ) = (|S i | −n) 2 n<label>(15)</label></formula><p>wheren = p|R| is the expected number of points in S i . where d i,j represents the distance to the nearest neighbor for the j-th point in S i , andd is roughly 2πp |Si| √ 3 if S i has a uniform distribution <ref type="bibr" target="#b51">[52]</ref>. <ref type="table">Table 9</ref> shows the completion results for cars in the LiDAR scans from the KITTI dataset. Experimental results indicate that GRNet outperforms other competitive methods in terms of Consistency and Uniformity. Benefited from Gridding and Gridding Reverse, GRNet is more sensitive to the spatial structure of the input points, which leads to better consistency between the two consecutive frames. As shown in <ref type="figure">Figure 4</ref>, the cars are barely recognizable due to incompleteness of the input data. In contrast, the completed point clouds provide more geometric information. In addition, the qualitative results also demonstrate the proposed method generates more reasonable shape completion.</p><formula xml:id="formula_21">U clutter (S i ) = 1 |S i | |Si| j=1 (d i,j −d) 2 d<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation Study</head><p>The performance improvement of GRNet should be attributed to three key components, including Gridding, Cubic Feature Sampling, and Gridding Loss. To demonstrate the effectiveness of each component in the proposed method, we evaluate the performance with different parameters. Gridding. <ref type="table">Table 5</ref> shows the results of different resolutions of 3D grids generated by Gridding. The F-Score of final completed point clouds increases with the 3D grids' resolutions. However, the numbers of parameters and the backward time  <ref type="table">Table 6</ref> indicate that Cubic Feature Sampling improves the point cloud completion results significantly. In addition, with more feature maps are fed, the completion quality becomes better without a significant increase in the numbers of parameters and backward time.</p><p>Gridding Loss. We further validate the effects of Gridding Loss, as shown in <ref type="table" target="#tab_5">Table 7</ref>. There is a decrease in terms of both CD and F-Score when removing Gridding Loss. When increasing the resolution of 3D grids from 64 3 to 128 3 , there are 25.9% and 5.4% improvements in CD and F-Score, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we study how to recover the complete 3D point cloud from an incomplete one. The main motivation of this work is to enable the convolutions on 3D point clouds while preserving their structural and context information.</p><p>To this aim, we introduce 3D grids as intermediate representations to regularize unordered point clouds. We then propose a novel Gridding Residual Network (GRNet) for point cloud completion, which contains three novel differentiable layers: Gridding, Gridding Reverse, and Cubic Feature Sampling, as well as a new Gridding Loss. Extensive comparisons are conducted on the ShapeNet, Com-pletion3D, and KITTI benchmarks, which indicate that the proposed GRNet performs favorably against state-of-the-art methods.</p><p>In this supplementary material, we provide additional information to complement the manuscript. First, we present details of Gridding, Gridding Reverse, and Cubic Feature Sampling (Section A). Second, we provide additional quantitative results on ShapeNet, Completion3D, and KITTI (Sections B, C, and D). Third, we present additional ablation studies (Section E). At last, we present more qualitative results compared to other methods (Section F).</p><p>A More Explanations on Gridding, Gridding Reverse, and Cubic Feature Sampling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Gridding</head><p>According to the manuscript, given a vertex v i and its neighboring points p ∈ N (v i ). The proposed Gridding layer computes the corresponding value w i of this vertex v i as</p><formula xml:id="formula_22">w i = p∈N (vi) w(v i , p) |N (v i )|<label>(17)</label></formula><p>where |N (v i )| is the number of neighboring points of v i and w(v i , p) is defined as</p><formula xml:id="formula_23">w(v i , p) = (1 − |x v i − x|)(1 − |y v i − y|)(1 − |z v i − z|)<label>(18)</label></formula><p>Based on Equations 17 and 18, the partial derivative with respect to x can be calculated as follows</p><formula xml:id="formula_24">∂w i ∂x = − 1 |N (vi)| p∈N (vi) (1 − |y v i − y|)(1 − |z v i − z|), x &gt; x v i 1 |N (vi)| p∈N (vi) (1 − |y v i − y|)(1 − |z v i − z|), x ≤ x v i<label>(19)</label></formula><p>where x and x v i are the x-coordinates of the point p and vertex v i , respectively. Similarly, the partial derivative with respect to y and z can be calculated as follows</p><formula xml:id="formula_25">∂w i ∂y = − 1 |N (vi)| p∈N (vi) (1 − |x v i − x|)(1 − |z v i − z|), y &gt; y v i 1 |N (vi)| p∈N (vi) (1 − |x v i − x|)(1 − |z v i − z|), y ≤ y v i<label>(20)</label></formula><formula xml:id="formula_26">∂w i ∂z = − 1 |N (vi)| p∈N (vi) (1 − |x v i − x|)(1 − |y v i − y|), z &gt; z v i 1 |N (vi)| p∈N (vi) (1 − |x v i − x|)(1 − |y v i − y|), z ≤ z v i<label>(21)</label></formula><p>where y and y v i are the y-coordinates of the point p and vertex v i , respectively. z and z v i are the z-coordinates of the point p and vertex v i , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Cubic Feature Sampling</head><p>Point Coordinates Normalization. Cubic Feature Sampling aggregates fea-  <ref type="table" target="#tab_6">Table 8</ref> shows the results of point cloud completion using the Chamfer Distance calculated with Equation <ref type="bibr" target="#b33">34</ref>. The values of PCN are exactly the same as <ref type="table" target="#tab_3">Table 4</ref> in the original paper 3 . </p><formula xml:id="formula_27">tures F c = {f c i } m i=1 of the coarse point cloud P c = {p c i } m i=1 from the 3D feature map F = {f v i } t 3 i=1 , where f c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Quantitative Results on Completion3D</head><p>where I denotes the input point cloud. MMD is the Chamfer Distance (CD) between the output and the car point cloud from ShapeNet that is the closest to the output point cloud in terms of CD. The Fidelity and MMD on KITTI of the compared methods are shown in <ref type="table">Table 9</ref>. However, both FD and MMD are not suitable metrics for KITTI. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, real-world LiDAR scans usually contain clutters which should be removed in the recovered point cloud. MSN <ref type="bibr" target="#b29">[30]</ref> incorporates the minimum density sampling (MDS) to preserve the structure of the input point cloud. Although MSN outperforms other methods in terms of FD, the clutters in the input point cloud are also preserved. MMD s measures how much the output resembles the cars in ShapeNet. However, cars from ShapeNet cannot cover all types of cars in the real-world.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Ablation Studies</head><p>Number of Sampling Points. Gridding Reverse generates a coarse point cloud from a 3D grid. We randomly sample 2,048 points from the coarse point cloud to generate a point cloud containing a fixed number of points for the following MLP. <ref type="table" target="#tab_1">Table 10</ref> shows the Chamfer Distance (CD) and F-Score@1% with different numbers of points sampled.</p><p>Experimental results indicate that sampling 2,048 points from the coarse point clouds archives the best performance in terms of CD and F-Score. The coarse point cloud of an object usually contains about 3,000-4,000 points, over- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Qualitative Comparisons</head><p>In this section, we provide more visual comparisons with the state-of-the-art methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref> for point cloud completion on ShapeNet <ref type="bibr" target="#b45">[46]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of the proposed (a) GRNet, (b) Gridding, (c) Gridding Reverse, (d) Cubic Feature Sampling, and (e) Gridding Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>and t 3</head><label>3</label><figDesc>is the size of the feature map. For a point p c i of the coarse point cloud P c , its features f c i are computed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative completion results on the ShapeNet testing set. GT stands for the ground truth of the 3D object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5</head><label>5</label><figDesc>is the screenshot of the leaderboard results on the Completion3D benchmark, which is available online at https://completion3d.stanford.edu/results. D Additional Quantitative Results on KITTI PCN [9] uses the Fidelity Distance (FD) and Minimal Matching Distance (MMD) as evaluation metrics for KITTI. FD is the average distance from each point in the input to its nearest neighbor in the output, which can be defined as follows FD = 1 n I i∈I min r∈R ||i − r|| 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The screenshot of the Completion3D benchmark results. Available online at https://completion3d.stanford.edu/results .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The clutters in the KITTI LiDAR Scan, as shown in the blue bounding box. Compared to MSN, GRNet recovers the complete point cloud while removing the clutters in the input point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Point completion results on ShapeNet compared using Chamfer Distance (CD) with L2 norm computed on 16,384 points and multiplied by 10 4 . The best results are highlighted in bold.</figDesc><table><row><cell>Methods</cell><cell cols="4">Airplane Cabinet Car Chair Lamp Sofa Table Watercraft Overall</cell></row><row><cell>AtlasNet [29]</cell><cell>1.753</cell><cell>5.101 3.237 5.226 6.342 5.990 4.359</cell><cell>4.177</cell><cell>4.523</cell></row><row><cell>PCN [9]</cell><cell cols="2">1.400 4.450 2.445 4.838 6.238 5.129 3.569</cell><cell>4.062</cell><cell>4.016</cell></row><row><cell cols="2">FoldingNet [51] 3.151</cell><cell>7.943 4.676 9.225 9.234 8.895 6.691</cell><cell>7.325</cell><cell>7.142</cell></row><row><cell>TopNet [11]</cell><cell>2.152</cell><cell>5.623 3.513 6.346 7.502 6.949 4.784</cell><cell>4.359</cell><cell>5.154</cell></row><row><cell>MSN [30]</cell><cell>1.543</cell><cell>7.249 4.711 4.539 6.479 5.894 3.797</cell><cell>3.853</cell><cell>4.758</cell></row><row><cell>GRNet</cell><cell cols="3">1.531 3.620 2.752 2.945 2.649 3.613 2.552 2.122</cell><cell>2.723</cell></row><row><cell cols="5">Table 2. Point completion results on ShapeNet compared using F-Score@1%. Note</cell></row><row><cell cols="5">that the F-Score@1% is computed on 16,384 points. The best results are highlighted</cell></row><row><cell>in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">Airplane Cabinet Car Chair Lamp Sofa Table Watercraft Overall</cell></row><row><cell>AtlasNet [29]</cell><cell>0.845</cell><cell>0.552 0.630 0.552 0.565 0.500 0.660</cell><cell>0.624</cell><cell>0.616</cell></row><row><cell>PCN [9]</cell><cell cols="2">0.881 0.651 0.725 0.625 0.638 0.581 0.765</cell><cell>0.697</cell><cell>0.695</cell></row><row><cell cols="2">FoldingNet [51] 0.642</cell><cell>0.237 0.382 0.236 0.219 0.197 0.361</cell><cell>0.299</cell><cell>0.322</cell></row><row><cell>TopNet [11]</cell><cell>0.771</cell><cell>0.404 0.544 0.413 0.408 0.350 0.572</cell><cell>0.560</cell><cell>0.503</cell></row><row><cell>MSN [30]</cell><cell cols="3">0.885 0.644 0.665 0.657 0.699 0.604 0.782 0.708</cell><cell>0.705</cell></row><row><cell>GRNet</cell><cell>0.843</cell><cell>0.618 0.682 0.673 0.761 0.605 0.751</cell><cell>0.750</cell><cell>0.708</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table Watercraft</head><label>Watercraft</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overall</cell></row><row><cell>AtlasNet [29]</cell><cell>10.36</cell><cell>23.40 13.40 24.16 20.24 20.82 17.52</cell><cell>11.62</cell><cell>17.77</cell></row><row><cell cols="2">FoldingNet [51] 12.83</cell><cell>23.01 14.88 25.69 21.79 21.31 20.71</cell><cell>11.51</cell><cell>19.07</cell></row><row><cell>PCN [9]</cell><cell>9.79</cell><cell>22.70 12.43 25.14 22.72 20.26 20.27</cell><cell>11.73</cell><cell>18.22</cell></row><row><cell>TopNet [11]</cell><cell>7.32</cell><cell>18.77 12.88 19.82 14.60 16.29 14.89</cell><cell>8.82</cell><cell>14.25</cell></row><row><cell>GRNet</cell><cell>6.13</cell><cell>16.90 8.27 12.23 10.22 14.93 10.08</cell><cell>5.86</cell><cell>10.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Point completion results on LiDAR scans from KITTI compared using Consistency and Uniformity. The best results are highlighted in bold. Qualitative completion results on the LiDAR scans from KITTI. The incomplete input point cloud is extracted and normalized from the scene according to its 3D bounding box.</figDesc><table><row><cell>Methods</cell><cell>Consistency (×10 −3 )</cell><cell>0.4%</cell><cell cols="3">Uniformity for different p 0.6% 0.8% 1.0%</cell><cell>1.2%</cell></row><row><cell>AtlasNet [29]</cell><cell>0.700</cell><cell>1.146</cell><cell>1.005</cell><cell>0.874</cell><cell>0.761</cell><cell>0.686</cell></row><row><cell>PCN [9]</cell><cell>1.557</cell><cell>3.662</cell><cell>5.812</cell><cell>7.710</cell><cell>9.331</cell><cell>10.823</cell></row><row><cell>FoldingNet [51]</cell><cell>1.053</cell><cell>1.245</cell><cell>1.303</cell><cell>1.262</cell><cell>1.162</cell><cell>1.063</cell></row><row><cell>TopNet [11]</cell><cell>0.568</cell><cell>1.353</cell><cell>1.326</cell><cell>1.219</cell><cell>1.073</cell><cell>0.950</cell></row><row><cell>MSN [30]</cell><cell>1.951</cell><cell>0.822</cell><cell>0.675</cell><cell>0.523</cell><cell>0.462</cell><cell>0.383</cell></row><row><cell>GRNet</cell><cell>0.313</cell><cell>0.632</cell><cell>0.572</cell><cell>0.489</cell><cell>0.410</cell><cell>0.352</cell></row><row><cell>Fig. 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>The Chamfer Distance (CD), F-Score@1%, numbers of parameters, and backward time on ShapeNet with different resolutions of 3D grids generated by Gridding. The backward time is measured on an NVIDIA TITAN Xp GPU with batch size of 1. The Chamfer Distance (CD), F-Score@1%, and numbers of parameters of MLPs on ShapeNet with different features maps feeding into Cubic Feature Sampling. The backward time is measured on an NVIDIA TITAN Xp GPU with batch size of 1.</figDesc><table><row><cell>Resolutions</cell><cell cols="4">CD (×10 −4 ) Coarse Complete Coarse Complete F-Score@1%</cell><cell cols="2"># Parameters Backward Time (M) (ms)</cell></row><row><cell>32 3</cell><cell>23.339</cell><cell>5.943</cell><cell>0.329</cell><cell>0.549</cell><cell>69.54</cell><cell>64</cell></row><row><cell>64 3</cell><cell>11.259</cell><cell>2.723</cell><cell>0.340</cell><cell>0.708</cell><cell>76.70</cell><cell>100</cell></row><row><cell>128 3</cell><cell>12.383</cell><cell>2.732</cell><cell>0.366</cell><cell>0.712</cell><cell>76.77</cell><cell>302</cell></row><row><cell cols="3">The Size of Feature Maps</cell><cell>CD</cell><cell cols="3">F-Score # Parameters Backward Time</cell></row><row><cell cols="4">128 × 8 3 64 × 16 3 32 × 32 3 (×10 −4 )</cell><cell>@1%</cell><cell>(M)</cell><cell>(ms)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>11.375</cell><cell>0.343</cell><cell>0</cell><cell>72</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.922</cell><cell>0.640</cell><cell>0.11</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.805</cell><cell>0.686</cell><cell>0.96</cell><cell>88</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.723</cell><cell>0.708</cell><cell>4.07</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>The Chamfer Distance (CD) and F-Score@1% on ShapeNet with different resolutions of 3D grids generated by Gridding Loss. The backward time is measured on an NVIDIA TITAN Xp GPU with batch size of 1. To archive a balance between effect and efficiency, we choose the resolution of size 64 3 for Gridding in GRNet. Cubic Feature Sampling. To quantitatively evaluate the effect of Cubic Feature Sampling, we compare the performance without Cubic Feature Sampling and with different feature maps fed into it. The experimental results presented in</figDesc><table><row><cell>Resolutions</cell><cell cols="2">CD (×10 −4 ) Coarse Complete</cell><cell cols="2">F-Score@1% Coarse Complete</cell><cell>Backward Time (ms)</cell></row><row><cell>Not Used</cell><cell>11.259</cell><cell>4.460</cell><cell>0.340</cell><cell>0.624</cell><cell>86</cell></row><row><cell>64 3</cell><cell>10.275</cell><cell>3.427</cell><cell>0.364</cell><cell>0.672</cell><cell>92</cell></row><row><cell>128 3</cell><cell>9.324</cell><cell>2.723</cell><cell>0.386</cell><cell>0.708</cell><cell>100</cell></row><row><cell>also increases.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Results of point cloud completion on ShapeNet compared using the Chamfer Distance (CD) with L1 norm computed on 16,384 points and multiplied by 10 3 . The best results are highlighted in bold.</figDesc><table><row><cell>Methods</cell><cell cols="3">Airplane Cabinet Car Chair Lamp Sofa Table Watercraft Overall</cell></row><row><cell>AtlasNet [29]</cell><cell cols="2">6.366 11.943 10.105 12.063 12.369 12.990 10.331 10.607</cell><cell>10.847</cell></row><row><cell>PCN [9]</cell><cell>5.502 10.625 8.696 10.998 11.339 11.676 8.590</cell><cell>9.665</cell><cell>9.636</cell></row><row><cell cols="3">FoldingNet [51] 9.491 15.796 12.611 15.545 16.413 15.969 13.649 14.987</cell><cell>14.308</cell></row><row><cell>TopNet [11]</cell><cell cols="2">7.614 13.311 10.898 13.823 14.439 14.779 11.224 11.124</cell><cell>12.151</cell></row><row><cell>MSN [30]</cell><cell>5.596 11.963 10.776 10.620 10.712 11.895 8.704</cell><cell>9.485</cell><cell>9.969</cell></row><row><cell>GRNet</cell><cell cols="2">6.450 10.373 9.447 9.408 7.955 10.512 8.444 8.039</cell><cell>8.828</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Results of point cloud completion on KITTI compared using Fidelity Distance (CD) and Minimal Matching Distance (MMD) computed on 16,384 points. Note that both FD and MMD are with L2 norm. The best results are highlighted in bold. The Chamfer Distance (CD) and F-Score@1% on ShapeNet with different numbers of points sampled from the coarse point cloud. The best results are highlighted in bold. ,096 points from the coarse point cloud leads to redundant information in the sampled point cloud. Sampling 1,024 points from the coarse point cloud may lose too much information for the subsequent processing.</figDesc><table><row><cell>Methods</cell><cell>FD (×10 3 )</cell><cell>MMD (×10 3 )</cell></row><row><cell>AtlasNet [29]</cell><cell>1.759</cell><cell>2.108</cell></row><row><cell>PCN [9]</cell><cell>2.235</cell><cell>1.366</cell></row><row><cell>FoldingNet [51]</cell><cell>7.467</cell><cell>0.537</cell></row><row><cell>TopNet [11]</cell><cell>5.354</cell><cell>0.636</cell></row><row><cell>MSN [30]</cell><cell>0.434</cell><cell>2.259</cell></row><row><cell>GRNet</cell><cell>0.816</cell><cell>0.568</cell></row><row><cell># Points</cell><cell>CD (×10 −4 )</cell><cell>F-Score@1%</cell></row><row><cell>1024</cell><cell>2.775</cell><cell>0.697</cell></row><row><cell>2048</cell><cell>2.723</cell><cell>0.708</cell></row><row><cell>4096</cell><cell>2.832</cell><cell>0.681</cell></row><row><cell>sampling 4</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is available at https://github.com/hzxie/GRNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://completion3d.stanford.edu/results</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://arxiv.org/pdf/1808.00671</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Gridding Reverse</head><p>Point Coordinates Normalization. Gridding Reverse generates point p c i = (x c i , y c i , z c i ) for the i-th grid cell by a weighted combination of eight vertices {v θ |θ ∈ Θ i } and the corresponding values {w θ |θ ∈ Θ i } in this cell, which is calculated as</p><p>where</p><p>Backward of Cubic Feature Sampling. During backward propagation, the partial derivative with respect to f v θ i j can be presented as</p><p>where j ∈ {1, 2, . . . , 8} and f c i,j denotes the j-th element in f c i . Since · and · is not differentiable, the partial derivatives with respect to x c i , y c i , and z c i are 0 <ref type="bibr" target="#b52">[53]</ref>, which can be formulated as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Quantitative Results on ShapeNet</head><p>According to the manuscript, the Chamfer Distance is with L2 norm. However, PCN <ref type="bibr" target="#b8">[9]</ref> adopts the Chamfer Distance with L1 norm as an evaluation metric, which can be formulated as follows</p><p>where</p><p>is the ground truth and R = {(x i , y i , z i )} n R i=1 is the reconstructed point set being evaluated. n T and n R are the numbers of points of T and R, respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape completion using 3D-encoder-predictor CNNs and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VConv-DAE: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning 3D shape completion from laser scan data with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A field model for repairing 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape completion enabled robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Point-voxel CNN for efficient 3D deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PCN: point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense 3D point cloud reconstruction using a deep pyramid nxetwork</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mandikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TopNet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep cascade generation on point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3D point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GAL: geometric adversarial loss for single-view 3D-object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV 2018</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">DISN: deep implicit surface network for high-quality single-view 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient dense point cloud object reconstruction using deformation vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV 2018</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning efficient point cloud generation for dense 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI 2018</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICML 2018</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Justlookup: One millisecond deep feature extraction for point clouds by lookup tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A papier-mâché approach to learning 3D surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR 2018</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Morphing and sampling network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020. (2020)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Linked Dynamic Graph CNN: learning on point cloud via linking hierarchical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<idno>arXiv 1904.10014</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised multi-task feature learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shape completion from a single RGBD image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">VoxSegNet: Volumetric CNNs for semantic part segmentation of 3D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.28963104</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR 2018</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Octree guided CNN with spherical kernels for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling local geometric structure of 3D point clouds using Geo-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NeurIPS 2018</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SpiderCNN: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV 2018</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DensePoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PointConv: Deep convolutional networks on 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<idno>235:1-235:12 4</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pix2Vox: Context-aware 3D reconstruction from single and multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pix2Vox++: Multi-scale contextaware 3D object reconstruction from single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">DOI10.1007/s11263-020-01347-6</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">What do single-view 3D reconstruction networks learn?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">FoldingNet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PU-GAN: a point cloud upsampling adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
