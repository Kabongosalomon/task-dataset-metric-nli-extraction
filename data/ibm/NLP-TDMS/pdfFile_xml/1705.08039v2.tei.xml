<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Poincaré Embeddings for Learning Hierarchical Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<email>dkiela@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Poincaré Embeddings for Learning Hierarchical Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning representations of symbolic data such as text, graphs and multi-relational data has become a central paradigm in machine learning and artificial intelligence. For instance, word embeddings such as WORD2VEC <ref type="bibr" target="#b16">[17]</ref>, GLOVE <ref type="bibr" target="#b22">[23]</ref> and FASTTEXT <ref type="bibr" target="#b3">[4]</ref> are widely used for tasks ranging from machine translation to sentiment analysis. Similarly, embeddings of graphs such as latent space embeddings <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr">NODE2VEC</ref>  <ref type="bibr" target="#b10">[11]</ref>, and DEEPWALK <ref type="bibr" target="#b23">[24]</ref> have found important applications for community detection and link prediction in social networks. Embeddings of multi-relational data such as RESCAL <ref type="bibr" target="#b18">[19]</ref>, TRANSE <ref type="bibr" target="#b5">[6]</ref>, and Universal Schema <ref type="bibr" target="#b26">[27]</ref> are being used for knowledge graph completion and information extraction.</p><p>Typically, the objective of embedding methods is to organize symbolic objects (e.g., words, entities, concepts) in a way such that their similarity in the embedding space reflects their semantic or functional similarity. For this purpose, the similarity of objects is usually measured either by their distance or by their inner product in the embedding space. For instance, Mikolov et al. <ref type="bibr" target="#b16">[17]</ref> embed words in R d such that their inner product is maximized when words co-occur within similar contexts in text corpora. This is motivated by the distributional hypothesis <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref>, i.e., that the meaning of words can be derived from the contexts in which they appear. Similarly, Hoff et al. <ref type="bibr" target="#b12">[13]</ref> embed social networks such that the distance between social actors is minimized if they are connected in the network. This reflects the homophily property found in many real-world networks, i.e. that similar actors tend to associate with each other.</p><p>Although embedding methods have proven successful in numerous applications, they suffer from a fundamental limitation: their ability to model complex patterns is inherently bounded by the dimensionality of the embedding space. For instance, Nickel et al. <ref type="bibr" target="#b19">[20]</ref> showed that linear embeddings of graphs can require a prohibitively large dimensionality to model certain types of relations. Although non-linear embeddings can mitigate this problem <ref type="bibr" target="#b6">[7]</ref>, complex graph patterns can still require a computationally infeasible embedding dimensionality. As a consequence, no method yet exists that is able to compute embeddings of large graph-structured data -such as social networks, knowledge graphs or taxonomies -without loss of information. Since the ability to express information is a precondition for learning and generalization, it is therefore important to increase the representation capacity of embedding methods such that they can realistically be used to model complex patterns on a large scale. In this work, we focus on mitigating this problem for a certain class of symbolic data, i.e., large datasets whose objects can be organized according to a latent hierarchy -a property that is inherent in many complex datasets. For instance, the existence of power-law distributions in datasets can often be traced back to hierarchical structures <ref type="bibr" target="#b24">[25]</ref>. Prominent examples of power-law distributed data include natural language (Zipf's law <ref type="bibr" target="#b34">[35]</ref>) and scale-free networks such as social and semantic networks <ref type="bibr" target="#b27">[28]</ref>. Similarly, the empirical analysis of Adcock et al. <ref type="bibr" target="#b0">[1]</ref> indicated that many real-world networks exhibit an underlying tree-like structure.</p><p>To exploit this structural property for learning more efficient representations, we propose to compute embeddings not in Euclidean but in hyperbolic space, i.e., space with constant negative curvature. Informally, hyperbolic space can be thought of as a continuous version of trees and as such it is naturally equipped to model hierarchical structures. For instance, it has been shown that any finite tree can be embedded into a finite hyperbolic space such that distances are preserved approximately <ref type="bibr" target="#b9">[10]</ref>. We base our approach on a particular model of hyperbolic space, i.e., the Poincaré ball model, as it is well-suited for gradient-based optimization. This allows us to develop an efficient algorithm for computing the embeddings based on Riemannian optimization, which is easily parallelizable and scales to large datasets. Experimentally, we show that our approach can provide high quality embeddings of large taxonomies -both with and without missing data. Moreover, we show that embeddings trained on WORDNET provide state-of-the-art performance for lexical entailment. On collaboration networks, we also show that Poincaré embeddings are successful in predicting links in graphs where they outperform Euclidean embeddings, especially in low dimensions.</p><p>The remainder of this paper is organized as follows: In Section 2 we briefly review hyperbolic geometry and discuss related work regarding hyperbolic embeddings. In Section 3 we introduce Poincaré embeddings and discuss how to compute them. In Section 4 we evaluate our approach on tasks such as taxonomy embedding, link prediction in networks and predicting lexical entailment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Embeddings and Hyperbolic Geometry</head><p>Hyperbolic geometry is a non-Euclidean geometry which studies spaces of constant negative curvature. It is, for instance, associated with Minkowski spacetime in special relativity. In network science, hyperbolic spaces have started to receive attention as they are well-suited to model hierarchical data. For instance, consider the task of embedding a tree into a metric space such that its structure is reflected in the embedding. A regular tree with branching factor b has (b + 1)b −1 nodes at level and ((b + 1)b − 2)/(b − 1) nodes on a level less or equal than . Hence, the number of children grows exponentially with their distance to the root of the tree. In hyperbolic geometry this kind of tree structure can be modeled easily in two dimensions: nodes that are exactly levels below the root are placed on a sphere in hyperbolic space with radius r ∝ and nodes that are less than levels below the root are located within this sphere. This type of construction is possible as hyperbolic disc area and circle length grow exponentially with their radius. 1 See <ref type="figure">Figure 1b</ref> for an example. Intuitively, hyperbolic spaces can be thought of as continuous versions of trees or vice versa, trees can be thought of as "discrete hyperbolic spaces" <ref type="bibr" target="#b15">[16]</ref>. In R 2 , a similar construction is not possible as circle length (2πr) and disc area (2πr 2 ) grow only linearly and quadratically with regard to r in Euclidean geometry. Instead, it is necessary to increase the dimensionality of the embedding to model increasingly complex hierarchies. As the number of parameters increases, this can lead to computational problems in terms of runtime and memory complexity as well as to overfitting.</p><p>Due to these properties, hyperbolic space has recently been considered to model complex networks. For instance, Kleinberg <ref type="bibr" target="#b14">[15]</ref> introduced hyperbolic geometry for greedy routing in geographic communication networks. Similarly, Boguñá et al. <ref type="bibr" target="#b2">[3]</ref> proposed hyperbolic embeddings of the AS Internet topology to perform greedy shortest path routing in the embedding space. Krioukov et al. <ref type="bibr" target="#b15">[16]</ref> developed a framework to model complex networks using hyperbolic spaces and discussed <ref type="bibr" target="#b0">1</ref> For instance, in a two dimensional hyperbolic space with constant curvature K = −1, the length of a circle is given as 2π sinh r while the area of a disc is given as 2π(cosh r − 1). Since sinh r = 1 2 (e r − e −r ) and cosh r = 1 2 (e r + e −r ), both disc area and circle length grow exponentially with r. how typical properties such as heterogeneous degree distributions and strong clustering emerges by assuming an underlying hyperbolic geometry to these networks. Adcock et al. <ref type="bibr" target="#b0">[1]</ref> proposed a measure based on Gromov's δ-hyperbolicity <ref type="bibr" target="#b9">[10]</ref> to characterize the tree-likeness of graphs.</p><p>In machine learning and artificial intelligence on the other hand, Euclidean embeddings have become a popular approach for learning from symbolic data. For instance, in addition to the methods discussed in Section 1, Paccanaro and Hinton <ref type="bibr" target="#b21">[22]</ref> proposed one of the first embedding methods to learn from relational data. More recently, Holographic <ref type="bibr" target="#b20">[21]</ref> and Complex Embeddings <ref type="bibr" target="#b28">[29]</ref> have shown state-of-the-art performance in Knowledge Graph completion. In relation to hierarchical representations, Vilnis and McCallum <ref type="bibr" target="#b30">[31]</ref> proposed to learn density-based word representations, i.e., Gaussian embeddings, to capture uncertainty and asymmetry. Given information about hierarchical relations in the form of ordered input pairs, Vendrov et al. <ref type="bibr" target="#b29">[30]</ref> proposed Order Embeddings to model visual-semantic hierarchies over words, sentences, and images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Poincaré Embeddings</head><p>In the following, we are interested in finding embeddings of symbolic data such that their distance in the embedding space reflects their semantic similarity. We assume that there exists a latent hierarchy in which the symbols can be organized. In addition to the similarity of objects, we intend to also reflect this hierarchy in the embedding space to improve over existing methods in two ways:</p><p>1. By inducing an appropriate bias on the structure of the embedding space, we aim at learning more parsimonious embeddings for superior generalization performance and decreased runtime and memory complexity.</p><p>2. By capturing the hierarchy explicitly in the embedding space, we aim at gaining additional insights about the relationships between symbols and the importance of individual symbols.</p><p>However, we do not assume that we have direct access to information about the hierarchy, e.g., via ordered input pairs. Instead, we consider the task of inferring the hierarchical relationships fully unsupervised, as is, for instance, necessary for text and network data. For these reasons -and motivated by the discussion in Section 2 -we embed symbolic data into hyperbolic space H. In contrast to Euclidean space R, there exist multiple, equivalent models of H such as the Beltrami-Klein model, the hyperboloid model, and the Poincaré half-plane model. In the following, we will base our approach on the Poincaré ball model, as it is well-suited for gradient-based optimization. <ref type="bibr" target="#b1">2</ref> In </p><formula xml:id="formula_0">particular, let B d = {x ∈ R d | x &lt; 1}</formula><formula xml:id="formula_1">g x = 2 1 − x 2 2 g E ,</formula><p>where x ∈ B d and g E denotes the Euclidean metric tensor. Furthermore, the distance between points u, v ∈ B d is given as</p><formula xml:id="formula_2">d(u, v) = arcosh 1 + 2 u − v 2 (1 − u 2 )(1 − v 2 )</formula><p>.</p><p>The boundary of the ball is denoted by ∂B. It corresponds to the sphere S d−1 and is not part of the hyperbolic space, but represents infinitely distant points. Geodesics in B d are then circles that are orthogonal to ∂B (as well as all diameters). See <ref type="figure">Figure 1a</ref> for an illustration.</p><p>It can be seen from Equation <ref type="formula" target="#formula_3">(1)</ref>, that the distance within the Poincaré ball changes smoothly with respect to the location of u and v. This locality property of the Poincaré distance is key for finding continuous embeddings of hierarchies. For instance, by placing the root node of a tree at the origin of B d it would have a relatively small distance to all other nodes as its Euclidean norm is zero. On the other hand, leaf nodes can be placed close to the boundary of the Poincaré ball as the distance grows very fast between points with a norm close to one. Furthermore, please note that Equation <ref type="formula" target="#formula_3">(1)</ref> is symmetric and that the hierarchical organization of the space is solely determined by the distance of points to the origin. Due to this self-organizing property, Equation <ref type="formula" target="#formula_3">(1)</ref> is applicable in an unsupervised setting where the hierarchical order of objects is not specified in advance such as text and networks.</p><p>Remarkably, Equation <ref type="formula" target="#formula_3">(1)</ref> allows us therefore to learn embeddings that simultaneously capture the hierarchy of objects (through their norm) as well a their similarity (through their distance).</p><p>Since a single hierarchical structure can already be represented in two dimensions, the Poincaré disk (B 2 ) is typically used to represent hyperbolic geometry. In our method, we instead use the Poincaré ball (B d ), for two main reasons: First, in many datasets such as text corpora, multiple latent hierarchies can co-exist, which can not always be modeled in two dimensions. Second, a larger embedding dimension can decrease the difficulty for an optimization method to find a good embedding (also for single hierarchies) as it allows for more degrees of freedom during the optimization process.</p><p>To compute Poincaré embeddings for a set of symbols S = {x i } n i=1 , we are then interested in finding embeddings Θ = {θ i } n i=1 , where θ i ∈ B d . We assume we are given a problem-specific loss function L(Θ) which encourages semantically similar objects to be close in the embedding space according to their Poincaré distance. To estimate Θ, we then solve the optimization problem</p><formula xml:id="formula_4">Θ ← arg min Θ L(Θ) s.t. ∀ θ i ∈ Θ : θ i &lt; 1.<label>(2)</label></formula><p>We will discuss specific loss functions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Optimization</head><p>Since the Poincaré Ball has a Riemannian manifold structure, we can optimize Equation <ref type="formula" target="#formula_4">(2)</ref> via stochastic Riemannian optimization methods such as RSGD <ref type="bibr" target="#b4">[5]</ref> or RSVRG <ref type="bibr" target="#b33">[34]</ref>. In particular, let T θ B denote the tangent space of a point θ ∈ B d . Furthermore, let ∇ R ∈ T θ B denote the Riemannian gradient of L(θ) and let ∇ E denote the Euclidean gradient of L(θ). Using RSGD, parameter updates to minimize Equation (2) are then of the form</p><formula xml:id="formula_5">θ t+1 = R θt (−η t ∇ R L(θ t ))</formula><p>where R θt denotes the retraction onto B at θ and η t denotes the learning rate at time t. Hence, for the minimization of Equation <ref type="formula" target="#formula_4">(2)</ref>, we require the Riemannian gradient and a suitable retraction. Since the Poincaré ball is a conformal model of hyperbolic space, the angles between adjacent vectors are identical to their angles in the Euclidean space. The length of vectors however might differ. To derive the Riemannian gradient from the Euclidean gradient, it is sufficient to rescale ∇ E with the inverse of the Poincaré ball metric tensor, i.e., g −1 θ . Since g θ is a scalar matrix, the inverse is trivial to compute. Furthermore, since Equation <ref type="formula" target="#formula_3">(1)</ref> is fully differentiable, the Euclidean gradient can easily be derived using standard calculus. In particular, the Euclidean gradient ∇ E = ∂L(θ) ∂d(θ,x) ∂d(θ,x) ∂θ depends on the gradient of L, which we assume is known, and the partial derivatives of the Poincaré distance, which can be computed as follows: Let α = 1 − θ 2 , β = 1 − x 2 and let</p><formula xml:id="formula_6">γ = 1 + 2 αβ θ − x 2<label>(3)</label></formula><p>The partial derivate of the Poincaré distance with respect to θ is then given as</p><formula xml:id="formula_7">∂d(θ, x) ∂θ = 4 β γ 2 − 1 x 2 − 2 θ, x + 1 α 2 θ − x α .<label>(4)</label></formula><p>Since d(·, ·) is symmetric, the partial derivative ∂d(x,θ) ∂θ can be derived analogously. As retraction operation we use R θ (v) = θ + v. In combination with the Riemannian gradient, this corresponds then to the well-known natural gradient method <ref type="bibr" target="#b1">[2]</ref>. Furthermore, we constrain the embeddings to remain within the Poincaré ball via the projection</p><formula xml:id="formula_8">proj(θ) = θ/ θ − ε if θ ≥ 1 θ otherwise ,</formula><p>where ε is a small constant to ensure numerical stability. In all experiments we used ε = 10 −5 . In summary, the full update for a single embedding is then of the form</p><formula xml:id="formula_9">θ t+1 ← proj θ t − η t (1 − θ t 2 ) 2 4 ∇ E .<label>(5)</label></formula><p>It can be seen from Equations <ref type="formula" target="#formula_7">(4)</ref> and <ref type="formula" target="#formula_9">(5)</ref> that this algorithm scales well to large datasets, as the computational and memory complexity of an update depends linearly on the embedding dimension. Moreover, the algorithm is straightforward to parallelize via methods such as Hogwild <ref type="bibr" target="#b25">[26]</ref>, as the updates are sparse (only a small number of embeddings are modified in an update) and collisions are very unlikely on large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Details</head><p>In addition to this optimization procedure, we found that the following training details were helpful for obtaining good representations: First, we initialize all embeddings randomly from the uniform distribution U(−0.001, 0.001). This causes embeddings to be initialized close to the origin of B d . Second, we found that a good initial angular layout can be helpful to find good embeddings. For this reason, we train during an initial "burn-in" phase with a reduced learning rate η/c. In combination with initializing close to the origin, this can improve the angular layout without moving too far towards the boundary. In our experiments, we set c = 10 and the duration of the burn-in to 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we evaluate the quality of Poincaré embeddings for a variety of tasks, i.e., for the embedding of taxonomies, for link prediction in networks, and for modeling lexical entailment. We compare the Poincaré distance as defined in Equation <ref type="formula" target="#formula_3">(1)</ref> to the following two distance functions:</p><p>Euclidean In all cases, we include the Euclidean distance d(u, v) = u − v 2 . As the Euclidean distance is flat and symmetric, we expect that it requires a large dimensionality to model the hierarchical structure of the data. Translational For asymmetric data, we also include the score function d(u, v) = u − v + r 2 , as proposed by Bordes et al. <ref type="bibr" target="#b5">[6]</ref> for modeling large-scale graph-structured data. For this score function, we also learn the global translation vector r during training.</p><p>Note that the translational score function has, due to its asymmetry, more information about the nature of an embedding problem than a symmetric distance when the order of (u, v) indicates the hierarchy of elements. This is, for instance, the case for is-a(u, v) relations in taxonomies. For the Poincaré distance and the Euclidean distance we could randomly permute the order of (u, v) and obtain the identical embedding, while this is not the case for the translational score function. As such, it is not fully unsupervised and only applicable where this hierarchical information is available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding Taxonomies</head><p>In the first set of experiments, we are interested in evaluating the ability of Poincaré embeddings to embed data that exhibits a clear latent hierarchical structure. For this purpose, we conduct experiments on the transitive closure of the WORDNET noun hierarchy <ref type="bibr" target="#b17">[18]</ref> in two settings:</p><p>Reconstruction To evaluate representation capacity, we embed fully observed data and reconstruct it from the embedding. The reconstruction error in relation to the embedding dimension is then a measure for the capacity of the model. Link Prediction To test generalization performance, we split the data into a train, validation and test set by randomly holding out observed links. Links in the validation and test set do not include the root or leaf nodes as these links would either be trivial to predict or impossible to predict reliably.</p><p>Since we are using the transitive closure, the hypernymy relations form a directed acyclic graph such that the hierarchical structure is not directly visible from the raw data but has to be inferred. The transitive closure of the WORDNET noun hierarchy consists of 82,115 nouns and 743,241 hypernymy relations. On this data, we learn embeddings in both settings as follows: Let D = {(u, v)} be the set of observed hypernymy relations between noun pairs. We then learn embeddings of all symbols in D such that related objects are close in the embedding space. In particular, we minimize the loss function</p><formula xml:id="formula_10">L(Θ) = (u,v)∈D log e −d(u,v) v ∈N (u) e −d(u,v ) ,<label>(6)</label></formula><p>where N (u) = {v | (u, v) ∈ D} ∪ {u} is the set of negative examples for u (including u). For training, we randomly sample 10 negative examples per positive example. Equation <ref type="formula" target="#formula_10">(6)</ref> can be interpreted as a soft ranking loss where related objects should be closer than objects for which we didn't observe a relationship. This choice of loss function is motivated by the fact that we don't want to push symbols belonging to distinct subtrees arbitrarily far apart as their subtrees might still be close. Instead we want them to be farther apart than symbols with an observed relation.</p><p>We evaluate the quality of the embeddings as commonly done for graph embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>: For each observed relationship (u, v), we rank its distance d(u, v) among the ground-truth negative examples for u, i.e., among the set {d(u, v ) | (u, v ) ∈ D)}. In the Reconstruction setting, we evaluate the ranking on all nouns in the dataset. We then record the mean rank of v as well as the mean average precision (MAP) of the ranking. The results of these experiments are shown in <ref type="table" target="#tab_1">Table 1</ref>. It can be seen that Poincaré embeddings are very successful in the embedding of large taxonomies -both with regard to their representation capacity and their generalization performance. Even compared to Translational embeddings, which have more information about the structure of the task, Poincaré embeddings show a greatly improved performance while using an embedding that is smaller by an order of magnitude. Furthermore, the results of Poincaré embeddings in the link prediction task are very robust with regard to the embedding dimension. We attribute this result to the structural bias of Poincaré embeddings, what could lead to reduced overfitting on this kind of data with a clear latent hierarchy. In <ref type="figure" target="#fig_1">Figure 2</ref> we show additionally a visualization of a two-dimensional Poincaré embedding. For purpose of clarity, this embedding has been trained only on the mammals subtree of WORDNET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Embeddings</head><p>Next, we evaluated the performance of Poincaré embeddings for link prediction in networks. Since edges in complex networks can often be explained via latent hierarchies over their nodes <ref type="bibr" target="#b7">[8]</ref>, we are interested in the benefits of Poincaré embeddings both in terms representation size and generalization performance. We performed our experiments on four commonly used social networks, i.e, ASTROPH, CONDMAT, GRQC, and HEPPH. These networks represent scientific collaborations such that there exists an undirected edge between two persons if they co-authored a paper. For these networks, we model the probability of an edge as proposed by Krioukov et al. <ref type="bibr" target="#b15">[16]</ref> via the Fermi-Dirac distribution</p><formula xml:id="formula_11">P ((u, v) = 1 | Θ) = 1 e (d(u,v)−r)/t + 1<label>(7)</label></formula><p>where r, t &gt; 0 are hyperparameters. Here, r corresponds to the radius around each point u such that points within this radius are likely to have an edge with u. The parameter t specifies the steepness of the logistic function and influences both average clustering as well as the degree distribution <ref type="bibr" target="#b15">[16]</ref>. We use the cross-entropy loss to learn the embeddings and sample negatives as in Section 4.1.</p><p>For evaluation, we split each dataset randomly into train, validation, and test set. The hyperparameters r and t where tuned for each method on the validation set. <ref type="table" target="#tab_2">Table 2</ref> lists the MAP score of Poincaré and Euclidean embeddings on the test set for the hyperparameters with the best validation score. Additionally, we again list the reconstruction performance without missing data. Translational embeddings are not applicable to these datasets as they consist of undirected edges. It can be seen that Poincaré embeddings perform again very well on these datasets and -especially in the low-dimensional regime -outperform Euclidean embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lexical Entailment</head><p>An interesting aspect of Poincaré embeddings is that they allow us to make graded assertions about hierarchical relationships as hierarchies are represented in a continuous space. We test this property on HYPERLEX <ref type="bibr" target="#b31">[32]</ref>, which is a gold standard resource for evaluating how well semantic models  Note that these embeddings were not specifically trained for this task.</p><p>To determine to what extent is-a(u, v) is true, we used the score function: score(is-a(u, v)) = −(1 + α( v − u ))d(u, v). (8) Here, the term α( v − u ) acts as a penalty when v is lower in the embedding hierarchy, i.e., when v has a higher norm than u. The hyperparameter α determines the severity of the penalty. In our experiments we set α = 10 3 .</p><p>Using Equation <ref type="formula">(8)</ref>, we scored all noun pairs in HYPERLEX and recorded Spearman's rank correlation with the ground-truth ranking. The results of this experiment are shown in <ref type="table" target="#tab_3">Table 3</ref>. It can be seen that the ranking based on Poincaré embeddings clearly outperforms all state-of-the-art methods evaluated in <ref type="bibr" target="#b31">[32]</ref>. Methods in <ref type="table" target="#tab_3">Table 3</ref> that are prefixed with WN also use WORDNET as a basis and therefore are most comparable. The same embeddings also achieved a state-of-the-art accuracy of 0.86 on WBLESS <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref>, which evaluates non-graded lexical entailment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>In this paper, we introduced Poincaré embeddings for learning representations of symbolic data and showed how they can simultaneously learn the similarity and the hierarchy of objects. Furthermore, we proposed an efficient algorithm to compute the embeddings and showed experimentally, that Poincaré embeddings provide important advantages over Euclidean embeddings on hierarchical data: First, Poincaré embeddings enable very parsimonious representations whats allows us to learn high-quality embeddings of large-scale taxonomies. Second, excellent link prediction results indicate that hyperbolic geometry can introduce an important structural bias for the embedding of complex symbolic data. Third, state-of-the-art results for predicting lexical entailment suggest that the hierarchy in the embedding space corresponds well to the underlying semantics of the data.</p><p>The main focus of this work was to evaluate the general properties of hyperbolic geometry for the embedding of symbolic data. In future work, we intend, to both expand the applications of Poincaré embeddings -for instance to multi-relational data -and also to derive models that are tailored to specific applications such as word embeddings. Furthermore, we have shown that natural gradient based optimization already produces very good embeddings and scales to large datasets. We expect that a full Riemannian optimization approach can further increase the quality of the embeddings and lead to faster convergence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 (Figure 1 :</head><label>51</label><figDesc>a) Geodesics of the Poincaré disk (b) Embedding of a tree in B 2 (c) Growth of Poincaré distance (a) Due to the negative curvature of B, the distance of points increases exponentially (relative to their Euclidean distance) the closer they are to the boundary. (c) Growth of the Poincaré distance d(u, v) relative to the Euclidean distance and the norm of v (for fixed u = 0.9). (b) Embedding of a regular tree in B 2 such that all connected nodes are spaced equally far apart (i.e., all black line segments have identical hyperbolic length).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two-dimensional Poincaré embeddings of transitive closure of the WORDNET mammals subtree. Ground-truth is-a relations of the original WORDNET tree are indicated via blue edges. A Poincaré embedding with d = 5 achieves mean rank 1.26 and MAP 0.927 on this subtree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>be the open d-dimensional unit ball, where · denotes the Euclidean norm. The Poincaré ball model of hyperbolic space corresponds then to the Riemannian manifold (B d , g x ), i.e., the open unit ball equipped with the Riemannian metric tensor</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on the transitive closure of the WORDNET noun hierarchy. Highlighted cells indicate the best Euclidean embeddings as well as the Poincaré embeddings which acheive equal or better results. Bold numbers indicate absolute best results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dimensionality</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>200</cell></row><row><cell>WORDNET</cell><cell>Reconstruction</cell><cell>Euclidean Translational Poincaré</cell><cell cols="6">Rank 3542.3 2286.9 1685.9 1281.7 1187.3 1157.3 MAP 0.024 0.059 0.087 0.140 0.162 0.168 Rank 205.9 179.4 95.3 92.8 92.7 91.0 MAP 0.517 0.503 0.563 0.566 0.562 0.565 Rank 4.9 4.02 3.84 3.98 3.9 3.83 MAP 0.823 0.851 0.855 0.86 0.857 0.87</cell></row><row><cell>WORDNET</cell><cell>Link Pred.</cell><cell>Euclidean Translational Poincaré</cell><cell cols="2">Rank 3311.1 2199.5 MAP 0.024 0.059 Rank 65.7 56.6 MAP 0.545 0.554 Rank 5.7 4.3 MAP 0.825 0.852</cell><cell>952.3 0.176 52.1 0.554 4.9 0.861</cell><cell>351.4 0.286 47.2 0.56 4.6 0.863</cell><cell>190.7 0.428 43.2 0.562 4.6 0.856</cell><cell>81.5 0.490 40.4 0.559 4.6 0.855</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Mean average precision for Reconstruction and Link Prediction on network data.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dimensionality</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Reconstruction</cell><cell></cell><cell></cell><cell cols="2">Link Prediction</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell></row><row><cell>ASTROPH</cell><cell cols="8">Euclidean 0.376 0.788 0.969 0.989 0.508 0.815 0.946 0.960</cell></row><row><cell>N=18,772; E=198,110</cell><cell>Poincaré</cell><cell cols="7">0.703 0.897 0.982 0.990 0.671 0.860 0.977 0.988</cell></row><row><cell>CONDMAT</cell><cell cols="8">Euclidean 0.356 0.860 0.991 0.998 0.308 0.617 0.725 0.736</cell></row><row><cell>N=23,133; E=93,497</cell><cell>Poincaré</cell><cell cols="7">0.799 0.963 0.996 0.998 0.539 0.718 0.756 0.758</cell></row><row><cell>GRQC</cell><cell cols="8">Euclidean 0.522 0.931 0.994 0.998 0.438 0.584 0.673 0.683</cell></row><row><cell>N=5,242; E=14,496</cell><cell>Poincaré</cell><cell cols="7">0.990 0.999 0.999 0.999 0.660 0.691 0.695 0.697</cell></row><row><cell>HEPPH</cell><cell cols="8">Euclidean 0.434 0.742 0.937 0.966 0.642 0.749 0.779 0.783</cell></row><row><cell>N=12,008; E=118,521</cell><cell>Poincaré</cell><cell cols="7">0.811 0.960 0.994 0.997 0.683 0.743 0.770 0.774</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Spearman's ρ for Lexical Entailment on HYPERLEX. entailment by quantifying to what degree X is a type of Y via ratings on a scale of [0, 10]. Using the noun part of HYPERLEX, which consists of 2163 rated noun pairs, we then evaluated how well Poincaré embeddings reflect these graded assertions. For this purpose, we used the Poincaré embeddings that were obtained in Section 4.1 by embedding WORDNET with a dimensionality d = 5.</figDesc><table><row><cell>FR</cell><cell cols="7">SLQS-Sim WN-Basic WN-WuP WN-LCh Vis-ID Euclidean Poincaré</cell></row><row><cell>ρ 0.283</cell><cell>0.229</cell><cell>0.240</cell><cell>0.214</cell><cell>0.214</cell><cell>0.253</cell><cell>0.389</cell><cell>0.512</cell></row><row><cell cols="2">capture graded lexical</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It can be seen easily that the distance function of the Poincare ball in Equation(1)is differentiable. Hence, for this model, an optimization algorithm only needs to maintain the constraint that x &lt; 1 for all embeddings. Other models of hyperbolic space however, would be more more difficult to optimize, either due to the form of their distance function or due to the constraints that they introduce. For instance, the hyperboloid model is constrained to points where x, x = −1, while the distance function of the Beltrami-Klein model requires to compute the location of ideal points on the boundary ∂B of the unit ball.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree-like structure in large social and information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sustaining the internet with hyperbolic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boguñá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krioukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical structure and the prediction of missing links in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="issue">7191</biblScope>
			<biblScope unit="page" from="98" to="101" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rupert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firth</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperbolic groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhael</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in group theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="75" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american Statistical association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">460</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting image generality for lexical entailment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Geographic routing using hyperbolic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM 2007. 26th IEEE International Conference on Computer Communications. IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1902" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marián</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1310.4546</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wordnet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML</title>
		<meeting>the 28th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts using linear relational embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Paccanaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="244" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical organization in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erzsébet</forename><surname>Ravasz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26112</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="78" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06361</idno>
		<title level="m">Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hyperlex: A large-scale evaluation of graded lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02117</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics COLING</title>
		<meeting>the 25th International Conference on Computational Linguistics COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Riemannian SVRG: fast stochastic optimization on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4592" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Human Behaviour and the Principle of Least Effort: an Introduction to Human Ecology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipf</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
