<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FANTrack: 3D Multi-Object Tracking with Feature Association Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkan</forename><surname>Baser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkateshwaran</forename><surname>Balasubramanian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prarthana</forename><surname>Bhattacharyya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
						</author>
						<title level="a" type="main">FANTrack: 3D Multi-Object Tracking with Feature Association Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a data-driven approach to online multi-object tracking (MOT) that uses a convolutional neural network (CNN) for data association in a tracking-by-detection framework. The problem of multi-target tracking aims to assign noisy detections to a-priori unknown and time-varying number of tracked objects across a sequence of frames. A majority of the existing solutions focus on either tediously designing cost functions or formulating the task of data association as a complex optimization problem that can be solved effectively. Instead, we exploit the power of deep learning to formulate the data association problem as inference in a CNN. To this end, we propose to learn a similarity function that combines cues from both image and spatial features of objects. Our solution learns to perform global assignments in 3D purely from data, handles noisy detections and a varying number of targets, and is easy to train. We evaluate our approach on the challenging KITTI dataset and show competitive results. Our code is available at https://git.uwaterloo.ca/wise-lab/fantrack.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multi-object tracking (MOT) is a critical problem in computer vision and has received great attention due to its widespread use in applications such as autonomous driving, robot navigation, and activity recognition. It is the problem of finding the optimal set of trajectories of objects of interest over a sequence of consecutive frames. Most of the successful computer vision approaches to MOT have focused on the tracking-by-detection principle <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. This paradigm allows the problem to be divided into two steps. First, an object detector is used to identify the potential locations of objects in the form of bounding boxes, and then a discrete combinatorial problem is solved to link these noisy detections over time to form trajectories. Despite decades of research, the status quo of tracking is far from reaching human accuracy. Current challenges to the problem include a varying and a-priori unknown number of targets; incorrect and missing detections; changing appearances of targets due The linking step called data association is arguably the most difficult component of MOT. Traditional batch methods usually formulate MOT as a global optimization problem, with the assumption that detections from all future frames are available, and solve it by mapping it to a graph based mincost flow algorithm <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Online Markovian formulations of MOT on the other hand often employ greedy or bipartite graph matching methods like the Hungarian algorithm to solve the assignment problem <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Online approaches are well suited to real-time applications such as tracking roadtraffic participants. The success of the final associations is also dependent on the similarity functions used to match the targets and detections. Traditionally cost functions have been handcrafted with representations based on color histograms, bounding box position, and linear motion models <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, but have failed to generalize across tasks and for complex tracking scenarios. Recently, deep neural network architectures have shown superior performance in many vision based tasks. <ref type="bibr">Milan et al.</ref> proposed the first end-to-end formulation for MOT, using a recurrent neural network (RNN) to solve the assignment problem for each target independently based on Euclidean cost <ref type="bibr" target="#b9">[10]</ref>. However, the use of convolutional neural networks (CNNs), which are easier to train than RNNs, in order to solve the association problem while also learning the cost function has not yet been investigated.</p><p>In this paper, we propose an online MOT formulation that casts the assignment problem as inference in a CNN. We present a two-step learning based approach (see <ref type="figure" target="#fig_0">Fig.  1</ref>). The first step learns a similarity function that takes advantage of both visual and 3D bounding box data to yield robust matching costs. The second step trains a CNN to predict discrete target assignments from the computed pairwise similarities. The benefit of our proposal is that it is easy to train, takes care of a varying number of targets and noisy detections, and provides a simple way to consider all the targets while making associations. We empirically demonstrate on the KITTI tracking dataset <ref type="bibr" target="#b10">[11]</ref> that: (i) Our approach can solve the multi-target association problem by performing inference using CNNs. (ii) It can integrate image based appearance and 3D bounding box features to get a discriminative as well as generalized feature representation, thereby learning a robust cost function for association. (iii) We show competitive qualitative and quantitative 3D tracking results compared to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. LITERATURE REVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Association in MOT</head><p>Classical approaches solve the data association problem by considering multiple hypotheses for an assignment (MHT) <ref type="bibr" target="#b11">[12]</ref>, or by jointly considering all possible assignment hypotheses (JPDA) <ref type="bibr" target="#b12">[13]</ref>. These formulations prove to be very computationally intensive, however.</p><p>Many recent works process sequences in batch mode, using a graph-based representation with detections as nodes and possible assignments as edges. The optimization is then cast as a linear program solved to (near) global optimality with relaxation, min-cost or shortest path algorithms <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. More complex optimization schemes include MCMC <ref type="bibr" target="#b16">[17]</ref> and discrete-continuous settings <ref type="bibr" target="#b17">[18]</ref>. However, global optimization formulations are unsuited to real-time applications like autonomous navigation.</p><p>Online methods estimate the current state using the information only from the past frames and the current one. Commonly used state-estimators include the Kalman filter <ref type="bibr" target="#b18">[19]</ref> for linear motion and particle filters <ref type="bibr" target="#b19">[20]</ref> for multimodal posteriors. The two-frame association problem is often solved using a greedy or Hungarian algorithm <ref type="bibr" target="#b5">[6]</ref>. Approaches based on local associations tend to be susceptible to track fragmentation and noisy detections, however.</p><p>Deep learning has achieved state-of-the-art results in perception tasks like image classification, segmentation, and single object tracking. Milan et al. proposed the first fully endto-end multi-object tracking method based on deep learning. The method predicts the assignment of each target, one at a time, using an RNN <ref type="bibr" target="#b9">[10]</ref>. In contrast, our approach feeds all detections and their learned similarity scores at once into a CNN to predict the assignments. Our model is easier to optimize than an RNN, handles noisy detections and a varying number of targets, and considers all targets at once when performing assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Measuring Similarity</head><p>Tracking algorithms have used distance functions such as Euclidean <ref type="bibr" target="#b20">[21]</ref> and Mahalanobis distance <ref type="bibr" target="#b21">[22]</ref> as matching costs for data association. Other similarity measures include color-based appearance features <ref type="bibr" target="#b22">[23]</ref>, SIFT-like features <ref type="bibr" target="#b23">[24]</ref>, and linear and non-linear motion models and their various weighted combinations <ref type="bibr" target="#b24">[25]</ref>. These tediously hand-crafted features fail to generalize across complex scenarios and backgrounds, however.</p><p>Recent works explore learning pairwise costs using deep structured SVM <ref type="bibr" target="#b2">[3]</ref>, CNNs <ref type="bibr" target="#b25">[26]</ref>, and RNNs <ref type="bibr" target="#b26">[27]</ref>. For CNNs, similarity learning often exploits Siamese networks. Leal-Taixe et al. <ref type="bibr" target="#b27">[28]</ref> and Frossard et al. <ref type="bibr" target="#b28">[29]</ref> use them to learn descriptors for matching with multi-modal inputs. While we also use Siamese networks to learn generalized and discriminative features from 3D object configurations and visual information conditioned on similarity, we adapt our objective function to use the cosine-similarity metric with hard-mining which has a positive impact on convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>Our proposed framework is based on tracking by detection paradigm. Our problem setup assumes at any time instant t we have N number of targets, M number of detections and track labels for every i th track. We use AVOD <ref type="bibr" target="#b29">[30]</ref> as our 3D object detector since it achieves state-of-the-art results on KITTI and is open-source, but in principle, any other 3D object detector could be used. The motivation for building FANTrack is to leverage the power of Siamese networks to model the similarities between targets and detections, CNNs to solve the data association problem in MOT, and an online track management module to update, initialize and prune tracks. We describe these modules in the following sections. <ref type="figure" target="#fig_1">Figure 2</ref> gives an overview of our proposed similarity network SimNet. The network has two input pairs with each pair corresponding to target and detection data, and consists of 3D bounding box parameters (1 × 7 dimensional vector) and image convolutional features (7 × 7 × 320 dimensional vector). The output from SimNet is a set of N max number of maps for each existing target corresponding to a local 5m × 5m region around the target and of 0.5m resolution. These output maps contain the similarity scores in each target's local neighbourhood with respect to all detections at a particular time step. The SimNet output is subsequently used for data association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Similarity Network</head><p>Functionally, SimNet computes a similarity score for every detection and target pair. It has two branches: a bounding box branch and an appearance branch, each of which uses a trainable Siamese network to learn object representations conditioned on whether two objects are similar or not. The outputs of these branches are vector representations of targets and detections. Their respective contribution towards the final similarity score computation is weighted using the importance branch. Finally, cosine-similarities of each target-detection vector representation are computed and the scalars are mapped to their corresponding positions on the above-mentioned set of local maps.</p><p>We describe our formulation of SimNet in the remainder of this sub-section.</p><p>1) Bounding Box Branch: The bounding box branch outputs a discriminative, robust vector representation for 3D object configurations of targets and detections, conditioned on whether they are similar or not. We train a Siamese network with stacked input pairs of target and detection 3D bounding boxes for this purpose. The 3D bounding boxes are defined by their centroids (x, y, z), axis-aligned dimensions (l, w, h), and rotation around the z-axis (θ z ) in the egocar's IMU/GPS coordinates. To prevent learning variations induced due to ego-motion, detection centroids are converted to coordinates at a common time-step using GPS data.</p><p>Architecture: The input to this branch is a (N + M ) × 1 × 7 tensor where the third dimension consists of the 7 bounding box parameters defined above. The inputs are fed to a convolutional layer with 256 1 × 1 filters to capture complex interactions across the 7 channels by pooling the parameters of each targets and detections independently <ref type="bibr" target="#b30">[31]</ref>. These object-independent features are then fed into two fullyconnected layers with 512 neurons, with dropout regularization. We apply L2 normalization on the output features, and henceforth refer to the result as unit features. Finally, the unit features of dimensions (N + M ) × 512 are sliced along the first dimension into target features and detection features using their respective counts (see <ref type="figure" target="#fig_2">Fig 3)</ref>. These are used to compute the bounding box cosine similarities as described in subsection A.4. We use batch normalization and leaky-ReLU across all layers.</p><p>2) Appearance Branch: The appearance branch outputs a robust and invariant vector representation for 2D visual cues of targets and detections conditioned on whether they belong to similar or dissimilar objects. We train another stacked Siamese network for this purpose. As its input, we concatenate convolutional features of targets and detections obtained from AVOD's ( <ref type="bibr" target="#b29">[30]</ref>) image feature extractors. Specifically, we use the second layer's convolutional features and the interpolated fourth layer's convolutional features. This is because the low-level features are local and more discriminative whereas high-level features are abstract, and are more invariant to appearance changes <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Architecture: The input to this branch is a (N + M ) × 7 × 7 × 320 convolutional feature. The architecture of the branch is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. First, we apply 256 3 × 3 convolutions to obtain promising features for similarity learning by preserving the spatial size of the input. Before flattening the feature maps for the fully-connected layers with 512 neurons, the Global Average Pooling (GAP) <ref type="bibr" target="#b33">[34]</ref> layer extracts one abstract feature from each feature map. Similar to the bounding box branch, L2 normalization yields a vector of dimension (N +M )×512. As in the case of the bounding box branch, the (N + M ) × 512 features are sliced along the first dimension to obtain appearance features of detections and targets to compute the appearance cosine similarities.</p><p>3) Importance Branch: The aim of this branch is to determine the relative importance of the bounding box and appearance features in the computation of the final cosine similarity score (see <ref type="figure" target="#fig_6">Fig. 5</ref>). Architecture: The inputs to this branch are the unit features from the other two branches. First, the vector representation of an object obtained from the appearance and bounding box branches is concatenated to form a single vector (dimension 1024). Then, a fully-connected layer with two neurons, ReLU activation, and a softmax layer that computes two scalars indicating importance weights (probabilities) of the two branches, for each target and detection. The importance weights obtained (ω bbox and ω appear ) are normalized to sum up to unity. 4) Similarity Maps: A similarity map (see <ref type="figure" target="#fig_7">Fig. 6</ref>) is computed for every target (for N targets we have N similarity maps) and contains its cosine similarity scores with all the detections within a 2D region of interest ([−40, 40] × [0, 80] m) in the ego-car's IMU/GPS coordinates. This map is referred to as the global similarity map. To compute the similarity scores, we perform the following: i) Each global map is split into grids with 0.5 m resolution.</p><p>ii) The detection appearance and bounding box features are positioned into the appropriate grid locations based on their location of detection.</p><p>iii) The target appearance and bounding box features are used as kernels to compute the similarity scores by the convolution with strides equal to 1. The computed scores correspond to cosine similarities as the features are normalized to unit vectors by the network branches.</p><p>Local similarity maps are then obtained from the global similarity maps for each target by cropping them around the target's local 5m×5m region corresponding to 10×10 cells. SimNet thus finally outputs N ×21×21 local similarity maps to be used for data association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Loss Function:</head><p>To learn the trainable parameters of the appearance branch, bounding box branch, and importance branch, we use the weighted cosine distance given by:</p><formula xml:id="formula_0">L(Θ 1 ) = 1 N + N i=1 w (i) skew × w (i) cost × 1 − y (i) ×ŷ (i) (Θ 1 )<label>(1)</label></formula><p>where Θ 1 is the network parameters, N + is the number of examples with nonzero weights, y (i) denotes the ground truth value of the i th example, i.e., y (i) ∈ {−1, 1}.ŷ (i) is the estimated cosine similarity score computed using the cosine similarities from the two branches and their normalized importance weights as follows:    be at least greater than those used for the final detections in AVOD (0.65) <ref type="bibr" target="#b29">[30]</ref>. In addition, the diversity among bounding box proposals for each object is maintained by rejecting a new proposal whose IoUs with existing ones are greater than 0.95. <ref type="figure" target="#fig_8">Fig. 7</ref> introduces our proposed data association network, referred to as AssocNet. The purpose of this network is to associate targets to the detections. The input to the network is the set of local similarity maps of dimension N × 21 × 21 obtained from SimNet, containing cosine similarity scores for probable target-detection pairs. The output from the network is the target-to-detection association probabilities for each existing target.</p><formula xml:id="formula_1">y (i) (Θ 1 ) = ω bbox (Θ 1 ) (i) ×ŷ (i) bbox (Θ 1 ) + ω (i) appear (Θ 1 ) ×ŷ (i) appear (Θ 1 )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Association Network</head><p>We first describe how our framework handles noisy detections and a varying number of targets. In order to deal with varying number of inputs, we create N max − N extra channels with dummy maps, where N max denotes the maximum number of targets that can be tracked. The dummy maps are a matrix of zeros -a reasonable representation since zero inputs don't have any impact on the output of the convolutional layers. We deal with missed detections by introducing an extra cell for each of the N max targets to account for spurious detections and concatenate it to their map of logits as described in the architecture below. Architecture: The main building blocks of the AssocNet are convolutional, dilated convolutional (d-Conv), and fullyconnected layers, with batch-normalization and leaky-ReLU activation used in all the layers. We take advantage of the increased receptive field of dilated convolutions <ref type="bibr" target="#b35">[36]</ref> to compensate for the sparsity of local similarity maps.</p><p>We now discuss the flow of information through Assoc-Net. The network processes the input using three dilated convolutional (d-Conv) layers with dilation factors of 2, 4, and 6 respectively. The neighbouring fields have slightly overlapping fields of view due to increased dilation size <ref type="bibr" target="#b36">[37]</ref>. The convolutional layer enables interactions between these neighbouring units which effectively results in considering all the detections simultaneously while making assignments. Thus to aggregate information, we employ a 3 × 3 convolutional layer at the end to compute the maps of logits (the vector of non-normalized predictions).</p><p>AssocNet is to be trained to predict assignment probabilities between a target and its probable detections. Since the locations of probable detections are known in each local similarity map, there is no need to train AssocNet to predict assignment probabilities of other locations as zero. To implement this idea, we generate association masks for each local similarity map. In the association masks, cells of probable detections are set to zero, while the other cells are set to a minimum negative number. Then the association masks are added to the map of logits obtained from the convolutional layer with 3 × 3 × 21 filters (see <ref type="figure" target="#fig_8">Fig.7</ref>). This maintains the values of the logits computed for probable detections, but makes other logits insignificant for further computation.</p><p>After masking the maps of logits, Assocnet is split into two branches. One branch consisting of fully-connected layers predicts the N max logit values of spurious detections. The other branch reshapes the logit map into 1D vectors to concatenate logits of spurious detections with those of probable detections. This results in a N max × (21 × 21 + 1) tensor. The softmax then computes the association probabilities for each target, which are is our required output.</p><p>The association probabilities are sliced and reshaped in order to obtain 2D association maps. The probabilities computed for spurious detections are missed-detection probabilities. Finally, we get rid of the association maps corresponding to the N max − N dummy channels.</p><p>1) Loss Function: Training AssocNet can be considered as training a classification problem in which labels are association maps showing the true data association for each existing target. To train the data association network we use a multi-task loss function given by:</p><formula xml:id="formula_2">L (Θ) = l (Θ) assoc + l (Θ) reg<label>(3)</label></formula><p>where Θ is the set parameters of the association network, l (Θ) reg is the regularization loss. l (Θ) assoc is the binary cross-entropy computed for the association maps as follows:</p><formula xml:id="formula_3">q vec = q (t) assoc (i, j) × log min q (t) assoc (i, j; Θ) + 0.01, 1 p vec = p (t) assoc (i, j) × log min p (t) assoc (i, j; Θ) + 0.01, 1 l (Θ) assoc = N t=1 21+1 i,j=1 (−q vec ) + (−p vec )<label>(4)</label></formula><p>where q (t)</p><formula xml:id="formula_4">assoc (i, j) = 1 − p (t)</formula><p>assoc (i, j) and 0.01 is the margin used to ignore negligible errors in the predicted probabilitieŝ p (t) assoc (i, j; Θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Track Management</head><p>The track management module takes care of state estimation, initiation, update, and termination of tracks. We use a Kalman filter for motion prediction and state estimation. We initiate, update and prune tracks with a Bayesian estimation model as specified in <ref type="bibr" target="#b37">[38]</ref> with a probability of existence P e . Our complete tracking algorithm is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we describe the dataset, training parameters, and experimental evaluation results for the tracker built using our proposed data association networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We used the KITTI Tracking benchmark dataset for training and evaluation of our approach. The KITTI Tracking dataset consists of 21 training sequences and 29 test sequences. As the training sequences have different levels Update P e ∀i ∈ T ;</p><formula xml:id="formula_5">∀(t k i , m k j ) Update track i with m k j ∀(t k i , N one) Propagate predicted t k i to τ = k + 1 ∀(N one, m k j ) Create a new track;</formula><p>∀i ∈ T if P k ei &lt; θ ex then P rune i end end end of difficulty, occlusion, and clutter, we split the 20% of every training sequence for validation. This way, training and validation datasets are not skewed. For training SimNet, we construct a training dataset from the training sequences by generating positive and negative examples in consecutive frames using ground truth information. Geometric transformations (translation, rotation, and scaling) are applied to the ground-truth bounding box parameters to model partial occlusion and detector noise. This gives a large training set in which the ratio of negatives to positives is approximately 18 : 25. We trained the object detector using a combined dataset consisting of the KITTI 3D object detection dataset and the 80% split of the KITTI training dataset mentioned earlier after pre-training on a synthetic dataset <ref type="bibr" target="#b43">[44]</ref>.</p><p>B. Training Parameters 1) Similarity Network: SimNet is trained with minibatches of size 128. Each mini-batch consists of the spatial indices of detections in the global map, the number of targets (N ), target centroids in x-y coordinates, target and detection appearance features, their bounding box parameters, and the labels of each example. To optimize the loss function (1) we used Adam optimizer and exponentially-decaying learning rate <ref type="bibr" target="#b44">[45]</ref>. The learning rate is initially set to 1e − 5 and then decreased every 100 epochs with a base of 0.95.</p><p>2) AssocNet: To optimize the loss function in (3) we used Adam optimizer and exponentially-decaying learning rate. The learning rate was initially set to 1e−6 and then decreased every 20 epochs with a base of 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>We use the popular CLEAR MOT metrics <ref type="bibr" target="#b45">[46]</ref> for evaluating our tracker. Multiple Object Tracking Accuracy (MOTA) gives us an estimate of the tracker's overall performance. However, this is dependent on the performance of the object detector. Hence, we also look at tracking specific metrics like Mostly Tracked (MT), Mostly Lost (ML), ID Switches (IDS) and fragmentation (FRAG), which evaluate the efficiency of the tracker in assigning the right IDs with reduced switches or fragmentation in the tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We do an ablation study to evaluate the components in our approach by comparing them with traditional approaches. Firstly, we study the impact of the similarity network. In Table II, Euclidean and Manhattan denote the baseline distances modeled with the 3D position estimates. Bhattacharyya and ChiSquare metrics are built from the image histograms of the cropped targets and detections to study the image-only configuration. SimNet and AssocNet denote our similarity and Association networks respectively. From <ref type="table" target="#tab_1">Table II</ref>, we could infer that conventional similarity approaches were not able to achieve comparable accuracy (MOTA) as the features involved in the computation of the similarity scores were not robust. We also study the impact of our association network by replacing it with a baseline Hungarian approach. Again, we could observe that the baseline approaches like Hungarian couldn't fare better than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Evaluation</head><p>We perform a qualitative evaluation by running our tracker on the KITTI tracking validation and testing sequences. We analyze different scenarios including occlusions, clutter, parked vehicles and false negatives from the detector. <ref type="figure">Fig. 8</ref> shows an example from sequence 0 in the test set. Different tracks representing the vehicles are color coded and the track IDs are displayed for reference. The tracker is able to perform well in spite of the clutter due to the closely parked cars. In <ref type="figure">Fig. 9</ref> we see an example from test sequence 17 in which the false negative by the detector is overcome with the help of the prediction of the tracker. These examples show the robustness of the tracker and its ability to perform better even with an average object detector. There were also some cases where the data association fails and as a result ID switching and fragmentation happen. In <ref type="figure" target="#fig_0">Fig. 10</ref> the track 38 was previously assigned to a nearby car but after an occlusion in the detection ID switching happens. This could be due to the low-lit conditions of the two cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Benchmark Results</head><p>We evaluate our approach on the test sequences on the KITTI evaluation server for the 'Car' class. The results are presented in <ref type="table" target="#tab_1">Table I</ref>. Due to the challenging nature of online tracking approach and to do a fair comparison, we only consider published online tracking approaches for our comparison. We achieve competitive results with respect to the state of the art in online tracking with improved MOTP which is better than most of the online methods. Our Mostly Tracked and Mostly Lost (MT &amp; ML) values are also competitive which show the effectiveness of our data association approach. Further, our approach gives inferences in 3D and KITTI evaluations are done in 2D, which is not completely representative of our approach. It should also be noted that none of these approaches use deep learning for data association. On the other side, we have used a simple Kalman filter for state estimation and motion prediction which could potentially be improved by better tuning of <ref type="figure">Fig. 9</ref>. Qualitative Evaluation -In this example (video 17 in test set) the detection was missed by the detector and reappears in the next frame. But the tracker was able to successfully maintain the track <ref type="figure" target="#fig_0">Fig. 10</ref>. Qualitative Evaluation -An example from video 15 in test set where ID switching occurs for Track 38 due to low-lit conditions parameters or trying out more sophisticated approaches for track management.</p><p>After optimizing the convolution operation in subsection A.4 with selective dot products our tracking algorithm has an average runtime of 0.04s per frame ( 25 Hz) on Nvidia GeForce GTX 1080 Ti and with a single thread on Intel Core i7-7700 CPU @ 3.60GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we presented a solution to the problem of data association in 3D online multi-object tracking using deep learning with multi-modal data. We have shown that a learning-based data association framework helps in combining different similarity cues in the data and provides more accurate associations than conventional approaches, which helps in increased overall tracking performance. We demonstrated the effectiveness of the tracker built using this model with a multitude of experiments and evaluations and show competitive results in the KITTI tracking benchmark. In the future, we plan to integrate this solution with an object detection framework more tightly and perform end-to-end training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overall architecture of the proposed approach to sensor motion, illumination, and angle of view; frequent occlusions, and abrupt changes in motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the proposed Siamese network for similarity learning. The branches highlighted in blue have trainable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Detailed architecture of the bounding box branch. Inputs are the concatenated bounding boxes of targets and detections. Outputs are sliced unit feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Detailed architecture of the appearance branch. Inputs are concatenated appearance feature maps of targets and detections. Outputs are sliced unit feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>skew is the weight used to remove the imbalance of negative examples in the training dataset. w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>cost scales the loss function according to how easy or hard it is to distinguish between each pair of examples so that the training can revolve around a sparse set of the selected hard examples [35]. 6) Creating training examples for SimNet: In this section, we describe creating positive and negative pairs of examples by augmentation from the KITTI training set to train the similarity network. A new bounding box proposal is a positive pair if its intersection over union (IoU) with its ground truth on images exceeds 0.8. The selected IoU thresholds should</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Detailed architecture of the importance branch. The inputs are the unit bounding box and appearance features of both targets and detections. Outputs are branch weights for bounding box and appearance branches. These weights can be further sliced for targets and detections separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Construction of global similarity map, one for each target. The target feature vector (yellow solid) is convolved with those of the detections to compute the similarity scores. Locations that do not include a detection feature vector are filled by zero vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>The architecture of the proposed association network. The inputs are local similarity maps from the proposed Siamese network. The outputs are the association maps which provide target-to-detection association and detection probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON KITTI TEST SET FOR 'CAR' CLASS higher values are better. ↓ denotes lower values are better) Fig. 8. Qualitative Evaluation -An example from video 14 in test set where the tracker performs well in a cluttered scene with parked cars.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">MOTA ↑</cell><cell cols="2">MOTP ↑</cell><cell cols="2">MT ↑</cell><cell>ML ↓</cell><cell>IDS ↓</cell><cell>FRAG ↓</cell></row><row><cell cols="2">MOTBeyondPixels [39]</cell><cell cols="2">84.24 %</cell><cell cols="2">85.73 %</cell><cell cols="2">73.23 %</cell><cell>2.77 %</cell><cell>468</cell><cell>944</cell></row><row><cell>JCSTD [40]</cell><cell></cell><cell cols="2">80.57 %</cell><cell cols="2">81.81 %</cell><cell cols="2">56.77 %</cell><cell>7.38 %</cell><cell>61</cell><cell>643</cell></row><row><cell cols="2">3D-CNN/PMBM [41]</cell><cell cols="2">80.39 %</cell><cell cols="2">81.26 %</cell><cell cols="2">62.77 %</cell><cell>6.15 %</cell><cell>121</cell><cell>613</cell></row><row><cell>extraCK [42]</cell><cell></cell><cell cols="2">79.99 %</cell><cell cols="2">82.46 %</cell><cell cols="2">62.15 %</cell><cell>5.54 %</cell><cell>343</cell><cell>938</cell></row><row><cell>MDP [43]</cell><cell></cell><cell cols="2">76.59 %</cell><cell cols="2">82.10 %</cell><cell cols="2">52.15 %</cell><cell>13.38 %</cell><cell>130</cell><cell>387</cell></row><row><cell>FANTrack (Ours)</cell><cell></cell><cell cols="2">77.72 %</cell><cell cols="2">82.32 %</cell><cell cols="2">62.61 %</cell><cell>8.76 %</cell><cell>150</cell><cell>812</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">TABLE II</cell><cell></cell></row><row><cell cols="9">ABLATION STUDY ON KITTI VALIDATION SET FOR 'CAR' CLASS</cell></row><row><cell>Method</cell><cell cols="2">MOTA ↑</cell><cell cols="2">MOTP ↑</cell><cell cols="2">MT ↑</cell><cell cols="2">PT ↑</cell><cell>ML ↓</cell><cell>IDS ↓ FRAG ↓</cell></row><row><cell>Euclidean+AssocNet</cell><cell cols="2">56.16 %</cell><cell cols="2">84.84 %</cell><cell cols="2">72.22 %</cell><cell cols="2">18.51 %</cell><cell>9.25 %</cell><cell>269</cell><cell>320</cell></row><row><cell>Manhattan+AssocNet</cell><cell cols="2">56.75 %</cell><cell cols="2">84.83 %</cell><cell cols="2">73.14 %</cell><cell cols="2">17.59 %</cell><cell>9.25 %</cell><cell>265</cell><cell>319</cell></row><row><cell>Bhattacharyya+AssocNet</cell><cell cols="2">56.69 %</cell><cell cols="2">84.81 %</cell><cell cols="2">72.22 %</cell><cell cols="2">18.51 %</cell><cell>9.25 %</cell><cell>256</cell><cell>307</cell></row><row><cell>ChiSquare+AssocNet</cell><cell cols="2">57.17 %</cell><cell cols="2">84.81 %</cell><cell cols="2">73.14 %</cell><cell cols="2">18.51 %</cell><cell>8.33 %</cell><cell>262</cell><cell>311</cell></row><row><cell>SimNet+Hungarian</cell><cell cols="2">74.59 %</cell><cell cols="2">84.92 %</cell><cell cols="2">65.74 %</cell><cell cols="2">23.14 %</cell><cell>11.11 %</cell><cell>26</cell><cell>93</cell></row><row><cell>SimNet+AssocNet</cell><cell cols="2">76.52 %</cell><cell cols="2">84.81 %</cell><cell cols="2">73.14 %</cell><cell cols="2">17.59 %</cell><cell>9.25 %</cell><cell>1</cell><cell>54</cell></row><row><cell cols="2">(↑ denotes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A boosted particle filter: Multitarget detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Okuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taleghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="247" to="266" />
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking multiple targets based on min-cost network flows with detection in rgb-d data</title>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="330" to="339" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Türetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1955</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust tracking-by-detection using a detector confidence particle filter, what</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1515" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4696" to="4704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple objects fusion tracker using a matching network for adaptively represented instance pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online multi-target tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<idno>abs/1604.03635</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJRR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1229" to="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">an algorithm for tracking multiple targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1202" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-target tracking using joint probabilistic data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Fortmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-target tracking by lagrangian relaxation to min-cost network flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1846" to="1853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Global data association for multiobject tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multiple object tracking using the shortest path faster association algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TheScientificWorldJournal</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page">481719</biblScope>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mcmc-based particle filtering for tracking a variable number of interacting targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1805" to="1819" />
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discrete-continuous optimization for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kálmán</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Novel approach to nonlinear / non-gaussian bayesian state estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F M</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A skeletonization algorithm by maxima tracking on euclidean distance transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="331" to="341" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">6d-vision: Fusion of stereo and motion for robust environment perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-08" />
			<biblScope unit="volume">3663</biblScope>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gmcp-tracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">7573</biblScope>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3786" to="3795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1701.01909</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese CNN for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno>abs/1604.07866</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end learning of multi-sensor 3d tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1806.11534</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno>arxiv:1712.02294</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>arxiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>arxiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Effective use of dilated convolutions for segmenting small object instances in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ryuhei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keisuke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tomoyuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuhei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in WACV</title>
		<imprint>
			<biblScope unit="page" from="1442" to="1450" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Joint target detection and tracking filter for chilbolton advanced meteorological radar data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Houssineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<idno>abs/1802.09298</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Vehicle detection and tracking in wide field-of-view aerial video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mono-camera 3d multi-object tracking using deep learning detections and pmbm filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benjaminsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Granstrom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09975</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A lightweight online multiple object vehicle tracking method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gündüz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Acarman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="427" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to track: Online multiobject tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4705" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Precise synthetic image and lidar (presil) dataset for autonomous vehicle perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hurl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00160v1</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arxiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
