<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphing and Sampling Network for Dense Point Cloud Completion *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Liu</surname></persName>
							<email>minghua@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
							<email>lsheng@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yang</surname></persName>
							<email>shengyang93fs@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>4 Sensetime</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
							<email>shaojing@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>4 Sensetime</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Morphing and Sampling Network for Dense Point Cloud Completion *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D point cloud completion, the task of inferring the complete geometric shape from a partial point cloud, has been attracting attention in the community. For acquiring highfidelity dense point clouds and avoiding uneven distribution, blurred details, or structural loss of existing methods' results, we propose a novel approach to complete the partial point cloud in two stages. Specifically, in the first stage, the approach predicts a complete but coarse-grained point cloud with a collection of parametric surface elements. Then, in the second stage, it merges the coarse-grained prediction with the input point cloud by a novel sampling algorithm. Our method utilizes a joint loss function to guide the distribution of the points. Extensive experiments verify the effectiveness of our method and demonstrate that it outperforms the existing methods in both the Earth Mover's Distance (EMD) and the Chamfer Distance (CD).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Acquiring high-fidelity 3D models from real-world scans is challenging, which not only depends on the capability of sensors but also relies on sufficient views for scanning. Based on such restricted raw data, shape completion is required to compensate for the structural loss and enhance the quality, in order to benefit subsequent applications, such as shape classification <ref type="bibr" target="#b23">(Sarmad, Lee, and Kim 2019)</ref> and point cloud registration <ref type="bibr" target="#b39">(Yuan et al. 2018)</ref>.</p><p>Existing learning-based methods represent 3D shapes as volumetric grids <ref type="bibr" target="#b5">(Dai, Ruizhongtai Qi, and Nießner 2017;</ref><ref type="bibr" target="#b10">Han et al. 2017;</ref><ref type="bibr" target="#b28">Stutz and Geiger 2018)</ref> or view-based projection <ref type="bibr" target="#b17">(Park et al. 2017</ref>) and then leverage 3D/2D convolution operations. These methods suffer from high computational cost or loss of geometric information. With the advances in deep learning for point cloud analysis and generation, some reasonable works on 3D point cloud completion have been presented <ref type="bibr" target="#b39">(Yuan et al. 2018;</ref><ref type="bibr" target="#b9">Gurumurthy and Agrawal 2019;</ref><ref type="bibr" target="#b23">Sarmad, Lee, and Kim 2019;</ref><ref type="bibr" target="#b33">Tchapmi et al. 2019)</ref>, which prevents high memory footprints and artifacts caused by discretization. <ref type="figure">Figure 1</ref>: Our network predicts realistic structures from partial views and completes the point clouds evenly. Each pair (before and after completion) is visualized in the same color, with 32,768 points after completion.</p><p>However, due to the limited capability of analyzing and generating point clouds, these works sometimes produce distorted results or even fail to preserve some of the actual structures which have been revealed in the input. For example, they may be able to complete the overall shape of a chair, but may neglect the connectors between the chair legs although they appear in the input point cloud. On the other hand, the similarity metric for point cloud comparison plays an important role. The widely used Chamfer Distance (CD) tends to cause uneven density distribution and blurred details <ref type="bibr" target="#b0">(Achlioptas et al. 2018)</ref>. As an alternative, the Earth Mover's Distance (EMD) is more sensitive to details and the density distribution, yet suffers from high computational cost and thus have not been applied to dense point clouds.</p><p>To tackle these problems, we propose a novel network which completes the partial point cloud in two stages. In the first stage, we follow the auto-encoder architecture, and utilize a collection of 2-manifold like surface elements, which can be 2D parameterized, to assemble a complete point cloud. In order to prevent surface elements from overlapping, we propose an expansion penalty which motivates each surface element to be concentrated in a local area. Although we can predict a complete point cloud with only such an auto-encoder, the surface generation may be coarse-grained, and the prediction may also neglect some structures within the input. To this end, in the second stage, we combine the coarse-grained prediction with the input point cloud and employ a novel sampling algorithm to obtain an evenly distributed subset point cloud from the combination. A pointwise residual is then learned for the point cloud which enables fine-grained details. We use EMD to compare with the ground truth and utilize an auction algorithm <ref type="bibr" target="#b1">(Bertsekas 1992)</ref> for the EMD approximation, which can be applied to dense point clouds.</p><p>Extensive experiments verify the effectiveness of our novelties. Our method outperforms the existing methods with regard to both EMD and CD. <ref type="figure">Figure 1</ref> shows some completion results. The contribution of our work mainly includes: • a novel approach for dense point cloud completion, which preserves known structures and generates continuous and smooth details; • expansion penalty for preventing overlaps between the surface elements; • a novel sampling algorithm for obtaining an evenly distributed subset point cloud; • an implementation of the EMD approximation, which can be applied to dense point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>3D Shape Completion Conventional methods for 3D shape completion mainly includes geometry-based approaches and example-based approaches. Geometry-based approaches may interpolate smooth surfaces based on existing structures <ref type="bibr" target="#b6">(Davis et al. 2002;</ref><ref type="bibr" target="#b40">Zhao, Gao, and Lin 2007)</ref> or rely on some geometric assumptions, such as symmetry <ref type="bibr" target="#b34">(Thrun and Wegbreit 2005;</ref><ref type="bibr" target="#b26">Sipiran, Gregor, and Schreck 2014)</ref>. However, interpolation-based methods do not apply to the cases with large-scale incompleteness, and geometric assumptions do not always hold true for real-world 3D data. Examplebased approaches <ref type="bibr" target="#b19">(Pauly et al. 2005;</ref><ref type="bibr" target="#b30">Sung et al. 2015;</ref><ref type="bibr" target="#b24">Shen et al. 2012)</ref> first retrieve some similar models in a large shape database, and then deform and assemble the retrieved models to complete the partial shape. The shape database plays an important role in example-based approaches, making them impractical for completing rare-seen novel shapes.</p><p>Learning-based approaches utilize a parametric model (e.g., neural network) to learn a mapping between the partial shape and its completion. Lots of works resort to volumetric grids and leverage 3D convolution networks <ref type="bibr" target="#b5">(Dai, Ruizhongtai Qi, and Nießner 2017;</ref><ref type="bibr" target="#b10">Han et al. 2017;</ref><ref type="bibr" target="#b28">Stutz and Geiger 2018)</ref>. 3D shapes can be projected into 2D views and some methods use 2D convolution operations for novel view generation <ref type="bibr" target="#b32">(Tatarchenko, Dosovitskiy, and Brox 2016;</ref><ref type="bibr" target="#b17">Park et al. 2017)</ref>. Representing 3D shapes as polygon meshes, Litany et al. completes the partial human body and face meshes with the help of graph convolution and the reference mesh models. There are also some recent methods exploring continuous implicit filed for representing 3D shape <ref type="bibr" target="#b3">(Chen and Zhang 2019;</ref><ref type="bibr" target="#b18">Park et al. 2019</ref>). However, these methods have their own limitations, such as high computational cost, loss of geometric details, and applicability to only certain shape categories.</p><p>Point Cloud Analysis Without the loss of geometric information and the artifact from the discretization, point clouds can be a more efficient representation. However, since point clouds are unordered and may have varying densities, deep learning on irregular point clouds faces many challenges and we cannot apply traditional convolution on point clouds directly. PointNet <ref type="bibr" target="#b21">(Qi et al. 2017a</ref>) uses symmetric functions to aggregate information from individual points, followed by some improvements on local feature learning <ref type="bibr" target="#b22">(Qi et al. 2017b;</ref>. Some methods project point clouds to regular structures, which allows traditional convolution <ref type="bibr" target="#b29">(Su et al. 2018;</ref><ref type="bibr" target="#b31">Tatarchenko et al. 2018)</ref>. By constructing graphs for point clouds, some approaches employ graph-based analysis <ref type="bibr" target="#b11">(Hu, Cai, and Lai 2018;</ref><ref type="bibr" target="#b12">Landrieu and Simonovsky 2018;</ref><ref type="bibr" target="#b36">Wang et al. 2019b</ref>). There are also lots of works exploring specialized convolution operation for point clouds <ref type="bibr" target="#b12">(Jiang, Wu, and Lu 2018;</ref><ref type="bibr" target="#b13">Li et al. 2018;</ref><ref type="bibr" target="#b15">Liu et al. 2019)</ref>.</p><p>Point Cloud Generation Decoding point clouds from latent features has not been fully explored. Fan, Su, and Guibas generate point cloud coordinates using a fullyconnected branch and a 2D deconvolution branch. Fold-ingNet ) deforms a 2D plane into a 3D shape, which favors continuous and smooth structures. Combining the merits of the fully-connected layer and Fold-ingNet, PCN <ref type="bibr" target="#b39">(Yuan et al. 2018</ref>) proposes a coarse-to-fine point cloud generator. AlasNet <ref type="bibr" target="#b8">(Groueix et al. 2018</ref>) further represents 3D shape as a collection of parametric surface elements and learns the mappings from the 2D square to 3D surface elements, which enables generating complex shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Given a point cloud lying on the partial surface of an object, our approach is expected to predict a point cloud indicating the complete shape of the object. The output point cloud should be dense enough and evenly distributed so that it can capture the details of the shape. Our approach leverages supervised learning and is trained end-to-end.</p><p>As shown in <ref type="figure">Figure 2</ref>, our approach takes a partial point cloud as input and completes it in two stages. In the first stage, the auto-encoder predicts a complete point cloud by morphing the unit squares into a collection of surface elements. The expansion penalty is proposed to prevent the overlaps between the surface elements. In the second stage, we merge the coarse output with the input point cloud. Through a special sampling algorithm, we obtain an evenly Figure 2: Architecture of our approach. "GFV" denotes the generalized feature vector, L expansion and L EMD denotes the expansion penalty and Earth Mover's Distance respectively. "[0, 1] 2 Sampling" denotes sampling 2D points on a unit square. The morphing-based decoder morphs the unit squares into a collection of surface elements, which are assembled into the coarse output. The minimum density sampling outputs an evenly distributed subset point cloud.</p><p>distributed subset point cloud from the combination, and then feed it into a residual network for point-wise residual prediction. By adding the residual, our approach outputs the final point cloud. Unlike many existing approaches, we employ EMD for dense point cloud comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphing-Based Prediction</head><p>In the first stage, we hope to predict a point cloud, which captures the overall shape of the object, with an autoencoder. For efficiency, the encoder is designed following the idea of PointNet <ref type="bibr" target="#b21">(Qi et al. 2017a</ref>), though we could use other networks for feature extracting as well. Inspired by the AtlasNet <ref type="bibr" target="#b8">(Groueix et al. 2018)</ref>, we then feed the extracted features into a morphing-based decoder for predicting continuous and smooth shapes.</p><p>As shown in the bottom left of <ref type="figure">Figure 2</ref>, the decoder employs K (16 in experiments) surface elements to form a complex shape. Each surface element is expected to focus on a local area which is relatively simple, making the generation of local surfaces easier. For each element, the decoder learns a mapping from the unit square [0, 1] 2 to the 3D surface using a multilayer perceptron (MLP), which mimics the morphing of a 2D square into a 3D surface. In each forward pass, we randomly sample N (512 in experiments) points in the unit square. The encoded feature vector, which describes the prediction, is then concatenated with the sampled point coordinates, before passing them as input to the K MLPs. Each sampled 2D point will be mapped to K 3D points lying on the K different surface elements. As a result, each forward pass outputs <ref type="bibr">KN (8,</ref><ref type="bibr">192</ref> in experiments) points describing the predicted shape. Since the MLPs learn continuous mappings from 2D to 3D, the decoder can generate smooth surfaces by dense sampling on 2D. While many approaches, like the fully-connected layer, output fix-sized discrete point coordinates, our approach can combine the results from multiple forward passes to generate point clouds with arbitrary resolution. For instance, <ref type="figure">Figure 1</ref> shows dense point clouds generated by 4 forward passes.</p><p>Although we will use similarity metrics (e.g., EMD) to guide the union of the MLPs cover the whole shape of the prediction, the MLPs in the AtlasNet <ref type="bibr" target="#b8">(Groueix et al. 2018)</ref> are not explicitly prevented from generating the same area of space, which may result in overlaps between the surface elements. <ref type="figure" target="#fig_0">Figure 3</ref> shows such an example, in the left point cloud of each pair, points from different surface elements tend to mix with each other. The overlaps may lead to the uneven density distribution of the point cloud. It may also cause surface elements to expand and cover larger areas, which makes it more difficult to morph the 2D square and capture local details.</p><p>To this end, we propose an expansion penalty, which serves as a regularizer for surface elements. It encourages each surface element to be compact and concentrated in a local area. Specifically, in each forward pass, we regard the generated 3D points from each MLP as a vertex set and construct a minimum spanning tree T i <ref type="bibr" target="#b20">(Prim 1957)</ref> for each of them based on the Euclidean distances between the points. We then choose the middle vertex of T i 's diameter (i.e., the simple path containing the most vertices) as the root vertex and direct T i by making all its edges point toward the root. <ref type="figure" target="#fig_1">Figure 4</ref> shows an example of the construction. This way, we have K directed minimum spanning trees which describe the distribution of points from each MLP. As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, edges with longer length (i.e., the distance between the pair of points) suggest more sparsely distributed points which tend to mix with points from other MLPs. The expansion penalty thus makes those points shrink along the edges toward the more compact areas. It can be formulated as:</p><formula xml:id="formula_0">Lexpansion = 1 KN 1≤i≤K (u,v)∈T i 1{dis(u, v) ≥ λli}dis(u, v)</formula><p>where dis(u, v) denotes the Euclidean distance between vertex u and vertex v, l i = ( (u,v)∈Ti dis(u, v))/(N − 1) denotes the average length of edges in T i , and 1 is the indicator function filtering edges whose length are shorter than λl i (λ is 1.5 in experiments). The expansion penalty is differentiable almost everywhere, since the constructed spanning tree is invariant under the infinitesimal movement of the points. For each directed edge (u, v) ∈ T i , whose length is longer than λl i , we only give u a gradient in the backward passes. So that u is motivated to shrink toward v, making a more compact surface element.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, thanks to the expansion penalty, the overlaps between the surface elements are mitigated. The MLPs divide the whole shape into K parts and each MLP covers a local part. The partition even corresponds to the semantic parts of the object, which shows the potential for the downstream semantic applications. Although there may be more intuitive methods to motivate each surface element to be concentrated (e.g., the distances to the mean point), we find them over-constrain the shape of each element. The key idea of our spanningtree based method is that we only want to penalize those points which are sparsely distributed (e.g., those sparsely distributed points on the boundary of each surface element) instead of all the points. Our expansion penalty thus allows each surface element to generate more flexible shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging and Refining</head><p>With the morphing-based auto-encoder, we can generate a smooth point cloud predicting the overall shape. However, due to the limited capabilities, the auto-encoder may neglect some structures, which have been revealed in the input point cloud. Also, the fixed-sized surface elements are not flexible enough for fine-grained local details. Therefore, we merge the coarse output from the auto-encoder with the input and then learn a point-wise residual for the combination.</p><p>Since the density of the two point clouds may be different and there may be overlapping between them, the merged point cloud is probably unevenly distributed. We thus hope to sample a subset point cloud, which has a uniform distribution, from the combination. Existing sampling algorithms for point clouds, such as the farthest point sampling (FPS) and Poisson disk sampling (PDS) <ref type="bibr" target="#b37">(Wei 2008)</ref>, cannot guarantee the global density distribution of the results. <ref type="figure" target="#fig_2">Figure 5</ref> shows such an example: the input point cloud consists of two parts, with the right part being twice as dense as the left part. The sampling results of FPS and PDS are unevenly distributed. Although the area is the same, the number of points on the right side is much larger than the number of points on the left side. Inspired by the point cloud uniformization algorithm using graph Laplacian <ref type="bibr" target="#b16">(Luo, Ge, and Wang 2018)</ref>, we employ the summation of Gaussian weights to estimate the "density" of a point and thus propose a novel sampling algorithm namely minimum density sampling (MDS). We denote the ith sampled point as p i and the set of first i sampled points as P i = {p j |1 ≤ j ≤ i}. Unlike FPS returning the farthest point from P i−1 as p i , in each iteration, MDS returns a point that has the minimum "density":</p><formula xml:id="formula_1">pi = argmin x / ∈P i−1 p j ∈P i−1 exp(− x − pj 2 /(2σ 2 ))</formula><p>where the parameter σ is a positive quantity, which corresponds to the size of the neighborhood considered. As shown in <ref type="figure" target="#fig_2">Figure 5</ref>, MDS outputs a subset point cloud whose global distribution is more uniform than that of FPS and PDS.</p><p>Taking the evenly distributed subset point cloud as input, we then learn a point-wise residual for refinement, which enables the generation of fine-grained structures. Since the points from the input are more reliable, in addition to the channels of the point coordinates, we add another binary channel to the input to distinguish the source of each point, where "0" stands for the input point cloud and "1" for the coarse output. The architecture of the residual network resembles PointNet <ref type="bibr" target="#b21">(Qi et al. 2017a</ref>), which consumes a point cloud and outputs a three-channel residual. We output the final point cloud after adding the residual point by point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Metric</head><p>One of the challenges in point cloud completion is the comparison with the ground truth. Existing similarity metrics mainly include the Chamfer Distance (CD) and the Earth Mover's Distance (EMD) <ref type="bibr" target="#b7">(Fan, Su, and Guibas 2017)</ref>. For two point clouds S 1 and S 2 , CD measures the mean distance between each point in one point cloud to its nearest neighbor in the other point cloud:</p><formula xml:id="formula_2">LCD(S1,S2)= 1 2 1 |S1| x∈S 1 min y∈S 2 x−y + 1 |S2| y∈S 2 min x∈S 1</formula><p>x−y EMD is only defined when S 1 and S 2 have the same size:</p><formula xml:id="formula_3">LEMD (S1, S2) = min φ:S 1 →S 2 1 |S1| x∈S 1 x − φ(x) 2</formula><p>where φ is a bijection. Most existing works employ CD as a loss function, since it's more efficient to compute. However, CD is blind to some visual inferiority <ref type="bibr" target="#b0">(Achlioptas et al. 2018)</ref>. <ref type="figure" target="#fig_3">Figure 6</ref> shows such an example. In the outputs of the second method, points tend to over-populate in those locations where most objects of that category have mass (e.g., table tops for tables), and the details of the changeable parts are always blurred. However, it's hard for CD to penalize this type of cheat since one of its summands can be very small and the other one can also be not so large.</p><p>By solving the linear assignment problem, EMD forces the output to have the same density distribution as the ground truth and is thus more discriminative to the local details and the density distribution. Most existing methods employ an implementation for EMD approximation which needs O(n 2 ) memory footprints (n denotes the number of points) and cannot be applied to dense point clouds due to the memory bottleneck. Therefore, many existing works <ref type="bibr" target="#b39">(Yuan et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018;</ref><ref type="bibr" target="#b35">Wang et al. 2019a</ref>) use EMD only for point clouds with about 2,000 points, which is not sufficient to capture many details.</p><p>Inspired by the auction algorithm <ref type="bibr" target="#b1">(Bertsekas 1992</ref>), a constant approximation for the linear assignment problem, we implement an approximation for EMD, which only needs O(n) memory. It can thus be applied to dense point clouds for comparing more details. Specifically, the algorithm treats the points from the two point clouds as persons and objects respectively and finds an economic equilibrium by proceeding the bidding phases and assignment phases iteratively. The algorithm terminates in finite iterations and outputs an assignment of points whose the mean distance is within of being optimal. Here, is a parameter which balances the error rate and the speed of convergence. In training processes, to accelerate the calculation of the loss function, we fix the number of iterations and assign the remaining points greedily. The whole procedure of our implementation is parallelized enabling deep learning with GPUs.</p><p>In fact, there are many other variants of the transportation distances <ref type="bibr" target="#b4">(Cuturi 2013;</ref><ref type="bibr" target="#b27">Solomon et al. 2015)</ref>, and it's promising to explore whether they can be applied to comparing two point clouds in an efficient and effective manner.</p><p>Our joint loss function L can thus be calculated as:</p><formula xml:id="formula_4">L = LEMD(Scoarse, Sgt) + αLexpansion + βLEMD(S final , Sgt)</formula><p>where S coarse denotes the coarse output, S final denotes the final output, and S gt denotes the ground truth. α and β are weighting factors (α is 0.1 and β is 1.0 in experiments). The EMD terms and the expansion penalty work in opposite directions. The former motivates the point cloud to cover the whole shape of the object, while the latter serves as a regularizer which encourages each surface elements to shrink. Their mutual restraint allows each surface element to be centralized in a local area while the union of the elements is as close as possible to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Data Generation and Model Training</head><p>We evaluate our methods on the ShapeNet dataset <ref type="bibr" target="#b2">(Chang et al. 2015)</ref>. We choose 30,974 synthetic CAD models from the dataset, which cover eight categories: table, chair, car, airplane, sofa, lamp, vessel, and cabinet. For a fair comparison, the train/test split is the same as in PCN <ref type="bibr" target="#b39">(Yuan et al. 2018)</ref>. We generate 50 pairs of partial and complete point clouds for each of the CAD model, resulting in 30,974 × 50 pairs of point clouds for training and test. Specifically, the CAD models are normalized and located at the origin. For each of them, we uniformly sample 8,192 points on the surface, which form the complete point cloud. We then randomly sample 50 camera poses on the unit sphere and lift the 2.5D captured images into 3D partial point clouds, which mimics obtaining 3D raw data from different views in the real-world applications. After denoising, for convenience of training, the partial point clouds are unified into the size of 5,000 points by randomly dropping or replicating points. We trained our models on 8 Nvidia GPUs for 2.3 × 10 5 iterations (i.e., 25 epochs) with a batch size of 160. The initial learning rate is 1e-3 and is decayed by 0.1 per 10 epochs. Adam is used as the optimizer. All activation functions are ReLU except for the final tanh layers producing the point coordinates and residuals. The models are trained end-to-end for all object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Existing Methods</head><p>We compare our approach to the following methods. FCAE uses an intuitive auto-encoder where the encoder follows PointNet <ref type="bibr" target="#b21">(Qi et al. 2017a</ref>) and the decoder is a fully connected layer generating the coordinates of 8,192 points directly. CD is used as the loss function. AtlasNet <ref type="bibr" target="#b8">(Groueix et al. 2018</ref>) employs the similar encoder but generates the point cloud with a set of parametric surface elements. Since AtlasNet outputs 2,500 points per forward pass, we combine the generated points of 4 different passes and then randomly sample 8,192 points from the combination. PCN <ref type="bibr" target="#b39">(Yuan et al. 2018)</ref> completes the partial point cloud through an autoencoder as well. It utilizes a stacked version of PointNet <ref type="bibr" target="#b21">(Qi et al. 2017a)</ref> as the encoder and generates point clouds in a coarse-to-fine fashion. We randomly sample 8,192 points from the output for comparison. The Oracle method randomly samples 8,192 points on the complete surface of the object. The reported distances between the randomly sam-pled points and the ground truth point cloud provide an estimation of the upper bound of the performance. Ours-CD is the same as our method except for replacing EMD with CD.</p><p>The quantitative results are reported in <ref type="table" target="#tab_0">Table 1</ref>. It can be seen that our methods outperform existing methods with regard to both EMD and CD. As mentioned earlier, compared to CD, EMD is more discriminative and convincing for point cloud comparison. Our method achieves the lowest EMD in all object categories and the average EMD is only 57.9% and 51.5% to that of AtlasNet and PCN, which demonstrates the superiority of our method. The EMD differs in various object categories suggesting the difficulties of completing different categories varies. Specifically, there are more airplanes and cars in the dataset and their structures are relatively simple and stable, making them easier to complete. In contrast, various lamps are relatively isolated in the dataset, which are more difficult to complete. As for CD, the discrepancies among different methods and object categories are relatively small. Although "Ours" uses EMD as the loss function while the existing methods use CD, "Ours" outperforms them with regard to CD and "Ours-CD" achieves lower results. However, there is still a huge gap between the results of completion methods and the "Oracle", which indicates the completion task is arduous.   <ref type="figure" target="#fig_4">Figure 7</ref> shows qualitative results. All methods work well with simple cases, such as the first airplane, but the discrepancy of the methods appears in complex cases. Specifically, FCAE and PCN tend to generate blurred details which may be the results of using the fully connected layers to output coordinates directly. In contrast, our method predicts more realistic structures and generates continuous and smooth details. Also, our method employs EMD as the loss function, which guarantees the even distribution of the points, while existing methods are more likely to overpopulate points in some parts. It can also be seen that our method can preserve the known structures, while other methods always distort or even neglect the structures revealed in the input. Moreover, unlike FCAE and PCN outputting the fix-sized point clouds, our method is capable of generating dense point cloud with arbitrary resolution. <ref type="figure">Figure 1</ref> shows such an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We compare our method to some ablated versions which are described in <ref type="table" target="#tab_1">Table 2</ref>. With regard to the expansion penalty, by remeshing the point clouds with the Ball-Pivoting Algorithm <ref type="bibr" target="#b0">(Bernardini et al. 1999)</ref>, we compute the sum of the areas of the surface elements. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>, which suggests that, without the expansion penalty, each surface element tends to cover a significantly larger area. <ref type="figure" target="#fig_0">Figure 3</ref> demonstrates that the expansion penalty can motivate each surface element to be centralized in a local area and prevent surface elements from overlapping. As shown in Ta-  ble 3, version A, without the expansion penalty, typically produces point clouds with larger EMD and CD. With regard to the merging operation, merging the coarse output with the input point cloud can preserve some known structures resulting in a more reliable output. Version B, without the merging operation and the subsequent refining operation, typically produces point clouds with larger EMD and CD. Moreover, we use MDS to obtain an evenly distributed subset point cloud. Replacing the MDS with the FPS, version C cannot guarantee the even distribution of the points, which causes a significantly larger EMD. However, since the FPS may preserve more points from the reliable input, version C produces point clouds with smaller CD. With regard to the refining operation, the point-wise residual enables the generation of fine-grained details. Without the refining operation, version D tends to produce point clouds with larger EMD and CD. With regard to the similarity metric, we implement an approximation which allows calculating EMD for dense point clouds. Version E replaces EMD with CD and produces solutions with larger EMD and smaller CD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have presented a novel approach for point cloud completion, which completes the partial point cloud in two stages. With the expansion penalty, we can effectively control the distribution of the points. The novel sampling operation enables us to preserve the known structures. Moreover, we have discussed the similarity metrics and implemented an efficient approximation for EMD. Extensive experiments demonstrate that our approach predicts more realistic structures and generates dense point clouds evenly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The figure shows pairs of point clouds. Points from the same surface elements are in the same color. In each pair, the left one is the result without the expansion penalty, where different surface elements tend to mix with each other. The right one is the result with the expansion penalty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The left figure shows the point cloud of a surface element (2D for clear presentation). The right figure shows the directed minimum spanning tree, where the green point indicates the root. For all red edges (u, v), u is motivated to shrink toward v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The figure shows the results of sampling 400 points from 600 points. The global distribution of MDS's result is more uniform than that of FPS and PDS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The figure shows the completion results of two different methods. The EMD is more reliable to distinguish the visual quality of the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The figure shows the completion results of different methods. Each output point cloud consists of 8,192 points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison between our methods and existing methods. For both EMD and CD, lower is better. methods vessel cabinet table airplane car chair sofa lamp average Oracle 0.93 1.44 1.21 0.69 1.25 1.17 1.28 0.91 1.11 FCAE 7.22 11.20 7.77 4.10 7.00 7.64 7.00 14.64 8.32 AtlasNet 8.11 8.91 5.07 3.27 4.20 5.03 6.97 10.71 6.53 PCN 6.56 8.79 6.84 3.44 4.44 6.89 6.28 15.45 7.34</figDesc><table><row><cell>Ours 3.83 4.16 3.66 2.18 3.28 3.63 3.47 6.04 3.78</cell></row><row><cell>(a) EMD × 100</cell></row><row><cell>methods vessel cabinet table airplane car chair sofa lamp average</cell></row><row><cell>Oracle 0.42 0.72 0.55 0.28 0.63 0.50 0.60 0.35 0.50</cell></row><row><cell>FCAE 1.33 1.40 1.16 0.70 1.10 1.29 1.37 1.72 1.26</cell></row><row><cell>AtlasNet 2.30 2.49 1.46 0.85 1.42 1.58 2.67 1.82 1.82</cell></row><row><cell>PCN 1.23 1.35 1.14 0.66 1.10 1.41 1.36 1.46 1.21</cell></row><row><cell>Ours 1.17 1.37 1.15 0.60 1.11 1.16 1.31 1.30 1.14</cell></row><row><cell>Ours-CD 0.99 1.19 0.96 0.56 1.03 1.02 1.16 1.07 1.00</cell></row><row><cell>(b) CD × 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The ablated versions of our method. " " indicates including that component while "×" indicates not.</figDesc><table><row><cell>methods</cell><cell cols="4">Lexpansion merging refining CD/EMD</cell></row><row><cell>w/o Lexpansion (A)</cell><cell>×</cell><cell>MDS</cell><cell></cell><cell>EMD</cell></row><row><cell>w/o merging (B)</cell><cell></cell><cell>×</cell><cell>×</cell><cell>EMD</cell></row><row><cell>w/o MDS (C)</cell><cell></cell><cell>FPS</cell><cell></cell><cell>EMD</cell></row><row><cell>w/o refining (D)</cell><cell></cell><cell>MDS</cell><cell>×</cell><cell>EMD</cell></row><row><cell>Ours-CD (E)</cell><cell></cell><cell>MDS</cell><cell></cell><cell>CD</cell></row><row><cell>Ours</cell><cell></cell><cell>MDS</cell><cell></cell><cell>EMD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison between our method and the ablated versions. For descriptions of the methods, seeTable 2 and the text. For both EMD and CD, lower is better. methods vessel cabinet table airplane car chair sofa lamp average A 3.94 4.33 3.85 2.23 3.47 3.78 3.59 6.08 3.91 B 4.18 4.37 4.08 2.39 3.46 3.89 3.75 6.51 4.08 C 4.30 5.30 4.24 2.59 4.01 4.41 4.18 6.38 4.43 D 3.93 4.32 3.73 2.38 3.41 3.73 3.64 6.02 3.89 E 5.44 6.81 4.52 3.01 4.39 5.44 5.62 8.93 5.52 Ours 3.83 4.16 3.66 2.18 3.28 3.63 3.47 6.04 3.78 (a) EMD × 100</figDesc><table><row><cell cols="2">methods vessel cabinet table airplane car chair sofa lamp average</cell></row><row><cell>A</cell><cell>1.20 1.46 1.22 0.62 1.15 1.23 1.38 1.37 1.20</cell></row><row><cell>B</cell><cell>1.36 1.48 1.29 0.70 1.19 1.30 1.45 1.59 1.29</cell></row><row><cell>C</cell><cell>1.09 1.38 1.12 0.58 1.11 1.10 1.27 1.23 1.11</cell></row><row><cell>D</cell><cell>1.24 1.44 1.21 0.64 1.15 1.23 1.40 1.39 1.21</cell></row><row><cell>E</cell><cell>0.99 1.19 0.96 0.56 1.03 1.02 1.16 1.07 1.00</cell></row><row><cell cols="2">Ours 1.17 1.37 1.15 0.60 1.11 1.16 1.31 1.30 1.14</cell></row><row><cell></cell><cell>(b) CD × 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The surface area for the ground truth, and the sum of the areas of the surface elements for the two methods. The numbers are the averages over all object categories.</figDesc><table><row><cell cols="3">ground truth w/o L expansion Ours</cell></row><row><cell>1.11</cell><cell>2.41</cell><cell>1.56</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
	<note>The ball-pivoting algorithm for surface reconstruction</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Auction algorithms for network flow problems: A tutorial introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="66" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<title level="m">ShapeNet: An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape completion using 3D-encoder-predictor CNNs and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhongtai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5868" to="5877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Filling holes in complex surfaces using volumetric diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symposium on 3D Data Processing Visualization and Transmission</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Guibas ; Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A papier-mâché approach to learning 3D surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groueix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High fidelity semantic shape completion for point clouds using latent optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurumurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrawal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1099" to="1108" />
		</imprint>
	</monogr>
	<note>Gurumurthy and Agrawal 2019</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-resolution shape completion using deep neural networks for global structure and local geometry inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic labeling and instance segmentation of 3D point clouds using patch context analysis and multiscale processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai ;</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TVCG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PointSIFT: A SIFT-like network module for 3D point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu ;</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>abs/1807.00652</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
	<note>Large-scale point cloud semantic segmentation with superpoint graphs</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deformable shape completion with graph convolutional autoencoders</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1886" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relationshape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uniformization and density adaptation for point cloud data via graph Laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="325" to="337" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformation-grounded image generation network for novel 3D view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3500" to="3509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Example-based 3D scan completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauly</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shortest connection networks and some generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Prim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1389" to="1401" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Point-Net: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RL-GAN-Net: A reinforcement learning agent controlled GAN network for real-time point cloud shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Sarmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">;</forename><surname>Sarmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5898" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structure recovery by part assembly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">180</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximate symmetry detection in partial 3D meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Sipiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schreck ; Sipiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="131" to="140" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional wasserstein distances: Efficient optimal transportation on geometric domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning 3D shape completion under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Stutz and Geiger. IJCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SplatNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data-driven structural priors for shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Sung et al. 2015</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tatarchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view 3D models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dosovitskiy</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brox ; Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TopNet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tchapmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shape from symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1824" to="1831" />
		</imprint>
	</monogr>
	<note>Thrun and Wegbreit</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3DN: 3D deformation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1038" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parallel poisson disk sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Folding Net: Point cloud auto-encoder via deep grid deformation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PCN: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A robust hole-filling algorithm for triangular mesh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin ;</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="987" to="997" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
