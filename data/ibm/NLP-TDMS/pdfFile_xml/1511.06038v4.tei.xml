<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Variational Inference for Text Processing Phil Blunsom 12</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><forename type="middle">Blunsom@cs Ac</forename><surname>Ox</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google Deepmind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Variational Inference for Text Processing Phil Blunsom 12</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bagof-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Probabilistic generative models underpin many successful applications within the field of natural language processing (NLP). Their popularity stems from their ability to use unlabelled data effectively, to incorporate abundant linguistic features, and to learn interpretable dependencies among data. However these successes are tempered by the fact that as the structure of such generative models becomes deeper and more complex, true Bayesian inference becomes intractable due to the high dimensional integrals required. Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b33">(Neal, 1993;</ref><ref type="bibr" target="#b0">Andrieu</ref> Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s). <ref type="bibr">et al., 2003)</ref> and variational inference <ref type="bibr" target="#b20">(Jordan et al., 1999;</ref><ref type="bibr" target="#b1">Attias, 2000;</ref><ref type="bibr" target="#b4">Beal, 2003)</ref> are the standard approaches for approximating these integrals. However the computational cost of the former results in impractical training for the large and deep neural networks which are now fashionable, and the latter is conventionally confined due to the underestimation of posterior variance. The lack of effective and efficient inference methods hinders our ability to create highly expressive models of text, especially in the situation where the model is non-conjugate. This paper introduces a neural variational framework for generative models of text, inspired by the variational autoencoder . The principle idea is to build an inference network, implemented by a deep neural network conditioned on text, to approximate the intractable distributions over the latent variables. Instead of providing an analytic approximation, as in traditional variational Bayes, neural variational inference learns to model the posterior probability, thus endowing the model with strong generalisation abilities. Due to the flexibility of deep neural networks, the inference network is capable of learning complicated non-linear distributions and processing structured inputs such as word sequences. Inference networks can be designed as, but not restricted to, multilayer perceptrons (MLP), convolutional neural networks (CNN), and recurrent neural networks (RNN), approaches which are rarely used in conventional generative models. By using the reparameterisation method , the inference network is trained through back-propagating unbiased and low variance gradients w.r.t. the latent variables. Within this framework, we propose a Neural Variational Document Model (NVDM) for document modelling and a Neural Answer Selection Model (NASM) for question answering, a task that selects the sentences that correctly answer a factoid question from a set of candidate sentences.</p><p>The NVDM <ref type="figure">(Figure 1)</ref> is an unsupervised generative model of text which aims to extract a continuous semantic latent variable for each document. This model can be interpreted as a variational auto-encoder: an MLP encoder (inference network) compresses the bag-of-words document representation into a continuous latent distribution, and a softmax decoder (generative model) reconstructs the document by generating the words independently. A primary feature of NVDM is that each word is generated directly from a dense continuous document representation instead of the more common binary semantic vector <ref type="bibr" target="#b15">(Hinton &amp; Salakhutdinov, 2009;</ref><ref type="bibr" target="#b25">Larochelle &amp; Lauly, 2012;</ref><ref type="bibr" target="#b36">Srivastava et al., 2013;</ref>. Our experiments demonstrate that our neural document model achieves the stateof-the-art perplexities on the 20NewsGroups and RCV1-v2.</p><p>The NASM ( <ref type="figure" target="#fig_0">Figure 2)</ref> is a supervised conditional model which imbues LSTMs <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997)</ref> with a latent stochastic attention mechanism to model the semantics of question-answer pairs and predict their relatedness. The attention model is designed to focus on the phrases of an answer that are strongly connected to the question semantics and is modelled by a latent distribution. This mechanism allows the model to deal with the ambiguity inherent in the task and learns pair-specific representations that are more effective at predicting answer matches, rather than independent embeddings of question and answer sentences. Bayesian inference provides a natural safeguard against overfitting, especially as the training sets available for this task are small. The experiments show that the LSTM with a latent stochastic attention mechanism learns an effective attention model and outperforms both previously published results, and our own strong nonstochastic attention baselines.</p><p>In summary, we demonstrate the effectiveness of neural variational inference for text processing on two diverse tasks. These models are simple, expressive and can be trained efficiently with the highly scalable stochastic gradient back-propagation. Our neural variational framework is suitable for both unsupervised and supervised learning tasks, and can be generalised to incorporate any type of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Neural Variational Inference Framework</head><p>Latent variable modelling is popular in many NLP problems, but it is non-trivial to carry out effective and efficient inference for models with complex and deep structure. In this section we introduce a generic neural variational inference framework that we apply to both the unsupervised NVDM and supervised NASM in the follow sections.</p><p>We define a generative model with a latent variable h, which can be considered as the stochastic units in deep neural networks. We designate the observed parent and child nodes of h as x and y respectively. Hence, the joint distribution of the generative model is p θ (x, y) = h p θ (y|h)p θ (h|x)p(x), and the variational lower bound L is derived as:</p><formula xml:id="formula_0">L = E q(h) [log p θ (y|h)p θ (h|x)p(x) − log q(h)] (1) log q(h) q(h) p θ (y|h)p θ (h|x)p(x)dh = log p θ (x, y)</formula><p>where θ parameterises the generative distributions p θ (y|h) and p θ (h|x). In order to have a tight lower bound, the variational distribution q(h) should approach the true posterior p(h|x, y). Here, we employ a parameterised diagonal Gaussian N (h|µ(x, y), diag(σ 2 (x, y))) as q φ (h|x, y).</p><p>The three steps to construct the inference network are:</p><p>1. Construct vector representations of the observed variables: u = f x (x), v = f y (y).</p><p>2. Assemble a joint representation: π = g(u, v).</p><p>3. Parameterise the variational distribution over the latent variable: µ = l 1 (π), log σ = l 2 (π). f x (·) and f y (·) can be any type of deep neural networks that are suitable for the observed data; g(·) is an MLP that concatenates the vector representations of the conditioning variables; l(·) is a linear transformation which outputs the parameters of the Gaussian distribution. By sampling from the variational distribution, h ∼ q φ (h|x, y), we are able to carry out stochastic back-propagation to optimise the lower bound (Eq. 1).</p><p>During training, the model parameters θ together with the inference network parameters φ are updated by stochastic back-propagation based on the samples h drawn from q φ (h|x, y). For the gradients w.r.t. θ, we have the form:</p><formula xml:id="formula_1">∇ θ L 1 L L l=1 ∇ θ log p θ (y|h (l) )p θ (h (l) |x) (2)</formula><p>For the gradients w.r.t. φ we reparameterise h = µ + σ · and sample (l) ∼ N (0, I) to reduce the variance in stochastic estimation . The update of φ can be carried out by backpropagating the gradients w.r.t. µ and σ:</p><formula xml:id="formula_2">s(h) = log p θ (y|h)p θ (h|x) − log q φ (h|x, y) ∇ µ L 1 L L l=1 ∇ h (l) [s(h (l) )] (3) ∇ σ L 1 2L L l=1 (l) ∇ h (l) [s(h (l) )]<label>(4)</label></formula><p>It is worth mentioning that unsupervised learning is a special case of the neural variational framework where h has no parent node x. In that case h is directly drawn from the prior p(h) instead of the conditional distribution p θ (h|x), and s(h) = log p θ (y|h)p θ (h) − log q φ (h|y).</p><p>Here we only discuss the scenario where the latent variables are continuous and the parameterised diagonal Gaussian is employed as the variational distribution. However the framework is also suitable for discrete units, and the only modification needed is to replace the Gaussian with a multinomial parameterised by the outputs of a softmax function. Though the reparameterisation trick for continuous variables is not applicable for this case, a policy gradient approach  can help to alleviate the high variance problem during stochastic estimation.  proposed a variational inference framework for semi-supervised learning, but the prior distribution over the hidden variable p(h) remains as the standard Gaussian prior, while we apply a conditional parameterised Gaussian distribution, which is jointly learned with the variational distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Variational Document Model</head><p>The Neural Variational Document Model ( <ref type="figure">Figure 1</ref>) is a simple instance of unsupervised learning where a continuous hidden variable h ∈ R K , which generates all the words in a document independently, is introduced to represent its semantic content. Let X ∈ R |V | be the bag-of-words representation of a document and x i ∈ R |V | be the one-hot representation of the word at position i.</p><p>As an unsupervised generative model, we could interpret NVDM as a variational autoencoder: an MLP encoder q(h|X) compresses document representations into continuous hidden vectors (X → h); a softmax decoder p(X|h) = N i=1 p(x i |h) reconstructs the documents by independently generating the words (h → {x i }). To maximise the log-likelihood log h p(X|h)p(h) of documents, we derive the lower bound:</p><formula xml:id="formula_3">L=E q φ (h|X) N i=1 log p θ (x i |h) −D KL [q φ (h|X) p(h)] (5)</formula><p>where N is the number of words in the document and p(h) is a Gaussian prior for h. Here, we consider N is observed for all the documents. The conditional probability over words p θ (x i |h) (decoder) is modelled by multinomial logistic regression and shared across documents:</p><formula xml:id="formula_4">p θ (x i |h) = exp{−E(x i ; h, θ))} |V | j=1 exp{−E(x j ; h, θ)} (6) E(x i ; h, θ) = −h T Rx i − b xi<label>(7)</label></formula><p>where R ∈ R K×|V | learns the semantic word embeddings and b xi represents the bias term.</p><p>As there is no supervision information for the latent semantics, h, the posterior approximation q φ (h|X) is only conditioned on the current document X. The inference network q φ (h|X) = N (h|µ(X), diag(σ 2 (X))) is modelled as:</p><formula xml:id="formula_5">π = g(f MLP X (X)) (8) µ = l 1 (π), log σ = l 2 (π)<label>(9)</label></formula><p>For each document X, the neural network generates its own parameters µ and σ that parameterise the latent distribution over document semantics h. Based on the samples h ∼ q φ (h|X), the lower bound (Eq. 5) can be optimised by back-propagating the stochastic gradients w.r.t. θ and φ.</p><p>Since p(h) is a standard Gaussian prior, the Gaussian KL-</p><formula xml:id="formula_6">Divergence D KL [q φ (h|X) p(h)]</formula><p>can be computed analytically to further lower the variance of the gradients. Moreover, it also acts as a regulariser for updating the parameters of the inference network q φ (h|X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Neural Answer Selection Model</head><p>Answer sentence selection is a question answering paradigm where a model must identify the correct sentences answering a factual question from a set of candidate sentences. Assume a question q is associated with a set of answer sentences {a 1 , a 2 , ..., a n }, together with their judgements {y 1 , y 2 , ..., y n }, where y m = 1 if the answer a m is correct and y m = 0 otherwise. This is a classification task where we treat each training data point as a triple (q, a, y) while predicting y for the unlabelled question-answer pair (q, a).</p><p>The Neural Answer Selection Model ( <ref type="figure" target="#fig_0">Figure 2</ref>) is a supervised model that learns the question and answer representations and predicts their relatedness. It employs two different LSTMs to embed raw question inputs q and answer inputs a. Let s q (j) and s a (i) be the state outputs of the two LSTMs, and i, j be the positions of the states. Conventionally, the last state outputs s q (|q|) and s a (|a|), as the independent question and answer representations, can be used for relatedness prediction. In NASM, however, we aim to learn pair-specific representations through a latent attention mechanism, which is more effective for pair relatedness prediction.</p><p>NASM applies an attention model to focus on the words in the answer sentence that are prominent for predicting the answer matched to the current question. Instead of using a deterministic question vector, such as s q (|q|), NASM employs a latent distribution p θ (h|q) to model the question semantics, which is a parameterised diagonal Gaussian N (h|µ(q), diag(σ 2 (q))). Therefore, the attention model extracts a context vector c(a, h) by iteratively attending to the answer tokens based on the stochastic vector h ∼ p θ (h|q). In doing so the model is able to adapt to the ambiguity inherent in questions and obtain salient information through attention. Compared to its deterministic counterpart (applying s q (|q|) as the question semantics), the stochastic units incorporated into NASM allow multi-modal attention distributions. Further, by marginalising over the latent variables, NASM is more robust against overfitting, which is important for small question answering training sets.</p><p>In this model, the conditional distribution p θ (h|q) is:</p><formula xml:id="formula_7">π θ = g θ (f LSTM q (q)) = g θ (s q (|q|)) (10) µ θ = l 1 (π θ ), log σ θ = l 2 (π θ )<label>(11)</label></formula><p>For each question q, the neural network generates the corresponding parameters µ and σ that parameterise the latent distribution over question semantics h. Following Bahdanau et al. <ref type="formula" target="#formula_9">(2015)</ref>, the attention model is defined as:</p><formula xml:id="formula_8">α(i) ∝ exp(W T α tanh(W h h + W s s a (i))) (12) c(a, h) = i s a (i)α(i) (13) z a (a, h) = tanh (W a c(a, h) + W n s a (|a|))<label>(14)</label></formula><p>where α(i) is the normalised attention score at answer token i, and the context vector c(a, h) is the weighted sum of all the state outputs s a (i). We adopt z q (q), z a (a, h) as the question and answer representations for predicting their relatedness y. z q (q) is a deterministic vector that is equal to s q (|q|), while z a (a, h) is a combination of the sequence output s a (|a|) and the context vector c(a, h) (Eq. 14).</p><p>For the prediction of pair relatedness y, we model the conditional probability distribution p θ (y|z q , z a ) by sigmoid function:</p><formula xml:id="formula_9">p θ (y = 1|z q , z a ) = σ z T q Mz a + b<label>(15)</label></formula><p>To maximise the log-likelihood log p(y|q, a) we use the variational lower bound:</p><formula xml:id="formula_10">L=E q φ (h) [log p θ (y|z q (q), z a (a, h))]−D KL (q φ (h)||p θ (h|q)) log p θ (y|z q (q), z a (a, h))p θ (h|q)dh = log p(y|q, a)<label>(16)</label></formula><p>Following the neural variational inference framework, we construct a deep neural network as the inference network</p><formula xml:id="formula_11">q φ (h|q, a, y) = N (h|µ φ (q, a, y), diag(σ 2 φ (q, a, y))): π φ = g φ (f LSTM q (q), f LSTM a (a), f y (y)) = g φ (s q (|q|), s a (|a|), s y ) (17) µ φ = l 3 (π φ ), log σ φ = l 4 (π φ )<label>(18)</label></formula><p>where q and a are also modelled by LSTMs 1 , and the relatedness label y is modelled by a simple linear transformation into the vector s y . According to the joint representation π φ , we then generate the parameters µ φ and σ φ , which parameterise the variational distribution over the question semantics h. To emphasise, though both p θ (h|q) and q φ (h|q, a, y) are modelled as parameterised Gaussian distributions, q φ (h|q, a, y) as an approximation only functions during inference by producing samples to compute the stochastic gradients, while p θ (h|q) is the generative distribution that generates the samples for predicting the question-answer relatedness y.</p><p>Based on the samples h ∼ q φ (h|q, a, y), we use SGVB to optimise the lower bound (Eq.16). The model parameters θ and the inference network parameters φ are updated jointly using their stochastic gradients. In this case, similar to the NVDM, the Gaussian KL divergence D KL [q φ (h|q, a, y)) p θ (h|q)] can be analytically computed during training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset &amp; Setup for Document Modelling</head><p>We experiment with NVDM on two standard news corpora: the 20NewsGroups 2 and the Reuters RCV1-v2 3 . The former is a collection of newsgroup documents, consisting of 11,314 training and 7,531 test articles. The latter is a large collection from Reuters newswire stories with 794,414 training and 10,000 test cases. The vocabulary size of these two datasets are set as 2,000 and 10,000.</p><p>To make a direct comparison with the prior work we follow the same preprocessing procedure and setup as Hinton &amp; Salakhutdinov <ref type="formula" target="#formula_5">(2009)</ref>  <ref type="table">weapons medical  companies  define  israel  book  guns  medicine  expensive  defined  israeli  books  weapon  health  industry  definition  arab  reference  NVDM  gun  treatment  company  printf  arabs  guide  militia  disease  market  int  lebanon  writing  armed  patients  buy  sufficient  lebanese  pages  weapon treatment  demand  defined  israeli  reading  shooting medecine commercial definition  israelis  read  NADE  firearms  patients  agency  refer  arab  books  assault  process  company  make  palestinian  relevent  armed</ref> studies credit examples arabs collection (b) The five nearest words in the semantic space.  <ref type="bibr" target="#b5">(Blei et al., 2003)</ref> is a traditional topic model that models documents by mixtures of topics, RSM <ref type="bibr" target="#b15">(Hinton &amp; Salakhutdinov, 2009)</ref> is an undirected topic model implemented by restricted Boltzmann machines, and docNADE <ref type="bibr" target="#b25">(Larochelle &amp; Lauly, 2012)</ref> is a neural topic model based on autoregressive assumption. The models based on Sigmoid Belief Networks (SBN) and Deep AutoRegressive Neural Network (DARN) structures are implemented by , which employs an MLP to build a Monte Carlo control variate estimator for stochastic estimation.</p><p>Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2015)</ref> and tuned by hold-out validation perplexity. We alternately optimise the generative model and the inference network by fixing the parameters of one while updating the parameters of the other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on Document Modelling</head><formula xml:id="formula_12">(− 1 D N d n 1 N d log p(X d ))</formula><p>, where D is the number of documents, N d represents the length of the dth document and log p(X) = log p(X|h)p(h)dh is the log probability of the words in the document. Since log p(X) is intractable in the NVDM, we use the variational lower bound (which is an upper bound on perplexity) to compute the perplexity following . <ref type="table" target="#tab_1">Table 1a</ref> apply discrete latent variables, here NVDM employs a continuous stochastic document representation. The experimental results indicate that NVDM achieves the best performance on both datasets. For the experiments on RCV1-v2 dataset, the NVDM with latent variable of 50 dimension performs even better than the fDARN with 200 dimension. It demonstrates that our document model with continuous latent variables has higher expressiveness and better generalisation ability. <ref type="table" target="#tab_1">Table 1b</ref> compares the 5 nearest words selected according to the semantic vector learned from NVDM and docNADE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>While all the baseline models listed in</head><p>In addition to the perplexities, we also qualitatively evaluate the semantic information learned by NVDM on the 20NewsGroups dataset with latent variables of 50 dimension. We assume each dimension in the latent space represents a topic that corresponds to a specific semantic meaning. <ref type="table" target="#tab_3">Table 2</ref> presents 5 randomly selected topics with 10 words that have the strongest positive connection with the topic. Based on the words in each column, we can deduce their corresponding topics as: Space, Religion, Encryption, Sport and Policy. Although the model does not impose independent interpretability on the latent representation dimensions, we still see that the NVDM learns locally interpretable structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Dataset &amp; Setup for Answer Sentence Selection</head><p>We experiment on two answer selection datasets, the QASent and the WikiQA datasets. QASent <ref type="bibr" target="#b40">(Wang et al., 2007)</ref> is created from the TREC QA track, and the WikiQA <ref type="bibr" target="#b43">(Yang et al., 2015)</ref> is constructed from Wikipedia, which is less noisy and less biased towards lexical overlap 4 . <ref type="table" target="#tab_4">Table 3</ref> summarises the statistics of the two datasets.  <ref type="bibr" target="#b27">(Le &amp; Mikolov, 2014)</ref>. Bigram-CNN is the simple convolutional model reported in <ref type="bibr" target="#b46">(Yu et al., 2014)</ref>. Deep CNN is the deep convolutional model from <ref type="bibr" target="#b35">(Severyn, 2015)</ref>. WA is a model based on word alignment <ref type="bibr" target="#b41">(Wang &amp; Ittycheriah, 2015)</ref>. LCLR is the SVM-based classifier trained using a set of features. Model + Cnt means that the result is obtained from a combination of a lexical overlap feature and the output from the distributional model.</p><p>In order to investigate the effectiveness of our NASM model we also implemented two strong baseline modelsa vanilla LSTM model (LSTM) and an LSTM model with a deterministic attention mechanism (LSTM+Att). The former directly applies the QA matching function (Eq. 15) on the independent question and answer representations which are the last state outputs s q (|q|) and s a (|a|) from the question and answer LSTM models. The latter adds an attention model to learn pair-specific representation for prediction on the basis of the vanilla LSTM. Moreover, LSTM+Att is the deterministic counterpart of NASM, which has the same neural network architecture as NASM. The only difference is that it replaces the stochastic units h with deterministic ones, and no inference network is required to carry out stochastic estimation. Following previous work, for each of our models we also add a lexical overlap feature by combining a co-occurrence word count feature with the probability generated from the neural model. MAP and MRR are adopted as the evaluation metrics for this task.</p><p>To facilitate direct comparison with previous work we follow the same experimental setup as <ref type="bibr" target="#b46">Yu et al. (2014)</ref> and <ref type="bibr" target="#b35">Severyn (2015)</ref>. The word embeddings (K = 50) are obtained by running the word2vec tool <ref type="bibr" target="#b31">(Mikolov et al., 2013)</ref> on the English Wikipedia dump and the AQUAINT 5 corpus. We use LSTMs with 3 layers and 50 hidden units, 5 https://catalog.ldc.upenn.edu/LDC2002T31</p><p>and apply 40% dropout after the embedding layer. For the construction of the inference network, we use an MLP (Eq. 10) with 2 layers and tanh units of 50 dimension, and an MLP (Eq. 17) with 2 layers and tanh units of 150 dimension for modelling the joint representation. During training we carry out stochastic estimation by taking one sample for computing the gradients, while in prediction we use 20 samples to calculate the expectation of the lower bound. <ref type="figure">Figure 3</ref> presents the standard deviation of NASM's MAP scores while using different numbers of samples. Considering the trade-off between computational cost and variance, we chose 20 samples for prediction in all the experiments. The models are trained using Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2015)</ref>, with hyperparameters selected by optimising the MAP score on the development set.  <ref type="bibr" target="#b44">(Yih et al., 2013)</ref> that the evaluation scripts used by previous work are noisy -4 out of 72 questions in the test set are treated answered incorrectly. This makes the MAP and MRR scores ∼ 4% lower than the true scores. Since <ref type="bibr" target="#b35">Severyn (2015)</ref> and <ref type="bibr" target="#b41">Wang &amp; Ittycheriah (2015)</ref> use a cleaned-up evaluation scripts, we apply the original noisy scripts to re-evaluate their outputs in A NASM the actress who played lolita , sue lyon , was fourteen at the time of filming . MAP and 6% on MRR. The LSTM+Att performs slightly better than the vanilla LSTM model, and our NASM improves the results further. Since the QASent dataset is biased towards lexical overlapping features, after combining with a co-occurrence word count feature, our best model NASM outperforms all the previous models, including both neural network based models and classifiers with a set of hand-crafted features (e.g. LCLR). Similarly, on the Wik-iQA dataset, all of our models outperform the previous distributional models by a large margin. By including a word count feature, our models improve further and achieve the state-of-the-art. Notably, on both datasets, our two LSTMbased models have set strong baselines and NASM works even better, which demonstrates the effectiveness of introducing stochastic units to model question semantics in this answer sentence selection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiments on Answer Sentence Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q1 how old was sue lyon when she made lolita</head><p>In <ref type="figure">Figure 4</ref>, we compare the effectiveness of the latent attention mechanism (NASM) and its deterministic counterpart (LSTM+Att) by visualising the attention scores on the answer sentences. For most of the negative answer sentences, neither of the two attention models can attend to reasonable words that are beneficial for predicting relatedness. But for the correct answer sentences, such as the ones in <ref type="figure">Figure 4</ref>, both attention models are able to capture crucial information by attending to different parts of the sentence based on the question semantics. Interestingly, compared to the deterministic counterpart LSTM+Att, our NASM assigns higher attention scores on the prominent words that are relevant to the question, which forms a more peaked distribution and in turn helps the model achieve better performance.</p><p>In order to have an intuitive observation on the latent distributions, we present Hinton diagrams of their log standard deviation parameters ( <ref type="figure">Figure 5)</ref>. In a Hinton diagram, the size of a square is proportional to a value's magnitude, and the colour (black/white) indicates its sign (positive/negative). In this case, we visualise the parameters order to make the results directly comparable with previous work. of 50 conditional distributions p θ (h|q) with the questions selected from 5 different groups, which start with 'how <ref type="bibr">', 'what', 'who', 'when' and '</ref>where'. All the log standard deviations are initialised as zero before training. According to <ref type="figure">Figure 5</ref>, we can see that the questions starting with 'how' have more white areas, which indicates higher variances or more uncertainties are in these dimensions. By contrast, the questions starting with 'what' have black squares in almost every dimension. Intuitively, it is more difficult to understand and answer the questions starting with 'how' than the others, while the 'what' questions commonly have explicit words indicating the possible answers. To validate this, we compute the stratified MAP scores based on different question type. The MAP of 'how' questions is 0.524 which is the lowest among the five groups. Hence empirically, 'how' questions are harder to 'understand and answer'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>As shown in the experiments, neural variational inference brings consistent improvements on the performance of both NLP tasks. The basic intuition is that the latent distributions grant the ability to sum over all the possibilities in terms of semantics. From the perspective of optimisation, one of the most important reasons is that Bayesian learning guards against overfitting.</p><p>According to Eq. 5 in NVDM, since we adopt p(h) as a standard Gaussian prior, the KL divergence term D KL [q φ (h|X) p(h)] can be analytically computed as 1 2 (K − µ 2 − σ 2 + log | diag(σ 2 )|). It is not difficult to find that it actually acts as L2 regulariser when we update the µ. Similarly, in NASM (Eq. 16), we also have the KL divergence term D KL [q φ (h|q, a, y)) p θ (h|q)]. Different from NVDM, it attempts to minimise the distance between q φ (h|q, a, y)) and p θ (h|q) that are both conditional distributions. Because p θ (h|q) as well as q φ (h|q, a, y)) are learned during training, the two distributions are mutually restrained while being updated. Therefore, NVDM simply penalises the large µ and encourages q φ (h|X) to approach the prior p(h) for every document X, but in NASM, p θ (h|q) acts like a moving baseline distribution which regularises the update of q φ (h|q, a, y)) for every different conditions. In practice, we carry out early stopping by observing the prediction performance on development dataset for the question answer selection task. Using the same learning rate and neural network structure, LSTM+Att reaches optimal performance and starts to overfit on training dataset generally at the 20th iteration, while NASM starts to overfit around the 35th iteration.</p><p>More interestingly, in the question answer selection experiments, NASM learns more peaked attention scores than its deterministic counterpart LSTM+Att. For the update process of LSTM+Att, we find there exists a relatively big variance in the gradients w.r.t. question semantics (LSTM+Att applies deterministic s q (|q|) while NASM applies stochastic h). This is because the training dataset is small and contains many negative answer sentences that brings no benefit but noise to the learning of the attention model. In contrast, for the update process of NASM, we observe more stable gradients w.r.t. the parameters of latent distributions. The optimisation of the lower bound on one hand maximises the conditional log-likelihood (that the deterministic counterpart cares about) and on the other hand minimises the KL-divergence (that regularises the gradients). Hence, each update of the lower bound actually keeps the gradients w.r.t. µ from swinging heavily. Besides, since the values of σ are not very significant in this case, the distribution of attention scores mainly depends on µ. Therefore, the learning of the attention model benefits from the regularisation as well, and it explains the fact that NASM learns more peaked attention scores which in turn helps achieve a better prediction performance.</p><p>Since the computations of NVDM and NASM can be parallelised on GPU and only one sample is required during training process, it is very efficient to carry out the neural variational inference. Moreover, for both NVDM and NASM, all the parameters are updated by backpropagation. Thus, the increased computation time for the stochastic units only comes from the added parameters of the inference network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Work</head><p>Training an inference network to approximate the variational distribution was first proposed in the context of Helmholtz machines <ref type="bibr" target="#b16">(Hinton &amp; Zemel, 1994;</ref><ref type="bibr" target="#b17">Hinton et al., 1995;</ref><ref type="bibr" target="#b10">Dayan &amp; Hinton, 1996)</ref>, but applications of these directed generative models come up against the problem of establishing low variance gradient estimators. Recent advances in neural variational inference mitigate this problem by reparameterising the continuous random variables , using control variates  or approximating the posterior with importance sampling <ref type="bibr" target="#b8">(Bornschein &amp; Bengio, 2015)</ref>. The instantiations of these ideas <ref type="bibr" target="#b2">Ba et al., 2015)</ref> have demonstrated strong performance on the tasks of image processing. The recent variants of generative auto-encoder <ref type="bibr" target="#b28">(Louizos et al., 2015;</ref><ref type="bibr" target="#b30">Makhzani et al., 2015)</ref> are also very competitive. <ref type="bibr" target="#b38">Tang &amp; Salakhutdinov (2013)</ref> applies the similar idea of introducing stochastic units for expression classification, but its inference is carried out by Monte Carlo EM algorithm with the reliance on importance sampling, which is less efficient and lack of scalability.</p><p>Another class of neural generative models make use of the autoregressive assumption <ref type="bibr" target="#b26">(Larochelle &amp; Murray, 2011;</ref><ref type="bibr" target="#b39">Uria et al., 2014;</ref><ref type="bibr" target="#b11">Germain et al., 2015;</ref><ref type="bibr" target="#b12">Gregor et al., 2014)</ref>. Applications of these models on document modelling achieve significant improvements on generating documents, compared to conventional probabilistic topic models <ref type="bibr" target="#b19">(Hofmann, 1999;</ref><ref type="bibr" target="#b5">Blei et al., 2003)</ref> and also the RBMs <ref type="bibr" target="#b15">(Hinton &amp; Salakhutdinov, 2009;</ref><ref type="bibr" target="#b36">Srivastava et al., 2013)</ref>. While these models that use binary semantic vectors, our NVDM employs dense continuous document representations which are both expressive and easy to train. The semantic word vector model <ref type="bibr" target="#b29">(Maas et al., 2011)</ref> also employs a continuous semantic vector to generate words, but the model is trained by MAP inference which does not permit the calculation of the posterior distribution. A very similar idea to NVDM is <ref type="bibr" target="#b9">Bowman et al. (2015)</ref>, which employs VAE to generate sentences from a continuous space.</p><p>Apart from the work mentioned above, there is other interesting work on question answering with deep neural networks. One of the popular streams is mapping factoid questions with answer triples in the knowledge base <ref type="bibr" target="#b6">(Bordes et al., 2014a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b45">Yih et al., 2014)</ref>. Moreover, <ref type="bibr" target="#b42">Weston et al. (2015)</ref>; <ref type="bibr" target="#b37">Sukhbaatar et al. (2015)</ref>; <ref type="bibr" target="#b24">Kumar et al. (2015)</ref> further exploit memory networks, where long-term memories act as dynamic knowledge bases. Another attention-based model <ref type="bibr" target="#b14">(Hermann et al., 2015)</ref> applies the attentive network to help read and comprehend for long articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper introduced a deep neural variational inference framework for generative models of text. We experimented on two diverse tasks, document modelling and question answer selection tasks to demonstrate the effectiveness of this framework, where in both cases our models achieve state of the art performance. Apart from the promising results, our model also has the advantages of (1) simple, expressive, and efficient when training with the SGVB algorithm;</p><p>(2) suitable for both unsupervised and supervised learning tasks; and (3) capable of generalising to incorporate any type of neural network. (1) Inference Network q φ (h|X):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. t-SNE Visualisation of Document Representations</head><formula xml:id="formula_13">λ = ReLU(W 1 X + b 1 ) (19) π = ReLU(W 2 λ + b 2 ) (20) µ = W 3 π + b 3 (21) log σ = W 4 π + b 4 (22) h ∼ N (µ(X), diag(σ 2 (X)))<label>(23)</label></formula><p>(2) Generative Model p θ (X|h):</p><formula xml:id="formula_14">e i = exp(−h T Rx i + b xi ) (24) p θ (x i |h) = ei |V | j ej (25) p θ (X|h) = N i p θ (x i |h) (26) (3) KL Divergence D KL [q φ (h|X)||p(h)]: D KL = − 1 2 (K − µ 2 − σ 2 + log | diag(σ 2 )|)(27)</formula><p>The variational lower bound to be optimised:</p><formula xml:id="formula_15">L =E q φ (h|X) N i=1 log p θ (x i |h) − D KL [q φ (h|X)||p(h)]<label>(28)</label></formula><formula xml:id="formula_16">≈ L l=1 N i=1</formula><p>log p θ (x i |h (l) ) + 1 2 (K − µ 2 − σ 2 + log | diag(σ 2 )|) (29)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Neural Answer Selection Model</head><p>(1) Inference Network q φ (h|q, a, y):</p><formula xml:id="formula_17">s q (|q|) = f LSTM q (q) (30) s a (|a|) = f LSTM a (a)</formula><p>(31) s y = W 5 y + b 5 (32) γ = s q (|q|)||s a (|a|)||s y (33) λ φ = tanh(W 6 γ + b 6 ) (34) π φ = tanh(W 7 λ φ + b 7 )</p><p>(35) µ φ = W 8 π φ + b 8 (36) log σ φ = W 9 π φ + b 9 (37) h ∼ N (µ φ (q, a, y), diag(σ 2 φ (q, a, y)))</p><p>(2) Generative Model p θ (h|q): </p><formula xml:id="formula_19">λ θ = tanh(W 1 s q (|q|) + b 1 ) (39) π θ = tanh(W 2 λ θ + b 2 ) (40) µ θ = W 3 π θ + b 3 (41) log σ θ = W 4 π θ + b 4<label>(42)</label></formula><p>c(a, h) = i s a (i)α(i) (45) z a (a, h) = tanh(W a c(a, h) + W n s a (|a|)) (46) z q (q) = s q (|q|) (47) p θ (y = 1|q, a, h) = σ(z T q M z a + b)</p><p>(3) KL Divergence D KL [q φ (h|q, a, y)||p θ (h|q)]:</p><formula xml:id="formula_22">D KL = − 1 2 (K + log | diag(σ 2 φ )| − log | diag(σ 2 θ )| − Tr(diag(σ 2 φ ) diag −1 (σ 2 θ )) − (µ φ − µ θ ) T diag −1 (σ 2 θ )(µ φ − µ θ )) (49)</formula><p>The variational lower bound to be optimised: </p><formula xml:id="formula_23">L = E q φ (h|q,</formula><formula xml:id="formula_24">+ (1 − y) log(1 − σ(z T q M z (l) a + b))] + 1 2 (K + log | diag(σ 2 φ )| − log | diag(σ 2 θ )| − Tr(diag(σ 2 φ ) diag −1 (σ 2 θ )) − (µ φ − µ θ ) T diag −1 (σ 2 θ )(µ φ − µ θ ))<label>(51)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computational Complexity</head><p>The computational complexity of NVDM for a training document is C φ + C θ = O(LK 2 + KSV ). Here, C φ = O(LK 2 ) represents the cost for the inference network to generate a sample, where L is the number of the layers in the inference network and K is the average dimension of these layers. Besides, C θ = O(KSV ) is the cost of reconstructing the document from a sample, where S is the average length of the documents and V represents the volume of words applied in this document model, which is conventionally much lager than K.</p><p>The computational complexity of NASM for a training question-answer pair is C φ + C θ = O((L + S)K 2 + SW ).</p><p>The inference network needs C φ = 2SW + 2K + LK 2 = O(LK 2 + SW ). It takes 2SW + 2K to produce the joint representation for a question-answer pair and its label, where W is the total number of parameters of an LSTM and S is the average length of the sentences. Based on the joint representation, an MLP spends LK 2 to generate a sample, where L is the number of layers and K represents the average dimension. The generative model requires C θ = 2SW +LK 2 +SK 2 +5K 2 +2K 2 = O((L+S)K 2 +SW ). Similarly, it costs 2SW + LK 2 to construct the generative latent distribution , where 2SW can be saved if the LSTMs are shared by the inference network and the generative model. Besides, the attention model takes SK 2 +5K 2 and the relatedness prediction takes the last 2K 2 .</p><p>Since the computations of NVDM and NASM can be parallelised in GPU and only one sample is required during training process, it is very efficient to carry out the neural variational inference. As NVDM is an instantiation of variational auto-encoder, its computational complexity is the same as the deterministic auto-encoder. In addition, the computational complexity of LSTM+Att, the deterministic counterpart of NASM, is also O((L+S)K 2 +SW ). There is only O(LK 2 ) time increase by introducing an inference network for NASM when compared to LSTM+Att.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>NASM for question answer selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>LSTM the blue color of liquid oxygen in a dewar flask A NASM the blue color of liquid oxygen in a dewar flask Q3 what does a liquid oxygen plant look like A LSTM the peso is subdivided into 100 centavos , represented by " _UNK_ " A NASM the peso is subdivided into 100 centavos , represented by " _UNK_ " Q2 how much is centavos in mexico A LSTM the actress who played lolita , sue lyon , was fourteen at the time of filming .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>A visualisation of attention scores on answer sentences. Hinton diagrams of the log standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6</head><label>6</label><figDesc>. t-SNE visualisation of the document representations achieved by (a) NVDM and (b) SWV (Maas et al., 2011) on the held-out test dataset of 20NewsGroups. The documents are collected from 20 different news groups, which correspond to the points with different colour in the figure. B. Details of the Deep Neural Network Structures B.1. Neural Variational Document Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a,y) [log p θ (y|q, a, h)] − D KL [q φ (h|q, a, y)||p θ (h|q)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>For the experimental results in (a), LDA</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1a</head><label>1a</label><figDesc></figDesc><table /><note>presents the test document perplexity. The first column lists the models, and the second column shows the dimension of latent variables used in the experiments. The final two columns present the perplexity achieved by each topic model on the 20NewsGroups and RCV1-v2 datasets. In document modelling, perplexity is computed by exp</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The topics learned by NVDM on 20News.</figDesc><table><row><cell>Space</cell><cell cols="3">Religion Encryption Sport</cell><cell>Policy</cell></row><row><cell>orbit</cell><cell>muslims</cell><cell>rsa</cell><cell>goals</cell><cell>bush</cell></row><row><cell>lunar</cell><cell cols="3">worship cryptography pts</cell><cell>resources</cell></row><row><cell>solar</cell><cell>belief</cell><cell>crypto</cell><cell cols="2">teams charles</cell></row><row><cell cols="2">shuttle genocide</cell><cell>keys</cell><cell>league</cell><cell>austin</cell></row><row><cell>moon</cell><cell>jews</cell><cell>pgp</cell><cell>team</cell><cell>bill</cell></row><row><cell>launch</cell><cell>islam</cell><cell>license</cell><cell cols="2">players resolution</cell></row><row><cell>fuel</cell><cell>christianity</cell><cell>secure</cell><cell>nhl</cell><cell>mr</cell></row><row><cell>nasa</cell><cell>atheists</cell><cell>key</cell><cell>stats</cell><cell>misc</cell></row><row><cell>satellite</cell><cell>muslim</cell><cell>escrow</cell><cell>min</cell><cell>piece</cell></row><row><cell cols="2">japanese religious</cell><cell>trust</cell><cell>buf</cell><cell>marc</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Statistics of QASent and WikiQA.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell cols="2">QASent</cell><cell>WikiQA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell>MRR</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>Source QASent Dev Set Train</cell><cell cols="4">Questions QA Pairs Judgement 1,229 53,417 automatic 82 1,148 manual</cell><cell>Published Models PV Bigram-CNN</cell><cell cols="2">0.5213 0.6023 0.5693 0.6613</cell><cell>0.5110 0.5160 0.6190 0.6281</cell></row><row><cell>Test</cell><cell></cell><cell>100</cell><cell>1,517</cell><cell>manual</cell><cell>Deep CNN</cell><cell cols="2">0.5719 0.6621</cell><cell>-</cell><cell>-</cell></row><row><cell>Train WikiQA Dev Test</cell><cell></cell><cell>2,118 296 633</cell><cell>20,360 2,733 6,165</cell><cell>manual manual manual</cell><cell cols="3">PV + Cnt WA LCLR Bigram-CNN + Cnt 0.7113 0.7846 0.6762 0.7514 0.7063 0.7740 0.7092 0.7700</cell><cell>0.5976 0.6058 --0.5993 0.6068 0.6520 0.6652</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Deep CNN + Cnt</cell><cell cols="2">0.7186 0.7826</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Judgement denotes</cell><cell>Our Models</cell><cell></cell><cell></cell></row><row><cell cols="5">whether correctness was determined automatically or by human</cell><cell>LSTM</cell><cell cols="2">0.6436 0.7235</cell><cell>0.6552 0.6747</cell></row><row><cell cols="2">annotators. 1 10 20 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1e−3 1.28 0.77 Standard Deviation 0.46</cell><cell>50 0.39</cell><cell></cell><cell>100 0.32</cell><cell cols="4">LSTM + Att NASM LSTM + Cnt LSTM + Att + Cnt NASM + Cnt Table 4. Results of our models (LSTM, LSTM + Att, NASM) 0.6451 0.7316 0.6639 0.6828 0.6501 0.7324 0.6705 0.6914 0.7228 0.7986 0.6820 0.6988 0.7289 0.8072 0.6855 0.7041 0.7339 0.8117 0.6886 0.7069 in comparison with</cell></row><row><cell></cell><cell></cell><cell>Samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 3. The standard deviations of MAP scores computed by</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">running 10 NASM models on WikiQA with different numbers of</cell><cell></cell><cell></cell><cell></cell></row><row><cell>samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>other state of the art models on the QASent and WikiQA dataset. PV is the paragraph vector</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>compares the results of our models with current state-of-the-art models on both answer selection datasets. On the QASent dataset, our vanilla LSTM model outperforms the deep CNN 6 model by approximately 7% on</figDesc><table /><note>6 As stated in</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this case, the LSTMs for q and a are shared by the inference network and the generative model, but there is no restriction on using different LSTMs in the inference network. 2 http://qwone.com/ jason/20Newsgroups 3 http://trec.nist.gov/data/reuters/reuters.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4"><ref type="bibr" target="#b43">Yang et al. (2015)</ref> provide detailed explanation of the differences between the two datasets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An introduction to mcmc for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="43" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A variational bayesian framework for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagai</forename><surname>Attias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning wake-sleep recurrent attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Variational algorithms for approximate Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of London</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML</title>
		<meeting>ECML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reweighted wakesleep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
		<idno>abs/1511.06349</idno>
		<ptr target="http://arxiv.org/abs/1511.06349" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Varieties of helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1385" to="1403" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Made</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length, and helmholtz free energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The wake-sleep algorithm for unsupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page" from="1158" to="1161" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tommi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saul</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00830</idno>
		<title level="m">The variational fair auto encoder</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1511.05644</idno>
		<ptr target="http://arxiv.org/abs/1511.05644" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Probabilistic inference using markov chain monte carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<idno>: CRG-TR- 93-1</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modelling input texts: from Tree Kernels to Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Trento</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling documents with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainbayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning stochastic feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What is the jeopardy model? a quasi-synchronous grammar for qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02628</idno>
		<title level="m">Faq-based question answering via word alignment</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pastusiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><forename type="middle">-</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep Learning for Answer Sentence Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
