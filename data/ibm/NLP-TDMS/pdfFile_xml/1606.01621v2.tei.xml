<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Photo Aesthetics Ranking Network with Attributes and Content Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
							<email>skong2@ics.uci.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<email>xshen@adobe.comprojectwebpage</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<email>zlin@adobe.comprojectwebpage</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
							<email>rmech@adobe.comprojectwebpage</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
							<email>fowlkes@ics.uci.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC Irvine</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Photo Aesthetics Ranking Network with Attributes and Content Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional Neural Network</term>
					<term>Image Aesthetics Rating</term>
					<term>Rank Loss</term>
					<term>Attribute Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world applications could benefit from the ability to automatically generate a fine-grained ranking of photo aesthetics. However, previous methods for image aesthetics analysis have primarily focused on the coarse, binary categorization of images into high-or low-aesthetic categories. In this work, we propose to learn a deep convolutional neural network to rank photo aesthetics in which the relative ranking of photo aesthetics are directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated photo aesthetics rating problem. To train and analyze this model, we have assembled a new aesthetics and attributes database (AADB) which contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. Anonymized rater identities are recorded across images allowing us to exploit intra-rater consistency using a novel sampling strategy when computing the ranking loss of training image pairs. We show the proposed sampling strategy is very effective and robust in face of subjective judgement of image aesthetics by individuals with different aesthetic tastes. Experiments demonstrate that our unified model can generate aesthetic rankings that are more consistent with human ratings. To further validate our model, we show that by simply thresholding the estimated aesthetic scores, we are able to achieve state-or-the-art classification performance on the existing AVA dataset benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatically assessing image aesthetics is increasingly important for a variety of applications <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17]</ref>, including personal photo album management, automatic photo editing, and image retrieval. While judging image aesthetics is a subjective task, it has been an area of active study in recent years and substantial progress has been made in identifying and quantifying those image features that are predictive of favorable aesthetic judgements by most individuals <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">5]</ref>.</p><p>Early works formulate aesthetic analysis as a classification or a regression problem of mapping images to aesthetic ratings provided by human raters <ref type="bibr">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b25">26]</ref>. Some approaches have focused on designing hand-crafted features that encapsulate arXiv:1606.01621v2 [cs.CV] 27 Jul 2016 <ref type="figure">Fig. 1</ref>. Classification-based methods for aesthetic analysis can distinguish high-and low-quality images shown in the leftmost and rightmost columns, but fail to provide useful insights about borderline images displayed in the middle column. This observation motivates us to consider rating and ranking images w.r.t aesthetics rather than simply assigning binary labels. We observe that the contribution of particular photographic attributes to making an image aesthetically pleasing depends on the thematic content (shown in different rows), so we develop a model for rating that incorporates joint attributes and content. The attributes and ratings of aesthetics on a scale 1 to 5 are predicted by our model (displayed on top and right of each image, respectively). standard photographic practice and rules of visual design, utilizing both low-level statistics (e.g. color histogram and wavelet analysis) and high-level cues based on traditional photographic rules (e.g. region composition and rule of thirds). Others have adopted generic image content features, which are originally designed for recognition (e.g. SIFT <ref type="bibr" target="#b13">[14]</ref> and Fisher Vector <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>), that have been found to outperform methods using rule-based features <ref type="bibr" target="#b20">[21]</ref>. With the advance of deep Convolutional Neural Network (CNN) <ref type="bibr" target="#b11">[12]</ref>, recent works propose to train end-to-end models for image aesthetics classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, yielding state-of-the-art performance on a recently released Aesthetics Visual Analysis dataset (AVA) <ref type="bibr" target="#b22">[23]</ref>.</p><p>Despite notable recent progress towards computational image aesthetics classification (e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>), judging image aesthetics is still a subjective task, and it is difficult to learn a universal scoring mechanism for various kinds of images. For example, as demonstrated in <ref type="figure">Fig. 1</ref>, images with obviously visible high-or low-aesthetics are relatively easy to classify, but existing methods cannot generate reliable labels for borderline images. Therefore, instead of formulating image aesthetics analysis as an overall binary classification or regression problem, we argue that it is far more practical and useful to predict relative aesthetic rankings among images with similar visual content along with generating richer descriptions in terms of aesthetic attributes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>To this end, we propose to train a model through a Siamese network <ref type="bibr" target="#b2">[3]</ref> that takes a pair of images as input and directly predicts relative ranking of their aesthetics in addition to their overall aesthetic scores. Such a structure allows us to deploy different sampling strategies of image pairs and leverage auxiliary side-information to regularize the training, including aesthetic attributes <ref type="bibr">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref> and photo content <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15]</ref>. For example, <ref type="figure">Fig. 1</ref> demonstrates that photos with different contents convey different attributes to make them aesthetically pleasing. While such side information has been individually adopted to improve aesthetics classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>, it remains one open problem to systematically incorporate all the needed components in a single end-toend framework with fine-grained aesthetics ranking. Our model and training procedure naturally incorporates both attributes and content information by sampling image pairs with similar content to learn the specific relations of attributes and aesthetics for different content sub-categories. As we show, this results in more comparable and consistent aesthetics estimation results.</p><p>Moreover, as individuals have different aesthetics tastes, we argue that it is important to compare ratings assigned by an individual across multiple images in order to provide a more consistent training signal. To this end, we have collected and will publicly release a new dataset in which each image is associated with a detailed score distribution, meaningful attributes annotation and (anonymized) raters' identities. We refer to this dataset as the "Aesthetics with Attributes Database", or AADB for short. AADB not only contains a much more balanced distribution of professional and consumer photos and a more diverse range of photo qualities than available in the exiting AVA dataset, but also identifies ratings made by the same users across multiple images. This enables us to develop novel sampling strategies for training our model which focuses on relative rankings by individual raters. Interestingly, this rater-related information also enables us to compare the trained model to each individual's rating results by computing the ranking correlation over test images rated by that individual. Our experiments show the effectiveness of the proposed model in rating image aesthetics compared to human individuals. We also show that, by simply thresholding rated aesthetics scores, our model achieves state-of-the-art classification performance on the AVA dataset, even though we do not explicitly train or tune the model for the aesthetic classification task.</p><p>In summary, our main contributions are three-fold:</p><p>1. We release a new dataset containing not only score distributions, but also informative attributes and anonymized rater identities. These annotations enable us to study the use of individuals' aesthetics ratings for training our model and analyze how the trained model performs compared to individual human raters. 2. We propose a new CNN architecture that unifies aesthetics attributes and photo content for image aesthetics rating and achieves state-of-the-art performance on existing aesthetics classification benchmark. 3. We propose a novel sampling strategy that utilizes mixed within-and cross-rater image pairs for training models. We show this strategy, in combination with pairwise ranking loss, substantially improves the performance w.r.t. the ranking correlation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>CNN for aesthetics classification: In <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>, CNN-based methods are proposed for classifying images into high-or low-aesthetic categories. The authors also show that using patches from the original high-resolution images largely improves the performance. In contrast, our approach formulates aesthetic prediction as a combined regression and ranking problem. Rather than using patches, our architecture warps the whole input image in order to minimize the overall network size and computational workload while retaining compositional elements in the image, e.g. rule of thirds, which are lost in patch-based approaches.</p><p>Attribute-adaptive models: Some recent works have explored the use of highlevel describable attributes <ref type="bibr">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref> for image aesthetics classification. In early work, these attributes were modeled using hand-crafted features <ref type="bibr">[6]</ref>. This introduces some intrinsic problems, since (1) engineering features that capture high-level semantic attributes is a difficult task, and (2) the choice of describable attributes may ignore some aspects of the image which are relevant to the overall image aesthetics. For these reasons, Marchesotti et al. propose to automatically select a large number of useful attributes based on textual comments from raters <ref type="bibr" target="#b21">[22]</ref> and model these attributes using generic features <ref type="bibr" target="#b20">[21]</ref>. Despite good performance, many of the discovered textual attributes (e.g. so cute, those eyes, so close, very busy, nice try) do not correspond to well defined visual characteristics which hinders their detectability and utility in applications. Perhaps the closest work to our approach is that of Lu et al. , who propose to learn several meaningful style attributes <ref type="bibr" target="#b15">[16]</ref> in a CNN framework and use the hidden features to regularize aesthetics classification network training.</p><p>Content-adaptive models: To make use of image content information such as scene categories or choice of photographic subject, Luo et al. propose to segment regions and extract visual features based on the categorization of photo content <ref type="bibr" target="#b17">[18]</ref>.</p><p>Other work, such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15]</ref>, has also demonstrated that image content is useful for aesthetics analysis. However, it has been assumed that the category labels are provided both during training and testing. To our knowledge, there is only one paper <ref type="bibr" target="#b23">[24]</ref> that attempts to jointly predict content semantics and aesthetics labels. In <ref type="bibr" target="#b23">[24]</ref>, Murray et al. propose to rank images w.r.t aesthetics in a three-way classification problem (high-, medium-and low-aesthetics quality). However, their work has some limitations because (1) deciding the thresholds between nearby classes is non-trivial, and (2) the final classification model outputs a hard label which is less useful than a continuous rating.</p><p>Our work is thus unique in presenting a unified framework that is trained by jointly incorporating the photo content, the meaningful attributes and the aesthetics rating in a single CNN model. We train a category-level classification layer on top of our aesthetics rating network to generate soft weights of category labels, which are used to combine scores predicted by multiple content-adaptive branches. This allows category-specific subnets to complement each other in rating image aesthetics with shared visual content information while efficiently re-using front-end feature computations. While our primary focus is on aesthetic rating prediction, we believe that the content and attribute predictions (as displayed on the right side of images in <ref type="figure">Fig. 1</ref>) represented in hidden layers of our architecture could also be surfaced for use in other applications such as automatic image enhancement and image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Aesthetics and Attributes Database</head><p>To collect a large and varied set of photographic images, we download images from the Flickr website 1 which carry a Creative Commons license and manually curate the data set to remove non-photographic images (e.g. cartoons, drawings, paintings, ads images, adult-content images, etc.). We have five different workers then independently AADB AVA <ref type="bibr" target="#b22">[23]</ref> PN <ref type="bibr">[5]</ref> CUHKPQ <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> Rater's ID <ref type="table">Table 1</ref>. Comparison of the properties of current image aesthetics datasets. In addition to score distribution and meaningful style attributes, AADB also tracks raters' identities across images which we exploit in training to improve aesthetic ranking models. annotate each image with an overall aesthetic score and a fixed set of eleven meaningful attributes using Amazon Mechanical Turk (AMT) <ref type="bibr" target="#b1">2</ref> . The AMT raters work on batches, each of which contains ten images. For each image, we average the ratings of five raters as the ground-truth aesthetic score. The number of images rated by a particular worker follows long tail distribution, as shown later in <ref type="figure" target="#fig_3">Fig. 6</ref> in the experiment.</p><formula xml:id="formula_0">Y N N N All Real Photo Y N Y Y Attribute Label Y Y N N Score Dist. Y Y Y N</formula><p>After consulting professional photographers, we selected eleven attributes that are closely related to image aesthetic judgements: interesting content, object emphasis, good lighting, color harmony, vivid color, shallow depth of f ield, motion blur, rule of thirds, balancing element, repetition, and symmetry. These attributes span traditional photographic principals of color, lighting, focus and composition, and provide a natural vocabulary for use in applications, such as auto photo editing and image retrieval. The final AADB dataset contains 10,000 images in total, each of which have aesthetic quality ratings and attribute assignments provided by five different individual raters. Aggregating multiple raters allows us to assign a confidence score to each attribute, unlike, e.g., AVA where attributes are binary. Similar to previous rating datasets <ref type="bibr" target="#b22">[23]</ref>, we find that average ratings are well fit by a Gaussian distribution. For evaluation purposes, we randomly split the dataset into validation (500), testing (1,000) and training sets (the rest). The supplemental material provides additional details about dataset collection and statistics of the resulting data. <ref type="table">Table 1</ref> provides a summary comparison of AADB to other related public databases for image aesthetics analysis. Except for our AADB and the existing AVA dataset, many existing datasets have two intrinsic problems (as discussed in <ref type="bibr" target="#b22">[23]</ref>), (1) they do not provide full score distributions or style attribute annotation, and (2) images in these datasets are either biased or consist of examples which are particularly easy for binary aesthetics classification. Datasets such as CUHKPQ <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> only provide binary labels (low or high aesthetics) which cannot easily be used for rating prediction. A key difference between our dataset and AVA is that many images in AVA are heavily edited or synthetic (see <ref type="figure" target="#fig_0">Fig. 2</ref>) while AADB contains a much more balanced distribution of professional and consumer photos. More importantly, AVA does not provide any way to identify ratings provided by the same individual for multiple images. We report results of experiments, showing that rater identity on training data provides useful side information for training improved aesthetic predictors.</p><p>Consistency Analysis of the Annotation: One concern is that the annotations provided by five AMT workers for each image may not be reliable given the subjective nature of the task. Therefore, we conduct consistency analysis on the annotations. Since the same five workers annotate a batch of ten images, we study the consistency at batch level. We use Spearman's rank correlation ρ between pairs of workers to measure consistency within a batch and estimate p-values to evaluate statistical significance of the correlation relative to a null hypothesis of uncorrelated responses. We use the Benjamini-Hochberg procedure to control the false discovery rate (FDR) for multiple comparisons <ref type="bibr">[1]</ref>. At an FDR level of 0.05, we find 98.45% batches have significant agreement among raters. This shows that the annotations are reliable for scientific research. Further consistency analysis of the dataset can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fusing Attributes and Content for Aesthetics Ranking</head><p>Inspired by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17]</ref>, we start by fine-tuning AlexNet <ref type="bibr" target="#b11">[12]</ref> using regression loss to predict aesthetic ratings. We then fine-tune a Siamese network <ref type="bibr" target="#b2">[3]</ref> which takes image pairs as input and is trained with a joint Euclidean and ranking loss (Section 4.2). We then append attribute (Section 4.3) and content category classification layers (Section 4.4) and perform joint optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regression Network for Aesthetics Rating</head><p>The network used in our image aesthetics rating is fine-tuned from AlexNet <ref type="bibr" target="#b11">[12]</ref> which is used for image classification. Since our initial model predicts a continuous aesthetic score other than category labels, we replace the softmax loss with the Euclidean loss given by</p><formula xml:id="formula_1">loss reg = 1 2N N i=1 ŷ i − y i 2 2 ,</formula><p>where y i is the average ground-truth rating for image-i, andŷ i is the estimated score by the CNN model. Throughout our work, we re-scale all the ground-truth ratings to be in the range of [0, 1] when preparing the data. Consistent with observations in <ref type="bibr" target="#b16">[17]</ref>, we find that fine-tuning the pre-trained AlexNet <ref type="bibr" target="#b11">[12]</ref> model performs better than that training the network from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pairwise Training and Sampling Strategies</head><p>A model trained solely to minimize the Euclidean loss may still make mistakes in the relative rankings of images that have similar average aesthetic scores. However, more accurate fine-grained ranking of image aesthetics is quite important in applications (e.g. <ref type="figure">Fig. 3</ref>. Architectures for our different models. All models utilize the AlexNet front-end architecture which we augment by (a) replacing the top softmax layer with a regression net and adopting ranking loss in addition to Euclidean loss for training, (b) adding an attribute predictor branch which is then fused with the aesthetic branch to produce a final attribute-adapted rating and (c) incorporating image content scores that act as weights to gate the combination of predictions from multiple content-specific branches.</p><p>in automating photo album management <ref type="bibr">[4]</ref>). Therefore, based on the Siamese network <ref type="bibr" target="#b2">[3]</ref>, we adopt a pairwise ranking loss to explicitly exploit relative rankings of image pairs available in the AADB data (see <ref type="figure">Fig. 3</ref> (a)). The ranking loss is given by:</p><formula xml:id="formula_2">loss rank = 1 2N i,j max 0, α − δ(yi ≥ yj)(ŷi −ŷj) (1) where δ(y i ≥ y j ) = 1, if y i ≥ y j −1,</formula><p>if y i &lt; y j , and α is a specified margin parameter.</p><p>By adjusting this margin and the sampling of image pairs, we can avoid the need to sample triplets as done in previous work on learning domain-specific similarity metrics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref>. Note that the regression alone focuses the capacity of the network on predicting the commonly occurring range of scores, while ranking penalizes mistakes for extreme scores more heavily.</p><p>In order to anchor the scores output by the ranker to the same scale as user ratings, we utilize a joint loss function that includes both ranking and regression:</p><formula xml:id="formula_3">loss reg+rank = lossreg + ωrloss rank ,<label>(2)</label></formula><p>where the parameter ω r controls the relative importance of the ranking loss and is set based on validation data. The network structure is shown in <ref type="figure">Fig. 3 (a)</ref>. Such a structure allows us to utilize different pair-sampling strategies to narrow the scope of learning and provide more consistent training. In our work, we investigate two strategies for selecting pairs of images used in computing the ranking loss. First, we can bias sampling towards pairs of images with a relatively large difference in their average aesthetic scores. For these pairs, the ground-truth rank order is likely to be stable (agreed upon by most raters). Second, as we have raters' identities across images, we can sample image pairs that have been scored by the same individual. While different raters may have different aesthetics tastes which erode differences in the average aesthetic score, we expect a given individual should have more consistent aesthetic judgements across multiple images. We show the empirical effectiveness of these sampling strategies in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attribute-Adaptive Model</head><p>Previous work on aesthetic prediction has investigated the use of attribute labels as input features for aesthetics classification (e.g. <ref type="bibr">[6]</ref>). Rather than independently training attribute classifiers, we propose to include additional activation layers in our ranking network that are trained to encode informative attributes. We accomplish this by including an additional term in the loss function that encourages the appropriate attribute activations. In practice, annotating attributes for each training image is expensive and time consuming. This approach has the advantage that it can be used even when only a subset of training data comes with attribute annotations. Our approach is inspired by <ref type="bibr" target="#b15">[16]</ref> which also integrates attribute classifiers, but differs in that the attribute-related layer shares the same front-end feature extraction with the aesthetic score predictor (see <ref type="figure">Fig. 3(b)</ref>). The attribute prediction task can thus be viewed as a source of sideinformation or "deep supervision" <ref type="bibr" target="#b12">[13]</ref> that serves to regularize the weights learned during training even though it is not part of the test-time prediction, though could be enabled when needed.</p><p>We add an attribute prediction branch on top of the second fully-connected layer in the aesthetics-rating network described previously. The attribute predictions from this layer are concatenated with the base model to predict the final aesthetic score. When attribute annotations are available, we utilize a K-way softmax loss or Euclidean loss, denoted by loss att , for the attribute activations and combine it with the rating and ranking losses loss =loss reg + ω r loss rank + ω a loss att</p><p>where ω a controls relative importance of attribute fine-tuning. If we do not have enough data with attribute annotations, we can freeze the attribute layer and only fine-tune through the other half of the concatenation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Content-Adaptive Model</head><p>The importance of particular photographic attributes depends strongly on image content <ref type="bibr" target="#b17">[18]</ref>. For example, as demonstrated by <ref type="figure">Fig. 1</ref>, vivid color and rule of thirds are highly relevant in rating landscapes but not for closeup portraits. In <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b14">15]</ref>, contents at the category level are assumed to be given in both training and testing stages, and category-specific models are then trained or fine-tuned. Here we propose to incorporate the category information into our model for joint optimization and prediction, so that the model can also work on those images with unknown category labels. We fine-tune the top two layers of AlexNet <ref type="bibr" target="#b11">[12]</ref> with softmax loss to train a contentspecific branch to predict category labels 3 (as shown by ContClass layer in <ref type="figure">Fig. 3 (c)</ref>). Rather than making a hard category selection, we use the softmax output as a weighting vector for combining the scores produced by the category specific branches, each of which is a concatenation of attribute feature and content-specific features (denoted by Att fea and Cont fea respectively in <ref type="figure">Fig. 3 (c)</ref>). This allows for content categories to be non-exclusive (e.g. a photo of an individual in a nature scene can utilize attributes for either portrait and scenery photos). During training, When fine-tuning the whole net as in <ref type="figure">Fig. 3</ref> (c), we freeze the content-classification branch and fine-tune the rest network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Implementation Details</head><p>We warp images to 256×256 and randomly crop out a 227×227 window to feed into the network. The initial learning rate is set at 0.0001 for all layers, and periodically annealed by 0.1. We set weight decay 1e − 5 and momentum 0.9. We use Caffe toolbox <ref type="bibr" target="#b8">[9]</ref> extended with our ranking loss for training all the models.</p><p>To train attribute-adaptive layers, we use softmax loss on AVA dataset which only has binary labels for attributes, and the Euclidean loss on the AADB dataset which has finer-level attribute scores. We notice that, on the AVA dataset, our attribute-adaptive branch yields 59.11% AP and 58.73% mAP for attribute prediction, which are comparable to the reported results of style-classification model fine-tuned from AlexNet <ref type="bibr" target="#b16">[17]</ref>. When learning content-adaptive layers on the AVA dataset for classifying eight categories, we find the content branch yields 59% content classification accuracy on the testing set. If we fine-tune the whole AlexNet, we obtain 62% classification accuracy. Note that we are not pursuing the best classification performance on either attributes or categories. Rather, our aim is to train reasonable branches that perform well enough to help with image aesthetics rating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>To validate our model for rating image aesthetics, we first compare against several baselines including the intermediate models presented in Section 4, then analyze the dependence of model performance on the model parameters and structure, and finally compare performance of our model with human annotation in rating image aesthetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Datasets</head><p>AADB dataset contains 10,000 images in total, with detailed aesthetics and attribute ratings, and anonymized raters' identity for specific images. We split the dataset into training (8,500), validation (500) and testing (1,000) sets. Since our dataset does not include ground-truth image content tags, we use clustering to find semantic content groups prior to training content adaptive models. Specifically, we represent each image using the fc7 features, normalize the feature vector to be unit Euclidean length, and use unsupervised k-means for clustering. In our experimental comparison, we cluster training images into k = 10 content groups, and transform the distances between a testing image and the centroids into prediction weights using a softmax. The value of k was chosen using validation data (see Section 5.3). <ref type="figure" target="#fig_1">Fig. 4</ref> shows samples from four of these clusters, from which we observe consistencies within each cluster and distinctions across clusters. AVA dataset contains approximately 250,000 images, each of which has about 200 aesthetic ratings ranging on a one-to-ten scale. For fair comparison, we follow the experimental practices and train/test split used in literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> which results in about 230,000 training and 20,000 test images. When fine-tuning AlexNet for binary aesthetics classification, we divide the training set into two categories (low-and highaesthetic category), with a score threshold of 5 as used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>. We use the subset of images which contain style attributes and content tags for training and testing the attribute-adaptive and content-adaptive branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation</head><p>To evaluate the aesthetic scores predicted by our model, we report the ranking correlation measured by Spearman's ρ between the estimated aesthetics scores and the ground-truth scores in the test set <ref type="bibr" target="#b24">[25]</ref>. Let r i indicate the rank of the ith item when we sort the list by scores {y i } andr i indicate the rank when ordered by {ŷ i }. We can compute the disagreement in the two rankings of a particular element i as d i = r i −r i . The Spearman's ρ rank correlation statistic is calculated as ρ = 1 −</p><formula xml:id="formula_5">6 d 2 i N 3 −N ,</formula><p>where N is the total number of images ranked. This correlation coefficient lies in the range of [−1, 1], with larger values corresponding to higher correlation in the rankings. The ranking correlation is particularly useful since it is invariant to monotonic transformations of the aesthetic score predictions and hence avoids the need to precisely calibrate output scores against human ratings. For purposes of comparing to existing classification accuracy results reported on the AVA dataset, we simply threshold the estimated scores [ŷ i &gt; τ ] to produce a binary prediction where the threshold τ is determined on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>For comparison, we also train a model for binary aesthetics classification by fine-tuning AlexNet (AlexNet FT Conf). This has previously been shown to be a strong baseline for aesthetic classification <ref type="bibr" target="#b16">[17]</ref>. We use the softmax confidence score corresponding of the high-aesthetics class as the predicted aesthetic rating. As described in Section 4, we consider variants of our architecture including the regression network alone (Reg), along with the addition of the pairwise ranking loss (Reg+Rank), attribute-constraint branches (Reg+Rank+Att) and content-adaptive branches (Reg+Rank+Cont). We also evaluate different pair-sampling strategies including within-and cross-rater sampling.  Model Architecture and Loss Functions: <ref type="table" target="#tab_0">Table 2</ref> and 3 list the performance on AADB and AVA datasets, respectively. From these tables, we notice several interesting observations. First, AlexNet FT Conf model yields good ranking results measured by ρ. This indicates that the confidence score in softmax can provide information about relative rankings. Second, the regression net outperforms the AlexNet FT Conf model, and ranking loss further improves the ranking performance on both datasets. This shows the effectiveness of our ranking loss which considers relative aesthetics ranking of image pairs in training the model. More specifically, we can see from <ref type="table" target="#tab_0">Table 2</ref> that, by sampling image pairs according to the the averaged ground-truth scores, i.e. cross-rater sampling only, Reg+Rank (cross-rater) achieves the ranking coefficient ρ = 0.6308; whereas if only sampling image pairs within each raters, we have ρ = 0.6450 by by Reg+Rank (within-rater). This demonstrates the effectiveness of sampling image pairs within the same raters, and validates our idea that the same individual has consistent aesthetics ratings. When using both strategies to sample image pairs, the performance is even better by Reg+Rank (within-&amp; cross-), leading to ρ = 0.6515. This is possibly due to richer information contained in more training pairs. By comparing the results in <ref type="table" target="#tab_1">Table 3</ref> between "Reg" (0.4995) and "Reg+Rank" (0.5126), and between "Reg+Att" (0.5331) and "Reg+Rank+Att" (0.5445) , we clearly observe that the ranking loss improves the ranking correlation. In this case, we can only exploit the cross-rater sampling strategy since rater's identities are not available in AVA for the stronger within-rater sampling approach. We note that for values of ρ near 0.5 computed over 20000 test images on AVA dataset, differences in rank correlation of 0.01 are highly statistically significant. These results clearly show that the ranking loss helps enforce overall ranking consistency.</p><p>To show that improved performance is due to the side information (e.g. attributes) other than a wider architecture, we first train an ensemble of eight rating networks (Reg) and average the results, leading to a rho=0.5336 (c.f. Reg+Rank+Att which yields rho=0.5445). Second, we try directly training the model with a single Euclidean loss using a wider intermediate layer with eight times more parameters. In this case we observed severe overfitting. This suggests for now that the side-supervision is necessary to effectively train such an architecture.  Third, when comparing Reg+Rank with Reg+Rank+Att, and Reg+Rank with Reg+ Rank+Cont, we can see that both attributes and content further improve ranking performance. While image content is not annotated on the AADB dataset, our contentadaptive model based on unsupervised K-means clustering still outperforms the model trained without content information. The performance benefit of adding attributes is substantially larger for AVA than AADB. We expect this is due to (1) differences in the definitions of attributes between the two datasets, and (2) the within-rater sampling for AADB, which already provides a significant boost making further improvement using attributes more difficult. The model trained with ranking loss, attribute-constraint and content-adaptive branches naturally performs the best among all models. It is worth noting that, although we focus on aesthetics ranking during training, we also achieve the state-of-the-art binary classification accuracy in AVA. This further validates our emphasis on relative ranking, showing that learning to rank photo aesthetics can naturally lead to good classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Hyperparameters:</head><p>In training our content-adaptive model on the AADB dataset which lacks supervised content labels, the choice of cluster number is an important parameter. <ref type="figure" target="#fig_2">Fig. 5</ref> plots the ρ on validation data as a function of the number of clusters K for the Reg+Cont model (without ranking loss). We can see the finer clustering improves performance as each content specific model can adapt to a sub-category of images. However, because the total dataset is fixed, performance eventually drops as the amount of training data available for tuning each individual content-adaptive branch decreases. We thus fixed K = 10 for training our unified network on AADB.</p><p>The relative weightings of the loss terms (specified by ω r in Eq. 2) is another important parameter. <ref type="table" target="#tab_2">Table 4</ref> shows the ranking correlation test performance on both datasets w.r.t. different choices of ω r . We observe that larger ω r is favored in AADB than that in AVA, possibly due to the contribution from the within-rater image pair sampling strategy. We set ω a (in Eq. 3) to 0.1 for jointly fine-tuning attribute regression and aesthetic rating. For the rank loss, we used validation performance to set the margin α to 0.15 and 0.02 on AVA and AADB respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Sampled Image Pairs:</head><p>Is it possible that better performance can be obtained through more sampled pairs instead of leveraging rater's information? To test this, we sample 2 and 5 million image pairs given the fixed training images on the AADB dataset, and report in <ref type="table" target="#tab_3">Table 5</ref> the performance of model "Reg+Rank" using different sampling strategies, i.e. within-rater only, cross-rater only and within-&amp;crossrater sampling. It should be noted the training image set remains the same, we just sample more pairs from them. We can see that adding more training pairs yields little differences in the final results, and even declines slightly when using higher cross-   rater sampling rates. These results clearly emphasize the effectiveness of our proposed sampling strategy which (perhaps surprisingly) yields much bigger gains than simply increasing the number of training pairs by 2.5x.</p><p>Classification Benchmark Performance: Our model achieves state-of-the-art classification performance on the AVA dataset simply by thresholding the estimated score ( <ref type="table" target="#tab_1">Table 3</ref>). It is worth noting that our model uses only the whole warped down-sampled images for both training and testing, without using any high-resolution patches from original images. Considering the fact that the fine-grained information conveyed by highresolution image patches is especially useful for image quality assessment and aesthetics analysis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>, it is quite promising to see our model performing so well. The best reported results <ref type="bibr" target="#b16">[17]</ref> for models that use low resolution warped images for aesthetics classification are based on Spatial Pyramid Pooling Networks (SPP) <ref type="bibr" target="#b7">[8]</ref> and achieves an accuracy of 72.85%. Compared to SPP, our model achieves 77.33%, a gain of 4.48%, even though our model is not tuned for classification. Previous work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> has shown that leveraging the high-resolution patches could lead to additional 5% potential accuracy improvement. We expect a further accuracy boost would be possible by applying this strategy with our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Further Comparison with Human Rating Consistency</head><p>We have shown that our model achieves a high level of agreement with average aesthetic ratings and outperforms many existing models. The raters' identities and ratings for the images in our AADB dataset enable us to further analyze agreement between our model each individual as well as intra-rater consistency. While human raters produce rankings which are similar with high statistical significance, as evaluated in Section 3, there is variance in the numerical ratings between them. To this end, we calculate ranking correlation ρ between each individual's ratings and the ground-truth average score. When comparing an individual to the ground-truth, we do not exclude that individual's rating from the ground-truth average for the sake of comparable evaluations across all raters. <ref type="figure" target="#fig_3">Fig. 6</ref> shows the number of images each rater has rated and their corresponding performance with respect to other raters. Interestingly, we find that the hard workers tend to provide more consistent ratings. In <ref type="table" target="#tab_4">Table 6</ref>, we summarize the individuals' performance by choosing a subset raters based on the number of images they have rated. This clearly indicates that the different human raters annotate the images consistently, and when labeling more images, raters contribute more stable rankings of the aesthetic scores.</p><p>Interestingly, from <ref type="table" target="#tab_4">Table 6</ref>, we can see that our model actually performs above the level of human consistency (as measured by ρ) averaged across all workers. However, when concentrating on the "power raters" who annotate more images, we still see a gap between machine and human level performance in terms of rank correlation ρ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cross-Dataset Evaluation</head><p>As discussed in Section 3, AVA contains professional images downloaded from a community based rating website; while our AADB contains a much more balanced distribution of consumer photos and professional photos rated by AMT workers, so has better generalizability to wide range of real-world photos.</p><p>To quantify the differences between these datasets, we evaluate whether models trained on one dataset perform well on the other. <ref type="table">Table 7</ref> provides a comparison of the cross-dataset performance. Interestingly, we find the models trained on either dataset have very limited "transferability". We conjecture there are two reasons. First, different groups of raters have different aesthetics tastes. This can be verified that, when looking at the DPChallenge website where images and ratings in the AVA dataset were taken from. DPChallenge provides a breakdown of scores which shows notable differences between the average scores among commenters, participants and non-participants. Second, the two datasets contain photos with different distributions of visual characteristics. For example, many AVA photos are professionally photographed or heavily edited; while AADB contains many daily photos from casual users. This observation motivates the need for further exploration into mechanisms for learning aesthetic scoring that is adapted to the tastes of specific user groups or photo collections <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed a CNN-based method that unifies photo style attributes and content information to rate image aesthetics. In training this architecture, we leverage individual aesthetic rankings which are provided by a novel dataset that includes aesthetic and attribute scores of multiple images by individual users. We have shown that our model is also effective on existing classification benchmarks for aesthetic judgement. Despite not using high-resolution image patches, the model achieves state-of-the-art classification performance on the AVA benchmark by simple thresholding. Comparison to individual raters suggests that our model performs as well as the "average" mechanical turk worker but still lags behind more consistent workers who label large batches of images. These observations suggest future work in developing aesthetic rating systems that can adapt to individual user preferences.</p><p>These attributes span traditional photographic principals of color, lighting, focus and composition, and provide a natural vocabulary for use in applications, such as auto photo editing and image retrieval. To visualize images containing these attributes, please refer to the attached our AMT instruction in the end of this supplementary material. The instruction is used for teaching raters to pass the qualification test. To collect a varied set of photographic images, we download images from Flickr website 4 , which carry a Creative Commons license. We manually curate the dataset to remove non-photographic images (e.g. cartoons, drawings, paintings, ads images, adult-content images, etc.). We have multiple workers independently annotate each image with an overall aesthetic score and the eleven meaningful attributes using Amazon Mechanical Turk 5 . For each attribute, we allow workers to click "positive" if this attribute conveyed by the image can enhance the image aesthetic quality, or "negative" if the attribute degrades image aesthetics. The default is "null", meaning the attribute does not effect image aesthetics. For example, "positive" vivid color means the vividness of the color presented in the image has a positive effect on the image aesthetics; while the counterpart "negative" means, for example, there is dull color composition. Note that we do not let workers tag negative repetition and symmetry, as for the two attributes negative values do not make sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Collection By Amazon Mechanical Turk</head><p>We launch a task consisting of 10,000 images on AMT, and let five different workers label each image. All the workers must read instructions and pass a qualification exam before they become qualified to do our task. The images are split into batches, each of which contains ten images. Therefore, raters will annotate different numbers of batches. There are 190 workers in total doing our AMT task, and the workers follow long tail distribution, as demonstrated by <ref type="figure" target="#fig_4">Figure 7</ref>. <ref type="figure" target="#fig_5">Figure 8</ref> shows the interface of our AMT task.</p><p>Note that even though judging these attributes is also subjective, the averaged scores of these attributes indeed reflect good information if we visualize the ranked images w.r.t averaged scores. Therefore, we use the averaged score as the ground truth, for both aesthetic score and attributes. Furthermore, we normalize aesthetic score to the range of [0, 1], as shown by <ref type="figure" target="#fig_6">Figure 9</ref>, from which we can see that ratings are well fit by a Gaussian distribution. This observation is consistent with that reported in <ref type="bibr" target="#b22">[23]</ref>. In our experiments we normalize the attributes' scores to the range of [−1, 1]. The images are split into testing set (1,000 images), validation set (500 images) and training set (the rest). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Statistics of AADB</head><p>The final AADB dataset contains 10,000 images in total, each of which has aesthetic quality ratings and attribute assignments provided by five different individual raters. Therefore, we have rating scores for attributes as well, which is different from AVA dataset <ref type="bibr" target="#b22">[23]</ref> in which images only have binary labels for the attributes. <ref type="figure">Figure 10</ref> shows the distribution of each attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Consistency Analysis</head><p>As there are five individuals rating each image, one may argue that the annotations are not reliable for this subjective task. Therefore, we carry out consistency analysis. We use both Kendall's W and Spearman's ρ for the analysis. Kendall's W directly measures the agreement among multiple raters, and accounts for tied ranks. It ranges from 0 (no <ref type="figure">Fig. 10</ref>. The distributions of all the eleven attributes. Note that for attributes repetition and symmetry, we do not let AMT workers annotate negative labels, as these attributes are of neutral meaning. Instead, we only allow them to point out whether there exist repetition or symmetry. To solve the data imbalance problem in training attribute classifiers, we adopt some data augmentation tricks to sample more rare cases. First, we conduct a permutation test over global W to obtain the distribution of W under the null hypothesis. We plot the curve of W : p(W ) vs. W in <ref type="figure" target="#fig_7">Fig. 11</ref> and p(W &lt; t) vs. t in <ref type="figure" target="#fig_0">Fig 12.</ref> We can easily see that the empirical Kendall's W on our AADB dataest is statistically significant.</p><p>Then, for each batch, we can also evaluate the annotation consistency with Kendall's W , which directly calculates the agreement among multiple raters, and accounts for tied ranks. As there are ten images and only five possible ratings for each image, tied ranks may happen in a batch. The average Kendall's W over all batches is 0.5322. This shows significant consistency of the batches annotated by the AMT workers. To test the statistical significance of Kendall's W at batch level, we adopt the Benjamini-Hochberg procedure to control the false discovery rate (FDR) for multiple comparisons <ref type="bibr">[1]</ref>. At level Q = 0.05, 99.07% batches from 1, 013 in total have significant agreement. This shows that almost all the batches annotated by AMT workers have consistent labels and are reliable for scientific use.</p><p>Furthermore, we can also test the statistical significance w.r.t Spearman's ρ at batch levels using Benjamini-Hochberg procedure. The p-values of pairwise ranks of raters in a batch can be computed by the exact permutation distributions. We average the pairwise p-values as the p-value for the batch. With the FDR level Q = 0.05, we find that 98.45% batches have significant agreement. This further demonstrates the reliability of the annotations.  To show the effectiveness of utilizing content information as a weights for output scores by different content-specific aesthetics rating branches, we report the performance on AVA dataset of different methods in <ref type="table" target="#tab_5">Table 8</ref>. Our first method is named "con-catGT", which means we use the ground-truth content label of an image, and get the estimated aesthetic score by the content-specific branch; then we put all the estimated scores together to get the global Spearman's ρ and classification accuracy. In method "concatPred", we use the predicted content label to choose which category-specific branch to use for estimating aesthetic score, then use the same procedure as in "con-catGT". In method "avg.", we use all the content-specific aesthetics rating branches to get multiple scores, and average them to a single score as the final estimation. In "weightedSum", we use the classification confidence score output by softmax of the content classification branch to do weighted sum for the final score. In "weightedSum FT", we fine-tune the whole network but freezing the classification branch, and use the finetuned model to do weighted sum on the scores for the final aesthetics rating. From this table, we can clearly observe that "weightedSum FT" performs the best, which is the one described in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Demonstration of Our Model</head><p>In this section, we test our model on personal photos qualitatively, in which these photos are downloaded online and not part of our AADB dataset. As our model can predicts all the eleven attributes, we show the attributes' estimation as well as the rated aesthetic scores. For better visualization, we simple set thresholds as (−0.2) and (0.2) to characterize "negative", "null" and "positive" attributes, respectively. <ref type="figure" target="#fig_2">Figure 13 -15</ref> show the results for images with high, low and medium estimated scores. We can see, in general, our model reasonably captures attributes and gives aesthetic scores.  <ref type="figure" target="#fig_2">Fig. 15</ref>. Some images outside our database with medium estimated scores.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Our AADB dataset consists of a wide variety of photographic imagery of real scenes collected from Flickr. This differs from AVA which contains significant numbers of professional images that have been highly manipulated, overlayed with advertising text, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Example images from four content clusters found in the training set. These clusters capture thematic categories of image content present in AADB without requiring additional manual labeling of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Dependence of model performance by varying the number of content clusters. We select K = 10 clusters in our experiments on AADB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Panels show (left) the number of images labeled by each worker, and the performance of each individual rater w.r.t Spearman's ρ (Right). Red line shows our model's performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Long tail distribution of AMT workers: number of rated images vs. each worker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Interface of data collection by AMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>The distribution of rated image aesthetic scores by the AMT workers follows a Gaussian distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Permutation test on Kendall's W : p(W ) vs. W . agreement) to 1 (complete agreement). Spearman's ρ is used in our paper that compares a pair of ranking lists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Permutation test on Kendall's W : p(W &lt; t) vs. t. Appendix: Analysis of Content-Aware Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Some images outside our database with high estimated scores. Some images outside our database with low estimated scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison of different models on AADB dataset.</figDesc><table><row><cell>Methods</cell><cell>ρ</cell></row><row><cell>AlexNet FT Conf</cell><cell>0.5923</cell></row><row><cell>Reg</cell><cell>0.6239</cell></row><row><cell>Reg+Rank (cross-rater)</cell><cell>0.6308</cell></row><row><cell>Reg+Rank (within-rater)</cell><cell>0.6450</cell></row><row><cell cols="2">Reg+Rank (within-&amp; cross-) 0.6515</cell></row><row><cell>Reg+Rank+Att</cell><cell>0.6656</cell></row><row><cell>Reg+Rank+Cont</cell><cell>0.6737</cell></row><row><cell>Reg+Rank+Att+Cont</cell><cell>0.6782</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison of different models on AVA dataset.</figDesc><table><row><cell>Methods</cell><cell>ρ</cell><cell>ACC (%)</cell></row><row><cell>Murray et al. [23]</cell><cell>-</cell><cell>68.00</cell></row><row><cell>SPP [8]</cell><cell>-</cell><cell>72.85</cell></row><row><cell>AlexNet FT Conf</cell><cell>0.4807</cell><cell>71.52</cell></row><row><cell>DCNN [16]</cell><cell>-</cell><cell>73.25</cell></row><row><cell>RDCNN [16]</cell><cell>-</cell><cell>74.46</cell></row><row><cell>RDCNN semantic [15]</cell><cell>-</cell><cell>75.42</cell></row><row><cell>DMA [17]</cell><cell>-</cell><cell>74.46</cell></row><row><cell>DMA AlexNet FT [17]</cell><cell>-</cell><cell>75.41</cell></row><row><cell>Reg</cell><cell>0.4995</cell><cell>72.04</cell></row><row><cell>Reg+Rank</cell><cell>0.5126</cell><cell>71.50</cell></row><row><cell>Reg+Att</cell><cell>0.5331</cell><cell>75.32</cell></row><row><cell>Reg+Rank+Att</cell><cell>0.5445</cell><cell>75.48</cell></row><row><cell>Reg+Rank+Cont</cell><cell>0.5412</cell><cell>73.37</cell></row><row><cell cols="2">Reg+Rank+Att+Cont 0.5581</cell><cell>77.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ranking performance ρ vs. rank loss weighting ωr in Eq. 2.</figDesc><table><row><cell>ωr</cell><cell>0.0</cell><cell>0.1</cell><cell>1</cell><cell>2</cell></row><row><cell cols="5">AADB 0.6382 0.6442 0.6515 0.6276</cell></row><row><cell cols="5">AVA 0.4995 0.5126 0.4988 0.4672</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ranking performance (ρ) of "Reg+Rank" with different numbers of sampled image pairs on AADB dataset.</figDesc><table><row><cell>#ImgPairs</cell><cell cols="2">2 million 5 million</cell></row><row><cell>cross-rater</cell><cell>0.6346</cell><cell>0.6286</cell></row><row><cell>within-rater</cell><cell>0.6450</cell><cell>0.6448</cell></row><row><cell cols="2">within-&amp; cross-rater 0.6487</cell><cell>0.6515</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Human perf. on the AADB dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Table 7. Cross dataset train/test evaluation.</cell></row><row><cell>#images #workers &gt;0 190 &gt;100 65 &gt;200 42 Our best -</cell><cell>ρ 0.6738 0.7013 0.7112 0.6782</cell><cell>Spearman's ρ AADB train AVA</cell><cell>test AADB AVA 0.6782 0.1566 0.3191 0.5154</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Analysis of content-aware model on AVA dataset.</figDesc><table><row><cell>method</cell><cell cols="5">concatGT concatPred avg. weightedSum weightedSum FT</cell></row><row><cell cols="2">Spearman's ρ 0.5367</cell><cell cols="2">0.5327 0.5336</cell><cell>0.5335</cell><cell>0.5426</cell></row><row><cell cols="2">accuracy(%) 75.41</cell><cell>75.33</cell><cell>75.39</cell><cell>75.33</cell><cell>75.57</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.flickr.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">www.mturk.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Even though category classification uses different features from those in aesthetics rating, we assume the low-level features can be shared across aesthetics and category levels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">www.flickr.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">www.mturk.com</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix: Aesthetics and Attributes Database (AADB)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attributes in AADB</head><p>We select eleven attributes that are highly related to image aesthetics after consulting professional photographers, which are</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The control of the false discovery rate in multiple testing under dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yekutieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1165" to="1188" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collaborative personalization of image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Easyalbum: an interactive photo annotation system based on face clustering and re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The role of attractiveness in web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimedia</title>
		<meeting>the 19th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rapid: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="386" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering beautiful attributes for aesthetic image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning beautiful (and ugly) attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ava: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to rank images using semantic and aesthetic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Research design and statistical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Well</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Lorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Routledge</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aesthetic quality classification of photographs based on color harmony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Leveraging user comments for aesthetic aware image search reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>San Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">balancing element&quot; -whether the image contains balanced elements</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">content&quot; -whether the image has good/interesting content</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">depth of field&quot; -whether the image has shallow depth of field</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">lighting&quot; -whether the image has good/interesting lighting</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">motion blur&quot; -whether the image has motion blur</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">rule of thirds&quot; -whether the photography follows rule of thirds</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">symmetry&quot; -whether the photo has symmetric patterns</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
