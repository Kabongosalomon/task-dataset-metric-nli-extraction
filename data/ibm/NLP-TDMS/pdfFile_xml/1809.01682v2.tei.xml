<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Relevance Ranking Using Enhanced Document-Query Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios-Ioannis</forename><surname>Brokos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Relevance Ranking Using Enhanced Document-Query Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore several new models for document relevance ranking, building upon the Deep Relevance Matching Model (</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document relevance ranking, also known as adhoc retrieval <ref type="bibr" target="#b8">(Harman, 2005)</ref>, is the task of ranking documents from a large collection using the query and the text of each document only. This contrasts with standard information retrieval (IR) systems that rely on text-based signals in conjunction with network structure <ref type="bibr" target="#b22">(Page et al., 1999;</ref><ref type="bibr" target="#b15">Kleinberg, 1999)</ref> and/or user feedback <ref type="bibr" target="#b13">(Joachims, 2002)</ref>. Text-based ranking is particularly important when (i) click-logs do not exist or are small, and (ii) the network structure of the collection is non-existent or not informative for query-focused relevance. Examples include various domains in digital libraries, e.g., patents <ref type="bibr" target="#b0">(Azzopardi et al., 2010)</ref> or scientific literature <ref type="bibr" target="#b39">(Wu et al., 2015;</ref><ref type="bibr">Tsatsaronis et al., 2015)</ref>; enterprise search <ref type="bibr" target="#b9">(Hawking, 2004)</ref>; and personal search <ref type="bibr" target="#b4">(Chirita et al., 2005)</ref>.</p><p>We investigate new deep learning architectures for document relevance ranking, focusing on termbased interaction models, where query terms (qterms for brevity) are scored relative to a docu-  <ref type="bibr" target="#b7">(Guo et al., 2016)</ref> for a query of three terms and a document of m terms. ment's terms <ref type="bibr">(d-terms)</ref> and their scores are aggregated to produce a relevance score for the document. Specifically, we use the Deep Relevance Matching Model (DRMM) of <ref type="bibr" target="#b7">Guo et al. (2016)</ref>  <ref type="figure" target="#fig_0">(Fig. 1)</ref>, which was shown to outperform strong IR baselines and other recent deep learning methods. DRMM uses pre-trained word embeddings for q-terms and d-terms, and cosine similarity histograms (outputs of ⊗ in <ref type="figure" target="#fig_0">Fig. 1)</ref>, each capturing the similarity of a q-term to all the d-terms of a particular document. The histograms are fed to an MLP (dense layers of <ref type="figure" target="#fig_0">Fig. 1</ref>) that produces the (document-aware) score of each q-term. Each qterm score is then weighted using a gating mechanism (topmost box nodes in <ref type="figure" target="#fig_0">Fig. 1</ref>) that examines properties of the q-term to assess its importance for ranking (e.g., common words are less important). The sum of the weighted q-term scores is the relevance score of the document. This ignores entirely the contexts where the terms occur, in contrast to recent position-aware models such as PACRR <ref type="bibr" target="#b11">(Hui et al., 2017)</ref> or those based on recurrent representations <ref type="bibr" target="#b23">(Palangi et al., 2016)</ref>.</p><p>In order to enrich DRMM with context-sensitive representations, we need to change fundamentally how q-terms are scored. This is because rich context-sensitive representations -such as input term encodings based on RNNs or CNNs -require end-to-end training and histogram construction is not differentiable. To account for this we investigate novel query-document interaction mechanisms that are differentiable and show empirically that they are effective ways to enable end-to-end training of context-sensitive DRMM models. This is the primary contribution of this paper.</p><p>Overall, we explore several extensions to DRMM, including: PACRR-like convolutional ngram matching features ( §3.1); context-sensitive term encodings ( §3.2); query-focused attentionbased document representations ( §3.3); pooling to reward denser term matches and turn rich term representations into fixed-width vectors ( §3.4); multiple views of terms, e.g., context sensitive, insensitive, exact matches ( §3.5).</p><p>We test our models on data from the BIOASQ biomedical question answering challenge <ref type="bibr">(Tsatsaronis et al., 2015)</ref> and <ref type="bibr">TREC ROBUST 2004</ref><ref type="bibr" target="#b37">(Voorhees, 2005</ref>, showing that they outperform strong BM25-based baselines <ref type="bibr" target="#b27">(Robertson and Zaragoza, 2009</ref>), DRMM, and PACRR. 1 2 Related Work Document ranking has been studied since the dawn of IR; classic term-weighting schemes were designed for this problem <ref type="bibr" target="#b31">(Sparck Jones, 1988;</ref><ref type="bibr" target="#b28">Robertson and Sparck Jones, 1988)</ref>. With the advent of statistical NLP and statistical IR, probabilistic language and topic modeling were explored <ref type="bibr" target="#b41">(Zhai and Lafferty, 2001;</ref><ref type="bibr" target="#b38">Wei and Croft, 2006)</ref>, followed recently by deep learning IR methods <ref type="bibr" target="#b18">(Lu and Li, 2013;</ref><ref type="bibr" target="#b10">Hu et al., 2014;</ref><ref type="bibr" target="#b23">Palangi et al., 2016;</ref><ref type="bibr" target="#b7">Guo et al., 2016;</ref><ref type="bibr" target="#b11">Hui et al., 2017)</ref>.</p><p>Most document relevance ranking methods fall within two categories: representation-based, e.g., <ref type="bibr" target="#b23">Palangi et al. (2016)</ref>, or interaction-based, e.g., <ref type="bibr" target="#b18">Lu and Li (2013)</ref>. In the former, representations of the query and document are generated independently. Interaction between the two only happens at the final stage, where a score is generated indicating relevance. End-to-end learning and backpropagation through the network tie the two representations together. In the interactionbased paradigm, explicit encodings between pairs of queries and documents are induced. This al-lows direct modeling of exact-or near-matching terms (e.g., synonyms), which is crucial for relevance ranking. Indeed, <ref type="bibr" target="#b7">Guo et al. (2016)</ref> showed that the interaction-based DRMM outperforms previous representation-based methods. On the other hand, interaction-based models are less efficient, since one cannot index a document representation independently of the query. This is less important, though, when relevance ranking methods rerank the top documents returned by a conventional IR engine, which is the scenario we consider here.</p><p>One set of our experiments ranks biomedical texts. Several methods have been proposed for the BIOASQ challenge <ref type="bibr">(Tsatsaronis et al., 2015)</ref>, mostly based on traditional IR techniques. The most related work is of <ref type="bibr">Mohan et al. (2017)</ref>, who use a deep learning architecture. Unlike our work, they focus on user click data as a supervised signal, and they use context-insensitive representations of document-query interactions. The other dataset we experiment with, <ref type="bibr">TREC ROBUST 2004</ref><ref type="bibr" target="#b37">(Voorhees, 2005</ref>, has been used extensively to evaluate traditional and deep learning IR methods.</p><p>Document relevance ranking is also related to other NLP tasks. Passage scoring for question answering <ref type="bibr" target="#b32">(Surdeanu et al., 2008)</ref> ranks passages by their relevance to the question; several deep networks have been proposed, e.g., <ref type="bibr" target="#b33">Tan et al. (2015)</ref>. Short-text matching/ranking is also related and has seen recent deep learning solutions <ref type="bibr" target="#b18">(Lu and Li, 2013;</ref><ref type="bibr" target="#b10">Hu et al., 2014;</ref><ref type="bibr" target="#b29">Severyn and Moschitti, 2015)</ref>. In document relevance ranking, though, documents are typically much longer than queries, which makes methods from other tasks that consider pairs of short texts not directly applicable.</p><p>Our starting point is DRMM, to which we add richer representations inspired by PACRR. Hence, we first discuss DRMM and PACRR further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DRMM</head><p>We have already presented an overview of DRMM. For gating (topmost box nodes of <ref type="figure" target="#fig_0">Fig. 1</ref>), <ref type="bibr" target="#b7">Guo et al. (2016)</ref> use a linear self-attention:</p><formula xml:id="formula_0">g i = softmax w T g φ g (q i ); q 1 , . . . , q n where φ g (q i )</formula><p>is the embedding e(q i ) of the i-th qterm, or its IDF, idf(q i ), and w g is a weights vector. Gating aims to weight the (document-aware) score of each q-term (outputs of dense layers in <ref type="figure" target="#fig_0">Fig. 1</ref>) based on the importance of the term. We found that φ g (q i ) = [e(q i ); idf(q i )], where ';' is concatenation, was optimal for all DRMM-based models.  The crux of the original DRMM are the bucketed cosine similarity histograms (outputs of ⊗ nodes in <ref type="figure" target="#fig_0">Fig. 1</ref>), each capturing the similarity of a q-term to all the d-terms. In each histogram, each bucket counts the number of d-terms whose cosine similarity to the q-term is within a particular range. Consider a document with three terms, with cosine similarities, s, to a particular q-term q i 0.5, 0.1, −0.3, respectively. If we used two buckets −1 ≤ s &lt; 0 and 0 ≤ s ≤ 1, then the input to the dense layers for q i would be 1, 2 . The fixed number of buckets leads to a fixed-dimension input for the dense layers and makes the model agnostic to different document and query lengths -one of DRMM's main strengths. The main disadvantage is that bucketed similarities are independent of the contexts where terms occur. A q-term 'regulated' will have a perfect match with a d-term 'regulated', even if the former is 'up regulated' and the latter is 'down regulated' in context. Also, there is no reward for term matches that preserve word order, or multiple matches within a small window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">PACRR</head><p>In PACRR <ref type="bibr" target="#b11">(Hui et al., 2017)</ref>, a query-document term similarity matrix sim is computed ( <ref type="figure" target="#fig_2">Fig. 2A</ref>). Each cell (i, j) of sim contains the cosine similarity between the embeddings of a q-term q i and a dterm d j . To keep the dimensions l q ×l d of sim fixed across queries and documents of varying lengths, queries are padded to the maximum number of qterms l q , and only the first l d terms per document are retained. 2 Then, convolutions ( <ref type="figure" target="#fig_2">Fig. 2A</ref>) of different kernel sizes n × n (n = 2, . . . , l g ) are applied to sim to capture n-gram query-document similarities. For each size n × n, multiple ker-nels (filters) are used. Max pooling is then applied along the dimension of the filters (max value of all filters), followed by row-wise k-max pooling to capture the strongest k signals between each qterm and all the d-terms. The resulting matrices are concatenated into a single matrix where each row is a document-aware q-term encoding; the IDF of the q-term is also appended, normalized by applying a softmax across the IDFs of all the q-terms. Following <ref type="bibr" target="#b12">Hui et al. (2018)</ref>, we concatenate the rows of the resulting matrix into a single vector, which is passed to an MLP ( <ref type="figure" target="#fig_2">Fig. 2A</ref>, dense layers) that produces a query-document relevance score. <ref type="bibr">3</ref> The primary advantage of PACRR over DRMM is that it models context via the n-gram convolutions, i.e., denser n-gram matches and matches preserving word order are encoded. However, this context-sensitivity is weak, as the convolutions operate over the similarity matrix, not directly on terms or even term embeddings. Also, unlike DRMM, PACRR requires padding and hyperparameters for maximum number of q-terms (l q ) and dterms (l d ), since the convolutional and dense layers operate over fixed-size matrices and vectors. On the other hand, PACRR is end-to-end trainable -though <ref type="bibr" target="#b11">Hui et al. (2017)</ref> use fixed pre-trained embeddings -unlike DRMM where the bucketed histograms are not differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">New Relevance Ranking Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PACRR-DRMM</head><p>In a DRMM-like version of PACRR, instead of using an MLP (dense layers, <ref type="figure" target="#fig_2">Fig. 2A</ref>) to score the concatenation of all the (document-aware) q-term encodings, the MLP independently scores each qterm encoding (the same MLP for all q-terms, <ref type="figure" target="#fig_2">Fig. 2B</ref>); the resulting scores are aggregated via a linear layer. This version, PACRR-DRMM, performs better than PACRR, using the same number of hidden layers in the MLPs. Likely this is due to the fewer parameters of its MLP, which is shared across the q-term representations and operates on shorter input vectors. Indeed, in early experiments PACRR-DRMM was less prone to over-fitting.</p><p>In PACRR-DRMM, the scores of the q-terms (outputs of dense layers, <ref type="figure" target="#fig_2">Fig. 2B</ref>) are not weighted by a gating mechanism, unlike DRMM <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Nevertheless, the IDFs of the q-terms, which are appended to the q-term encodings <ref type="figure" target="#fig_2">(Fig. 2B)</ref>, are a form of term-gating (shortcut passing on information about the terms, here their IDFs, to upper layers) applied before scoring the q-terms. By contrast, in DRMM <ref type="figure" target="#fig_0">(Fig. 1</ref>) term-gating is applied after q-term scoring, and operates on [e(q i ); idf(q i )].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context-sensitive Term Encodings</head><p>In their original incarnations, DRMM and PACRR use pre-trained word embeddings that are insensitive to the context of a particular query or document where a term occurs. This contrasts with the plethora of systems that use context-sensitive word encodings (for each particular occurrence of a word) in virtually all NLP tasks <ref type="bibr" target="#b1">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b26">Plank et al., 2016;</ref><ref type="bibr" target="#b17">Lample et al., 2016)</ref>. In general, this is achieved via RNNs, e.g., LSTMs <ref type="bibr" target="#b6">(Gers et al., 2000)</ref>, or CNNs <ref type="bibr" target="#b2">(Bai et al., 2018)</ref>.</p><p>In the IR literature, context-sensitivity is typically viewed through two lenses: term proximity <ref type="bibr" target="#b3">(Büttcher et al., 2006)</ref> and term dependency <ref type="bibr" target="#b20">(Metzler and Croft, 2005)</ref>. The former assumes that the context around a term match is also relevant, whereas the latter aims to capture when multiple terms (e.g., an n-gram) must be matched together. An advantage of neural network architectures like RNNs and CNNs is that they can capture both.</p><p>In the models below ( § §3.3-3.4), an encoder produces the context-sensitive encoding of each qterm or d-term from the pre-trained embeddings. To compute this we use a standard BILSTM encoding scheme and set the context-sentence encoding as the concatenation of the last layer's hidden states of the forward and backward LSTMs at each position. As is common for CNNs and even recent RNN term encodings <ref type="bibr" target="#b25">(Peters et al., 2018)</ref>, we use the original term embedding e(t i ) as a residual and combine it with the BILSTM encodings.</p><formula xml:id="formula_1">Specifically, if − → h (t i ) and ← − h (t i )</formula><p>are the last layer's hidden states of the left-to-right and right-to-left LSTMs for term t i , respectively, then we set the context-sensitive term encoding as:</p><formula xml:id="formula_2">c(t i ) = [ − → h (t i ) + e(t i ); ← − h (t i ) + e(t i )]<label>(1)</label></formula><p>Since we are adding the original term embedding to each LSTM hidden state, we require the dimensionality of the hidden layers to be equal to that of the original embedding. Other methods were tried, including passing all representations through an MLP, but these had no effect on performance. This is an orthogonal way to incorporate context into the model relative to PACRR. PACRR creates a query-document similarity matrix and computes n-gram convolutions over the matrix. Here we incorporate context directly into the term encodings; hence similarities in this space are already contextsensitive. One way to view this difference is the point at which context enters the model -directly during term encoding (Eq. 1) or after term similarity scores have been computed (PACRR, <ref type="figure" target="#fig_2">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ABEL-DRMM</head><p>Using the context-sensitive q-term and d-term encodings of §3.2 (Eq. 1), our next extension to DRMM is to create document-aware q-term encodings that go beyond bucketed histograms of cosine similarities, the stage in <ref type="figure" target="#fig_0">Fig. 1</ref> indicated by ⊗. We focus on differentiable encodings to facilitate endto-end training from inputs to relevance scores. <ref type="figure">Figure 3</ref> shows the sub-network that computes the document-aware encoding of a q-term q i in the new model, given a document d = d 1 , . . . , d m of m d-terms. We first compute a dot-product 4 attention score a i,j for each d j relative to q i :</p><formula xml:id="formula_3">a i,j = softmax c(q i ) T c(d j ); d 1 , . . . , d m<label>(2)</label></formula><p>where c(t) is the context-sensitive encoding of t (Eq. 1). We then sum the context-sensitive encodings of the d-terms, weighted by their attention scores, to produce an attention-based representation d q i of document d from the viewpoint of q i :</p><formula xml:id="formula_4">d q i = j a i,j c(d j )<label>(3)</label></formula><p>The Hadamard product (element-wise multiplication, ) between the (L2-normalized) document representation d q i and the q-term encoding c(q i ) is then computed and used as the fixed-dimension document-aware encoding φ H (q i ) of q i <ref type="figure">(Fig. 3)</ref>:</p><formula xml:id="formula_5">φ H (q i ) = d q i ||d q i || c(q i ) ||c(q i )||<label>(4)</label></formula><p>The ⊗ nodes and lower parts of the DRMM network of <ref type="figure" target="#fig_0">Fig. 1</ref> are now replaced by (multiple copies of) the sub-network of <ref type="figure">Fig. 3</ref> (one copy per q-term), with the nodes replacing the ⊗ nodes. We call the resulting model Attention-Based ELement-wise DRMM (ABEL-DRMM).</p><p>Intuitively, if the document contains one or more terms d j that are similar to q i , the attention  <ref type="figure">Figure 3</ref>: ABEL-DRMM sub-net. From context-aware q-term and d-term encodings (Eq. 1), it generates fixeddimension document-aware q-term encodings to be used in DRMM <ref type="figure" target="#fig_0">(Fig. 1, replacing ⊗ nodes)</ref>.</p><p>mechanism will have emphasized mostly those terms and, hence, d q i will be similar to c(q i ), otherwise not. This similarity could have been measured by the cosine similarity between d q i and c(q i ), but the cosine similarity assigns the same weight to all the dimensions, i.e., to all the (L2 normalized) element-wise products in φ H (q i ), which cosine similarity just sums. By using the Hadamard product, we pass on to the upper layers of DRMM (the dense layers of <ref type="figure" target="#fig_0">Fig. 1</ref>), which score each q-term with respect to the document, all the (normalized) element-wise products of φ H (q i ), allowing the upper layers to learn which elementwise products (or combinations of them) are important when matching a q-term to the document. Other element-wise functions can also be used to compare d q i to c(q i ), instead of the Hadamard product (Eq. 4). For example, a vector containing the squared terms of the Euclidean distance between d q i and c(q i ) could be used instead of φ H (q i ). This change had no effect on ABEL-DRMM's performance on development data. We also tried using [d q i ; c(q i )] instead of φ H (q i ), but performance on development data deteriorated.</p><p>ABEL-DRMM is agnostic to document length, like DRMM. ABEL-DRMM, however, is trainable end-to-end, unlike the original DRMM. Still, both models do not reward higher density matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">POSIT-DRMM</head><p>Ideally, we want models to reward both the maximum match between a q-term and a document, but also the average match (between several q-terms and the document) to reward documents that have a higher density of matches. The document-aware q-term scoring of ABEL-DRMM does not account for this, as the attention summation hides whether a single or multiple terms were matched with high similarity. We also want models to be end-to-end trainable, like ABEL-DRMM. <ref type="figure">Figure 4</ref> (context-sensitive box) outlines a simple network that produces document-aware q-  <ref type="figure">Figure 4</ref>: POSIT-DRMM with multiple views (+MV). Three two-dimensional document-aware q-term encodings, one from each view, are produced, concatenated, and used in DRMM <ref type="figure" target="#fig_0">(Fig. 1, replacing ⊗ nodes)</ref>. term encodings, replacing the ABEL-DRMM subnetwork of <ref type="figure">Fig. 3</ref> in the DRMM framework. We call the resulting model POoled SImilariTy DRMM (POSIT-DRMM). As in ABEL-DRMM, we compute an attention score a i,j for each d j relative to q i , now using cosine similarity (cf. Eq. 2):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Doc-Aware Query Term Encoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-sensitive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-insensitive</head><formula xml:id="formula_6">a i,j = c(q i ) T c(d j ) ||c(q i )|| ||c(d j )||<label>(5)</label></formula><p>However, we do not use the a i,j scores to compute a weighted average of the encodings of the d-terms (cf. Eq. 3), which is also why there is no softmax in a i,j above (cf. Eq. 2). 5 Instead, we concatenate the attention scores of the m d-terms:</p><p>a i = a i,1 , . . . , a i,j , . . . , a i,m T and we apply two pooling steps on a i to create a 2dimensional document-aware encoding φ P (q i ) of the q-term q i <ref type="figure">(Fig. 4)</ref>. First max-pooling, which returns the single best match of q i in the document. Then average pooling over a k-max-pooled version of a i , which represents the average similarity for the top k matching terms:</p><formula xml:id="formula_7">φ P (q i ) = max(a i ), avg k-max(a i ) T</formula><p>POSIT-DRMM has many fewer parameters than the other models. The input to the upper qterm scoring dense layers of the DRMM framwork <ref type="figure" target="#fig_0">(Fig. 1)</ref> for ABEL-DRMM has the same dimensionality as pre-trained term embeddings, on the order of hundreds. By contrast, the input dimensionality here is 2. Hence, POSIT-DRMM does not require deep dense layers, but uses a single layer (depth 1). More information on hyperparameters is provided in Appendix A (supplementary material).</p><p>POSIT-DRMM is closely related to PACRR (and PACRR-DRMM). Like POSIT-DRMM, PACRR first computes cosine similarities between all q-terms and d-terms <ref type="figure" target="#fig_2">(Fig. 2)</ref>. It then applies n-gram convolutions to the similarity matrix to inject contextawareness, and then pooling to create documentaware q-term representations. Instead, POSIT-DRMM relies on the fact that the term encodings are now already context sensitive (Eq. 1) and thus skips the n-gram convolutions. Again, this is a choice of when context is injected -during term encoding or after computing similarity scores.</p><p>Mohan et al.'s work <ref type="formula" target="#formula_2">(2017)</ref> is related in the sense that for each q-term, document-aware encodings are built over the best matching (Euclidean distance) d-term. But again, term encodings are context-insensitive pre-trained word embeddings and the model is not trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multiple Views of Terms (+MV)</head><p>An extension to ABEL-DRMM and POSIT-DRMM (or any deep model) is to use multiple views of terms. The basic POSIT-DRMM produces a twodimensional document-aware encoding of each qterm ( <ref type="figure">Fig. 4</ref>, context-sensitive box) viewing the terms as their context-sensitive encodings (Eq. 1). Another two-dimensional document-aware q-term encoding can be produced by viewing the terms directly as their pre-trained embeddings without converting them to context-sensitive encodings <ref type="figure">(Fig. 4</ref>, context-insensitive box). A third view uses one-hot vector representations of terms, which allows exact term matches to be modeled, as opposed to near matches in embedding space. Concatenating the outputs of the 3 views, we obtain 6-dimensional document-aware q-term encodings, leading to a model dubbed POSIT-DRMM+MV. An example of this multi-view document-aware query term representation is given in <ref type="figure">Fig. 5</ref> for a querydocument pair from BIOASQ's development data.</p><p>The multi-view extension of ABEL-DRMM (ABEL-DRMM+MV) is very similar, i.e., it uses context-sensitive term encodings, pre-trained term embeddings, and one-hot term encodings in its three views. The resulting three document-aware q-term embeddings can be summed or concatenated, though we found the former more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Alternative Network Structures</head><p>The new models ( § §3.1-3.5) were selected by experimenting on development data. Many other extensions were considered, but not ultimately used as they were not beneficial empirically, including deeper and wider RNNs or CNN encoders <ref type="bibr" target="#b2">(Bai et al., 2018)</ref>; combining document-aware encodings from all models; different attention mechanisms, e.g., multi-head <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>.</p><p>Pointer Networks <ref type="bibr" target="#b36">(Vinyals et al., 2015)</ref> use the attention scores directly to select an input component. POSIT-DRMM does this via max and average pooling, not argmax. We implemented Pointer Networks -argmax over ABEL-DRMM attention to select the best d-term encoding -but empirically this was similar to ABEL-DRMM. Other architectures considered in the literature include the K-NRM model of <ref type="bibr" target="#b40">Xiong et al. (2017)</ref>. This is similar to both ABEL-DRMM and POSIT-DRMM in that it can be viewed as an end-to-end version of DRMM. However, it uses kernels over the query-document interaction matrix to produce features per q-term.</p><p>The work of <ref type="bibr" target="#b24">Pang et al. (2017)</ref> is highly related and investigates many different structures, specifically aimed at incorporating context-sensitivity. However, unlike our work, Pang et al. first extract contexts (n-grams) of documents that match q-terms. Multiple interaction matrices are then constructed for the entire query relative to each of these contexts. These document contexts may match one or more q-terms allowing the model to incorporate term proximity. These interaction matrices can also be constructed using exact string match similar to POSIT-DRMM+MV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experiment with ad-hoc retrieval datasets with hundreds of thousands or millions of documents. As deep learning models are computationally expensive, we first run a traditional IR system 6 using the BM25 score <ref type="bibr" target="#b27">(Robertson and Zaragoza, 2009)</ref> and then re-rank the top N returned documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods Compared</head><p>All systems use an extension proposed by <ref type="bibr" target="#b29">Severyn and Moschitti (2015)</ref>, where the relevance score is combined via a linear model with a set of extra features. We use four extra features: zscore normalized BM25 score; percentage of qterms with exact match in the document (regular and IDF weighted); and percentage of q-term bigrams matched in the document. The latter three features were taken from <ref type="bibr">Mohan et al. (2017)</ref>.</p><p>In addition to the models of § §2.1, 2.2, 3.1-3.5, we used the following baselines: Standard Okapi BM25 (BM25); and BM25 re-ranked with a linear model over the four extra features (BM25+extra). These IR baselines are very strong and most recently proposed deep learning models do not beat them. 7 DRMM and PACRR are also strong baselines and have shown superior performance over other deep learning models on a variety of data <ref type="bibr" target="#b7">(Guo et al., 2016;</ref><ref type="bibr" target="#b11">Hui et al., 2017)</ref>. <ref type="bibr">8</ref> All hyperparameters were tuned on development data and are available in Appendix A. All models were trained using Adam (Kingma and Ba, 2014) with batches containing a randomly sampled negative example per positive example 9 and a pair-wise loss. As the datasets contain only documents marked as relevant, negative examples were sampled from the top N documents (returned by BM25) that had not been marked as relevant.</p><p>We evaluated the models using the TREC ad-hoc retrieval evaluation script 10 focusing on MAP, Pre-cision@20 and nDCG@20 <ref type="bibr" target="#b19">(Manning et al., 2008)</ref>. We trained each model five times with different random seeds and report the mean and standard deviation for each metric on test data; in each run, the model selected had the highest MAP on the development data. We also report results for an oracle, which re-ranks the N documents returned by BM25 placing all human-annotated relevant documents at the top. To test for statistical significance between two systems, we employed twotailed stratified shuffling <ref type="bibr" target="#b30">(Smucker et al., 2007;</ref><ref type="bibr" target="#b5">Dror et al., 2018)</ref> using the model with the highest development MAP over the five runs per method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BioASQ Experiments</head><p>Our first experiment used the dataset of the document ranking task of BIOASQ <ref type="bibr">(Tsatsaronis et al., 2015)</ref>, years 1-5. 11 It contains 2,251 English biomedical questions, each formulated by a biomedical expert, who searched (via PubMed 12 ) for, and annotated relevant documents. Not all relevant documents were necessarily annotated, but the data includes additional expert relevance judg-7 See, for example, <ref type="table" target="#tab_3">Table 2</ref> of <ref type="bibr" target="#b7">Guo et al. (2016)</ref>. 8 For PACRR/PACRR-DRMM, we used/modified the code released by <ref type="bibr" target="#b11">Hui et al. (2017</ref><ref type="bibr" target="#b12">Hui et al. ( , 2018</ref>. We use our own implementation of DRMM, which performs roughly the same as <ref type="bibr" target="#b7">Guo et al. (2016)</ref>, though the results are not directly comparable due to different random partitions of the data. <ref type="bibr">9</ref> We limit positive examples to be in the top N documents. 10 https://trec.nist.gov/trec_eval/ (v9.0) 11 http://bioasq.org/. 12 https://www.ncbi.nlm.nih.gov/pubmed/ ments made during the official evaluation. <ref type="bibr">13</ref> The document collection consists of approx. 28M 'articles' (titles and abstracts only) from the 'MEDLINE/PubMed Baseline 2018' collection. <ref type="bibr">14</ref> We discarded the approx. 10M articles that contained only titles, since very few of these were annotated as relevant. For the remaining 18M articles, a document was the concatenation of each title and abstract. Consult Appendix B for further statistics of the dataset. Word embeddings were pre-trained by applying word2vec <ref type="bibr" target="#b21">(Mikolov et al., 2013</ref>) (see Appendix A for hyper-parameters) to the 28M 'articles' of the MEDLINE/PubMed collection. IDF values were computed over the 18M articles that contained both titles and abstracts.</p><p>The 1,751 queries of years 1-4 were used for training, the first 100 queries of year 5 (batch 1) for development, and the remaining 400 queries of year 5 (batches 2-5) as test set. We set N = 100, since even using only the top 100 documents of BM25, the oracle scores are high. PubMed articles published after 2015 for the training set, and after 2016 for the development and test sets, were removed from the top N (and replaced by lower ranked documents up to N ), as these were not available at the time of the human annotation. <ref type="table">Table 1</ref> reports results on the BIOASQ test set, averaged over five runs as well as the single best run (by development MAP) with statistical significance. The enhanced models of this paper perform better than BM25 (even with extra features), PACRR, and DRMM. There is hardly any difference between PACRR and DRMM, but our combination of the two (PACRR-DRMM) surpasses them both on average, though the difference is statistically significant (p &lt; 0.05) only when comparing to PACRR. Models that use context-sensitive term encodings (ABEL-DRMM, POSIT-DRMM) outperform other models, even PACRR-style models that incorporate context at later stages in the network. This is true both on average and by statistical significance over the best run. The best model on average is POSIT-DRMM+MV, though it is not significantly different than POSIT-DRMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TREC Robust 2004 Experiments</head><p>Our primary experiments were on the BIOASQ dataset as it has one of the largest sets of queries  (with manually constructed relevance judgments) and document collections, making it a particularly realistic dataset. However, in order to ground our models in past work we also ran experiments on <ref type="bibr">TREC ROBUST 2004</ref><ref type="bibr" target="#b37">(Voorhees, 2005</ref>, which is a common benchmark. It contains 250 queries 15 and 528K documents. As this dataset is quite small, we used a 5-fold cross-validation. In each fold, approx. 3 5 of the queries were used for training, 1 5 for development, and 1 5 for testing. We applied word2vec to the 528K documents to obtain pretrained embeddings. IDF values were computed over the same corpus. Here we used N = 1000, as the oracle scores for N = 100 were low. <ref type="table" target="#tab_3">Table 2</ref> shows the TREC ROBUST results, which largely mirror those of BIOASQ. POSIT-DRMM+MV is still the best model, though again not significantly different than POSIT-DRMM. Furthermore, ABEL-DRMM and POSIT-DRMM are clearly better than the deep learning baselines, 16 <ref type="bibr">15</ref> We used the 'title' fields of the queries. <ref type="bibr">16</ref> The results we report for our implementation of DRMM are slightly different than those of <ref type="bibr" target="#b7">Guo et al. (2016)</ref>. There but unlike BIOASQ, there is no statistically significant difference between PACRR-DRMM and the two deep learning baselines. Even though the scores are quite close (particularly MAP) both ABEL-DRMM and POSIT-DRMM are statistically different from PACRR-DRMM, which was not the case for BIOASQ. ABEL-DRMM+MV is significantly different than ABEL-DRMM on the best run for MAP and nDCG@20, unlike BIOASQ where there was no statistically significant difference between the two methods. However, on average over 5 runs, the systems show little difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>An interesting question is how well the deep models do without the extra features. For BIOASQ, the best model's (POSIT-DRMM+MV) MAP score drops from 48.1 to 46.2 on the development set, which is higher than the BM25 baseline (43.7), but on-par with BM25+EXTRA (46.0). We should are a number of reasons for why this might be the case: there is no standard split of the data; non-standard preprocessing of the documents; the original DRMM paper reranks the top documents returned by Query Likelihood and not BM25. note, however, that on this set, the DRMM baseline without the extra features (which include BM25) is actually lower than BM25 (MAP 42.5), though it is obviously adding a useful signal, since DRMM with the extra features performs better (46.5).</p><p>We also tested the contribution of contextsensitive term encodings (Eq. 1). Without them, i.e., using directly the pre-trained embeddings, MAP on BIOASQ development data dropped from 47.6 to 46.3, and from 48.1 to 47.0 for ABEL-DRMM and POSIT-DRMM, respectively. <ref type="figure">Fig. 5</ref> shows the cosine similarities (attention scores, Eq. 5) between q-terms and d-terms, using term encodings of the three views <ref type="figure">(Fig. 4)</ref>, for a query "Does Vitamin D induce autophagy?" and a relevant document from the BIOASQ development data. POSIT-DRMM indeed marks this as relevant. In the similarities of the context-insensitive view (middle left box) we see multiple matches around 'vitamin d' and 'induce autophagy'. The former is an exact match (white squares in lower left box) and the latter a soft match. The contextsensitive view (upper left box) smooths things out and one can see a straight diagonal white line matching 'vitamin d induce autophagy'. The right box of <ref type="figure">Fig. 5</ref> shows the 6 components <ref type="figure">(Fig. 4)</ref> of the document-aware q-term encodings. Although some terms are not matched exactly, the context sensitive max and average pooled components (two left-most columns) are high for all q-terms. Interestingly, 'induce' and 'induces' are not an exact match (leading to black cells for 'induce' in the two right-most columns) and the corresponding context-insensitive component of (third cell from left) is low. However, the two components of the context-sensitive view (two left-most cells of 'induce') are high, esp. the max-pooling component (left-most).Finally, 'vitamin d' has multiple matches leading to a high average k-max pooled value, which indicates that the importance of that phrase in the document. <ref type="figure">Fig. 6</ref> shows the 6 components of the documentaware q-term encodings for another query and the same document, which is now irrelevant. In the max pooling columns of the exact match and context-insensitive view (columns 3, 5), the values look quite similar to those of <ref type="figure">Fig. 5</ref>. However, POSIT-DRMM scores this query-document pair low for two reasons. First, in the average-k-max pooling columns (columns 2, 4, 6) we get lower values than <ref type="figure">Fig. 5</ref>, indicating that there is less support for this pair in terms of density. Second, the context sensitive values (columns 1, 2) are much worse, indicating that even though many exact matches exist, in context, the meaning is not the same.</p><p>We conclude by noting there is still quite a large gap between the current best models and the oracle re-ranking scores. Thus, there is head room for improvements through more data or better models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of DRMM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>PACRR<ref type="bibr" target="#b11">(Hui et al., 2017)</ref> andPACRR-DRMM.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>sen sit ive -m ax co nte xt-sen sit ive -av g-k -m ax co nte xt-ins en sit ive -m ax co nte xt-ins en sit ive -av g-k -m ax ex ac t-m atc h-m ax ex ac t-m atc h-a vg -kLeft: Cosine similarities (POSIT-DRMM attention) of query and document terms, with context-sensitive, context-insensitive, and exact match views of the terms (top to bottom). Document truncated to 50 words. White is stronger. Right: Corresponding POSIT-DRMM+MV 6-dimensional document-aware query term encodings. co nte xt-sen sit ive -m ax co nte xt-sen sit ive -av g-k -m ax co nte xt-ins en sit ive -m ax co nte xt-ins en sit ive -av g-k -m ax ex ac t-m atc h-m ax ex ac t-m atc h-a vg -k-ma POSIT-DRMM+MV 6-dimensional documentaware q-term encodings for 'Does autophagy induce apoptosis defense?' and the same document as Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± .0 25.5 ± .0 55.4 ± .0 BM25+extra 48.7 ± .0 26.6 ± .0 58.1 ± .0 Deep Learning Baselines PACRR 49.1 ± .2 27.1 ± .1 58.5 ± .2 DRMM 49.3 ± .2 27.2 ± .2 58.5 ± .3 Deep Learning with Enhanced Interactions PACRR-DRMM 49.9 ± .1 27.4 ± .1 59.3 ± .1 ; PACRR 3 ; DRMM 4 ; PACRR-DRMM 5 ; ABEL-DRMM 6 ; ABEL-DRMM+MV 7 .</figDesc><table><row><cell cols="4">AVERAGE OVER FIVE RUNS WITH STD. DEV.</cell><cell cols="4">BEST RUN WITH STAT. SIG.</cell></row><row><cell>System</cell><cell>MAP</cell><cell>P@20</cell><cell>nDCG@20</cell><cell>System</cell><cell>MAP</cell><cell>P@20</cell><cell>nDCG@20</cell></row><row><cell></cell><cell cols="2">Traditional IR Baselines</cell><cell></cell><cell></cell><cell cols="2">Traditional IR Baselines</cell><cell></cell></row><row><cell cols="4">BM25 46.1 ABEL-DRMM 50.3 ± .2 27.5 ± .1 59.6 ± .2</cell><cell cols="4">BM25 BM25+extra PACRR DRMM Deep Learning with Enhanced Interactions 46.1 25.5 55.4 48.7 (1) 26.6 (1) 58.1 (1) Deep Learning Baselines 49.1 (1) 27.0 (1-2) 58.6 (1) 49.3 (1-2) 27.1 (1-2) 58.8 (1-2) PACRR-DRMM 50.0 (1-3) 27.3 (1-2) 59.4 (1-3) ABEL-DRMM 50.2 (1-4) 27.5 (1-4) 59.4 (1-3)</cell></row><row><cell>+MV</cell><cell cols="3">50.4 ± .2 27.4 ± .2 59.7 ± .3</cell><cell>+MV</cell><cell>50.5 (1-4)</cell><cell cols="2">27.6 (1-4) 59.8 (1-4)</cell></row><row><cell>POSIT-DRMM</cell><cell cols="3">50.7 ± .2 27.8 ± .1 60.1 ± .2</cell><cell>POSIT-DRMM</cell><cell cols="3">50.7 (1-4,6) 27.9 (1-7) 60.1 (1-4,6)</cell></row><row><cell>+MV</cell><cell cols="3">51.0 ± .1 27.9 ± .1 60.3 ± .2</cell><cell>+MV</cell><cell>51.0 (1-7)</cell><cell cols="2">27.7 (1-4) 60.3 (1-7)</cell></row><row><cell>Oracle</cell><cell cols="3">72.8 ± .0 37.5 ± .0 80.7 ± .0</cell><cell>Oracle</cell><cell>72.8</cell><cell>37.5</cell><cell>80.7</cell></row><row><cell cols="8">Table 1: Performance on BIOASQ test data. Statistically significant (p &lt; 0.05) difference from BM25 1 ;</cell></row><row><cell cols="4">BM25+extra 2 AVERAGE OVER FIVE RUNS WITH STD. DEV.</cell><cell cols="4">BEST RUN WITH STAT. SIG.</cell></row><row><cell>System</cell><cell>MAP</cell><cell>P@20</cell><cell>nDCG@20</cell><cell>System</cell><cell>MAP</cell><cell>P@20</cell><cell>nDCG@20</cell></row><row><cell></cell><cell cols="2">Traditional IR Baselines</cell><cell></cell><cell></cell><cell cols="2">Traditional IR Baselines</cell><cell></cell></row><row><cell>BM25</cell><cell cols="3">23.8 ± .0 35.4 ± .0 42.5 ± .0</cell><cell>BM25</cell><cell>23.8</cell><cell>35.4</cell><cell>42.5</cell></row><row><cell>BM25+extra</cell><cell cols="3">25.0 ± .0 36.7 ± .0 43.2 ± .0</cell><cell>BM25+extra</cell><cell>25.0 (1)</cell><cell>36.7 (1)</cell><cell>43.2 (1)</cell></row><row><cell></cell><cell cols="2">Deep Learning Baselines</cell><cell></cell><cell></cell><cell cols="2">Deep Learning Baselines</cell><cell></cell></row><row><cell>PACRR</cell><cell cols="3">25.8 ± .2 37.2 ± .4 44.3 ± .4</cell><cell>PACRR</cell><cell cols="2">25.8 (1-2) 37.4 (1-2)</cell><cell>44.5 (1-2)</cell></row><row><cell>DRMM</cell><cell cols="3">25.6 ± .6 37.0 ± .8 44.4 ± .6</cell><cell>DRMM</cell><cell cols="2">25.9 (1-2) 37.2 (1-2)</cell><cell>44.4 (1-2)</cell></row><row><cell cols="4">Deep Learning with Enhanced Interactions</cell><cell cols="4">Deep Learning with Enhanced Interactions</cell></row><row><cell cols="4">PACRR-DRMM 25.9 ± .4 37.3 ± .7 44.4 ± .7</cell><cell cols="4">PACRR-DRMM 25.9 (1-2) 37.6 (1-2,4) 44.5 (1-2)</cell></row><row><cell>ABEL-DRMM</cell><cell cols="3">26.3 ± .4 38.0 ± .6 45.6 ± .4</cell><cell>ABEL-DRMM</cell><cell cols="2">26.1 (1-5) 38.0 (1-5)</cell><cell>45.4 (1-5)</cell></row><row><cell>+MV</cell><cell cols="3">26.5 ± .4 38.0 ± .5 45.5 ± .4</cell><cell>+MV</cell><cell cols="2">26.4 (1-6) 38.2 (1-5)</cell><cell>45.8 (1-6)</cell></row><row><cell>POSIT-DRMM</cell><cell cols="3">27.0 ± .4 38.3 ± .6 45.7 ± .5</cell><cell>POSIT-DRMM</cell><cell cols="2">27.1 (1-7) 38.8 (1-7)</cell><cell>46.2 (1-7)</cell></row><row><cell>+MV</cell><cell cols="3">27.2 ± .3 38.6 ± .6 46.1 ± .4</cell><cell>+MV</cell><cell cols="2">27.1 (1-7) 38.9 (1-7)</cell><cell>46.4 (1-7)</cell></row><row><cell>Oracle</cell><cell cols="3">68.0 ± .0 82.1 ± .0 93.1 ± .0</cell><cell>Oracle</cell><cell>68.0</cell><cell>82.1</cell><cell>93.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance on TREC ROBUST test data. Statistically significant (p &lt; 0.05) difference from BM25 1 ; BM25+extra 2 ; PACRR 3 ; DRMM 4 ; PACRR-DRMM 5 ; ABEL-DRMM 6 ; ABEL-DRMM+MV 7 .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code and data of our experiments, including word embeddings, are available at https://github.com/ nlpaueb/deep-relevance-ranking.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We usePACRR-firstk, which Hui et al. (2017)  recommend when documents fit in memory, as in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3"><ref type="bibr" target="#b11">Hui et al. (2017)</ref> used an additional LSTM, which was later replaced by the final concatanation<ref type="bibr" target="#b12">(Hui et al., 2018)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Dot-products have a larger range than other similarity functions, encouraging low entropy attention distributions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The ai,js still need to be normalized for input to the upper layers, but they do not need to be positive summing to 1. This is why we use cosine similarity in Eq. 5 instead of dot-products combined with softmax of Eq. 2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We used Galago (http://www.lemurproject. org/galago.php, v.3.10). We removed stop words and applied Krovetz's stemmer<ref type="bibr" target="#b16">(Krovetz, 1993)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">Our results are, thus, not comparable to those of participating systems, since experts did not consider our outputs. 14 See https://www.nlm.nih.gov/databases/ download/pubmed_medline.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">SunilMohan, Nicolas Fiorini, Sun Kim, and Zhiyong  Lu. 2017. Deep learning for biomedical information retrieval: Learning textual relevance from click logs. In BioNLP 2017, pages 222-231.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the reviewers for their constructive feedback that greatly improved this work. Oscar Täckström gave thorough input on an early draft of this work. Finally, AUEB's NLP group provided many suggestions over the course of the work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Search system requirements of patent analysts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><surname>Vanderbauwhede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="775" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Term proximity scoring for ad-hoc retrieval on very large text collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="621" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activity based metadata for semantic desktop search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Paul Alexandru Chirita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefania</forename><surname>Gavriloaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Ghita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raluca</forename><surname>Nejdl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<meeting><address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="439" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The hitchhiker&apos;s guide to testing statistical significance in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rotem</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gili</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Australia</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><forename type="middle">A</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management<address><addrLine>Indianapolis, IN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The TREC ad hoc experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Donna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Challenges in enterprise search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hawking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Australasian Database Conference</title>
		<meeting>the 15th Australasian Database Conference<address><addrLine>Dunedin, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PACRR: A position-aware neural IR model for relevance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-PACRR: A context-aware neural IR model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 11th ACM International Conference on Web Search and Data Mining<address><addrLine>Marina Del Rey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Viewing morphology as an inference process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep architecture for matching short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1367" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A markov random field model for term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual international ACM SI-GIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SI-GIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabab</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeprank: A new deep architecture for relevance ranking in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
	<note>Jingfang Xu, and Xueqi Cheng</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Document retrieval systems. chapter Relevance Weighting of Search Terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">Sparck</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Taylor Graham Publishing</publisher>
			<biblScope unit="page" from="143" to="160" />
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Document retrieval systems. chapter A Statistical Interpretation of Term Specificity and Its Application in Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen Sparck</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Taylor Graham Publishing</publisher>
			<biblScope unit="page" from="132" to="142" />
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to rank answers on large online QA collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting for the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT)</title>
		<meeting>the 46th Annual Meeting for the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT)<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">LSTM-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Eric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. 2015. An overview of the BioASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artiéres, Axel-Cyrille Ngonga Ngomo</title>
		<meeting><address><addrLine>Norman Heino</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The TREC robust retrieval track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Citeseerx: AI in a digital library search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><forename type="middle">Mark</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Hsuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suppawong</forename><surname>Tuarob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
