<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Event Specific and Chunk Span features to Extract COVID Events from tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Kaushal</surname></persName>
							<email>ayushk4@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kharagpur</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Vaidhya</surname></persName>
							<email>iamtejasvaidhya@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kharagpur</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Event Specific and Chunk Span features to Extract COVID Events from tweets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Twitter has acted as an important source of information during disasters and pandemic, especially during the times of COVID-19. In this paper, we describe our system entry for WNUT 2020 Shared Task-3. The task was aimed at automating the extraction of a variety of COVID-19 related events from Twitter, such as individuals who recently contracted the virus, someone with symptoms who were denied testing and believed remedies against the infection. The system consists of separate multi-task models for slot-filling subtasks and sentence-classification subtasks while leveraging the useful sentence-level information for the corresponding event. The system uses COVID-Twitter-Bert with attention-weighted pooling of candidate slot-chunk features to capture the useful information chunks. The system ranks 1st at the leader-board with F1 of 0.6598, without using any ensembles or additional datasets. The code and trained models are available at this https url 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The World Health Organization declared COVID-19, a global pandemic on March 11, 2020. As of 2020/09/21, there are over 30 million cases 2 and 900,000 deaths due to the infection. With the imposed lockdown, work from home and physical distancing, social media like twitter saw an increased usage. A large part of the use was posting and consuming information on the novel infection. These information include potential reasons for contraction of the disease, such as via exposure to a family member who tested positive, or someone who is showing COVID symptoms but was denied testing. Accompanying to the pandemic was an infodemic of misinformation about COVID-19, including fake remedies, treatments and prevention-suggestions in social media <ref type="bibr" target="#b3">(Alam et al., 2020)</ref>. <ref type="bibr" target="#b26">Zong et al. (2020)</ref> show the possibility to automatically extract structured knowledge on COVID-19 events from Twitter and released a dataset of COVID related tweets across 5 event types. We used this dataset in our experiments for the sharedtask. These tweets are annotated for whether they belong to an event (we refer to this as the event-prediction task in this paper) and their eventspecific questions (factual or opinion). We identify these event-specific questions into two types of subtasks, slot-filling and sentence classification.</p><p>Our system consists of separate multi-task models for slot-filling subtasks and sentenceclassification subtasks. Our contribution comprises improvement upon the baseline (mentioned in section 2) in three ways:</p><p>• We incorporate the event-prediction task as auxiliary subtask and fuse its features for all the event-specific subtasks.</p><p>• We perform an attention-weighted pooling over the candidate chunk span enabling the model to attend to subtask specific cues.</p><p>• We use the domain-specific Bert of Covid-Twitter Bert <ref type="bibr" target="#b17">(Müller et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Sentence classification tasks (such as opinion or sentiment mining) as well as slot-filling tasks have greatly progressed with deep learning advancements such as LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref>, Tree-LSTM <ref type="bibr" target="#b24">(Tai et al., 2015)</ref> and transfer learning over pre-trained models <ref type="bibr" target="#b20">(Peters et al., 2018;</ref><ref type="bibr" target="#b11">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b9">Devlin et al., 2019)</ref>. Among these, CT-Bert outperforms others on COVID related twitter tasks <ref type="bibr" target="#b17">(Müller et al., 2020)</ref>. Taking inspiration from the same, we use arXiv:2012.10052v1 [cs.CL] 18 Dec 2020</p><p>CT-Bert as part of our architecture. A variety of slot-filling approaches have been built on top of these deep learning advancements <ref type="bibr" target="#b14">(Kurata et al., 2016;</ref><ref type="bibr" target="#b21">Qin et al., 2019)</ref>. The proposed baseline for our task <ref type="bibr" target="#b26">(Zong et al., 2020)</ref>   <ref type="bibr">(2019)</ref>. Due to the excellent performance offered by Bert <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and Baldini Soares et al. <ref type="formula" target="#formula_2">(2019)</ref>, we build upon this baseline approach. Extraction of structured knowledge from tweets pertaining of events <ref type="bibr" target="#b7">(Benson et al., 2011)</ref> has been studied for disaster and crises management <ref type="bibr" target="#b0">(Abhik and Toshniwal, 2013;</ref><ref type="bibr" target="#b23">Rudra et al., 2018)</ref> and in pandemic scenarios <ref type="bibr" target="#b1">(Al-Garadi et al., 2016)</ref>. Extracting such entities can be useful for epidemiologists, deciding policies and preventing spread <ref type="bibr" target="#b1">(Al-Garadi et al., 2016;</ref><ref type="bibr" target="#b26">Zong et al., 2020)</ref>.</p><p>Due to the fast-spreading nature of the infection, it is also difficult to manually trace the spread of the pandemic. However, with twitter event-specific entity extraction and Geo-location, one could potentially build a real-time pandemic surveillance system <ref type="bibr" target="#b15">(Lwowski and Najafirad, 2020;</ref><ref type="bibr" target="#b2">Al-Garadi et al., 2020)</ref>. <ref type="bibr" target="#b4">Bal et al. (2020)</ref> show that healthissues related misinformation is prevalent in social media, while <ref type="bibr" target="#b3">Alam et al. (2020)</ref> talks about covidspecific misinformation. Such systems for extracting structured knowledge over the tweets talking about potential cures for COVID will help study how users perceive the COVID misinformation.</p><p>In §3, we describe the dataset and the problem statement. Then in §4, we discuss the details of our two multi-task models followed by experiments, results and conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset and Problem statement</head><p>Now, we will briefly go over the dataset. The reader may refer (Zong et al., 2020) for full details. Each of the 7500 tweets in the dataset belongs to one of the 5 event types: tested-positive, tested-negative, can-not-test, death, and cure. The first four events aimed at extracting structured reports of coronavirus related events, such as self-reported cases or news stories about public figures who were exposed to the virus. Each tweet was first annotated for whether it belongs to its respective event (e.g. Is the tweet belonging to the tested-positive event talking about someone who tested positive?). Throughout this paper, we refer to this as the Event-Prediction task. The tweets that correspond to its event were then annotated for event-specific questions or sub-  tasks about factual information and user's opinions. All annotations are done by multiple Amazon Mechanical Turks with inter-annotation agreement. The event-specific questions or subtasks (e.g. name, age, gender of the person tested positive) varies depending on the event. These subtasks are of two categories: slot-filling (e.g., Who tested positive/negative?, Where are they located?, Who is in close contact with person contracting the disease?) and sentence classification (e.g. Is author related to infected person?, Does the author experience any symptoms?, Does the author believe a cure method is effective?). The dataset released tweet IDs and their annotations. We obtain our text corresponding to tweets using the official Twitter API 3 . <ref type="table">Table 1</ref> shows the statistics for the dataset we scrapped in early July. 4 <ref type="figure" target="#fig_0">Figure 1</ref> shows an annotated example from the dataset. We identify the event-specific subtasks into two categories shown in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We now formally describe the two types of eventspecific subtasks:</p><p>Slot-filling subtasks: Assume n slot-filling subtasks {S 1 , S 2 ...S n }. We set up each slot-filling subtask S i as a supervised binary classification problem. Given the tweet t and the candidate slot s, the model f (t, s) → {0, 1} predicts whether s answers its designated question. We extract a list of  </p><formula xml:id="formula_0">1 , C 2 ...C m , }.</formula><p>Given a sentence classification subtask C i aims to learn a model g(t) → {l 1 , l 2 ...l k }, where t is a tweet and l j is a label. Here the number of labels can vary depending on the subtask, for example, gender is labelled with {Male, Female, Others/Not Specified}, Relation with {Yes, No}, Opinion with {effective, no cure, not effective, no opinion} and so on. All these subtasks are 'supervised' classification problems.</p><p>The dataset is also annotated with whether a tweet corresponds to its respective event or not. We treat this as an additional Event-Prediction task. This is a binary classification task that aims to learn a model h(t) → 0, 1 where t is a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>In the following subsections §4.1 and §4.2, we describe our multi-task model for slot-filling and sentence-classification respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Slot-filling</head><p>We improve upon the baseline <ref type="bibr" target="#b26">(Zong et al., 2020)</ref> by using domain-specific Bert, using attentionweighted pooling over the candidate chunk feature sequence, incorporating auxiliary Event-Prediction task and utilizing its logits for all the slot-filling subtasks. Before describing the approach, we first describe the Bert baseline. Our slot-filling model can be seen in figure 2.</p><p>The baseline consists of Bert based classifier. It takes a tweet t as input and encloses the candidate slot s, within the tweet, inside special entity start &lt; E &gt; and end &lt; /E &gt; markers. The Bert hidden representation of token &lt; E &gt; is then processed through a fully connected layer with softmax activation to make the binary prediction for a task <ref type="bibr" target="#b5">(Baldini Soares et al., 2019)</ref>. Since many slot-filling Tweet CT-Bert <ref type="bibr">[CLS]</ref> ... tasks within an event are semantically related to each other, they jointly trained the final softmax layers of all the subtasks S i in an event by sharing their Bert model parameters. COVID Twitter Bert (CT-Bert) is a Bert-Large model pretrained on Twitter Corpus on COVID-19 topics, leading to marginal improvements from Bert on tasks based on Twitter datasets <ref type="bibr" target="#b17">(Müller et al., 2020)</ref>. This motivates us to use CT-Bert instead of Bert from the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP</head><p>The baseline, uses the Bert hidden representation of token &lt; E &gt; for classification. Here, however, we use attention-weighted pool of the CT-Bert hidden representation of tokens between &lt; E &gt; and &lt; /E &gt; (both inclusive). Formally, let {x 0 , ...x p , ...x q , ...x n } be the output vectors from the hidden representation of CT-Bert where p and q are indices of &lt; E &gt; and &lt; /E &gt; respectively, then for any of the slot-filling subtask S j , we get its pooled vector as follows:</p><formula xml:id="formula_1">x S j = q i=p α S j i x i (1) α S j i = Sof tmax p to q (x T i a S j ) where x i</formula><p>T denotes the transpose of x i , a S j is a trainable vector. The motivation for attention weighted pooling is that depending on the task, model can attend to different portions of the candidate slot chunk. Next we obtain the binary classification score vector:</p><formula xml:id="formula_2">h S j = W S j x S j + b S j<label>(2)</label></formula><p>Here W S j and b S j are trainable parameters. We treat the Event-Prediction task as an auxiliary task and then fuse its logits to each of the other slot-filling subtasks. The motivation is that a taskspecific entity shall be present in a tweet only if the tweet belongs to its respective event.</p><p>To predict the label for Event-Prediction task, we take the CT-Bert features of [CLS] token and pass it through a MultiLayer Perceptron (MLP) to get logits h ces .</p><p>We fuse h ces prediction over each subtasks S j by adding it to h S j (from (2)) to get the logits h</p><formula xml:id="formula_3">S j f : h S j f = h S j + M LP S j (h ces )<label>(3)</label></formula><p>In practice, we share the parameters of the M LP S j across all the slot-filling subtasks S j . Given a tweet t and slot s, our loss for slot-filling model over n slot-filling subtasks {S 1 , S 2 ...S n } and Event-Prediction task looks like:</p><p>Loss(t, s, y ces , (y 1 , y 2 ...y n )) = λ 1 CE Loss (h ces , y ces ) + n k=1 CE Loss (h S k f , y k ) (4) where CE loss is softmax cross entropy loss, y ces is ground truth label for Event-Prediction task and (y 1 , y 2 ...y n ) are the labels for the candidate slot s of tweet t for the subtasks {S 1 , S 2 ...S n }. We keep λ 1 = 1.</p><p>Our preprocessing for this is same as baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence classification</head><p>Our Sentence classification model is shown in figure 3. We use a Bert based sentence classifier and improve it by using CT-Bert, incorporating the auxiliary Event-Prediction task and attention-weighted pooling over the entire sequence. This model uses CT-Bert instead of Bert and the auxiliary Event-Prediction task for same reason as the slot-filling model. An attention-weighted pooling is done over the feature sequences from CT-Bert to extract the most relevant information. Formally, let {x 0 , x 1 , ......x n } be the output vectors from CT-Bert (here 0 and n are indices of [CLS] and [SEP ] respectively). Then for any of the sentence classification subtask C j , we get its pooled vector x C j as follows:</p><formula xml:id="formula_4">x C j = n i=0 β C j i x i (5) β C j i = Sof tmax i (x T i a C j + c C j )</formula><p>where a C j , c C j are trainable vector and scalar respectively.</p><p>For the Event-Prediction task, we take the CT-Bert vector representation of [CLS] token and pass it through a MLP. Assume the MLP's final and hidden states to be v ces and h ces .</p><p>Next, we incorporate information from Event-Prediction task into sentence classification subtask C j . Since the sentence classification subtasks aren't binary classification, so, unlike the slot-filling model, we cannot merely add the Event-Prediction logits to all tasks. Additionally, we desire sentence-level event specific features for each of the sentence level predictions. Hence, we concatenate the hidden state features from the MLP of Event-Prediction task h ces to pooled vector x C j from 5 to get the logits h C j f for each subtask C j , as follows:</p><formula xml:id="formula_5">h C j f = [ x C j ; h ces ] T W C j + b C j (6)</formula><p>Here T denotes transpose, [; ] denotes vector concatenation. W C j and b C j are trainable.</p><p>Given a tweet t, our loss for sentence classification model over m sentence classification subtasks {C 1 , C 2 ...C m } and Event-Prediction task is:</p><formula xml:id="formula_6">Loss(t, y ces , (y 1 , y 2 ...y m )) = λ 2 CE Loss (v ces , y ces ) + m k=1 CE Loss (h C k f , y k ) (7)</formula><p>where CE Loss is softmax cross entropy loss, y ces is ground truth label for Event-Prediction task and (y 1 , y 2 ...y m ) are the labels for tweet t for the subtasks {C 1 , C 2 ...C m }. We keep λ 2 = 1.</p><p>Preprocessing for sentence classification is done using ekphrasis library <ref type="bibr" target="#b6">(Baziotis et al., 2017)</ref>. We remove Emoji, URL, Email, punctuation and normalize text by word segmenting, lower-casing and word decontraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>All the experiments were performed using PyTorch <ref type="bibr" target="#b18">(Paszke et al., 2019)</ref> and Hugging Face's transformers <ref type="bibr" target="#b25">(Wolf et al., 2019)</ref>. We use git and wandb <ref type="bibr" target="#b8">(Biewald, 2020)</ref> for experiment tracking. Optimization is done using Adam (Kingma and Ba, 2014) with a learning rate of 2e-5. Slot-filling models are trained for 8 epochs and sentence classification model for 10 epochs. Average training time per epoch on Tesla P100 is ≈ 4 minutes for slot-filling, and ≈ 30 second for sentence classification.</p><p>We use a 70-30 split for train-valid set. The valid set is used to obtain the best threshold for each of the slot classification tasks over the grid {0.1, 0.2, ..., 0.9}. We exclude labels with "No consensus" from our data. <ref type="bibr">5</ref> All the MLP have 1 hidden layer and 0.1 dropout. M LP S j has 4 hidden size, LeakyReLU activation <ref type="bibr" target="#b16">(Maas et al., 2013)</ref> with 0.1 negative slope, rest of the MLP have 50 hidden size and Tanh activation. <ref type="bibr">5</ref> As per the submission guidelines, some subtasks like opinion had their label classes merged. We incorporate these changes in our model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Our performance on the held-out test set is shown in <ref type="table" target="#tab_5">Table 3</ref>. Our system ranks 1st position in the W-NUT 2020 Shared <ref type="bibr">Task-3 (Zong et al., 2020)</ref>. We also independently rank 1st for 3 of the 5 events: 'Can Not Test', 'Death', and 'Cure'. Now we discuss our various experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slot-filling:</head><p>We experimented with a variety of architectures for slot-filling model. Our (SF) is our Slot-Filling Model from §4.1. Our (SF) w/o pool is our slot-filling model that uses the CT-Bert hidden representation of token &lt; E &gt; to classify instead of doing an attention-weighted pooling. Our (SF) w/o CES is our slot-filling model without Event-Prediction task. CT-Bert and Bertlarge are baseline models using CT-Bert and Bertlarge instead of Bert-base. <ref type="table" target="#tab_7">Table 4</ref> shows the performance of these models. There is a considerable performance difference by using CT-Bert instead of Bert, demonstrate the benefits of domain specific pre-training. Our (SF) w/o pool and Our (SF) w/o CES outperform CT-Bert demonstrating the importance of Event-Prediction task and attention-weighted pooling over slot-chunk respectively. Our (SF) using CT-Bert with Event-Prediction and attentionweighted pooling performs the best among these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence level tasks:</head><p>We experimented with various architectures for sentence level tasks. Our (SC) is our Sentence Classification architecture from §4.2. Our (SC) w/o CES is our Sentence Classification without Event-Prediction task. Bert multitask model predicts using the [CLS] representation from Bert <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. We also build an LSTM model <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> with GloVe embedding <ref type="bibr" target="#b19">(Pennington et al., 2014)</ref>, and twitter-tokenization using Word-   <ref type="table">Table 5</ref>: Results sentence classification models on our 70-30 split. We report results on the valid set across all sentence classification subtasks across the 5 events.</p><p>Tokenizers package <ref type="bibr" target="#b12">(Kaushal et al., 2020)</ref>. <ref type="table">Table 5</ref> shows the performance of these architectures. Our (SC) outperforms others on macro F1 and micro F1, followed by Our (SC) w/o CES. The performance difference between these two, shows the benefits of including the Event-Prediction task. While the performance difference between CT-Bert multitask and Our (SC) w/o CES shows the gains from attention weighted pooling. CT-Bert also outperforms Bert multitask, showing its usefulness in our proposed system over using Bert. Lastly, Bert multitask, and all the models using Bert/CT-Bert outperform LSTM by a very large margin demonstrating the superiority of these pretrained language models.</p><p>Separate Sentence classification and slot filling models: Consider Bert separate, a simple system treating the two categories of tasks separately. It has the Bert baseline as its slot filling model and a simple Bert sentence classifier using features from [CLS] for sentence prediction. Bert separate does not have the event-prediction auxilliary task or any attention weighted pooling. <ref type="table">Table 6</ref> shows the performance of Bert separate against the baseline. Bert separate outperforms the Bert baseline by a considerable margin, thus showing the importance of treating the two subtasks differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Micro F1 Macro F1 Bert Separate .631 .545 Bert Baseline .608 .512 <ref type="table">Table 6</ref>: Results comparing the systems treating the sentence classification and slot-filling subtasks separately vs those treating it similarly. We report results on the valid set across all the subtasks of both categories across the 5 events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we presented our system that bagged 1st position in the WNUT-2020 Shared Task-3 on Extracting COVID Entities from Twitter. We divided the event-specific subtasks into slot-filling and sentence classification subtasks, building separate architectures for the two. For both architectures, we used COVID-Twitter Bert, weightedattention pooling over chunk-spans/sentence and fused logits and features from auxiliary Event-Prediction task. Our ablation studies demonstrated the usefulness of each component in our system. There is a lot of scope of improvement for subtasks with few positive labels. Pretraining on relevant data (such as COVID-misinformation datasets for event cure) is a promising direction.</p><p>Another direction would be to reduce the training and inference time of slot-filling model by not enclosing the candidate chunk within special start &lt; E &gt; and special end &lt; /E &gt; tokens. We can instead use the attention-weighted pooling over candidate slot chunks. This will reduce the number of Bert forward passes from O(k) to O(1), where k is the number of candidate chunks in a tweet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example tweet from tested negative event.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Slot-Filling Model, described in Section §4.1. Here n is the number of slot-filling subtasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Sentence Classification model, described in section. §4.2. Here m is the number of Sentence Classification subtasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>modifies Bert model for slot-filling problem inspired by Baldini Soares et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Event Sentence Classification Slot-Filling task Tested positive gender, relation who,age,recent-visit,when,where,employer,c.-contact Tested negative gender, relation who,age,when,where,duration,close-contact Can Not Test relation, symptoms who,when,where Death relation, symptoms who,age,when,where Cure opinion what is the cure, who is promoting cure</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The proposed event-specific subtasks split into two subtask types: slot-filling and sentence classification candidate slot of all noun chunks and name entities in each of the tweets by using a Twitter tagging tool<ref type="bibr" target="#b22">(Ritter et al., 2011)</ref> same as the baseline.Sentence classification subtasks: Assume m sentence classification subtasks {C</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Micro averaged scores on the held out test set for our final submission.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results of slot-filling models on our 70-30 split. We report results on the valid set across all slot filling subtasks across the 5 events.</figDesc><table><row><cell>Model</cell><cell cols="2">Micro F1 Macro F1</cell></row><row><cell>Our (SC)</cell><cell>.788</cell><cell>.767</cell></row><row><cell cols="2">Our (SC) w/o CES .777</cell><cell>.731</cell></row><row><cell>CT-Bert multitask</cell><cell>.760</cell><cell>.717</cell></row><row><cell>Bert multitask</cell><cell>.715</cell><cell>.612</cell></row><row><cell>LSTM multitask</cell><cell>.614</cell><cell>.543</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Ayushk4/extract covid entity 2 https://coronavirus.jhu.edu/map.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://developer.twitter.com/ 4 We get about 350 fewer tweets than the corpus. Some tweets are not obtainable over time as the accounts/tweets get deleted, renamed, banned, or change-visibility etc.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are very grateful for the invaluable suggestions given by Nikhil Shah, Dibya Prakash Das and Sayan Sinha. We also thank the organizers of the Shared Task-3 at WNUT, EMNLP-2020.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sub-event detection during natural hazards using features of social media data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhekar</forename><surname>Abhik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durga</forename><surname>Toshniwal</surname></persName>
		</author>
		<idno type="DOI">10.1145/2487788.2488046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on World Wide Web</title>
		<meeting>the 22nd International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="783" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using online social networks to track a pandemic: A systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Al-Garadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasturi</forename><surname>Varathan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2016.05.005</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Ghulam Mujtaba, and Abdelkodose Abdulla</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Text classification approach for the automatic detection of twitter posts containing self</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed Ali Al-Garadi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Chi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahithi</forename><surname>Lakamana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeed</forename><surname>Sarker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>reported covid-19 symptoms</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fighting the covid-19 infodemic in social media: A holistic perspective and a call to arms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysing the extent of misinformation in cancer related tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Bal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swastika</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritam</forename><surname>Dutt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="924" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DataStories at SemEval-2017 task 4: Deep LSTM with attention for message-level and topic-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Pelekis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Doulkeridis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2126</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="747" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Event discovery in social media feeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases. Software available from wandb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordtokenizers.jl: Basic tools for tokenizing natural language in julia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Kaushal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyndon</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Innes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.01956</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page">1956</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging sentence-level information with encoder LSTM for semantic slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1223</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2077" to="2083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Covid-19 surveillance through twitter using selfsupervised learning and few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Lwowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Najafirad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Covid-twitter-bert: A natural language processing model to analyse covid-19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Per</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kummervold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>content on twitter</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stack-propagation framework with token-level intent detection for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1214</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identifying sub-events and summarizing disaster-related information from microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koustav</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210030</idno>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Extracting covid-19 events from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Shi Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
