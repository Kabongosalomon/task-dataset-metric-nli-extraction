<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLOBAL-TO-LOCAL MEMORY POINTER NETWORKS FOR TASK-ORIENTED DIALOGUE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<email>rsocher@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<email>cxiong@salesforce.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salesforce</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GLOBAL-TO-LOCAL MEMORY POINTER NETWORKS FOR TASK-ORIENTED DIALOGUE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-theart models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Task-oriented dialogue systems aim to achieve specific user goals such as restaurant reservation or navigation inquiry within a limited dialogue turns via natural language. Traditional pipeline solutions are composed of natural language understanding, dialogue management and natural language generation <ref type="bibr" target="#b32">(Young et al., 2013;</ref><ref type="bibr" target="#b28">Wen et al., 2017)</ref>, where each module is designed separately and expensively. In order to reduce human effort and scale up between domains, end-to-end dialogue systems, which input plain text and directly output system responses, have shown promising results based on recurrent neural networks <ref type="bibr" target="#b10">(Zhao et al., 2017;</ref><ref type="bibr" target="#b14">Lei et al., 2018)</ref> and memory networks <ref type="bibr" target="#b24">(Sukhbaatar et al., 2015)</ref>. These approaches have the advantages that the dialogue states are latent without hand-crafted labels and eliminate the needs to model the dependencies between modules and interpret knowledge bases (KB) manually.</p><p>However, despite the improvement by modeling KB with memory network <ref type="bibr" target="#b0">(Bordes &amp; Weston, 2017;</ref>, end-to-end systems usually suffer from effectively incorporating external KB into the system response generation. The main reason is that a large, dynamic KB is equal to a noisy input and hard to encode and decode, which makes the generation unstable. Different from chit-chat scenario, this problem is especially harmful in task-oriented one, since the information in KB is usually the expected entities in the response. For example, in <ref type="table">Table 1</ref> the driver will expect to get the correct address to the gas station other than a random place such as a hospital. Therefore, pointer networks <ref type="bibr" target="#b26">(Vinyals et al., 2015)</ref> or copy mechanism <ref type="bibr" target="#b8">(Gu et al., 2016)</ref> is crucial to successfully generate system responses because directly copying essential words from the input source to the output not only reduces the generation difficulty, but it is also more like a human behavior. For example, in <ref type="table">Table 1</ref>, when human want to reply others the Valero's address, they will need to "copy" the information from the table to their response as well. Therefore, in the paper, we propose the global-to-local memory pointer (GLMP) networks, which is composed of a global memory encoder, a local memory decoder, and a shared external knowledge. Unlike existing approaches with copy ability <ref type="bibr" target="#b9">(Gulcehre et al., 2016;</ref><ref type="bibr" target="#b8">Gu et al., 2016;</ref>  <ref type="table">Table 1</ref>: An in-car assistant example on the navigation domain. The left part is the KB information and the right part is the conversation between a driver and our system.  , which the only information passed to decoder is the encoder hidden states, our model shares the external knowledge and leverages the encoder and the external knowledge to learn a global memory pointer and global contextual representation. Global memory pointer modifies the external knowledge by softly filtering words that are not necessary for copying. Afterward, instead of generating system responses directly, the local memory decoder first uses a sketch RNN to obtain sketch responses without slot values but sketch tags, which can be considered as learning a latent dialogue management to generate dialogue action template. Then the decoder generates local memory pointers to copy words from external knowledge and instantiate sketch tags.</p><p>We empirically show that GLMP can achieve superior performance using the combination of global and local memory pointers. In simulated out-of-vocabulary (OOV) tasks in the bAbI dialogue dataset <ref type="bibr" target="#b0">(Bordes &amp; Weston, 2017)</ref>, GLMP achieves 92.0% per-response accuracy and surpasses existing end-to-end approaches by 7.5% in full dialogue. In the human-human dialogue dataset , GLMP is able to surpass the previous state of the art on both automatic and human evaluation, which further confirms the effectiveness of our double pointers usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GLMP MODEL</head><p>Our model 1 is composed of three parts: global memory encoder, external knowledge, and local memory decoder, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a). The dialogue history X = (x 1 , . . . , x n ) and the KB information B = (b 1 , . . . , b l ) are the input, and the system response Y = (y 1 , . . . , y m ) is the expected output, where n, l, m are the corresponding lengths. First, the global memory encoder uses a context RNN to encode dialogue history and writes its hidden states into the external knowledge. Then the last hidden state is used to read the external knowledge and generate the global memory pointer at the same time. On the other hand, during the decoding stage, the local memory decoder first generates sketch responses by a sketch RNN. Then the global memory pointer and the sketch RNN hidden state are passed to the external knowledge as a filter and a query. The local memory pointer returns from the external knowledge can copy text from the external knowledge to replace the sketch tags and obtain the final system response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">EXTERNAL KNOWLEDGE</head><p>Our external knowledge contains the global contextual representation that is shared with the encoder and the decoder. To incorporate external knowledge into a learning framework, end-to-end memory networks (MN) are used to store word-level information for both structural KB (KB memory) and temporal-dependent dialogue history (dialogue memory), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). In addition, the MN is well-known for its multiple hop reasoning ability <ref type="bibr" target="#b24">(Sukhbaatar et al., 2015)</ref>, which is appealing to strengthen copy mechanism.</p><p>Global contextual representation. In the KB memory module, each element b i ∈ B is represented in the triplet format as (Subject, Relation, Object) structure, which is a common format used to represent KB nodes <ref type="bibr" target="#b18">(Miller et al., 2016;</ref>. For example, the KB in the <ref type="table">Table 1</ref> will be denoted as {(Tom's house, distance, 3 miles), ..., (Starbucks, address, 792 Bedoin St)}. On the other hand, the dialogue context X is stored in the dialogue memory module, where the speaker and temporal encoding are included as in <ref type="bibr" target="#b0">Bordes &amp; Weston (2017)</ref> like a triplet format. For instance, the first utterance from the driver in the <ref type="table">Table 1</ref> will be denoted as {($user, turn1, I), ($user, turn1, need), ($user, turn1, gas)}. For the two memory modules, a bag-of-word representation is used as the memory embeddings. During the inference time, we copy the object word once a memory position is pointed to, for example, 3 miles will be copied if the triplet (Toms house, distance, 3 miles) is selected. We denote Object(.) function as getting the object word from a triplet. Knowledge read and write. Our external knowledge is composed of a set of trainable embedding matrices C = (C 1 , . . . , C K+1 ), where C k ∈ R |V |×d emb , K is the maximum memory hop in the MN, |V | is the vocabulary size and d emb is the embedding dimension. We denote memory in the external knowledge as M = [B; X] = (m 1 , . . . , m n+l ), where m i is one of the triplet components mentioned. To read the memory, the external knowledge needs a initial query vector q 1 . Moreover, it can loop over K hops and computes the attention weights at each hop k using</p><formula xml:id="formula_0">p k i = Softmax((q k ) T c k i ),<label>(1)</label></formula><p>where c k i = B(C k (m i )) ∈ R d emb is the embedding in i th memory position using the embedding matrix C k , q k is the query vector for hop k, and B(.) is the bag-of-word function. Note that p k ∈ R n+l is a soft memory attention that decides the memory relevance with respect to the query vector. Then, the model reads out the memory o k by the weighted sum over c k+1 and update the query vector q k+1 . Formally,</p><formula xml:id="formula_1">o k = i p k i c k+1 i , q k+1 = q k + o k .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GLOBAL MEMORY ENCODER</head><p>In <ref type="figure" target="#fig_1">Figure 2</ref>(a), a context RNN is used to model the sequential dependency and encode the context X. Then the hidden states are written into the external knowledge as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Afterward, the last encoder hidden state serves as the query to read the external knowledge and get two outputs, the global memory pointer and the memory readout. Intuitively, since it is hard for MN architectures to model the dependencies between memories , which is a serious drawback especially in conversational related tasks, writing the hidden states to the external knowledge can provide sequential and contextualized information. With meaningful representation, our pointers can correctly copy out words from external knowledge, and the common OOV challenge can be mitigated. In addition, using the encoded dialogue context as a query can encourage our external knowledge to read out memory information related to the hidden dialogue states or user intention. Moreover, the global memory pointer that learns a global memory distribution is passed to the decoder along with the encoded dialogue history and KB information.</p><p>Context RNN. A bi-directional gated recurrent unit (GRU) <ref type="bibr" target="#b2">(Chung et al., 2014)</ref> is used to encode dialogue history into the hidden states H = (h e 1 , . . . , h e 1 ), and the last hidden state h e n is used to query the external knowledge as the encoded dialogue history. In addition, the hidden states H are written into the dialogue memory module in the external knowledge by summing up the original memory representation with the corresponding hidden states. In formula,</p><formula xml:id="formula_2">c k i = c k i + h e mi if m i ∈ X and ∀k ∈ [1, K + 1],<label>(3)</label></formula><p>Global memory pointer. Global memory pointer G = (g 1 , . . . , g n+l ) is a vector containing real values between 0 and 1. Unlike conventional attention mechanism that all the weights sum to one, each element in G is an independent probability. We first query the external knowledge using h e n until the last hop, and instead of applying the Softmax function as in (1), we perform an inner product followed by the Sigmoid function. The memory distribution we obtained is the global memory pointer G, which is passed to the decoder. To further strengthen the global pointing ability, we add an auxiliary loss to train the global memory pointer as a multi-label classification task. We show in the ablation study that adding this additional supervision does improve the performance. Lastly, the memory readout q K+1 is used as the encoded KB information.</p><p>In the auxiliary task, we define the label G label = (g l 1 , . . . , g l n+l ) by checking whether the object words in the memory exists in the expected system response Y . Then the global memory pointer is trained using binary cross-entropy loss Loss g between G and G label . In formula,</p><formula xml:id="formula_3">g i = Sigmoid((q K ) T c K i ), g l i = 1 if Object(m i ) ∈ Y 0 otherwise , Loss g = − n+l i=1 [g l i × log g i + (1 − g l i ) × log (1 − g i )].<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LOCAL MEMORY DECODER</head><p>Given the encoded dialogue history h e n , the encoded KB information q K+1 , and the global memory pointer G, our local memory decoder first initializes its sketch RNN using the concatenation of h e n and q K+1 , and generates a sketch response that excludes slot values but includes the sketch tags. For example, sketch RNN will generate "@poi is @distance away", instead of "Starbucks is 1 mile away." At each decoding time step, the hidden state of the sketch RNN is used for two purposes: 1) predict the next token in vocabulary, which is the same as standard sequence-to-sequence (S2S) learning; 2) serve as the vector to query the external knowledge. If a sketch tag is generated, the global memory pointer is passed to the external knowledge, and the expected output word will be picked up from the local memory pointer. Otherwise, the output word is the word that generated by the sketch RNN. For example in <ref type="figure" target="#fig_1">Figure 2</ref>(b), a @poi tag is generated at the first time step, therefore, the word Starbucks is picked up from the local memory pointer as the system output word.</p><p>Sketch RNN. We use a GRU to generate a sketch response Y s = (y s 1 , . . . , y s m ) without real slot values. The sketch RNN learns to generate a dynamic dialogue action template based on the encoded dialogue (h e n ) and KB information (q K+1 ). At each decoding time step t, the sketch RNN hidden state h d t and its output distribution P vocab t are defined as</p><formula xml:id="formula_4">h d t = GRU(C 1 (ŷ s t−1 ), h d t−1 ), P vocab t = Softmax(W h d t )<label>(5)</label></formula><p>We use the standard cross-entropy loss to train the sketch RNN, we define Loss v as.</p><formula xml:id="formula_5">Loss v = m t=1 − log(P vocab t (y s t )).<label>(6)</label></formula><p>We replace the slot values in Y into sketch tags based on the provided entity table. The sketch tags ST are all the possible slot types that start with a special token, for example, @address stands for all the addresses and @distance stands for all the distance information.</p><p>Local memory pointer. Local memory pointer L = (L 1 , . . . , L m ) contains a sequence of pointers. At each time step t, the global memory pointer G first modify the global contextual representation using its attention weights,</p><formula xml:id="formula_6">c k i = c k i × g i , ∀i ∈ [1, n + l] and ∀k ∈ [1, K + 1],<label>(7)</label></formula><p>and then the sketch RNN hidden state h d t queries the external knowledge. The memory attention in the last hop is the corresponding local memory pointer L t , which is represented as the memory distribution at time step t. To train the local memory pointer, a supervision on top of the last hop memory attention in the external knowledge is added. We first define the position label of local memory pointer L label at the decoding time step t as</p><formula xml:id="formula_7">L label t = max(z) if ∃z s.t. y t = Object(m z ), n + l + 1 otherwise.<label>(8)</label></formula><p>The position n+l+1 is a null token in the memory that allows us to calculate loss function even if y t does not exist in the external knowledge. Then, the loss between L and L label is defined as</p><formula xml:id="formula_8">Loss l = m t=1 − log(L t (L label t )).<label>(9)</label></formula><p>Furthermore, a record R ∈ R n+l is utilized to prevent from copying same entities multiple times. All the elements in R are initialized as 1 in the beginning. During the decoding stage, if a memory position has been pointed to, its corresponding position in R will be masked out. During the inference time,ŷ t is defined aŝ</p><formula xml:id="formula_9">y t = arg max(P vocab t ) if arg max(P vocab t ) ∈ ST, Object(m arg max(Lt R) ) otherwise,<label>(10)</label></formula><p>where is the element-wise multiplication. Lastly, all the parameters are jointly trained by minimizing the weighted-sum of three losses (α, β, γ are hyper-parameters):</p><formula xml:id="formula_10">Loss = αLoss g + βLoss v + γLoss l<label>(11)</label></formula><p>3 EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATASETS</head><p>We use two public multi-turn task-oriented dialogue datasets to evaluate our model: the bAbI dialogue <ref type="bibr" target="#b0">(Bordes &amp; Weston, 2017)</ref> and Stanford multi-domain dialogue (SMD) . The bAbI dialogue includes five simulated tasks in the restaurant domain. Task 1 to 4 are about calling API calls, modifying API calls, recommending options, and providing additional information, respectively. Task 5 is the union of tasks 1-4. There are two test sets for each task: one follows the same distribution as the training set and the other has OOV entity values. On the other hand, SMD is a human-human, multi-domain dialogue dataset. It has three distinct domains: calendar scheduling, weather information retrieval, and point-of-interest navigation. The key difference between these two datasets is, the former has longer dialogue turns but the regular user and system behaviors, the latter has few conversational turns but variant responses, and the KB information is much more complicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TRAINING DETAILS</head><p>The model is trained end-to-end using Adam optimizer <ref type="bibr" target="#b12">(Kingma &amp; Ba, 2015)</ref>, and learning rate annealing starts from 1e −3 to 1e −4 . The number of hop K is set to 1,3,6 to compare the performance difference. The weights α, β, γ summing up the three losses are set to 1. All the embeddings are initialized randomly, and a simple greedy strategy is used without beam-search during the decoding stage. The hyper-parameters such as hidden size and dropout rate are tuned with grid-search over  <ref type="bibr" target="#b20">(Seo et al., 2017)</ref>, End-to-end Memory Network <ref type="bibr" target="#b0">(Bordes &amp; Weston, 2017)</ref>, Gated Memory Network <ref type="bibr" target="#b15">(Liu &amp; Perez, 2017)</ref>, Point to Unknown Word <ref type="bibr" target="#b9">(Gulcehre et al., 2016)</ref>, and Memory-to-Sequence .</p><formula xml:id="formula_11">Task QRN MN GMN S2S+Attn</formula><p>Ptr-Unk Mem2Seq GLMP K1 GLMP K3 GLMP K6 T1 99.4 (-) 99.9 (99.6) 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)  T2 99.5 (-) 100 (100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref> 100 <ref type="formula" target="#formula_0">(100)</ref>   the development set (per-response accuracy for bAbI Dialogue and BLEU score for the SMD). In addition, to increase model generalization and simulate OOV setting, we randomly mask a small number of input source tokens into an unknown token. The model is implemented in PyTorch and the hyper-parameters used for each task and the dataset statistics are reported in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RESULTS</head><p>bAbI Dialogue. In <ref type="table" target="#tab_1">Table 2</ref>, we follow <ref type="bibr" target="#b0">Bordes &amp; Weston (2017)</ref> to compare the performance based on per-response accuracy and task-completion rate. Note that for utterance retrieval methods, such as QRN, MN, and GMN, cannot correctly recommend options (T3) and provide additional information (T4), and a poor generalization ability is observed in OOV setting, which has around 30% performance difference in Task 5. Although previous generation-based approaches (Ptr-Unk, Mem2Seq) have mitigated the gap by incorporating copy mechanism, the simplest cases such as generating and modifying API calls (T1, T2) still face a 6-17% OOV performance drop. On the other hand, GLMP achieves a highest 92.0% task-completion rate in full dialogue task and surpasses other baselines by a big margin especially in the OOV setting. No per-response accuracy loss for T1, T2, T4 using only the single hop, and only decreases 7-9% in task 5.</p><p>Stanford Multi-domain Dialogue. For human-human dialogue scenario, we follow previous dialogue works <ref type="bibr" target="#b10">Zhao et al., 2017;</ref>) to evaluate our system on two automatic evaluation metrics, BLEU and entity F1 score 2 . As shown in <ref type="table" target="#tab_3">Table 3</ref>, GLMP achieves a highest 14.79 BLEU and 59.97% entity F1 score, which is a slight improvement in BLEU but a huge gain in entity F1. In fact, for unsupervised evaluation metrics in task-oriented dialogues, we argue that the entity F1 might be a more comprehensive evaluation metric than per-response accuracy or BLEU, as shown in  that humans are able to choose the right entities but have very diversified responses. Note that the results of rule-based and KVR are not directly comparable because they simplified the task by mapping the expression of entities to a canonical form using named entity recognition and linking 3 .  <ref type="bibr">(-11.47)</ref> Moreover, human evaluation of the generated responses is reported. We compare our work with previous state-of-the-art model Mem2Seq 4 and the original dataset responses as well. We randomly select 200 different dialogue scenarios from the test set to evaluate three different responses. Amazon Mechanical Turk is used to evaluate system appropriateness and human-likeness on a scale from 1 to 5. As the results shown in <ref type="table" target="#tab_3">Table 3</ref>, we see that GLMP outperforms Mem2Seq in both measures, which is coherent to previous observation. We also see that human performance on this assessment sets the upper bound on scores, as expected. More details about the human evaluation are reported in the Appendix.</p><p>Ablation Study. The contributions of the global memory pointer G and the memory writing of dialogue history H are shown in <ref type="table" target="#tab_4">Table 4</ref>. We compare the results using GLMP with K = 1 in bAbI OOV setting and SMD. GLMP without H means that the context RNN in the global memory encoder does not write the hidden states into the external knowledge. As one can observe, our model without H has 5.3% more loss in the full dialogue task. On the other hand, GLMP without G means that we do not use the global memory pointer to modify the external knowledge, and an 11.47% entity F1 drop can be observed in SMD dataset. Note that a 0.4% increase can be observed in task 5, it suggests that the use of global memory pointer may impose too strong prior entity probability. Even if we only report one experiment in the table, this OOV generalization problem can be mitigated by increasing the dropout ratio during training.</p><p>Visualization and Qualitative Evaluation. Analyzing the attention weights has been frequently used to interpret deep learning models. In <ref type="figure">Figure 3</ref>, we show the attention vector in the last hop for each generation time step. Y-axis is the external knowledge that we can copy, including the KB information and the dialogue history. Based on the question "what is the address?" asked by the driver in the last turn, the gold answer and our generated response are on the top, and the global memory pointer G is shown in the left column. One can observe that in the right column, the final memory pointer successfully copy the entity chevron in step 0 and its address 783 Arcadia Pl in step 3 to fill in the sketch utterance. On the other hand, the memory attention without global weighting is reported in the middle column. One can find that even if the attention weights focus on several point of interests and addresses in step 0 and step 3, the global memory pointer can mitigate the issue as expected. More dialogue visualization and generated results including several negative examples and error analysis are reported in the Appendix. Delexicalized Generation: @poi is at @address Final Generation: chevron is at 783_arcadia_pl Gold: 783_arcadia_pl is the address for chevron gas_station <ref type="figure">Figure 3</ref>: Memory attention visualization in the SMD navigation domain. Left column is the global memory pointer G, middle column is the memory pointer without global weighting, and the right column is the final memory pointer. these approaches can encourage more flexible and diverse system responses by generating utterances token-by-token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORKS</head><p>Pointer network. <ref type="bibr" target="#b26">Vinyals et al. (2015)</ref> uses attention as a pointer to select a member of the input source as the output. Such copy mechanisms have also been used in other natural language processing tasks, such as question answering <ref type="bibr" target="#b3">(Dehghani et al., 2017;</ref><ref type="bibr" target="#b10">He et al., 2017)</ref>, neural machine translation <ref type="bibr" target="#b9">(Gulcehre et al., 2016;</ref><ref type="bibr" target="#b8">Gu et al., 2016)</ref>, language modeling <ref type="bibr" target="#b17">(Merity et al., 2017)</ref>, and text summarization <ref type="bibr" target="#b19">(See et al., 2017)</ref>. In task-oriented dialogue tasks,  first demonstrated the potential of the copy-augmented Seq2Seq model, which shows that generationbased methods with simple copy strategy can surpass retrieval-based ones. Later,  augmented the vocabulary distribution by concatenating KB attention, which at the same time increases the output dimension. Recently,  combines end-to-end memory network into sequence generation, which shows that the multi-hop mechanism in MN can be utilized to improve copy attention. These models outperform utterance retrieval methods by copying relevant entities from the KBs.</p><p>Others. <ref type="bibr" target="#b10">Zhao et al. (2017)</ref> proposes entity indexing and  introduces recorded delexicalization to simplify the problem by record entity tables manually. In addition, our approach utilized recurrent structures to query external memory can be viewed as the memory controller in Memory augmented neural networks (MANN) <ref type="bibr" target="#b6">(Graves et al., 2014;</ref>. Similarly, memory encoders have been used in neural machine translation <ref type="bibr" target="#b27">(Wang et al., 2016)</ref> and meta-learning applications . However, different from other models that use a single matrix representation for reading and writing, GLMP leverages end-to-end memory networks to perform multiple hop attention, which is similar to the stacking self-attention strategy in the Transformer <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In the work, we present an end-to-end trainable model called global-to-local memory pointer networks for task-oriented dialogues. The global memory encoder and the local memory decoder are designed to incorporate the shared external knowledge into the learning framework. We empirically show that the global and the local memory pointer are able to effectively produce system responses even in the out-of-vocabulary scenario, and visualize how global memory pointer helps as well. As a result, our model achieves state-of-the-art results in both the simulated and the human-human dialogue datasets, and holds potential for extending to other tasks such as question answering and text summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TABLES</head><p>A.1 TRAINING PARAMETERS <ref type="table">Table 5</ref>: Selected hyper-parameters in each dataset for different hops. The values is the embedding dimension and the GRU hidden size, and the values between parenthesis is the dropout rate. For all the models we used learning rate equal to 0.001, with a decay rate of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T1</head><p>T2 <ref type="table" target="#tab_3">T3  T4  T5  SMD  GLMP K1</ref>    Appropriateness 5: Correct grammar, correct logic, correct dialogue flow, and correct entity provided 4: Correct dialogue flow, logic and grammar but has slightly mistakes in entity provided 3: Noticeable mistakes about grammar or logic or entity provided but acceptable 2: Poor grammar, logic and entity provided 1: Wrong grammar, wrong logic, wrong dialogue flow, and wrong entity provided Human-Likeness (Naturalness) 5: The utterance is 100% like what a person will say 4: The utterance is 75% like what a person will say 3: The utterance is 50% like what a person will say 2: The utterance is 25% like what a person will say 1: The utterance is 0% like what a person will say B ERROR ANALYSIS For bAbI dialogues, the mistakes are mainly from task 3, which is recommending restaurants based on their rating from high to low. We found that sometimes the system will keep sending those restaurants with the higher score even if the user rejected them in the previous turns. On the other hand, SMD is more challenging for response generation. First, we found that the model makes mistakes when the KB has several options corresponding to the user intention. For example, once the user has more than one doctor appointment in the table, the model can barely recognize. In addition, since we do not include the domain specific and user intention supervision, wrong delexicalized responses may be generated, which results in an incorrect entity copy. Lastly, we found that the copied entities may not be matched to the generated sketch tags. For example, an address tag may result in a distance entity copy. We leave the space of improvement to future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL DISCUSSION</head><p>One of the reviewers suggested us to compare our work to some existing dialogue framework such as PyDial 5 . To the best of our knowledge, in the PyDial framework, it requires to have the dialogue acts labels for the NLU module and the belief states labels for the belief tracker module. The biggest challenge is we do not have such labels in the SMD and bAbI datasets. Moreover, the semi tracker in PyDial is rule-based, which need to re-write rules whenever it encounters a new domain or new datasets. Even its dialogue management module could be a learning solution like policy networks, the input of the policy network is still the hand-crafted state features and labels. Therefore, without the rules and labels predefined in the NLU and belief tracker modules, PyDial could not learn a good policy network.</p><p>Truly speaking, based on the data we have (not very big size) and the current state-of-the-art machine learning algorithms and models, we believe that a well and carefully constructed task-oriented dialogue system using PyDial in a known domain using human rules (in NLU and Belief Tracker) with policy networks may outperform the end-to-end systems (more robust). However, in this paper, without additional human labels and human rules, we want to explore the potential and the advantage of end-to-end systems. Besides easy to train, for multi-domain cases, or even zero-shot domain cases, we believe end-to-end approaches will have better adaptability compared to any rule-based systems. Delexicalized Generation: the nearest @poi_type is @poi , @distance away at @address Final Generation: the nearest grocery_store is willows_market , 3_miles away at 409_bollard_st</p><p>Gold: we are 3_miles away from willows_market but there is a car_collision_nearby <ref type="figure">Figure 5</ref>: Memory attention visualization from the SMD navigation domain. Delexicalized Generation: the nearest @poi_type is @poi at @address Final Generation: the nearest hospital is stanford_express_care at 214_el_camino_real</p><p>Gold: stanford_express_care is at 214_el_camino_real <ref type="figure">Figure 6</ref>: Memory attention visualization from the SMD navigation domain. Delexicalized Generation: the nearest @poi_type is @poi , @distance away at @address Final Generation: the nearest parking_garage is civic_center_garage , 4_miles away at 5_miles Gold: the closest parking_garage is civic_center_garage , located 4_miles away at 270_altaire_walk  Delexicalized Generation: the nearest @poi_type is @poi , @distance away at @address Final Generation: the nearest grocery_store is sigona_farmers_market , 4_miles away at 819_alma_st Gold: there are whole_foods 2_miles away and sigona_farmers_market 4_miles away where do we go ?  Delexicalized Generation: okay , setting a reminder for your @event with @party @date at @time Final Generation: okay , setting a reminder for your dinner with marie 7pm at the_6th Gold: i will set a reminder for dinner at 7pm , for the_6th of this month with marie Delexicalized Generation: your @event is on @date at @time Final Generation: your tennis_activity is on the_4th at 5pm Gold: your tennis_activity is on the_4th at 5pm and your sister will be attending  Delexicalized Generation: your next @event is on @date at @time Final Generation: your next tennis_activity is on the_13th at 6pm Gold: your next tennis_activity is on the_4th at 5pm with your mother <ref type="figure" target="#fig_0">Figure 11</ref>: Memory attention visualization from the SMD schedule domain. Delexicalized Generation: your @event is on @date at @time with @party Final Generation: your doctor is on the_5th at 6pm with alex Gold: are you talking about the doctor_appointment on wednesday or the one on the_5th ? Delexicalized Generation: it will not be @weather_attribute in @location @weekly_time Final Generation: it will not be drizzle in redwood_city weekend Gold: there will be no drizzle in redwood_city this weekend <ref type="figure" target="#fig_0">Figure 13</ref>: Memory attention visualization from the SMD weather domain. 18 will there be clear_skies in danville on thursday ? 0 1 2 3 4 5 6 7</p><p>Pointer w/o G 0 1 2 3 4 5 6 7</p><p>Final Pointer Delexicalized Generation: there will be @weather_attribute in @location on @date Final Generation: there will be clear_skies in danville on thursday Gold: dew is predicted in danville on thursday <ref type="figure" target="#fig_0">Figure 14</ref>: Memory attention visualization from the SMD weather domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed (a) global-to-local memory pointer networks for task-oriented dialogue systems and the (b) external knowledge architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed (a) global memory encoder and the (b) local memory decoder architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Appropriateness and human-likeness scores according to 200 dialogue scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Memory attention visualization from the SMD navigation domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Memory attention visualization from the SMD navigation domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Memory attention visualization from the SMD schedule domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Memory attention visualization from the SMD schedule domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>There is a gas station locally Valero is 4 miles away Coupa 2 miles moderate coffee or tea place 394 Van Ness Ave Gold: Valero is 4 miles away</figDesc><table><row><cell cols="3">Point of interest (poi) Distance Traffic</cell><cell>Poi type</cell><cell>Address</cell><cell>Driver</cell><cell>I need gas</cell></row><row><cell cols="7">Toms house System GLMP: Panda express 3 miles heavy friend's house 580 Van Ness Ave 2 miles no Chinese restaurant 842 Arrowhead Way Driver What is the address ?</cell></row><row><cell>Stanford express care</cell><cell>5 miles</cell><cell>no</cell><cell>hospital</cell><cell>214 El Camino Real</cell><cell cols="2">System GLMP: Valero is located at 200 Alester Ave</cell></row><row><cell>Valero</cell><cell>4 miles</cell><cell>heavy</cell><cell>gas station</cell><cell>200 Alester Ave</cell><cell></cell><cell>Gold: Valero is at 200 Alester Ave</cell></row><row><cell>Starbucks</cell><cell>1 miles</cell><cell>heavy</cell><cell cols="2">coffee or tea place 792 Bedoin St</cell><cell>Driver</cell><cell>Thank you!</cell></row><row><cell cols="2">Manning, 2017;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Per-response accuracy and completion rate (in the parentheses) on bAbI dialogues. GLMP achieves the least out-of-vocabulary performance drop. Baselines are reported from Query Reduction Network</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>In SMD dataset, our model achieves highest BLEU score and entity F1 score over baselines, including previous state-of-the-art result from. (Models with * are reported from, where the problem is simplified to the canonicalized forms.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Automatic Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">Rule-Based* KVR* S2S S2S + Attn Ptr-Unk Mem2Seq GLMP K1 GLMP K3 GLMP K6</cell></row><row><cell>BLEU</cell><cell>6.6</cell><cell>13.2</cell><cell>8.4</cell><cell>9.3</cell><cell>8.3</cell><cell>12.6</cell><cell>13.83</cell><cell>14.79</cell><cell>12.37</cell></row><row><cell>Entity F1</cell><cell>43.8</cell><cell>48.0</cell><cell>10.3</cell><cell>19.9</cell><cell>22.7</cell><cell>33.4</cell><cell>57.25</cell><cell>59.97</cell><cell>53.54</cell></row><row><cell>Schedule F1</cell><cell>61.3</cell><cell>62.9</cell><cell>9.7</cell><cell>23.4</cell><cell>26.9</cell><cell>49.3</cell><cell>68.74</cell><cell>69.56</cell><cell>69.38</cell></row><row><cell>Weather F1</cell><cell>39.5</cell><cell>47.0</cell><cell>14.1</cell><cell>25.6</cell><cell>26.7</cell><cell>32.8</cell><cell>60.87</cell><cell>62.58</cell><cell>55.89</cell></row><row><cell>Navigation F1</cell><cell>40.4</cell><cell>41.3</cell><cell>7.0</cell><cell>10.8</cell><cell>14.9</cell><cell>20.0</cell><cell>48.62</cell><cell>52.98</cell><cell>43.08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Human Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mem2Seq</cell><cell></cell><cell></cell><cell>GLMP</cell><cell></cell><cell></cell><cell>Human</cell><cell></cell></row><row><cell>Appropriate</cell><cell></cell><cell>3.89</cell><cell></cell><cell></cell><cell>4.15</cell><cell></cell><cell></cell><cell>4.6</cell><cell></cell></row><row><cell>Humanlike</cell><cell></cell><cell>3.80</cell><cell></cell><cell></cell><cell>4.02</cell><cell></cell><cell></cell><cell>4.54</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study using single hop model.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">bAbI Dialogue OOV</cell><cell></cell><cell>SMD</cell></row><row><cell></cell><cell></cell><cell cols="3">Per-response Accuracy</cell><cell></cell><cell>Entity F1</cell></row><row><cell></cell><cell>T1</cell><cell>T2</cell><cell>T3</cell><cell>T4</cell><cell>T5</cell><cell>All</cell></row><row><cell>GLMP</cell><cell>100 (-)</cell><cell>100 (-)</cell><cell>95.5 (-)</cell><cell>100 (-)</cell><cell>92.0 (-)</cell><cell>57.25 (-)</cell></row><row><cell cols="6">GLMP w/o H 90.4 (-9.6) 85.6 (-14.4) 95.4 (-0.1) 100 (-0) 86.2 (-5.3)</cell><cell>47.96 (-9.29)</cell></row><row><cell>GLMP w/o G</cell><cell>100 (-0)</cell><cell>91.7 (-8.3)</cell><cell>95.5 (-0)</cell><cell cols="3">100 (-0) 92.4 (+0.4) 45.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Dataset statistics for 2 datasets.</figDesc><table><row><cell>Task</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell></cell><cell>SMD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Calendar Weather Navigation</cell></row><row><cell>Avg. User turns</cell><cell>4</cell><cell>6.5</cell><cell>6.4</cell><cell>3.5</cell><cell>12.9</cell><cell></cell><cell>2.6</cell></row><row><cell>Avg. Sys turns</cell><cell>6</cell><cell>9.5</cell><cell>9.9</cell><cell>3.5</cell><cell>18.4</cell><cell></cell><cell>2.6</cell></row><row><cell>Avg. KB results</cell><cell>0</cell><cell>0</cell><cell>24</cell><cell>7</cell><cell>23.7</cell><cell></cell><cell>66.1</cell></row><row><cell>Avg. Sys words</cell><cell>6.3</cell><cell>6.2</cell><cell>7.2</cell><cell>5.7</cell><cell>6.5</cell><cell></cell><cell>8.6</cell></row><row><cell>Max. Sys words</cell><cell>9</cell><cell>9</cell><cell>9</cell><cell>8</cell><cell>9</cell><cell></cell><cell>87</cell></row><row><cell>Nb. Slot Types</cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell><cell>6</cell><cell>4</cell><cell>5</cell></row><row><cell>Nb. Distinct Slot values</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>79</cell><cell>65</cell><cell>140</cell></row><row><cell>Vocabulary</cell><cell></cell><cell></cell><cell>3747</cell><cell></cell><cell></cell><cell></cell><cell>1601</cell></row><row><cell>Train dialogues</cell><cell></cell><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell>2425</cell></row><row><cell>Val dialogues</cell><cell></cell><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell>302</cell></row><row><cell>Test dialogues</cell><cell></cell><cell cols="3">1000 + 1000 OOV</cell><cell></cell><cell></cell><cell>304</cell></row><row><cell cols="6">Total Nb. Dialogues 4000 4000 4000 4000 4000</cell><cell>1034</cell><cell>997</cell><cell>1000</cell></row><row><cell cols="2">A.3 HUMAN EVALUATION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Figure 9: Memory attention visualization from the SMD schedule domain.</figDesc><table><row><cell></cell><cell>Pointer w/o G</cell><cell>Final Pointer</cell></row><row><cell>[sister] party swimming_activity</cell><cell></cell><cell></cell></row><row><cell>[the_4th] date swimming_activity</cell><cell></cell><cell></cell></row><row><cell>[10am] time swimming_activity</cell><cell></cell><cell></cell></row><row><cell>[tom] party doctor_appointment</cell><cell></cell><cell></cell></row><row><cell>[the_5th] date doctor_appointment</cell><cell></cell><cell></cell></row><row><cell>[3pm] time doctor_appointment</cell><cell></cell><cell></cell></row><row><cell>[martha] party yoga_activity</cell><cell></cell><cell></cell></row><row><cell>[the_10th] date yoga_activity</cell><cell></cell><cell></cell></row><row><cell>[11am] time yoga_activity</cell><cell></cell><cell></cell></row><row><cell>[sister] party optometrist_appointment</cell><cell></cell><cell></cell></row><row><cell>[thursday] date optometrist_appointment</cell><cell></cell><cell></cell></row><row><cell>[2pm] time optometrist_appointment</cell><cell></cell><cell></cell></row><row><cell>[aunt] party dinner</cell><cell></cell><cell></cell></row><row><cell>[sunday] date dinner</cell><cell></cell><cell></cell></row><row><cell>[7pm] time dinner</cell><cell></cell><cell></cell></row><row><cell>[sister] party tennis_activity</cell><cell></cell><cell></cell></row><row><cell>[the_4th] date tennis_activity</cell><cell></cell><cell></cell></row><row><cell>[5pm] time tennis_activity</cell><cell></cell><cell></cell></row><row><cell>car</cell><cell></cell><cell></cell></row><row><cell>,</cell><cell></cell><cell></cell></row><row><cell>find</cell><cell></cell><cell></cell></row><row><cell>the</cell><cell></cell><cell></cell></row><row><cell>date</cell><cell></cell><cell></cell></row><row><cell>and</cell><cell></cell><cell></cell></row><row><cell>time</cell><cell></cell><cell></cell></row><row><cell>for</cell><cell></cell><cell></cell></row><row><cell>my</cell><cell></cell><cell></cell></row><row><cell>tennis_activity</cell><cell></cell><cell></cell></row><row><cell>G</cell><cell>0 1 2 3 4 5 6</cell><cell>0 1 2 3 4 5 6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/jasonwu0731/GLMP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">BLEU: multi-bleu.perl script; Entity F1: Micro-average over responses.3  For example, they compared in "@poi is @poi distance away," instead of "Starbucks is 1 mile away."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Mem2Seq code is released and we achieve similar results stated in the original paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.camdial.org/pydial/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog. International Conference on Learning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1605.07683</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to attend, copy, and generate for session-based query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fleury</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3132847.3133010</idno>
		<ptr target="http://doi.acm.org/10.1145/3132847.3133010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A copy-augmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-2075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="468" to="473" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Key-value retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W17-5506" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<title level="m">Neural turing machines. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1154" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany, Au</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="1014" />
		</imprint>
	</monogr>
	<note>Pointing the unknown words</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Learning to remember rare events. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Task lineages: Dialog state tracking for flexible interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gated end-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-1001" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P18-1136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1468" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1147" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query-reduction networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural language generation in dialogue using lexicalized and delexicalized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On-line active reward learning for policy optimisation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5866-pointer-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memory-enhanced decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1027" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="278" to="286" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A network-based end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hao Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end recurrent entity network for entity-value independent goal-oriented dialog learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genta</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dialog System Technology Challenges Workshop, DSTC6</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end dynamic query memory network for entity-value independent task-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genta</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="6154" to="6158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative encoder-decoder models for task-oriented spoken dialog systems with chatting capability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th</title>
		<meeting>the 18th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<ptr target="http://aclweb.org/anthology/W17-5505" />
		<title level="m">Annual SIGdial Meeting on Discourse and Dialogue</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Global-locally self-attentive dialogue state tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>60f] high sunday manhattan [50f] low sunday manhattan [rain] sunday manhattan</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Low Monday Manhattan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>monday manhattan [100f] high sunday compton [90f] low sunday compton [raining] sunday compton</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High Wednesday Compton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carson</forename><surname>High Saturday</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carson</forename><surname>High Friday</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carson</forename><surname>High Thursday</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carson</forename><surname>High Wednesday</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High Tuesday Seattle</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Low</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>cloudy] wednesday fresno [50f] high tuesday fresno [40f] low tuesday fresno [frost] tuesday fresno</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High Sunday Oakland</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Low</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>saturday oakland [100f] high friday oakland [90f] low friday oakland [dry] friday oakland</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High Friday Exeter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>High Friday Alhambra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
