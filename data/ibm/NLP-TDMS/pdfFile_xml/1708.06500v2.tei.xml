<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparsity Invariant CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-01">September 1, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
							<email>jonas.uhrig@daimler.com</email>
							<affiliation key="aff0">
								<orgName type="department">Daimler R&amp;D Sindelfingen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<addrLine>3 KIT Karlsruhe 4 ETH</addrLine>
									<settlement>Zürich 5 MPI Tübingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
							<email>nick.schneider@daimler.com</email>
							<affiliation key="aff0">
								<orgName type="department">Daimler R&amp;D Sindelfingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Daimler R&amp;D Sindelfingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Daimler R&amp;D Sindelfingen</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<addrLine>3 KIT Karlsruhe 4 ETH</addrLine>
									<settlement>Zürich 5 MPI Tübingen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
						</author>
						<title level="a" type="main">Sparsity Invariant CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-01">September 1, 2017</date>
						</imprint>
					</monogr>
					<note>The first two authors contributed equally to this work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth upsampling from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the benefits of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising 93k depth annotated RGB images. Our dataset allows for training and evaluating depth upsampling and depth prediction techniques in challenging real-world settings and will be made available upon publication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last few years, convolutional neural networks (CNNs) have impacted nearly all areas of computer vision. In most cases, the input to the CNN is an image or video, represented by a densely populated matrix or tensor. By combining convolutional layers with non-linearites and pooling layers, CNNs are able to learn distributed representations, extracting low-level features in the first layers, followed by successively higher-level features in subsequent layers. However, when the input to the network is sparse and irregular (e.g., when only 10% of the pixels carry information), it becomes less clear how the convolution operation should be defined as for each filter location the number and placement of the inputs varies.</p><p>The naïve approach to this problem is to assign a default value to all non-informative sites <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">39]</ref>. Unfortunately, this approach leads to suboptimal results as the learned filters must be invariant to all possible patterns of activation whose number grows exponentially with the filter size. In this paper, we investigate a simple yet effective solution to this problem which outperforms the naïve approach and several other baselines. In particular, we introduce a novel sparse convolutional layer which weighs the elements of the convolution kernel according to the validity of the input pixels. Additionally, a second stream stream carries information about the validity of pixels to subsequent layers of the network. This enables our approach to handle large levels of sparsity without significantly compromising accuracy. Importantly, our representation is invariant to the level of sparsity in the input. As evidenced by our experiments, training our network at a sparsity level different from the sparsity level at test time does not significantly deteriorate the results. This has important applications, e.g., in the context of robotics where algorithms must be robust to changes in sensor configuration.</p><p>One important area of application for the proposed technique is enhancement of 3D laser scan data, see <ref type="figure" target="#fig_0">Fig. 1</ref> for an illustration. While laser scanners provide valuable information about depth and reflectance, the resulting point clouds are typically very sparse, in particular when considering mobile scanners like the Velodyne HDL-64e 1 used in autonomous driving <ref type="bibr" target="#b15">[16]</ref>.</p><p>Learning models which are able to increase the density of such scans is thus highly desirable. Unfortunately, processing high-resolution data directly in 3D is challenging without compromising accuracy <ref type="bibr" target="#b53">[53]</ref>.</p><p>An alternative, which we follow in this paper, is to project the laser scan onto a virtual or real 2D image plane resulting in a 2.5D representation. Besides modeling depth prediction as a 2D regression problem, this representation has the advantage that additional dense information (e.g., RGB values from a color camera) can be easily integrated. However, projected laser scans are typically very sparse and not guaranteed to align with a regular pixel grid, hence leading to poor results when processed with standard CNNs. In contrast, the proposed method produces compelling results even when the input is sparse and irregularly distributed.</p><p>We evaluate our method in ablation studies and against several state-of-the-art baselines. For our evaluation, we leverage the synthetic Synthia dataset <ref type="bibr" target="#b54">[54]</ref> as well as a newly proposed real-world dataset with 93k depth annotated images derived from the KITTI raw dataset <ref type="bibr" target="#b14">[15]</ref>. Our dataset is the first to provide a significant number of highquality depth annotations for this scenario. Besides attaining higher accuracy in terms of depth and semantics we demonstrate our method's ability to generalize across varying datasets and levels of sparsity. Our code and dataset will be released upon publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we discuss methods which operate on sparse inputs followed by techniques that consider sparsity within the CNN. We briefly discuss the state-of-the-art in invariant representation learning and conclude with a review on related depth upsampling techniques.</p><p>CNNs with Sparse Inputs: The naïve approach to handling sparse inputs is to either zero the invalid values or to create an additional input channel for the network which encodes the validity of each pixel. For detecting objects in laser scans, Chen et al. <ref type="bibr" target="#b2">[3]</ref> and Li et al. <ref type="bibr" target="#b39">[39]</ref> project the 3D point clouds from the laser scanner onto a low resolution image, zero the missing values and run a standard CNN 1 http://velodynelidar.com/hdl-64e.html on this input. For optical flow interpolation and inpainting, Zweig et al. <ref type="bibr" target="#b70">[70]</ref> and Köhler et al. <ref type="bibr" target="#b33">[33]</ref> pass an additional binary validity mask to the network. As evidenced by our experiments, both strategies are suboptimal compared to explicitly considering sparsity inside the convolution layers.</p><p>Jampani et al. <ref type="bibr" target="#b29">[30]</ref> use bilateral filters as layers inside a CNN and learn the parameters of the corresponding permutohedral convolution kernel. While their layer handles sparse irregular inputs, it requires guidance information to construct an effective permutohedral representation and is computationally expensive for large grids. Compared to their approach our sparse convolutional networks yield significantly better results for depth upsampling while being as efficient as regular CNNs.</p><p>Graham <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and Riegler et al. <ref type="bibr" target="#b53">[53]</ref> consider sparse 3D inputs. In contrast to our work, their focus is on improving computational efficiency and memory demands by partitioning the space according to the input. However, regular convolution layers are employed which suffer from the same drawbacks as the naïve approach described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparsity in CNNs:</head><p>A number of works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b65">65]</ref> also consider sparsity within convolutional neural networks. Liu et al. <ref type="bibr" target="#b40">[40]</ref> show how to reduce the redundancy in the parameters using a sparse decomposition. Their approach eliminates more than 90% of parameters, with a drop of accuracy of less than 1% on ILSVRC2012. Wen et al. <ref type="bibr" target="#b65">[65]</ref> propose to regularize the structures (i.e., filters, channels and layer depth) of deep neural networks to obtain a hardware friendly representation. They report speed-up factors of 3 to 5 with respect to regular CNNs. While these works focus on improving efficiency of neural networks by exploiting sparsity within the network, we consider the problem of sparse inputs and do not tackle efficiency. A combination of the two lines of work will be an interesting direction for future research.</p><p>Invariant Representations: Learning models robust to variations of the input is a long standing goal of computer vision. The most commonly used solution to ensure robustness is data augmentation <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b59">59]</ref>. More recently, geometric invariances (e.g., rotation, perspective transformation) have been incorporated directly into the filters of CNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b69">69]</ref>. In this paper, we consider the problem of learning representations invariant to the level of sparsity in the input. As evidenced by our experiments, our model performs well even when the sparsity level differs significantly between the training and the test set. This has important implications as it allows for replacing the sensor (e.g., laser scanner) without retraining the network.</p><p>Depth Upsampling: We evaluate the effectiveness of our approach for the task of depth upsampling, which is an active area of research with applications in, e.g., stereo vision, optical flow and 3D reconstruction from laser scan data.</p><p>While some methods operate directly on the depth input, others require guidance, e.g., from a high resolution image.</p><p>Methods for non-guided depth upsampling are closely related to those for single image superresolution. Early approaches have leveraged repetitive structures to identify similar patches across different scales in 2D <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">45]</ref> and 3D <ref type="bibr" target="#b26">[27]</ref>. More recently, deep learning based methods for depth <ref type="bibr" target="#b52">[52]</ref> and image superresolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b67">67]</ref> have surpassed traditional upsampling techniques in terms of accuracy and efficiency. However, all aforementioned methods assume that the data is located on a regular grid and therefore cannot be applied for upsampling sparse and irregularly distributed 3D laser scan data as considered in this paper.</p><p>Guided upsampling, on the other hand, uses the underlying assumption that the target domain shares commonalities with a high-resolution guidance image, e.g., that image edges align with depth discontinuities. A popular choice for guided upsampling is guided bilateral filtering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b68">68]</ref>. More advanced approaches are based on global energy minimization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51]</ref>, compressive sensing <ref type="bibr" target="#b21">[22]</ref>, or incorporate semantics for improved performance <ref type="bibr" target="#b58">[58]</ref>. Several approaches also exploit end-to-end models for guided depth upsampling of regular data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b61">61]</ref>. While some of the aforementioned techniques are able to handle sparse inputs, they heavily rely on the guidance signal. In contrast, here we propose a learning based solution to the problem, yielding compelling results even without image guidance. Unlike existing CNN-based approaches for depth upsampling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b61">61]</ref>, the proposed convolution layer handles sparse irregular inputs which occur, e.g., in 3D laser scan data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Let f denote a mapping from input domain X (e.g., intensity, depth) to output domain Y (e.g., depth, semantics), implemented via a convolutional neural network. In this paper, we consider the case, where the inputs x = {x u,v } ∈ X are only partially observed. Let o = {o u,v } denote corresponding binary variables indicating if an input is observed (o u,v = 1) or not (o u,v = 0). The output of a standard convolutional layer in a CNN is computed via</p><formula xml:id="formula_0">f u,v (x) = k i,j=−k x u+i,v+j w i,j + b<label>(1)</label></formula><p>with kernel size 2k + 1, weight w and bias b. If the input comprises multiple features, x u,v and w i,j represent vectors whose length depends on the number of input channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Naïve Approach</head><p>There are two naïve ways to deal with unobserved inputs. First, invalid inputs x u,v can be encoded using a default value, e.g., zero. The problem with this approach is that the network must learn to distinguish between observed inputs and those being invalid. This is a difficult task as the number of possible binary patterns grows exponentially with the kernel size. Alternatively, o can be used as an additional input to the network in the hope that the network learns the correspondence between the observation mask and the inputs. Unfortunately, both variants struggle to learn robust representations from sparse inputs (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sparse Convolutions</head><p>To tackle this problem, we propose a convolution operation which explicitly considers sparsity by evaluating only observed pixels and normalizing the output appropriately:</p><formula xml:id="formula_1">f u,v (x, o) = k i,j=−k o u+i,v+j x u+i,v+j w i,j k i,j=−k o u+i,v+j + + b (2)</formula><p>Here, a small is added to the denominator to avoid division by zero at filter locations where none of the input pixels x u+i,v+j are observed. Note that Equation 2 evaluates to a (scaled) standard convolution when the input is dense.</p><p>The primary motivation behind the proposed sparse convolution operation is to render the filter output invariant to the actual number of observed inputs which varies significantly between filter locations due to the sparse and irregular input. Note that in contrast to other techniques <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">52]</ref> which artificially upsample the input (e.g., via interpolation), our approach operates directly on the input and doesn't introduce additional distractors.</p><p>When propagating information to subsequent layers, it is important to keep track of the visibility state and make it available to the next layers of the network. In particular, we like to mark output locations as "unobserved" when none of the filter's inputs has been observed. We thus determine subsequent observation masks in the network f o u,v (o) via the max pooling operation</p><formula xml:id="formula_2">f o u,v (o) = max i,j=−k,..,k o u+i,v+j<label>(3)</label></formula><p>which evaluates to 1 if at least one observed variable is visible to the filter and 0 otherwise. In combination with the output of the convolution this serves as input for the next sparse convolution layer. The complete architecture of our network is illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Skip Connections</head><p>So far we have only considered the convolution operation. However, state-of-the-art CNNs comprise many different types of layers implementing different mathematical operations. Many of those can be easily generalized to consider observation masks. Layers that take the outputs of multiple preceding layers and combine them to a single output,  Here, denotes elementwise multiplication, * convolution, 1/x inversion and "max pool" the max pooling operation. The input feature can be single channel or multi-channel. e.g., by summation, are used frequently in many different network architectures, e.g., summation in inception modules <ref type="bibr" target="#b62">[62]</ref> or skip connections in ResNets <ref type="bibr" target="#b23">[24]</ref> as well as fully convolutional networks <ref type="bibr" target="#b43">[43]</ref>. With additional observation indicators, the summation of input layers for each channel c and location (u, v) can be redefined as a normalized sum over the observed inputs</p><formula xml:id="formula_3">x l f + (x, o) = n l=1 o l x l n l=1 o l (4)</formula><p>where n denotes the number of input layers. If all pixels are observed, this expression simplifies to the standard operation n l=1 x l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Large-Scale Dataset</head><p>Training and evaluating the proposed depth upsampling approach requires access to a large annotated dataset. While evaluation on synthetic datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b54">54]</ref> is possible, it remains an open question if the level of realism attained by such datasets is sufficient to conclude about an algorithm's performance in challenging real-world situations.</p><p>Unfortunately, all existing real-world datasets with sanitized depth ground truth are small in scale. The Middlebury benchmark <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref> provides depth estimates only for a dozen images and only in controlled laboratory conditions. While the Make3D dataset <ref type="bibr" target="#b55">[55]</ref> considers more realistic scenarios, only 500 images of small resolution are provided. Besides, KITTI <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">46]</ref> provides 400 images of street scenes with associated depth ground truth. However, none of these datasets is large enough for end-to-end training of high-capacity deep neural networks.</p><p>For our evaluation, we therefore created a new largescale dataset based on the KITTI raw datasets <ref type="bibr" target="#b14">[15]</ref> which comprises 93k frames with semi-dense depth ground truth. While the KITTI raw datasets provide depth information in the form of raw Velodyne scans, significant manual effort is typically required to remove noise in the laser scans, artifacts due to occlusions (e.g., due to the different centers of projection of the laser scanner and the camera) or reflecting/transparent surfaces in the scene <ref type="bibr" target="#b15">[16]</ref>. It is therefore highly desirable to automate this task.</p><p>In this paper, we propose to remove outliers in the laser scans by comparing the scanned depth to results from a stereo reconstruction approach using semi-global matching <ref type="bibr" target="#b25">[26]</ref>. While stereo reconstructions typically lead to depth bleeding artifacts at object boundaries, LiDaR sensors create streaking artifacts along their direction of motion. To remove both types of outliers, we enforce consistency between laser scans and stereo reconstruction and remove all LiDaR points exhibiting large relative errors. For comparing both measurements, we transform the SGM disparity maps to depth values using KITTI's provided calibration files. We further follow <ref type="bibr" target="#b15">[16]</ref> and accumulate 11 laser scans to increase the density of the generated depth maps. While the environment is mostly static, some of the KITTI sequences comprise dynamic objects, where a laser scan accumulation causes many outliers on dynamic objects. Therefore, we use the SGM depth maps only once to clean the accumulated laser scan projection (instead of cleaning each laser scan separately) in order to remove all outliers in one step: Occlusions, dynamic motion and measurement artifacts. We also observed that most errors due to reflecting and transparent surfaces can be removed with this simple technique as SGM and LiDaR rarely agree in those regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset Evaluation</head><p>Before using the proposed dataset for evaluation in Section 5, we verify its quality. Towards this goal, we exploit the manually cleaned training set of the KITTI 2015 stereo benchmark as reference data. We compute several error measures for our generated depth maps using the provided depth ground truth and compare ourself to the raw and accumulated LiDaR scans as well as the SGM depth maps in <ref type="table">Table 9</ref>. The SGM reconstruction is very dense but also rather inaccurate compared to the raw laser scans. In terms of mean absolute error (MAE) our dataset reaches approximately the same accuracy level as the raw LiDaR KITTI 2015 <ref type="bibr" target="#b46">[46]</ref> Our Dataset Raw LiDaR Acc. LiDaR SGM <ref type="bibr" target="#b25">[26]</ref> RGB Image Error Maps wrt. KITTI 2015 <ref type="bibr" target="#b46">[46]</ref> Figure 3: Large-scale Dataset. Qualitative results of our depth annotated dataset. From left to right we compare: depth maps of the manually curated KITTI 2015 dataset, our automatically generated data, raw and accumulated LiDaR scans, and SGM <ref type="bibr" target="#b25">[26]</ref> results. Differences to the KITTI 2015 depth maps are shown in the last row from 0 (green) to 2 (red) meters.</p><p>scans. However, for the metrics "root mean squared error (RMSE)", "KITTI outliers" (disparity error ≥ 3px and ≥ 5%), as well as the δ inlier ratios (maximal mean relative error of δ i = 1.25 i for i ∈ {1, 2, 3}), our dataset outperforms all baseline results. At the same time, we achieve four times denser depth maps than raw LiDaR scans. A qualitative comparison is presented in <ref type="figure">Fig. 3</ref>. After manually separating the foreground and background regions on the benchmark depth maps, we evaluate the errors present on dynamic objects and background in Table <ref type="bibr" target="#b9">10</ref>. The result indicates that our proposed accumulation and clean-up pipeline is able to remove outliers in the raw LiDaR scans and at the same time significantly increases the density of the data. Qualitatively, we find only little errors in our dataset. Most of the remaining errors are located on dynamic objects or at high distances, cf. <ref type="figure">Fig. 3</ref> (bottom). In comparison, SGM results are inaccurate at large distances and LiDaR scans result in occlusion errors due to the different placement of the LiDaR sensor and the virtual camera used for projection (we use the image plane of the KITTI reference camera for all our experiments). Note that dynamic objects (e.g., car on the left) lead to significant errors in the accumulated LiDaR scans which are largely reduced with our technique.</p><p>For our experimental evaluation in the next section, we split our dataset into 85k images for training, 3k images for testing and 4k images for validation. For all splits we ensure a similar distribution over KITTI scene categories (city, road, residential and campus) while keeping the sequence IDs unique for each split to avoid overfitting to nearby frames. On acceptance of this paper, we will setup an online evaluation server for evaluating the performance of depth upsampling and depth estimation algorithms.</p><p>In the following section, we leverage this dataset for our depth upsampling experiments using the proposed sparse CNN, which is the main focus of this paper.  <ref type="bibr" target="#b46">[46]</ref>. Note that our dataset is generated fully automatically and achieves highest accuracy while providing high density. All metrics are computed in disparity space.  5. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Depth Upsampling</head><p>We investigate the task of depth map completion to evaluate the effect of sparse input data for our Sparse Convolution Modules. For this task, a sparse, irregularly populated depth map from a projected laser scan is completed to full image resolution without any RGB guidance.</p><p>We first evaluate the performance of our method with varying degrees of sparsity in the input. Towards this goal, we leverage the Synthia dataset of Ros et al. <ref type="bibr" target="#b54">[54]</ref> which gives us full control over the sparsity level. To artificially adjust the sparsity of the input, we apply random dropout to the provided dense depth maps during training. The proba-    bility of a pixel to be dropped is set to different levels ranging from 0% to 95%. We train three different variants of a Fully Convolutional Network (FCN) with five convolutional layers of kernel size 11, 7, 5, 3, and 3. Each convolution has a stride of one, 16 output channels, and is followed by a ReLU as nonlinear activation function. The three variants we consider are: i) plain convolutions with only sparse depth as input, ii) plain convolutions with sparse depth and concatenated valid pixel map as input, and iii) the proposed Sparse Convolution Layers, cf. <ref type="figure" target="#fig_2">Fig. 2</ref>. We train separate networks for various levels of sparsity using the Synthia Summer sequences, whereas evaluation is performed on the Synthia Cityscapes dataset. To compare the performance of the different approaches we first evaluate them on the sparsity level they have been trained on. To test the generalization ability of the different models we further apply them to sparsity levels which they have not seen during training. <ref type="figure">Fig. 4</ref> shows our results. We observe that plain convolutions perform poorly with very sparse inputs as all pixels (valid and invalid) are considered in the convolution. This introduces a large degree of randomness during training and testing and results in strong variations in performance. Convolutions on sparse depth maps with the concatenated valid mask perform slightly better than using only the depth input. However, in contrast to our Sparse Convolutions they perform poorly, especially on very sparse input.</p><p>Invariance to the level of sparsity is an important property for depth upsampling methods as it increases robustness towards random perturbations in the data. Besides, this property allows to generalize to different depth sensors such as structured light sensors, PMD cameras or LiDaR scanners. As evidenced by <ref type="figure">Fig. 4</ref>, all methods perform reasonably well at the performance level they have been trained on (diagonal entries) with the sparse convolution variant performing best. However, both baselines fail completely in predicting depth estimates on more sparse and, surprisingly, also on more dense inputs. In contrast, our proposed Sparse Convolution Network performs equally well across all levels of sparsity no matter which sparsity level has been observed during training. This highlights the generalization ability of our approach. <ref type="figure">Fig. 5</ref> shows a qualitative comparison of the generated dense depth maps for the two baselines and our approach using 5% sparsity during training and testing. Note that the input in <ref type="figure">Fig. 5</ref> (a) has been visually enhanced using dilation to improve readability. It thus appears more dense than the actual input to the networks. For the same examples, <ref type="figure" target="#fig_5">Fig. 6</ref> shows the drastic drop in performance when training standard CNNs on 5% and evaluating on 20%, while our approach performs equally well. While ConvNets with input masks lead to noisy results, standard ConvNets even result in a systematic bias as they are unaware of the level of sparsity in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Synthetic-to-Real Domain Adaptation</head><p>To evaluate the domain adaption capabilities of our method, we conduct an experiment where we train on the Synthia dataset and evaluate on our proposed KITTI validation set. <ref type="table" target="#tab_4">Table 3</ref> shows the performance of our network (SparseConv) as well as the two regular CNN baselines using the same number of parameters. Our experiments demonstrate that sparse convolutions perform as well on KITTI as on Synthia, while the dense baselines are not able to adapt to the new input modality and fail completely. We show qualitative results of this experiment in <ref type="figure" target="#fig_6">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to Guided Upsampling</head><p>As discussed in the related work section, several approaches in the literature leverage a high resolution image to guide the depth map completion task which significantly facilitates the problem. Dense color information can be very useful to control the interpolation of sparse depth points, e.g., to distinguish between object boundaries and smooth surfaces. However, relying on camera information in multimodal sensor setups, such as used in e.g. autonomous cars, is not always recommended. Bad weather and night scenes can diminish the benefit of image data or even worsen the result. Therefore, we target an approach which leverages depth as the only input in this paper. In this section, we show that despite not relying on guidance information, our approach performs on par with the state-of-the-art in guided upsampling and even outperforms several methods which use image guidance. <ref type="table" target="#tab_5">Table 4</ref> (top) shows a comparison of several state-of-the-art methods for guided filtering. In particular, we evaluated the methods of Barron et al. <ref type="bibr" target="#b0">[1]</ref>, Schneider et al. <ref type="bibr" target="#b58">[58]</ref>, Ferstl et al. <ref type="bibr" target="#b11">[12]</ref>, and Jampani et al. <ref type="bibr" target="#b29">[30]</ref> which all require a non-sparse RGB image as guidance. For a fair comparison we added the same amount of convolutional layers as we use in our sparse convolutional network for Jampani et al. <ref type="bibr" target="#b30">[31]</ref>. For the other baseline methods we optimized the hyper parameters via grid search on the validation split.</p><p>In addition, we compare our method to several depthonly algorithms in <ref type="table" target="#tab_5">Table 4</ref> (bottom). We first evaluate a simple pooling approach that takes the closest (distance to sensor) valid point to fill in unseen regions within a given window. Second, we apply the Nadaraya-Watson regressor <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b64">64]</ref> using a Gaussian kernel on the sparse depth input. We optimized the hyperparameters of both approaches on the training data. While the first two baselines do not require large amounts of training data, we also compare our method to high-capacity baselines. In particular, we consider a standard ConvNet with and without visibility mask as additional feature channel.</p><p>It is notable that our approach performs comparable to state-of-the-art guided upsampling techniques despite not using any RGB information. In particular, it performs second in terms of RMSE on both validation and test split which we attribute to the Euclidean loss used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Sparsity Evaluation on KITTI</head><p>In the KITTI dataset, a 64-layer laser scanner with a rotational frequency of 10 Hz was used to acquire ground truth for various tasks such as stereo vision and flow estimation. If projected to the image, the depth measurements cover ap-  Simple CNN <ref type="bibr" target="#b7">[8]</ref> Simple CNN ours + mask <ref type="figure">Figure 8</ref>: Quantitative results in MAE (meters) on our depth annotated KITTI subset for varying levels of input density. We compare our unguided approach to several baselines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b58">58]</ref> which leverage RGB guidance for upsampling and two standard convolutional neural networks with and without valid mask concatenated to the input.</p><p>proximately 5 % of the image. For industrial applications such as autonomous driving, often scanners with only 32 or 16 layers and higher frequencies are used. This results in very sparse depth projections. To analyze the impact of extremely sparse information, we evaluate the Sparse Convolutional Network and several baselines with respect to different levels of sparsity on our newly annotated KITTI subset. In particular, we train all networks using all laser measurements and evaluate the performance when varying the density of the input using random dropout. Our results in <ref type="figure">Fig. 8</ref> demonstrate the generalization ability of our network for different levels of sparsity. Regular convolutions as well as several state-of-the-art approaches perform poorly in the presence of sparse inputs. Note that both Barron et al. <ref type="bibr" target="#b0">[1]</ref> and Ferstl et al. <ref type="bibr" target="#b11">[12]</ref> perform slightly better than our method on very sparse data but require a dense high-resolution RGB image for guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semantic Labeling from Sparse Depth</head><p>To demonstrate an output modality different from depth, we also trained the well-known VGG16 architecture <ref type="bibr" target="#b43">[43]</ref> for the task of semantic labeling from sparse depth in- puts. We modify VGG16 by replacing the regular convolutions using our sparse convolution modules. Additionally, we apply the weighted skip connections presented in Section 3.2.1 to generate high-resolution predictions from the small, spatially downsampled FC7 layer, while incorporating visibility masks of the respective network stages. <ref type="table" target="#tab_7">Table 5</ref> shows the mean performance after training on all Synthia "Sequence" frames (left camera to all directions, summer only) and evaluating on the Synthia "Cityscapes" subset 2 . Again, we observe that the proposed sparse convolution module outperforms the two baselines. The comparably small numbers can be explained by the different nature of the validation set which contains more people and also very different viewpoints (bird's eye vs. street-level).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a novel sparse convolution module for handling sparse inputs which can replace regular convolution modules and results in improved performance while generalizing well to novel domains or sparsity levels. Furthermore, we provide a newly annotated dataset with 93k depth annotated images for training and evaluating depth prediction and depth upsampling techniques.</p><p>In future work, we plan to combine the proposed sparse convolution networks with network compression techniques to handle sparse inputs while at the same time being more efficient. We further plan to investigate the effect of sparse irregular inputs for 3D CNNs <ref type="bibr" target="#b53">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Sparsity Invariant CNNs</head><p>September 1, 2017</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence Analysis</head><p>We find that Sparse Convolutions converge much faster than standard convolutions for most input-outputcombinations, especially for those on Synthia with irregularly sparse depth input, as considered in Section 5.1 of the main paper. In <ref type="figure">Figure 9</ref>, we show the mean average error in meters on our validation subset of Synthia over the process of training with identical solver settings (Adam with momentum terms of β 1 = 0.9, β 2 = 0.999 and delta 1e−8). We chose for each variant the maximal learning rate which still causes the network to converge (which turned out to be 1e−3 for all three variants). We find that Sparse Convolutions indeed train much faster and much smoother compared to both ConvNet variants, most likely caused by the explicit ignoring of invalid regions in the update step. Interestingly, the ConvNet variant with concatenated visibility mask in the input converges smoother than the variant with only sparse depth in the input, however, additionally incorporating visibility masks seems to reduce overall performance for the task of depth upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation Detailed Results on Synthia</head><p>Relating to Section 5.3 of the main paper, we show in Table 6 the class-wise IoU for semantic labeling on 5% sparse input data and compare the three proposed VGG-like variants: Convolutions on depth only, convolutions on depth with concatenated visibility mask, and sparse convolutions using depth and visibility mask. We find that sparse convolutions learn to predict also less likely classes, while standard convolutions on such sparse data even struggle to get the most likely classes correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation on Real Depth Maps</head><p>Many recent datasets provide RGB and aligned depth information along with densely annotated semantic labels, such as Cityscapes <ref type="bibr" target="#b4">[5]</ref> and SUN-RGBD <ref type="bibr" target="#b60">[60]</ref>. Many stateof-the-art approaches incorporate depth as well as RGB information in order to achieve highest performance for the task of semantic segmentation <ref type="bibr" target="#b22">[23]</ref>. As the provided depth maps are often not completely dense, we propose to use sparse convolutions on the depth channel instead of filling depth maps artificially and applying dense convolutions afterwards.</p><p>We conduct experiments on SUN-RGBD with only depth maps as input to show the benefit of using sparse convolutions over traditional convolutions. As seen in Section 5.3 of the main paper, sparse convolutions help to incorporate missing depth information in the input for very sparse (5%) depth maps. In <ref type="table">Table 7</ref> we show the performance of a VGG16 (with half the amount of channels than usual) trained from scratch for the task of semantic labeling from (sparse) depth maps. We apply skip connections as used throughout literature <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">44]</ref> up to half the input resolution. We compare performance on the provided raw sparse depth maps (raw, cf. <ref type="figure" target="#fig_0">Figure 11</ref>) as well as a dense depth map version obtained from a special inpainting approach using neighboring frames (filled) on the SUN-RGBD test dataset, as well as the used convolution type (sparse or standard). We find that sparse convolutions perform better than standard convolutions, on both raw and filled depth maps, no matter if a visibility map is concatenated to the input depth map or not. Like reported in <ref type="bibr" target="#b22">[23]</ref>, standard convolutions on the raw depth maps do perform very bad, however, we find that concatenating the visibility map to the input already doubles the achieved performance. A detailed classwise performance analysis can be found in <ref type="table" target="#tab_12">Table 8</ref>. Note that missing information in the input, like missing depth measurements in the SUN-RGBD dataset, does not always cause less information, which we discuss in the following section. This phenomenon boosts methods that explicitly learn convolutions on a visibility mask, such as the two standard convolution networks with concatenated visibility masks. Although we do not explicitly extract features of the visibility masks we still outperform the other convolution variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion: Missing data is not always missing information</head><p>In our experiments we recognized that missing data might sometimes be helpful for certain tasks. Let's consider e.g. digit classification <ref type="bibr" target="#b38">[38]</ref> or shape recognition from 3D CAD models as depicted in <ref type="figure" target="#fig_0">Figure 10</ref>. For both cases the relation between invalid (background) and valid pixels/voxels is indispensable information for the classification. We want to stress that our approach does not tackle  <ref type="figure">Figure 9</ref>: Convergence of the three considered network baselines from Section 5.1 of the main paper for the task of sparse depth upsampling on 5% dense input depth maps from our Synthia train subset.  such cases. Instead it handles cases where unobserved data is irregularly distributed and does not contain additional information. Therefore, the missing data harms the results of the convolution.</p><p>Data from active sensors, such as Time-of-Flight (ToF) cameras used in the SUN-RGBD dataset, is often sparse as shown in <ref type="figure" target="#fig_0">Figure 11</ref>. However, the missing data might contain a pattern if e.g. only certain materials do reflect the emitted light. This might be the reason why the results in <ref type="table">Table 7</ref> show a significant improvement for standard convolutions if the visibility mask is concatenated. Our Sparse Convolution Network does not consider any missing data. Therefore, it might miss information encoded in the visibility mask. Although, the proposed method outperforms the naïve approaches, considering the valid mask explicitly will  <ref type="bibr" target="#b37">[37]</ref> and Graham <ref type="bibr" target="#b19">[20]</ref>.</p><p>likely further improve the performance of our method. <ref type="table">Table 7</ref>: Performance comparison of different input and convolution variants for the task of semantic labeling on (sparse or filled) depth maps from the SUN-RGBD dataset <ref type="bibr" target="#b60">[60]</ref>. All networks are trained from scratch on the training split using 37 classes, performance is evaluated on the test split as mean IoU, cf. <ref type="bibr" target="#b22">[23]</ref>.  <ref type="figure" target="#fig_0">Figure 11</ref>: Active sensors such as ToF cameras might contain missing values because of strongly reflecting surfaces. However, the missing data clearly outlines the shape of certain objects and therefore gives a hint for semantic segmentation. This example is taken from the SUN-RGBD dataset <ref type="bibr" target="#b60">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Dataset Evaluation</head><p>Relating to Section 4.1 of the main paper, we manually extract regions in the image containing dynamic objects in order to compare our dataset's depth map accuracy for foreground and background separately. Various error metrics for the 142 KITTI images with corresponding raw sequences, where we differentiate between the overall average, cf. <ref type="table">Table 9</ref>, as well as foreground and background pixels, cf. <ref type="table" target="#tab_0">Tables 10 and 11</ref>.</p><p>We find that our generated depth maps have a higher accuracy than all other investigated depth maps. Compared to raw LiDaR, our generated depth maps are four times denser and contain five times less outliers in average. Even though we lose almost 50% of the density of the LiDaR accumulation through our cleaning procedure, we achieve almost 20 times less outliers on dynamic objects and even a similar boost also on the static environment. This might be explained through the different noise characteristics in critical regions, e.g. where LiDaR typically blurs in lateral direction on depth edges, SGM usually blurs in longitudinal direction. In comparison to the currently best published stereo algorithm on the KITTI 2015 stereo benchmark web-site <ref type="bibr" target="#b46">[46]</ref>, which achieves 2.48, 3.59, 2.67 KITTI outlier rates for background, foreground and all pixels (anonymous submission, checked on April 18th, 2017), the quality of our depth maps is in the range of 0.23, 2.99, 0.84. Therefore, besides boosting depth estimation from single images (as shown in Section 6), we hope to also boost learned stereo estimation approaches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Depth Upsampling Results</head><p>We show more results of our depth upsampling approach in <ref type="figure" target="#fig_0">Figure 12</ref>. The input data of the Velodyne HDL64 is sparse and randomly distributed when projected to the image. Our approach can handle fine structures while being smooth on flat surfaces. Sparse convolutions internally incorporate sparsity in the input and apply the learned convolutions only to those input pixels with valid depth measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boosting Single-Image Depth Prediction</head><p>As promised in Section 4 of the main paper, we conducted several experiments for a deep network predicting depth maps from a single RGB image, e.g. as done by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">41]</ref> and many more. Due to the lack of training code and to keep this study independent of current research in loss and architecture design, we chose the well-known VGG16 architecture <ref type="bibr" target="#b44">[44]</ref> with weights initialized on the Im-ageNet dataset <ref type="bibr" target="#b35">[35]</ref> and vary only the used ground truth. For a fair comparison, we use the same amount of images and the same sequence frames for all experiments but adapt the depth maps: Our generated dataset (denser than raw LiDaR and even more accurate), sparse LiDaR scans (as used by most approaches for depth prediction on KITTI scenes), as  Sparse Input Sparse Conv. Results Our Dataset <ref type="figure" target="#fig_0">Figure 12</ref>: Further qualitative results of our depth upsampling approach on the KITTI dataset with corresponding sparse depth input and our generated dense depth map dataset. <ref type="table">Table 9</ref>: Evaluation of differently generated depth map variants using the manually annotated ground truth disparity maps of 142 corresponding KITTI benchmark training images <ref type="bibr" target="#b46">[46]</ref>. Best values per metric are highlighted. Cleaned Accumulation describes the output of our automated dataset generation without manual quality assurance, the extension '+ SGM' describes an additional cleaning step of our depth maps with SGM depth maps, applied mainly to remove outliers on dynamic objects. All metrics are computed in the disparity space. well as depth maps from semi-global matching (SGM) <ref type="bibr" target="#b25">[26]</ref>, a common real-time stereo estimation approach, cf. <ref type="table" target="#tab_0">Table  12</ref> (bottom). We evaluate the effect of training with the standard L1 and L2 losses, but do not find large performance differences, cf. <ref type="table" target="#tab_0">Table 12</ref> (top). Also, we compare the difference between an inverse depth representation, as suggested in the literature <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b63">63]</ref>, as well as an absolute metric representation, cf. <ref type="table" target="#tab_0">Table 12 (top)</ref>. Surprisingly, we find that absolute depth values as ground truth representation outperform inverse depth values. We use the best setup (absolute depth with L2 loss due to faster convergence) to evaluate the performance on our test split, where our dataset outperforms the other most promising depth maps from raw LiDaR, cf. <ref type="table" target="#tab_0">Table 12</ref> (bottom). We find that our generated dataset produces visually more pleasant results and especially much less outliers in occluded regions, cf. the car on the left for the second and last row of <ref type="figure" target="#fig_0">Figure 13</ref>. Also, our dense depth maps seem to help the networks to generalize better to unseen areas, such as the upper half of the image. We hope that our dataset will be used in the future to further boost performance for this challenging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Lidar</head><p>Our Dataset Groundtruth Output Groundtruth Output <ref type="figure" target="#fig_0">Figure 13</ref>: Raw Lidar vs our dataset as training data for depth from mono: Qualitative examples of the depth-frommono CNN trained on our generated dense and outlier-cleaned dataset in contrast to the sparse raw LiDaR data. It becomes apparent that denser training data leads to improved results e.g. in the upper half of the image and at object boundaries (where most LiDaR outliers occur). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Depth Map Completion. Using sparse, irregular depth measurements (a) as inputs leads to noisy results when processed with standard CNNs (c). In contrast, our method (d) predicts smooth and accurate depth maps by explicitly considering sparsity during convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sparse Convolutional Network. (a) The input to our network is a sparse depth map (yellow) and a binary observation mask (red). It passes through several sparse convolution layers (dashed) with decreasing kernel sizes from 11×11 to 3 × 3. (b) Schematic of our sparse convolution operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Comparison of three different networks on the Synthia dataset<ref type="bibr" target="#b54">[54]</ref> while varying the sparsity level of the training split (left) and the sparsity of the test split (top). From left-to-right: ConvNet, ConvNet with concatenated validity mask and the proposed SparseConvNet. All numbers represent mean average errors (MAE).(a) Input (visually enhanced) (b) ConvNet (c) ConvNet + mask (d) SparseConvNet (ours) (e) Groundtruth Qualitative comparison of our sparse convolutional network to standard ConvNets on Synthia [54], trained and evaluated at 5% sparsity. (b) Standard ConvNets suffer from large invalid regions in the input leading to noisy results. (c) Using a valid mask as input reduces noise slightly. (d) In contrast, our approach predicts smooth and accurate outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Network predictions for scenes in Figs. 1 and 5, with all networks trained at 5% sparsity and evaluated at 20% sparsity. While ConvNets with and without visibility mask produce substantially worse results, the results of the proposed sparsity invariant CNN do not degrade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison of the best network variants from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>VGG -Depth Only 27 .</head><label>27</label><figDesc>1 30.3 25.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6.4 VGG -Depth + Mask 20.9 27.8 14.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4.9 VGG -Sparse Convolutions 95.3 59.0 33.0 17.2 1.0 60.5 28.7 33.0 12.5 35.6 6.1 0.5 22.4 31.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Missing data sometimes contains useful information as in the example of handwritten digit classification or 3D CAD model classification. Examples are taken from LeCun et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Depth, Mask concat. 59.4 80.2 28.7 53.3 49.0 37.3 42.9 2.7 21.7 17.7 7.3 22.3 0.0 5.2 0.9 35.6 11.4 21.4 14.0 0.0 6.3 34.8 10.0 6.4 4.6 0.1 8.6 0.0 4.7 12.1 2.7 0.1 35.0 30.2 9.2 23.3 2.7 19.0 Conv. Filled Depth, Mask concat. 59.9 81.6 29.2 52.5 50.7 38.6 42.6 0.6 15.3 16.9 11.1 17.0 0.1 0.5 0.2 20.5 12.5 11.3 16.2 0.0 4.0 38.0 18.9 4.3 5.4 0.0 5.5 0.0 3.6 15.7 0.0 9.3 32.9 27.4 17.0 29.9 0.6 18.6 Sparse Conv. Raw Depth 60.1 80.7 26.9 54.2 50.3 34.7 40.5 9.3 22.0 11.0 10.0 16.6 4.0 8.5 3.0 20.7 10.7 23.2 17.9 0.0 3.8 44.5 10.2 6.2 6.9 2.5 5.2 4.6 5.0 15.3 1.2 2.8 42.9 31.6 11.2 26.4 3.0 19.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of reference depth maps using the manually curated ground truth depth maps of the KITTI 2015 training set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation ofTable 9split according to foreground (car) / background (non-car) regions.</figDesc><table><row><cell>Depth Map</cell><cell>MAE RMSE KITTI [px] [px] outliers</cell><cell>δ1</cell><cell>δi inlier rates δ2</cell><cell>δ3</cell></row><row><cell>SGM Raw LiDaR Acc. LiDaR Our Dataset</cell><cell cols="4">1.2/1.1 3.0/2.8 5.9/4.4 97.6 /96.9 98.2 /98.7 98.5/99.3 3.7/0.2 10.0/1.9 17.4/0.9 84.3 /99.3 86.1 /99.6 88.6/99.7 7.7/1.1 12.0/4.8 59.7/4.3 55.7 /96.7 73.7 /98.0 83.0/98.8 0.9/0.3 2.2/0.8 3.0/0.2 98.6/99.8 99.0/99.9 99.3/99.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison (MAE) of different methods trained on different sparsity levels on Synthia and evaluated on our newly proposed KITTI depth dataset.</figDesc><table><row><cell>Sparsity at train:</cell><cell>5% 10% 20% 30% 40% 50% 60% 70%</cell></row><row><cell>ConvNet ConvNet+mask SparseConvNet</cell><cell>16.03 13.48 10.97 8.437 10.02 9.73 9.57 9.90 16.18 16.44 16.54 16.16 15.64 15.27 14.62 14.11 0.722 0.723 0.732 0.734 0.733 0.731 0.731 0.730</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of different methods on our KITTI depth dataset. Our method performs comparable to state-of-the-art methods that incorporate RGB (top), while outperforming all depth-only variants (bottom).</figDesc><table><row><cell>Method</cell><cell cols="2">MAE [m] val test</cell><cell cols="2">RMSE [m] val test</cell></row><row><cell>Bilateral NN [30] SGDU [58] Fast Bilateral Solver [1] TGVL [12]</cell><cell>1.09 0.72 0.65 0.59</cell><cell>1.09 0.57 0.52 0.46</cell><cell>4.19 2.5 1.98 4.85</cell><cell>5.233 2.02 1.75 4.08</cell></row><row><cell>Closest Depth Pooling Nadaraya Watson [47, 64] ConvNet ConvNet + mask SparseConvNet (ours)</cell><cell>0.94 0.74 0.78 0.79 0.68</cell><cell>0.68 0.66 0.62 0.62 0.54</cell><cell>2.77 2.99 2.97 2.24 2.01</cell><cell>2.30 2.86 2.69 1.94 1.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>trained on synthetic Synthia<ref type="bibr" target="#b54">[54]</ref> and evaluated on the proposed real-world KITTI depth dataset. While our SparseConvNet adapts well to the novel domain, standard convolutional neural networks fail completely in recovering sensible depth information.</figDesc><table><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>[1]</cell><cell>[44]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[22]</cell><cell></cell></row><row><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAE [meters]</cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6 Density [%]</cell><cell>0.8</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>IoU performance of different network variants on the Synthia Cityscapes subset after training on all Synthia sequences (mean over all 15 known classes).</figDesc><table><row><cell>Network</cell><cell>IoU [%]</cell></row><row><cell>VGG -Depth Only VGG -Depth + Mask VGG -Sparse Convolutions</cell><cell>6.4 4.9 31.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of the class-level performance for pixel-level semantic labeling on our Synthia validation split subset ('Cityscapes') after training on all Synthia 'Sequence' subsets using the Intersection over Union (IoU) metric. All numbers are in percent and larger is better.</figDesc><table><row><cell>sky</cell><cell>building</cell><cell>road</cell><cell>sidewalk</cell><cell>fence</cell><cell>vegetation</cell><cell>pole</cell><cell>car</cell><cell>traffic sign</cell><cell>pedestrian</cell><cell>bicycle</cell><cell>lanemarking</cell><cell>traffic light</cell><cell>mean IoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Evaluation as inTable 9but only for Foreground pixels.</figDesc><table><row><cell>Depth Map</cell><cell>MAE RMSE</cell><cell>KITTI outliers</cell><cell>δ1</cell><cell>δi inlier rates δ2 δ3</cell></row><row><cell>SGM Raw LiDaR Acc. LiDaR Cleaned Acc.</cell><cell>1.23 3.72 10.02 2.98 7.73 12.01 0.88 2.15</cell><cell cols="3">5.91 17.36 84.29 86.11 88.56 97.6 98.2 98.5 59.73 55.67 73.73 83.04 2.99 98.55 98.96 99.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Evaluation as inTable 9but only for Background pixels.</figDesc><table><row><cell>Depth Map</cell><cell cols="2">MAE RMSE</cell><cell>KITTI outliers</cell><cell>δ1</cell><cell>δi inlier rates δ2 δ3</cell></row><row><cell>SGM Raw LiDaR Acc. LiDaR Cleaned Acc.</cell><cell>1.05 0.22 1.09 0.34</cell><cell>2.77 1.90 4.81 0.77</cell><cell>4.36 0.94 4.25 0.23</cell><cell cols="2">96.93 98.72 99.27 99.25 99.56 99.73 96.74 97.99 98.78 99.83 99.94 99.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Evaluation of the class-level performance for pixel-level semantic labeling on Synthia Cityscapes subset after training on all Synthia Sequence subsets using the Intersection over Union (IoU) metric. All numbers are in percent and larger is better. Our sparse convolutions outperform the other variants on 18 classes, standard convolutions on filled depth with concatenated visibility mask outperform the others on 11 classes, and on 8 classes standard convolutions on raw depth with concatenated mask perform best.</figDesc><table><row><cell>wall</cell><cell>floor</cell><cell>cabinet</cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell>table</cell><cell>door</cell><cell>window</cell><cell>bookshelf</cell><cell>picture</cell><cell>counter</cell><cell>blinds</cell><cell>desk</cell><cell>shelves</cell><cell>curtain</cell><cell>dresser</cell><cell>pillow</cell><cell>mirror</cell><cell>floor mat</cell><cell>clothes</cell><cell>ceiling</cell><cell>books</cell><cell>fridge</cell><cell>tv</cell><cell>paper</cell><cell>towel</cell><cell>shower curtain</cell><cell>box</cell><cell>whiteboard</cell><cell>person</cell><cell>night stand</cell><cell>toilet</cell><cell>sink</cell><cell>lamp</cell><cell>bathtub</cell><cell>bag</cell><cell>mean IoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Evaluation of different depth ground truth and loss variants (top) used for training a VGG16 on singleimage depth prediction. L1 and L2 loss achieve comparable performance, while absolute depth representation for training instead of inverse depth performs significantly better. We compare performance on our generated validation and test split, as well as 142 ground truth depth maps from KITTI 2015<ref type="bibr" target="#b46">[46]</ref> for the best performing setup with L2 loss on absolute depth (bottom).</figDesc><table><row><cell>Depth Maps</cell><cell>Loss</cell><cell>Inverse Depth?</cell><cell>val</cell><cell>MAE test</cell><cell>KITTI'15</cell><cell>val</cell><cell>RMSE test</cell><cell>KITTI'15</cell></row><row><cell>Our Dataset Our Dataset Our Dataset Our Dataset</cell><cell>L2 L1 L2 L1</cell><cell>yes yes no no</cell><cell>2.980 2.146 2.094 2.069</cell><cell></cell><cell></cell><cell>6.748 4.743 3.634 3.670</cell><cell></cell><cell></cell></row><row><cell>Our Dataset Raw LiDaR Scans SGM</cell><cell>L2 L2 L2</cell><cell>no no no</cell><cell>2.094 2.184 3.278</cell><cell>1.913 1.940 2.973</cell><cell>1.655 1.790 3.652</cell><cell>3.634 3.942 5.826</cell><cell>3.266 3.297 4.811</cell><cell>3.275 3.610 8.927</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We map all unknown classes in the validation set to corresponding classes in the training set and ignore all other unknown classes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="617" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Noise-Aware Filter for Real-Time Depth Upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Buisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno>1611.07759</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An application of markov random fields to range sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Upsampling Range Data in Dynamic Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>scale convolutional architecture. arXiv.org, 1411.4734</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruether</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perforatedcnns: Acceleration through elimination of redundant convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ibraimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Super-Resolution from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Sparse 3D convolutional neural networks. Proc. of the British Machine Vision Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the British Machine Vision Conf. (BMVC)</title>
		<meeting>of the British Machine Vision Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense disparity maps from sparse disparity measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleinsteuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusionbased cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Asian Conf. on Computer Vision (ACCV)</title>
		<meeting>of the Asian Conf. on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Warped convolutions: Efficient invariance to spatial transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>arXiv.org, 1609.04382</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth super resolution by rigid body self-similarity in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hornacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth map super-resolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense CRFs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mask-specific inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the German Conference on Pattern Recognition (GCPR)</title>
		<meeting>of the German Conference on Pattern Recognition (GCPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Joint Bilateral Upsampling. ACM Trans. on Graphics (SIG-GRAPH)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TI-POOLING: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint geodesic upsampling of depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Brostow. Patch based synthesis for single depth image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On estimating regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nadaraya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability and its Applications</title>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High quality depth map upsampling for 3d-tof cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Holistic sparsecnn: Forging the trident of accuracy, speed, and size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno>arXiv.org, 1608.01409</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A deep primal-dual network for guided depth super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08569</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ATGV-net: Accurate depth super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Make3D: learning 3D scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nesic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the German Conference on Pattern Recognition (GCPR)</title>
		<meeting>of the German Conference on Pattern Recognition (GCPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the German Conference on Pattern Recognition (GCPR)</title>
		<meeting>of the German Conference on Pattern Recognition (GCPR)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep depth super-resolution: Learning depth super-resolution using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Asian Conf. on Computer Vision (ACCV)</title>
		<meeting>of the Asian Conf. on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>2017. 15</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Smooth regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sankhyā</title>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Spatial-Depth Super Resolution for Range Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Interponet</surname></persName>
		</author>
		<title level="m">A brain inspired neural network for optical flow dense interpolation. arXiv.org, 1611.09803</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
