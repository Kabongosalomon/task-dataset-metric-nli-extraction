<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
						</author>
						<title level="a" type="main">SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce SalsaNext for the uncertainty-aware semantic segmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next version of SalsaNet [1] which has an encoder-decoder architecture where the encoder unit has a set of ResNet blocks and the decoder part combines upsampled features from the residual blocks. In contrast to SalsaNet, we introduce a new context module, replace the ResNet encoder blocks with a new residual dilated convolution stack with gradually increasing receptive fields and add the pixel-shuffle layer in the decoder. Additionally, we switch from stride convolution to average pooling and also apply central dropout treatment. To directly optimize the Jaccard index, we further combine the weighted cross entropy loss with Lovász-Softmax loss <ref type="bibr" target="#b1">[2]</ref>. We finally inject a Bayesian treatment to compute the epistemic and aleatoric uncertainties for each point in the cloud. We provide a thorough quantitative evaluation on the Semantic-KITTI dataset <ref type="bibr" target="#b2">[3]</ref>, which demonstrates that the proposed SalsaNext outperforms other state-of-theart semantic segmentation networks and ranks first on the Semantic-KITTI leaderboard. We also release our source code https://github.com/TiagoCortinhal/SalsaNext.</p><p>• To capture the global context information in the full arXiv:2003.03653v3 [cs.CV] 9 Jul 2020</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene understanding is an essential prerequisite for autonomous vehicles. Semantic segmentation helps gaining a rich understanding of the scene by predicting a meaningful class label for each individual sensory data point. Achieving such a fine-grained semantic prediction in real-time accelerates reaching the full autonomy to a great extent.</p><p>Safety-critical systems, such as self-driving vehicles, however, require not only highly accurate but also reliable predictions with a consistent measure of uncertainty. This is because the quantitative uncertainty measures can be propagated to the subsequent units, such as decision making modules to lead to safe manoeuvre planning or emergency braking, which is of utmost importance in safety-critical systems. Therefore, semantic segmentation predictions integrated with reliable confidence estimates can significantly reinforce the concept of safe autonomy.</p><p>Advanced deep neural networks recently had a quantum jump in generating accurate and reliable semantic segmentation with real-time performance. Most of these approaches, however, rely on the camera images <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, whereas relatively fewer contributions have discussed the semantic segmentation of 3D LiDAR data <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The main reason is that unlike camera images which provide dense</p><p>The research leading to these results has received funding from the Vinnova FFI project SHARPEN, under grant agreement no. 2018-05001. <ref type="bibr" target="#b0">1</ref> Halmstad University, School of Information Technology, Center for Applied Intelligent Systems Research, Halmstad, Sweden.</p><p>2 Volvo Technology AB, Volvo Group Trucks Technology, Vehicle Automation, Gothenburg, Sweden. <ref type="figure">Fig. 1</ref>. Mean IoU versus runtime plot for the state-of-the-art 3D point cloud semantic segmentation networks on the Semantic-KITTI dataset <ref type="bibr" target="#b2">[3]</ref>. Inside parentheses are given the total number of network parameters in Millions. All deep networks visualized here use only 3D LiDAR point cloud data as input. Note that only the published methods are considered. measurements in a grid-like structure, LiDAR point clouds are relatively sparse, unstructured, and have non-uniform sampling, although LiDAR scanners have a wider field of view and return more accurate distance measurements.</p><p>As comprehensively described in <ref type="bibr" target="#b7">[8]</ref>, there exists two mainstream deep learning approaches addressing the semantic segmentation of 3D LiDAR data only: point-wise and projection-based neural networks (see <ref type="figure">Fig. 1</ref>). The former approach operates directly on the raw 3D points without requiring any pre-processing step, whereas the latter projects the point cloud into various formats such as 2D image view or high-dimensional volumetric representation. As illustrated in <ref type="figure">Fig. 1</ref>, there is a clear split between these two approaches in terms of accuracy, runtime and memory consumption. For instance, projection-based approaches (shown in green circles in <ref type="figure">Fig. 1</ref>) achieve the state-of-the-art accuracy while running significantly faster. Although point-wise networks (red squares) have slightly lower number of parameters, they cannot efficiently scale up to large point sets due to the limited processing capacity, thus, they take a longer runtime. It is also highly important to note that both point-wise and projection-based approaches in the literature lack uncertainty measures, i.e. confidence scores, for their predictions.</p><p>In this work, we introduce a novel neural network architecture to perform uncertainty-aware semantic segmentation of a full 3D LiDAR point cloud in real-time. Our proposed network is built upon the SalsaNet model <ref type="bibr" target="#b0">[1]</ref>, hence, named SalsaNext. The SalsaNet model has an encoder-decoder skeleton where the encoder unit consists of a series of ResNet blocks and the decoder part upsamples and fuses features extracted in the residual blocks. In the here proposed SalsaNext, our contributions lie in the following aspects: 360 • LiDAR scan, we introduce a new context module before encoder, which consists of a residual dilated convolution stack fusing receptive fields at various scales. • To increase the receptive field, we replaced the ResNet block in the encoder with a novel combination of a set of dilated convolutions (with a rate of 2) each of which has different kernel sizes <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7)</ref>. We further concatenated the convolution outputs and combined with residual connections yielding a branch-like structure. • To avoid any checkerboard artifacts in the upsampling process, we replaced the transposed convolution layer in the SalsaNet decoder with a pixel-shuffle layer <ref type="bibr" target="#b8">[9]</ref> which directly leverages on the feature maps to upsample the input with less computation. • To boost the roles of very basic features (e.g. edges and curves) in the segmentation process, the dropout treatment was altered by omitting the first and last network layers in the dropout process. • To have a lighter model, average pooling was employed instead of having stride convolutions in the encoder. • To enhance the segmentation accuracy by optimizing the mean intersection-over-union score, i.e. the Jaccard index, the weighted cross entropy loss in SalsaNet was combined with the Lovász-Softmax loss <ref type="bibr" target="#b1">[2]</ref>. • To further estimate the epistemic (model) and aleatoric (observation) uncertainties for each 3D LiDAR point, the deterministic SalsaNet model was transformed into a stochastic format by applying the Bayesian treatment. All these contributions form the here introduced SalsaNext model which is the probabilistic derivation of the SalsaNet with a significantly better segmentation performance. The input of SalsaNext is the rasterized image of the full LiDAR scan, where each image channel stores position, depth, and intensity cues in the panoramic view format. The final network output is the point-wise classification scores together with uncertainty measures.</p><p>To the best of our knowledge, this is the first work showing the both epistemic and aleatoric uncertainty estimation on the LiDAR point cloud segmentation task. Computing both uncertainties is of utmost importance in safe autonomous driving since the epistemic uncertainty can indicate the limitation of the segmentation model while the aleatoric one highlights the sensor observation noises for segmentation.</p><p>Quantitative and qualitative experiments on the Semantic-KITTI dataset <ref type="bibr" target="#b2">[3]</ref> show that the proposed SalsaNext significantly outperforms other state-of-the-art networks in terms of pixel-wise segmentation accuracy while having much fewer parameters, thus requiring less computation time. SalsaNext ranks first place on the Semantic-KITTI leaderboard.</p><p>Note that we also release our source code and trained model to encourage further research on the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, recent works in semantic segmentation of 3D point cloud data will be summarized. This will then be followed by a brief review of the literature related to Bayesian neural networks for uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation of 3D Point Clouds</head><p>Recently, great progress has been achieved in semantic segmentation of 3D LiDAR point clouds using deep neural networks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The core distinction between these advanced methods lies not only in the network design but also in the representation of the point cloud data.</p><p>Fully convolutional networks <ref type="bibr" target="#b11">[12]</ref>, encoder-decoder structures <ref type="bibr" target="#b12">[13]</ref>, and multi-branch models <ref type="bibr" target="#b4">[5]</ref>, among others, are the mainstream network architectures used for semantic segmentation. Each network type has a unique way of encoding features at various levels, which are then fused to recover the spatial information. Our proposed SalsaNext follows the encoder-decoder design as it showed promising performance in most state-of-the-art methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Regarding the representation of unstructured and unordered 3D LiDAR points, there are two common approaches as depicted in <ref type="figure">Fig. 1</ref>: point-wise representation and projection-based rendering. We refer the interested readers to <ref type="bibr" target="#b7">[8]</ref> for more details on the 3D data representation.</p><p>Point-wise methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> directly process the raw irregular 3D points without applying any additional transformation or pre-processing. Shared multi-layer perceptronbased PointNet <ref type="bibr" target="#b14">[15]</ref>, the subsequent work PointNet++ <ref type="bibr" target="#b15">[16]</ref>, and superpoint graph SPG networks <ref type="bibr" target="#b16">[17]</ref> are considered in this group. Although such methods are powerful on small point clouds, their processing capacity and memory requirement, unfortunately, becomes inefficient when it comes to the full 360 • LiDAR scans. To accelerate point-wise operations, additional cues, e.g. from camera images, are employed as successfully introduced in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Projection-based methods instead transform the 3D point cloud into various formats such as voxel cells <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, multi-view representation <ref type="bibr" target="#b20">[21]</ref>, lattice structure <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and rasterized images <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In the multi-view representation, a 3D point cloud is projected onto multiple 2D surfaces from various virtual camera viewpoints. Each view is then processed by a multi-stream network as in <ref type="bibr" target="#b20">[21]</ref>. In the lattice structure, the raw unorganized point cloud is interpolated to a permutohedral sparse lattice where bilateral convolutions are applied to occupied lattice sectors only <ref type="bibr" target="#b21">[22]</ref>. Methods relying on the voxel representation discretize the 3D space into 3D volumetric space (i.e. voxels) and assign each point to the corresponding voxel <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Sparsity and irregularity in point clouds, however, yield redundant computations in voxelized data since many voxel cells may stay empty. A common attempt to overcome the sparsity in LiDAR data is to project 3D point clouds into 2D image space either in the top-down Bird-Eye-View <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> or spherical Range-View (RV) (i.e. panoramic view) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b10">[11]</ref> formats. Unlike point-wise and other projection-based approaches, such 2D rendered image representations are more compact, dense and computationally cheaper as they can be processed by standard 2D convolutional layers. Therefore, our SalsaNext model initially projects the LiDAR point cloud into 2D RV image generated by mapping each 3D point onto a spherical surface.</p><p>Note that in this study we focus on semantic segmentation of LiDAR-only data and thus ignore multi-model approaches that fuse, e.g. LiDAR and camera data as in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Uncertainty Prediction with Bayesian Neural Networks</head><p>Bayesian Neural Networks (BNNs) learn approximate distribution on the weights to further generate uncertainty estimates, i.e. prediction confidences. There are two types of uncertainties: Aleatoric which can quantify the intrinsic uncertainty coming from the observed data, and epistemic where the model uncertainty is estimated by inferring with the posterior weight distribution, usually through Monte Carlo sampling. Unlike aleatoric uncertainty, which captures the irreducible noise in the data, epistemic uncertainty can be reduced by gathering more training data. For instance, segmenting out an object that has relatively fewer training samples in the dataset may lead to high epistemic uncertainty, whereas high aleatoric uncertainty may rather occur on segment boundaries or distant and occluded objects due to noisy sensor readings which are inherent in sensors. Bayesian modelling helps estimating both uncertainty types.</p><p>Gal et al. <ref type="bibr" target="#b27">[28]</ref> proved that dropout can be used as a Bayesian approximation to estimate the uncertainty in classification, regression and reinforcement learning tasks while this idea was also extended to semantic segmentation of RGB images by Kendall et al. <ref type="bibr" target="#b3">[4]</ref>. Loquercio et al. <ref type="bibr" target="#b28">[29]</ref> proposed a framework which extends the dropout approach by propagating the uncertainty that is produced from the sensors through the activation functions without the need of retraining. Recently, both uncertainty types were applied to 3D point cloud object detection <ref type="bibr" target="#b29">[30]</ref> and optical flow estimation <ref type="bibr" target="#b30">[31]</ref> tasks. To the best of our knowledge, BNNs have not been employed in modeling the uncertainty of semantic segmentation of 3D LiDAR point clouds, which is one of the main contributions in this work.</p><p>In this context, the closest work to ours is <ref type="bibr" target="#b31">[32]</ref> which introduces a probabilistic embedding space for point cloud instance segmentation. This approach, however, captures neither the aleatoric nor the epistemic uncertainty but rather predicts the uncertainty between the point cloud embeddings. Unlike our method, it has also not been shown how the aforementioned work can scale up to large and complex LiDAR point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we give a detailed description of our method starting with the point cloud representation. We then continue with the network architecture, uncertainty estimation, loss function, and training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LiDAR Point Cloud Representation</head><p>As in <ref type="bibr" target="#b6">[7]</ref>, we project the unstructed 3D LiDAR point cloud onto a spherical surface to generate the LIDAR's native Range View (RV) image. This process leads to dense and compact point cloud representation which allows standard convolution operations.</p><p>In the 2D RV image, each raw LiDAR point (x, y, z) is mapped to an image coordinate (u, v) as</p><formula xml:id="formula_0">u v = 1 2 [1 − arctan(y, x)π −1 ]w [1 − (arcsin(z, r −1 ) + f down )f −1 ]h ,</formula><p>where h and w denote the height and width of the projected image, r represents the range of each point as r = x 2 + y 2 + z 2 and f defines the sensor vertical field of view as f = |f down | + |f up |.</p><p>Following the work of <ref type="bibr" target="#b6">[7]</ref>, we considered the full 360 • field-of-view in the projection process. During the projection, 3D point coordinates (x, y, z), the intensity value (i) and the range index (r) are stored as separate RV image channels. This yields a [w × h × 5] image to be fed to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>The architecture of the proposed SalsaNext is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The input to the network is an RV image projection of the point cloud as described in section III-A.</p><p>SalsaNext is built upon the base SalsaNet model <ref type="bibr" target="#b0">[1]</ref> which follows the standard encoder-decoder architecture with a bottleneck compression rate of 16. The original SalsaNet encoder contains a series of ResNet blocks <ref type="bibr" target="#b32">[33]</ref> each of which is followed by dropout and downsampling layers. The decoder blocks apply transpose convolutions and fuse upsampled features with that of the early residual blocks via skip connections. To further exploit descriptive spatial cues, a stack of convolution is inserted after the skip connection. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, we in this study improve the base structure of SalsaNet with the following contributions:</p><p>Contextual Module: One of the main issues with the semantic segmentation is the lack of contextual information throughout the network. The global context information gathered by larger receptive fields plays a crucial role in learning complex correlations between classes <ref type="bibr" target="#b4">[5]</ref>. To aggregate the context information in different regions, we place a residual dilated convolution stack that fuses a larger receptive field with a smaller one by adding 1 × 1 and 3 × 3 kernels right at the beginning of the network. This helps us capture the global context alongside with more detailed spatial information.</p><p>Dilated Convolution: Receptive fields play a crucial role in extracting spatial features. A straightforward approach to capture more descriptive spatial features would be to enlarge the kernel size. This has, however, a drawback of increasing the number of parameters drastically. Instead, we replace the ResNet blocks in the original SalsaNet encoder with a novel combination of a set of dilated convolutions having effective receptive fields of 3, 5 and 7 (see Block I in <ref type="figure" target="#fig_0">Fig. 2</ref>). We further concatenate each dilated convolution output and apply a 1 × 1 convolution followed by a residual connection in order to let the network exploit more information from the fused features coming from various depths in the receptive field. Each of these new residual dilated convolution blocks (i.e. Block I) is followed by dropout and pooling layers as depicted in Block II in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Pixel-Shuffle Layer: The original SalsaNet decoder involves transpose convolutions which are computationally expensive layers in terms of number of parameters. We replace these standard transpose convolutions with the pixelshuffle layer <ref type="bibr" target="#b8">[9]</ref> (see Block III in <ref type="figure" target="#fig_0">Fig. 2</ref>) which leverages on the learnt feature maps to produce the upsampled feature maps by shuffling the pixels from the channel dimension to the spatial dimension. More precisely, the pixel-shuffle operator reshapes the elements of (H × W × Cr 2 ) feature map to a form of (Hr × W r × C), where H, W, C, and r represent the height, width, channel number and upscaling ratio, respectively.</p><p>We additionally double the filters in the decoder side and concatenate the pixel-shuffle outputs with the skip connection (Block IV in <ref type="figure" target="#fig_0">Fig. 2</ref>) before feeding them to the dilated convolutional blocks (Block V in <ref type="figure" target="#fig_0">Fig. 2</ref>) in the decoder.</p><p>Central Encoder-Decoder Dropout: As shown by quantitative experiments in <ref type="bibr" target="#b3">[4]</ref>, inserting dropout only to the central encoder and decoder layers results in better segmentation performance. It is because the lower network layers extract basic features such as edges and corners <ref type="bibr" target="#b33">[34]</ref> which are consistent over the data distribution and dropping out these layers will prevent the network to properly form the higher level features in the deeper layers. Central dropout approach eventually leads to higher network performance. We, therefore, insert dropout in every encoder-decoder layer except the first and last one highlighted by dashed edges in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Average Pooling: In the base SalsaNet model the downsampling was performed via a strided convolution which introduces additional learning parameters. Given that the down-sampling process is relatively straightforward, we hypothesize that learning at this level would not be needed. Thus, to allocate less memory SalsaNext switches to average pooling for the downsampling.</p><p>All these contributions from the proposed SalsaNext network. Furthermore, we applied a 1 × 1 convolution after the decoder unit to make the channel numbers the same with the total number of semantic classes. The final feature map is finally passed to a soft-max classifier to compute pixelwise classification scores. Note that each convolution layer in the SalsaNext model employs a leaky-ReLU activation function and is followed by batch normalization to solve the internal covariant shift. Dropout is then placed after the batch normalization. It can, otherwise, result in a shift in the weight distribution which can minimize the batch normalization effect during training as shown in <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Uncertainty Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Heteroscedastic Aleatoric Uncertainty:</head><p>We can define aleatoric uncertainty as being of two kinds: homoscedastic and heteroscedastic. The former defines the type of aleatoric uncertainty that remains constant given different input types, whereas the later may rather differ for different types of input. In the LiDAR semantic segmentation task, distant points might introduce a heteroscedastic uncertainty as it is increasingly difficult to assign them to a single class. The same kind of uncertainty is also observable in the object edges when performing semantic segmentation, especially when the gradient between the object and the background is not sharp enough.</p><p>LiDAR observations are usually corrupted by noise and thus the input that a neural network is processing is a noisy version of the real world. Assuming that the sensor's noise characteristic is known (e.g. available in the sensor data sheet), the input data distribution can be expressed by the <ref type="figure">normal N (x, v)</ref>, where x represents the observations and v the sensor's noise. In this case, the aleatoric uncertainty can be computed by propagating the noise through the network via Assumed Density Filtering (ADF). This approach was initially applied by Gast et al. <ref type="bibr" target="#b35">[36]</ref>, where the network's activation functions including input and output were replaced by probability distributions. A forward pass in this ADF-based modified neural network finally generates output predictions µ with their respective aleatoric uncertainties σ A .</p><p>2) Epistemic Uncertainty: In SalsaNext, the epistemic uncertainty is computed using the weight's posterior p(W|X, Y) which is intractable and thus impossible to present analytically. However, the work in <ref type="bibr" target="#b27">[28]</ref> showed that dropout can be used as an approximation to the intractable posterior. More specifically, dropout is an approximating distribution q θ (ω) to the posterior in a BNN with L layers, ω = [W l ] L l=1 where θ is a set of variational parameters. The optimization objective function can be written as:</p><formula xml:id="formula_1">L M C (θ) = − 1 M i∈S log p(y i |f ω (x i )) + 1 N KL(q θ ||p(ω))</formula><p>where the KL denotes the regularization from the Kullback-Leibler divergence, N is the number of data samples, S holds a random set of M data samples, y i denotes the ground-truth, f ω (x i ) is the output of the network for x i input with weight parameters ω and p(y i |f ω (x i )) likelihood. The KL term can be approximated as:</p><formula xml:id="formula_2">KL(q M (W)||p(W)) ∝ i 2 (1 − p) 2 ||M|| 2 − KH(p) where H(p) := −p log(p) − (1 − p) log(1 − p)</formula><p>represents the entropy of a Bernoulli random variable with probability p and K is a constant to balance the regularization term with the predictive term. For example, the negative log likelihood in this case will be estimated as</p><formula xml:id="formula_3">− log p(y i |f ω (x i )) ∝ 1 2 log σ + 1 2σ ||y i − f ω (x i )|| 2</formula><p>for a Gaussian likelihood with σ model's uncertainty.</p><p>To be able to measure the epistemic uncertainty, we employ a Monte Carlo sampling during inference: we run n trials and compute the average of the variance of the n predicted outputs:</p><formula xml:id="formula_4">Var epistemic p(y|f ω (x)) = σ epistemic = 1 n n i=1 (y i −ŷ) 2 .</formula><p>As introduced in <ref type="bibr" target="#b28">[29]</ref>, the optimal dropout rate p which minimizes the KL divergence, is estimated for an already trained network by applying a grid search on a log-range of a certain number of possible rates in the range [0, 1]. In practice, it means that the optimal dropout rates p will minimize: p = arg min p d∈D</p><formula xml:id="formula_5">1 2 log(σ d tot ) + 1 2σ d tot (y d − y d pred (p)) 2 ,</formula><p>where σ tot denotes the total uncertainty by summing the aleatoric and the epistemic uncertainty, D is the input data, y d pred (p) and y d are the predictions and labels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function</head><p>Datasets with imbalanced classes introduce a challenge for neural networks. Take an example of a bicycle or traffic sign which appears much less compared to the vehicles in the autonomous driving scenarios. This makes the network more biased towards to the classes that emerge more in the training data and thus yields significantly poor network performance.</p><p>To cope with the imbalanced class problem, we follow the same strategy in SalsaNet and add more value to the underrepresented classes by weighting the softmax cross-entropy loss L wce with the inverse square root of class frequency as</p><formula xml:id="formula_6">L wce (y,ŷ) = − i α i p(y i )log(p(ŷ i )) with α i = 1/ √ f i ,</formula><p>where y i andŷ i define the true and predicted class labels and f i stands for the frequency, i.e. the number of points, of the i th class. This reinforces the network response to the classes appearing less in the dataset. In contrast to SalsaNet, we here also incorporate the Lovász-Softmax loss <ref type="bibr" target="#b1">[2]</ref> in the learning procedure to maximize the intersection-over-union (IoU) score, i.e. the Jaccard index. The IoU metric (see section IV-A) is the most commonly used metric to evaluate the segmentation performance. Nevertheless, IoU is a discrete and not derivable metric that does not have a direct way to be employed as a loss. In <ref type="bibr" target="#b1">[2]</ref>, the authors adopt this metric with the help of the Lovász extension for submodular functions. Considering the IoU as a hypercube where each vertex is a possible combination of the class labels, we relax the IoU score to be defined everywhere inside of the hypercube. In this respect, the Lovász-Softmax loss (L ls ) can be formulated as follows:</p><formula xml:id="formula_7">L ls = 1 |C| c∈C ∆ Jc (m(c)) , and m i (c) = 1 − x i (c) if c = y i (c) x i (c) otherwise ,</formula><p>where |C| represents the class number, ∆ Jc defines the Lovász extension of the Jaccard index, x i (c) ∈ [0, 1] and y i (c) ∈ {−1, 1} hold the predicted probability and ground truth label of pixel i for class c, respectively.</p><p>Finally, the total loss function of SalsaNext is a linear combination of both weighted cross-entropy and Lovász-Softmax losses as follows: L = L wce + L ls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Optimizer And Regularization</head><p>As an optimizer, we employed stochastic gradient descent with an initial learning rate of 0.01 which is decayed by 0.01 after each epoch. We also applied an L2 penalty with λ = 0.0001 and a momentum of 0.9. The batch size and spatial dropout probability were fixed at 24 and 0.2, respectively.</p><p>To prevent overfitting, we augmented the data by applying a random rotation/translation, flipping randomly around the y-axis and randomly dropping points before creating the projection. Every augmentation is applied independently of each other with a probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Post-processing</head><p>The main drawback of the projection-based point cloud representation is the information loss due to discretization errors and blurry convolutional layer responses. This problem  emerges when, for instance, the RV image is re-projected back to the original 3D space. The reason is that during the image rendering process, multiple LiDAR points may get assigned to the very same image pixel which leads to misclassification of, in particular, the object edges. This effect becomes more obvious, for instance, when the objects cast a shadow in the background scene.</p><p>To cope with these back-projection related issues, we employ the kNN-based post-processing technique introduced in <ref type="bibr" target="#b6">[7]</ref>. The post-processing is applied to every LIDAR point by using a window around each corresponding image pixel, that will be translated into a subset of point clouds. Next, a set of closest neighbors is selected with the help of kNN. The assumption behind using the range instead of the Euclidian distances lies in the fact that a small window is applied, making the range of close (u, v) points serve as a good proxy for the Euclidian distance in the three-dimensional space. For more details, we refer the readers to <ref type="bibr" target="#b6">[7]</ref>.</p><p>Note that this post-processing is applied to the network output during inference only and has no effect on learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate the performance of SalsaNext and compare with the other state-of-the-art semantic segmentation methods on the large-scale challenging Semantic-KITTI dataset <ref type="bibr" target="#b2">[3]</ref> which provides over 43K point-wise annotated full 3D LiDAR scans. We follow exactly the same protocol in <ref type="bibr" target="#b6">[7]</ref> and divide the dataset into training, validation, and test splits. Over 21K scans (sequences between 00 and 10) are used for training, where scans from sequence 08 are particularly dedicated to validation. The remaining scans (between sequences 11 and 21) are used as test split. The dataset has in total 22 classes 19 of which are evaluated on the test set by the official online benchmark platform. We implement our model in PyTorch and release the code for public use https://github.com/TiagoCortinhal/SalsaNext</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metric</head><p>To evaluate the results of our model we use the Jaccard Index, also known as mean intersection-over-union (IoU) over all classes that is given by mIoU</p><formula xml:id="formula_8">= 1 C C i=1 |Pi∩Gi| |Pi∪Gi| ,</formula><p>where P i is the set of point with a class prediction i, G i the labelled set for class i and || the cardinality of the set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Results</head><p>Obtained quantitative results compared to other stateof-the-art point-wise and projection-based approaches are reported in <ref type="table" target="#tab_1">Table I</ref>. Our proposed model SalsaNext considerably outperforms the others by leading to the highest mean IoU score (59.5%) which is +3.6% over the previous stateof-the-art method <ref type="bibr" target="#b23">[24]</ref>. In contrast to the original SalsaNet, we also obtain more than 14% improvement in the accuracy. When it comes to the performance of each individual category, SalsaNext performs the best in 9 out of 19 categories. Note that in most of these remaining 10 categories (e.g. road, vegetation, and terrain) SalsaNext has a comparable performance with the other approaches.</p><p>Following the work of <ref type="bibr" target="#b28">[29]</ref>, we further computed the epistemic and aleatoric uncertainty without retraining the SalsaNext model (see sec. III-C). <ref type="figure" target="#fig_1">Fig. 3</ref> depicts the quantitative relationship between the epistemic (model) uncer- tainty and the number of points that each class has in the entire Semantic-KITTI test dataset. This plot has diagonally distributed samples, which clearly shows that the network becomes less certain about rare classes represented by low number of points (e.g. motorcyclist and motorcycle). There also exists, to some degree, an inverse correlation between the obtained uncertainty and the segmentation accuracy: when the network predicts an incorrect label, the uncertainty becomes high as in the case of motorcyclist which has the lowest IoU score (19.4%) in <ref type="table" target="#tab_1">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results</head><p>For the qualitative evaluation, <ref type="figure" target="#fig_2">Fig. 4</ref> shows some sample semantic segmentation and uncertainty results generated by SalsaNext on the Semantic-KITTI test set.</p><p>In this figure, only for visualization purposes, segmented object points are also projected back to the respective camera image. We, here, emphasize that these camera images have not been used for training of SalsaNext. As depicted in <ref type="figure" target="#fig_2">Fig. 4</ref>, SalsaNext can, to a great extent, distinguish road, car, and other object points. In <ref type="figure" target="#fig_2">Fig. 4</ref>, we additionally show the estimated epistemic and aleatoric uncertainty values projected on the camera image for the sake of clarity. Here, the light blue points indicate the highest uncertainty whereas darker points represent more certain predictions. In line with <ref type="figure" target="#fig_1">Fig. 3</ref>, we obtain high epistemic uncertainty for rare classes such as other-ground as shown in the last frame in <ref type="figure" target="#fig_2">Fig. 4</ref>. We also observe that high level of aleatoric uncertainty mainly appears around segment boundaries (see the second frame in <ref type="figure" target="#fig_2">Fig. 4</ref>) and on distant objects (e.g. last frame in <ref type="figure" target="#fig_2">Fig. 4</ref>). In the supplementary video 1 , we provide more qualitative results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>In this ablative analysis, we investigate the individual contribution of each improvements over the original SalsaNet model. <ref type="table" target="#tab_1">Table II</ref> shows the total number of model parameters and FLOPs (Floating Point Operations) with the obtained mIoU scores on the Semantic-KITTI test set before and after applying the kNN-based post processing (see section III-F).</p><p>As depicted in <ref type="table" target="#tab_1">Table II</ref>, each of our contributions on SalsaNet has a unique improvement in the accuracy. The post processing step leads to a certain jump (around 2%) in the accuracy. The peak in the model parameters is observed when dilated convolution stack is introduced in the encoder, which is vastly reduced after adding the pixel-shuffle layers in the decoder. Combining the weighted cross-entropy loss with Lovász-Softmax leads to the highest increment in the accuracy as the Jaccard index is directly optimized. We can achieve the highest accuracy score of 59.5% by having only 2.2% (i.e. 0.15M) extra parameters compared to the original SalsaNet model. <ref type="table" target="#tab_1">Table II</ref> also shows that the number of FLOPs is correlated with the number of parameters. We note that adding the epistemic and aleatoric uncertainty computations do not introduce any additional training parameter since they are computed after the network is trained. . At the bottom of each scene, the range-view image of the network response is shown. Note that the corresponding camera images on the right are only for visualization purposes and have not been used in the training. The top camera image on the right shows the projected segments whereas the middle and bottom images depict the projected epistemic and aleatoric uncertainties, respectively. Note that the lighter the color is, the more uncertain the network becomes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Runtime Evaluation</head><p>Runtime performance is of utmost importance in autonomous driving. <ref type="table" target="#tab_1">Table III</ref> reports the total runtime performance for the CNN backbone network and post-processing module of SalsaNext in contrast to other networks. To obtain fair statistics, all measurements are performed using the entire Semantic-KITTI dataset on the same single NVIDIA Quadro RTX 6000 -24GB card. As depicted in <ref type="table" target="#tab_1">Table III</ref>, our method clearly exhibits better performance compared to, for instance, RangeNet++ <ref type="bibr" target="#b6">[7]</ref> while having 7× less parameters. SalsaNext can run at 24 Hz when the uncertainty computation is excluded for a fair comparison with deterministic models. Note that this high speed we reach is significantly faster than the sampling rate of mainstream LiDAR sensors which typically work at 10 Hz <ref type="bibr" target="#b38">[39]</ref>. <ref type="figure">Fig. 1</ref> also compares the overall performance of SalsaNext with the other state-ofthe-art semantic segmentation networks in terms of runtime, accuracy, and memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We presented a new uncertainty-aware semantic segmentation network, named SalsaNext, that can process the full 360 • LiDAR scan in real-time. SalsaNext builds up on the SalsaNet model and can achieve over 14% more accuracy. In contrast to previous methods, SalsaNext returns +3.6% better mIoU score. Our method differs in that SalsaNext can also estimate both data and model-based uncertainty.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the proposed SalsaNext model. Blocks with dashed edges indicate those that do not employ the dropout. The layer elements k, d, and bn represent the kernel size, dilation rate and batch normalization, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The relationship between the epistemic (model) uncertainty and the number of points (in log scale) that each class has in the entire test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Sample qualitative results showing successes of our proposed SalsaNext method [best view in color]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>COMPARISON ON SEMANTIC-KITTI TEST SET (SEQUENCES 11<ref type="bibr" target="#b20">TO 21)</ref>. IOU SCORES ARE GIVEN IN PERCENTAGE (%).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TABLE III RUNTIME PERFORMANCE ON THE SEMANTIC-KITTI TEST SET</figDesc><table><row><cell></cell><cell cols="3">Processing Time (msec)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN</cell><cell>kNN</cell><cell>Total</cell><cell cols="2">Speed (fps) Parameters</cell><cell>FLOPs</cell></row><row><cell>RangeNet++ [7]</cell><cell>63.51</cell><cell>2.89</cell><cell>66.41</cell><cell>15 Hz</cell><cell>50 M</cell><cell>720.96 G</cell></row><row><cell>SalsaNet [1]</cell><cell>35.78</cell><cell>2.62</cell><cell>38.40</cell><cell>26 Hz</cell><cell>6.58 M</cell><cell>51.60 G</cell></row><row><cell>SalsaNext [Ours]</cell><cell>38.61</cell><cell>2.65</cell><cell>41.26</cell><cell>24 Hz</cell><cell>6.73 M</cell><cell>125.68 G</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cavdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV2020)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lovászsoftmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast-scnn: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1902.04502</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ICRA</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RangeNet++: Fast and Accurate LiDAR Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.05158</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pointseg: Real-time semantic segmentation based on 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient convolutions for realtime semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.03428" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05905</idno>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rt3d: Real-time 3-d vehicle detection in lidar point cloud for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RAL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3434" to="3440" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Complex-yolo: Realtime 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">3d-mininet: Learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Riazuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A general framework for uncertainty estimation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>RA-L, 2020</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3266" to="3273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Uncertainty estimates and multi-hypotheses networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="652" to="667" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Point cloud instance segmentation using probabilistic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Understanding the disharmony between dropout and batch normalization by variance shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lightweight probabilistic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3369" to="3378" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
