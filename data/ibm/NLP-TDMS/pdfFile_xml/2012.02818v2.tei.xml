<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Franchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>2 valeo.ai</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Aldea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Séverine</forename><surname>Dubuisson</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Aix Marseille University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Bloch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>2 valeo.ai</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bayesian Neural Networks (BNNs) have been long considered an ideal, yet unscalable solution for improving the robustness and the predictive uncertainty of deep neural networks. While they could capture more accurately the posterior distribution of the network parameters, most BNN approaches are either limited to small networks or rely on constraining assumptions, e.g., parameter independence. These drawbacks have enabled prominence of simple, but computationally heavy approaches such as Deep Ensembles, whose training and testing costs increase linearly with the number of networks. In this work we aim for efficient deep BNNs amenable to complex computer vision architectures, e.g., ResNet50 DeepLabV3+, and tasks, e.g., semantic segmentation, with fewer assumptions on the parameters. We achieve this by leveraging variational autoencoders (VAEs) to learn the interaction and the latent distribution of the parameters at each network layer. Our approach, Latent-Posterior BNN (LP-BNN), is compatible with the recent BatchEnsemble method, leading to highly efficient (in terms of computation and memory during both training and testing) ensembles. LP-BNNs attain competitive results across multiple metrics in several challenging benchmarks for image classification, semantic segmentation and out-of-distribution detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most top-performing approaches for predictive uncertainty estimation with Deep Neural Networks (DNNs) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b14">15]</ref> are essentially based on ensembles, in particular Deep Ensembles (DE) <ref type="bibr" target="#b38">[39]</ref>, which have been shown to display many strengths: stability, mode diversity, good calibration, etc. <ref type="bibr" target="#b12">[13]</ref>. In addition, through the Bayesian lens, ensembles enable a more straightforward separation and quantification of the sources and forms of uncertainty <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref>, which in turn allows a better communication of the decisions to humans <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref> or to connected modules in an autonomous system <ref type="bibr" target="#b48">[49]</ref>. This is crucial for real-world decision making systems. Although originally introduced as simple and scalable alternative to Bayesian Neural Networks <ref type="figure">Figure 1</ref>: In a standard NN each weight has a fixed value. In most BNNs all weights follow Gaussian distributions and are assumed to be mutually independent: each weight is factorized by a Gaussian distribution. For LP-BNN in each layer, weights follow a multivariate Gaussian distribution with a latent weight space composed of independent Gaussian distributions. This enables computing expressive weight distributions in a lower dimensional space.</p><p>(BNNs) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52]</ref>, DE still have notable drawbacks in terms of computational cost for both training and testing often make them prohibitive in practical applications.</p><p>In this work we address uncertainty estimation with BNNs, the departure point of DE. BNNs propose an intuitive and elegant formalism suited for this task by estimating the posterior distribution over the parameters of a network conditioned on training data. Performing exact inference BNNs is intractable and most approaches require approximations. The most common one is the mean-field assumption <ref type="bibr" target="#b32">[33]</ref>, i.e., the weights are assumed to be independent of each other and factorized by their own distribution, usually Gaussian <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51]</ref>. However, this approximation can be damaging <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b11">12]</ref> as a more complex organization can emerge within network layers, and that higher level correlations contribute to better performance and generalization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64</ref>]. Yet, even under such settings, BNNs are challenging to train at scale on modern DNN architectures <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b9">10]</ref>. In response, researchers have looked into structured-covariance approximations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b50">51]</ref>, however they further increase memory and time complexity over the original mean-field approximation.</p><p>Here, we revisit BNNs in a pragmatic manner. We propose an approach to estimate the posterior of a BNN with layer-level inter-weight correlations, in a stable and computationally efficient manner, compatible with modern DNNs and complex computer vision tasks, e.g., semantic segmentation.</p><p>We advance a novel deep BNN model, dubbed Latent Posterior BNN (LP-BNN), where the posterior distribution of the weights at each layer is encoded with a variational autoencoder (VAE) <ref type="bibr" target="#b35">[36]</ref> into a lower-dimensional latent space that follows a Gaussian distribution (see <ref type="figure">Figure 1</ref>). We switch from the inference of the posterior in the high dimensional space of the network weights to a lower dimensional space which is easier to learn and already encapsulates weight interaction information. LP-BNN is naturally compatible with the recent BatchEnsemble (BE) approach <ref type="bibr" target="#b69">[70]</ref> that enables learning a more diverse posterior from the weights of the BE sub-networks. Their combination outperforms most of related approaches across a breadth of benchmarks and metrics. In particular, LP-BNN is competitive with DE and has significantly lower costs for training and prediction. Contributions. To summarize, the contributions of our work are: <ref type="bibr" target="#b0">(1)</ref> We introduce a scalable approach for BNNs to implicitly capture layer-level weight correlations enabling more expressive posterior approximations, by foregoing the limiting mean-field assumption LP-BNN scales to high capacity DNNs (e.g., 50+ layers and 30M parameters for DeepLabv3+), while still training on a single V100 GPU. <ref type="bibr" target="#b1">(2)</ref> We propose to leverage VAEs for computing the posterior distribution of the weights by projecting them in the latent space. This improves significantly training stability while ensuring diversity of the sampled weights. (3) We extensively evaluate our method on a range of computer vision tasks and settings: image classification for in-domain uncertainty, out-of-distribution (OOD) detection, robustness to distribution shift, and semantic segmentation ( high-resolution images, strong class imbalance) for OOD detection. We demonstrate that LP-BNN achieves similar performances with high-performing Deep Ensembles, while being substantially more efficient computationally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this section, we present the chosen formalism for this work and offer a short background on BNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries</head><p>We consider a training dataset D = {(x i , y i )} n i=1 with n samples and labels, corresponding to two random variables X ∼ P X and Y ∼ P Y . Without loss of generality we represent x i ∈ R d as a vector, and y i as a scalar label. We process the input data x i with a neural network f Θ (·) with parameters Θ, that outputs a classification or regression prediction. We view the neural network as a probabilistic model with f Θ (x i ) = P (Y = y i | X = x i , Θ). In the following, when there are no ambiguities, we discard the random variable from notations. For classification, P (y i | x i , Θ) is a categorical distribution over the set of classes over the domain of Y , typically corresponding to the crossentropy loss function, while for regression P (y i | x i , Θ) is a Gaussian distribution of real values over the domain of Y when using the squared loss function. For simplicity we unroll our reasoning for the classification task.</p><p>In supervised learning, we leverage gradient descent for learning Θ that minimizes the cross-entropy loss, which is equivalent to finding the parameters that maximize the likelihood estimation (MLE) P (D | Θ) over the training set Θ MLE = arg max Θ (xi,yi)∈D log P (y i | x i , Θ), or equivalently minimize the following loss function:</p><formula xml:id="formula_0">L MLE (Θ) = − (xi,yi)∈D log P (y i | x i , Θ).<label>(1)</label></formula><p>The Bayesian approach enables adding prior information on the parameters Θ, by placing a prior distribution P(Θ) upon them. This prior represents some expert knowledge about the dataset and the model. Instead of maximizing the likelihood, we can now find the maximum a posteriori (MAP) weights for P(Θ | D) ∝ P(D | Θ)P(Θ) to compute Θ MAP = arg max Θ (xi,yi)∈D log P(y i | x i , Θ) + log P(Θ), i.e. to minimize the following loss function:</p><formula xml:id="formula_1">L MAP (Θ) = − (xi,yi)∈D log P (y i | x i , Θ) − log P (Θ), (2)</formula><p>inducing a specific distribution over the functions computed by the network and a regularization of the weights. For a Gaussian prior, Eq. (2) reads as L 2 regularization (weight decay).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bayesian Neural Networks</head><p>In most neural networks only the Θ MAP weights computed during training are kept for predictions. Conversely, in BNNs we aim to find the posterior distribution P (Θ | D) of the parameters given the training dataset, not only the values corresponding to the MAP. Here we can make a prediction y on a new sample x by computing the expectation of the predictions from an infinite ensemble corresponding to different configurations of the weights sampled from the posterior distribution:</p><formula xml:id="formula_2">P (y | x, D) = P (y | x, Θ)P (Θ | D)dΘ,<label>(3)</label></formula><p>which is also known as Bayes ensemble. The integral in Eq. (3), which is calculated over the domain of Θ, is intractable, and in practice it is approximated by averaging research across the years <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref>. Early approaches relied on Markov chain Monte Carlo variants for inference, while progress in variational inference (VI) <ref type="bibr" target="#b32">[33]</ref> has enabled a recent revival of BNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>. VI turns posterior inference into an optimization problem. In detail, VI finds the parameters ν of a distribution Q ν (Θ) on the weights that approximates the true Bayesian posterior distribution of the weights P (Θ | D) through KL-divergence minimization. This is equivalent to minimizing the following loss function, also known as expected lower bound (ELBO) loss <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_3">L BNN (Θ, ν) = − (xi,yi)∈D E Θ∼Qν (Θ) log (P (y i | x i , Θ)) + KL(Q ν (Θ)||P (Θ)).<label>(5)</label></formula><p>The loss function L BNN (Θ, ν) is composed of two terms: the KL term depends on the weights and the prior P (Θ), while the likelihood term is data dependent. This function strives to simultaneously capture faithfully the complexity and diversity of the information from data D, while preserving the simplicity of the prior P (Θ). To optimize this loss function, Blundell et al. <ref type="bibr" target="#b5">[6]</ref> proposed leveraging the re-parameterization trick <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b59">60]</ref>, foregoing the expensive MC estimates.</p><p>Discussion. BNNs are particularly appealing for uncertainty quantification thanks to the ensemble of predictions from multiple weight configurations sampled from the posterior distribution. However this brings an increased computational and memory cost. For instance, the simplest variant of BNNs with fully factorized Gaussian approximation distributions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, i.e. each weight consists of a Gaussian mean and variance, carries a double amount of parameters.</p><p>In addition, recent works <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b9">10]</ref> point out that BNNs often underfit, and need multiple tunings to stabilize training dynamics involved by the loss function and the variance from weight samplings at each forward pass. Due to computational limitations, most BNN approaches assume that parameters are not correlated. This hinders their effectiveness <ref type="bibr" target="#b11">[12]</ref>, as empirical evidence has shown that encouraging weight collaboration improves training stability and generalization <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>. In order to calculate a tractable weight correlation aware posterior distribution, we propose to leverage a VAE to compute compressed latent distributions from which we can sample new weight configurations. We rely on the recent BatchEnsemble (BE) method <ref type="bibr" target="#b69">[70]</ref> to further improve the parameter-efficiency of BNNs. We now proceed to describe BE and then derive our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">BatchEnsemble</head><p>Deep Ensembles (DEs) <ref type="bibr" target="#b38">[39]</ref> are a popular and pragmatic alternative to BNNs. While DEs boast outstanding accuracy and predictive uncertainty, their training and testing cost increases linearly with the number of networks. This drawback has motivated the emergence of a recent stream of works proposing efficient ensemble methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b69">70]</ref>. One of the most promising ones is BatchEnsemble <ref type="bibr" target="#b69">[70]</ref>, which mimics in a parameter-efficient manner one of the main strengths of DE, i.e. diverse predictions <ref type="bibr" target="#b12">[13]</ref>.</p><p>In a nutshell, BE builds up an ensemble from a single base network (shared among ensemble members) and a set of layer-wise weight matrices specific to each member. At each layer, the weight of each ensemble member is generated from the Hadamard product between a weight shared among all ensemble members, called "slow weight", and a Rank-1 matrix that varies among all members, called "fast weight". Formally, let W share ∈ R m×p be the slow weights in a neural network layer with input dimension m and with p outputs. Each member j from an ensemble of size J owns a fast weight matrix W j ∈ R m×p . W j is a Rank-1 matrix computed from a tuple of trainable vectors r j ∈ R m and s j ∈ R p , with W j = r j s j . BE generates from them a family of ensemble weights as follows: W j = W share W j , where is the Hadamard product. Each W j member of the ensemble is essentially a Rank-1 perturbation of the shared weights W share (see <ref type="figure" target="#fig_0">Figure 2</ref>). The sequence of operations during the forward pass reads:</p><formula xml:id="formula_4">h = a (W share (x s j )) r j ,<label>(6)</label></formula><p>where a is an activation function and h the output activations. The operations in BE can be efficiently vectorized, enabling each member to process in parallel the corresponding subset of samples from the mini-batch. W share is trained in a standard manner over all samples in the mini-batch. A BE network f Θ BE is parameterized by an extended set of parameters Θ BE = θ slow : {W share }, θ fast : {r j , s j } J j=1 . With its multiple sub-networks parameterized by a reduced set of weights, BE is a practical method that can potentially improve the scalability of BNNs. We take advantage of the small size of the fast weights to capture efficiently the interactions between units and to compute a latent distri-bution of the weights. We detail our approach below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Efficient Bayesian Neural Networks (BNNs)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoding the posterior weight distribution of a BNN</head><p>Most BNN variants assume full independence between weights, both inter-and intra-layer. Modeling precisely weight correlations in modern high capacity DNNs with thousands to millions of parameters per layer <ref type="bibr" target="#b21">[22]</ref> is however a daunting endeavor due to computational intractability. Yet, multiple strategies aiming to boost weight collaboration in one way or another, e.g. Dropout <ref type="bibr" target="#b63">[64]</ref>, WeightNorm <ref type="bibr" target="#b61">[62]</ref>, Weight Standardization <ref type="bibr" target="#b57">[58]</ref>, have proven to improve training speed, stability and generalization. Ignoring weight correlations might partially explain the shortcomings of BNNs in terms of underfitting <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b9">10]</ref>. This motivates us to find a scalable way to compute the posterior distribution of the weights without discarding their correlations.</p><p>Li et al. <ref type="bibr" target="#b40">[41]</ref> have recently found that the intrinsic dimension of DNNs can be in the order of hundreds to a few thousands. The good performances of BE, that builds on weights from a low-rank subspace, further confirm this finding. For efficiency, we leverage the Rank-1 subspace decomposition in BE and estimate here the distribution of the weights, leading to a novel form of BNNs. Formally, instead of computing the posterior distribution P (Θ | D), we aim now for P (θ fast | D).</p><p>A first approach would be to compute Rank-1 weight distributions by using r j and s j as variational layers, place priors on them and compute their posterior distributions in a similar manner to <ref type="bibr" target="#b5">[6]</ref>. Dusenberry et al. <ref type="bibr" target="#b9">[10]</ref> show that these Rank-1 BNNs stabilize training by reducing the variance of the sampled weights, due to sampling only from Rank-1 variational distributions instead of full weight matrices. However this raises the memory cost significantly, as training is performed simultaneously over all J sub-networks: on CIFAR-10 for ResNet-50 with J=4, the authors use 8 TPUv2 cores with mini-batches of size 64 per core.</p><p>We argue that a more efficient way of computing the posterior distribution of the fast weights would be to learn instead the posterior distribution of the lower dimensional latent variables of {r, s} ∈ θ fast . This can be efficiently done with a VAE <ref type="bibr" target="#b35">[36]</ref> that can find a variational approximation Q φ (z | r) to the intractable posterior distribution P ψ (z | r). VAEs can be seen as a generative model that can deal with complicated dependencies between input dimensions via a probabilistic encoder that projects the input into a latent space following a specific prior distribution. For simplicity and clarity, from here onward we derive our formalism only for r at a single layer and consider weights s to be deterministic. Here the input to the VAE are the weights r and we rely on it to learn the dependencies between weights and encode them into the latent representation.</p><p>In detail, for each layer of the network f Θ (·) we introduce a VAE composed of a one layer encoder g enc φ (·) with variational parameters φ and a one layer decoder g dec ψ (·) with parameters ψ. Let the prior over the latent variables be a centered isotropic Gaussian distribution P ψ (z) = N (z; 0, I).</p><p>Like common practice, we let the variational approximate posterior distribution Q φ (z | r) be a multivariate Gaussian with diagonal covariance. The encoder takes as input a minibatch of size J (the size of the ensemble) composed of all the r j weights of this layer and outputs as activations (µ j , σ 2 j ). We sample a latent variable z j ∼ N (µ j , σ 2 j I) and feed it to the decoder, which in turn outputs the reconstructed weightŝ r j = g dec ψ (z j ). In other words, at each forward pass, we sample new fast weightsr j from the latent posterior distribution to be further used for generating the ensemble. The weights of each member of the ensemble W j = W share (r j s j ) are now random variables depending on W share , s j and z j . Note that while in practice we sample J weight configurations, this approach allows us to generate larger ensembles by sampling multiple times from the same latent distribution. We illustrate an overview of an LP-BNN layer in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The VAE modules are trained in the standard manner with the ELBO loss function <ref type="bibr" target="#b35">[36]</ref> jointly with the rest of the network. The final loss function is:</p><formula xml:id="formula_5">L LP-BNN (Θ LP-BNN )=− (x i ,y i )∈D E z∼Q φ (z|r) log (P (y i | x i , Θ LP-BNN , z)) + 1/L KL(Q φ (z | r)||P ψ (z)) + r −r 2 , (7)</formula><p>where Θ LP-BNN = θ slow , θ fast :{r j , s j } J j=1 , θ variational :{φ, ψ} and L the number of layers. The loss function is applied to all J members of the ensemble.</p><p>At a first glance, the loss function L LP-BNN bears some similarities with L BNN (Eq. 5). Both functions include likelihood and KL terms. The likelihood in L BNN , i.e. the cross-entropy loss, depends on input data x i and on the parameters Θ sampled from Q ν (Θ), while L LP-BNN depends on the latent variables z j sampled from Q φ (z j | r j ) that lead to the fast weightsr j . It guides the weights towards useful values for the main task. The KL term in L BNN enforces the per-weight prior, while in L LP-BNN it preserves the consistency and simplicity of the common latent distribution of the weights r j . In addition, L LP-BNN has an input weight reconstruction loss (last term in Eq. 7) ensuring that the generated weightsr j are still compatible with the rest of parameters of the network and do not cause high variance and instabilities during training, as typically occurs in standard BNNs <ref type="bibr" target="#b9">[10]</ref>.</p><p>At test time, we generate the LP-BNN ensemble on the fly by sampling the weightsr j from the encodings of r j to compute W j . For the final prediction we compute the </p><formula xml:id="formula_6">P (y i |x i ) = 1 J J j=1 P (y i | x i , θ slow , s j ,r j )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discussion on LP-BNN</head><p>We discuss here the quality of the uncertainty from LP-BNN. The predictive uncertainty of a DNN stems from two main types of uncertainty <ref type="bibr" target="#b27">[28]</ref>: aleatoric uncertainty and epistemic uncertainty. The former is related to randomness, typically due to the noise in the data. The latter concerns finite size training datasets. The epistemic uncertainty captures the uncertainty in the DNN parameters and their lack of knowledge on the model that generated the training data.</p><p>In BNN approaches, through likelihood marginalization over weights, the prediction is computed by integrating the outputs from different DNNs weighted by the posterior distribution (Eq. 3), allowing us to conveniently capture both types of uncertainties <ref type="bibr" target="#b46">[47]</ref>. The quality of the uncertainty estimates depends on the diversity of predictions and views provided by the BNN. DE <ref type="bibr" target="#b38">[39]</ref> achieve excellent diversity <ref type="bibr" target="#b12">[13]</ref> by mimicking BNN ensembles through training of multiple individual models. Recently, Wilson and Izmailov <ref type="bibr" target="#b71">[72]</ref> proposed to combine DE and BNNs towards improving diversity further. However, as DE are already computationally demanding, we argue that BE is a more pragmatic choice for increasing the diversity of our BNN, leading to better uncertainty quantification. <ref type="figure" target="#fig_5">Figure S1</ref> shows a qualitative comparison of the prediction diversity from different methods. We compare LP-BNN, BE, and DE based on WRN-28-10 [75] trained on CIFAR-10 <ref type="bibr" target="#b37">[38]</ref> and analyze predictions on CIFAR-10, CIFAR10-C <ref type="bibr" target="#b23">[24]</ref>, and SVHN <ref type="bibr" target="#b52">[53]</ref> test images. SVHN contains digits which have a different distribution from the training data, i.e., predominant epistemic uncertainty, while CIFAR10-C  <ref type="bibr" target="#b69">[70]</ref>, and DE <ref type="bibr" target="#b38">[39]</ref>. For all methods we set the number of models to J=4.</p><p>displays a distribution shift via noise corruption, i.e., more aleatoric uncertainty. The expected behavior is that individual DNNs in an ensemble would predict different classes for OOD images and have higher entropy on the corrupted ones, reducing the confidence score of the ensemble. We can see that the diversity of BE is lower for CIFAR10-C and SVHN, leading to poorer results in <ref type="table">Table 3</ref>.</p><p>In the supplementary we include additional discussions on our posterior covariance matrix ( §A.1), the link between the size of the ensemble and the covariance matrix approximation ( §A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>Bayesian Deep Learning. Bayesian approaches and neural networks have a long joint history <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref>. Early approaches relied on Markov chain Monte Carlo variants for inference on BNNs, which was later replaced by variational inference (VI) <ref type="bibr" target="#b32">[33]</ref> in the context of deeper networks. Most of the modern approaches make use of VI with the mean-field approximation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51]</ref> which conveniently makes posterior inference tractable. However this limits the expressivity of the posterior <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b11">12]</ref>. This drawback became subject of multiple attempts for structuredcovariance approximations using matrix variate Gaussian priors <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b64">65]</ref> or natural gradients <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b50">51]</ref>. However they further increase memory and time complexity over the original mean-field approximation. Recent methods proposed more simplistic BNNs by performing inference with structured priors only over the first and last layer <ref type="bibr" target="#b55">[56]</ref> or just the last layer <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b53">54]</ref>. Full covariance can be computed for shallow networks thanks to a meta-prior in a low-dimensional space where the VI can be performed <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>. Most BNNs are still challenging to train, underfit and are difficult to scale to big DNNs <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b9">10]</ref>, while the issue of finding a proper prior is still open <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b13">14]</ref>. Our approach builds upon the low dimensional fast weights from BE and on the stability of the VAEs, foregoing many of the shortcomings of BNN training. Ensemble Approaches. Ensembles mimick and attain, to some extent, properties of BNNs <ref type="bibr" target="#b12">[13]</ref>. Deep Ensembles <ref type="bibr" target="#b38">[39]</ref> train multiple DNNs with different random initializations leading to excellent uncertainty quantification scores. The major inherent drawback in terms of computational and memory overhead has been subsequently addressed through multi-head networks <ref type="bibr" target="#b39">[40]</ref>, snapshot-ensembles from intermediate training checkpoints <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b17">18]</ref>, efficient computation of the posterior distribution from weight trajectories during training <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b14">15]</ref>, use of multiple Dropout masks at test time <ref type="bibr" target="#b16">[17]</ref>, multiple random perturbations to the weights of a pre-trained network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b2">3]</ref>, multiple perturbation of the input image <ref type="bibr" target="#b1">[2]</ref>, multiple low-rank weights tied to a backbone network <ref type="bibr" target="#b69">[70]</ref>, simultaneous processing of multiple images by the same DNN <ref type="bibr" target="#b20">[21]</ref>. Most approaches still have a significant computational overhead for training or for prediction, while struggling with diversity <ref type="bibr" target="#b12">[13]</ref>. Dirichlet Networks (DNs). DNs <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b65">66]</ref> bring a promising line of approaches that estimate uncertainty from a single network by parameterizing a Dirichlet distribution over its predictions. However, most of these methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> use OOD samples during training, which may be unrealistic in many applications <ref type="bibr" target="#b6">[7]</ref>, or do not scale to bigger DNNs <ref type="bibr" target="#b31">[32]</ref>.DNs have been developed only for classification tasks and extending them to regression requires further adjustments <ref type="bibr" target="#b45">[46]</ref>, unlike LP-BNN that can be equally used for classification and regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>We evaluate the performance of LP-BNN in assessing the uncertainty of its predictions. For our benchmark, we evaluate LP-BNN on different scenarios against several strong baselines with different advantages in terms of performance, training or runtime: BE <ref type="bibr" target="#b69">[70]</ref>, DE <ref type="bibr" target="#b38">[39]</ref>, Maximum Class Probability (MCP) <ref type="bibr" target="#b24">[25]</ref>, MC Dropout <ref type="bibr" target="#b16">[17]</ref>, TRADI <ref type="bibr" target="#b14">[15]</ref>, EDL <ref type="bibr" target="#b62">[63]</ref>, DUQ <ref type="bibr" target="#b66">[67]</ref>, and MIMO <ref type="bibr" target="#b20">[21]</ref>.</p><p>First, we evaluate the predictive performance in terms of accuracy for image classification and mIoU <ref type="bibr" target="#b10">[11]</ref> for semantic segmentation, respectively. Secondly, we evaluate the quality of the confidence scores provided by the DNNs by means of Expected Calibration Error (ECE) <ref type="bibr" target="#b19">[20]</ref>. For ECE we use M -bin histograms of confidence scores and accuracy, and compute the average of M bin-to-bin differences between the two histograms. Similarly to <ref type="bibr" target="#b19">[20]</ref> we set M = 15. To evaluate the robustness to dataset shift via corrupted images, we first train the DNNs on CIFAR-10 <ref type="bibr" target="#b37">[38]</ref> or CIFAR-100 <ref type="bibr" target="#b37">[38]</ref> and then test on the corrupted versions of these datasets <ref type="bibr" target="#b23">[24]</ref>. The corruptions include different types of noise, blurring, and some other transformations that alter the quality of the images. For this scenario, similarly to <ref type="bibr" target="#b68">[69]</ref>, we use as evaluation measures the Corrupted Accuracy (cA) and Corrupted Expected Calibration Error (cE), that offer a better understanding of the behavior of our DNN when facing shift of data distribution and aleatoric uncertainty. In order to evaluate the epistemic uncertainty, we propose to assess the OOD detection performance. This scenario typically consists in training a DNN over a dataset following a given distribution, and testing it on data coming from this distribution and data from another distribution , not seen during training. We quantify the confidence of the DNN predictions in this setting through their prediction scores, i.e., output softmax values. We use the same indicators of the accuracy of detecting OOD data as in <ref type="bibr" target="#b24">[25]</ref>: AUC, AUPR, and the FPR-95%-TPR. These indicators measure whether the DNN model lacks knowledge regarding some specific data and how reliable are its predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image classification with CIFAR-10/100 [38]</head><p>Protocol. Here we train on CIFAR-10 [38] composed of 10 classes. For CIFAR-10 we consider as OOD the SVHN dataset <ref type="bibr" target="#b52">[53]</ref>. Since SVHN is a color image dataset of digits, it guarantees that the OOD data comes from a distribution different from those of CIFAR-10. We use WRN-28-10 <ref type="bibr" target="#b74">[75]</ref> for all methods, a popular architecture for this dataset, and evaluate on CIFAR-10-C <ref type="bibr" target="#b23">[24]</ref>. For CIFAR-100 <ref type="bibr" target="#b37">[38]</ref> we use again WRN-28-10 and evaluate on the test sets of CIFAR-100 and CIFAR-100-C <ref type="bibr" target="#b23">[24]</ref>. Note that for all DNNs, even for DE, we average results over three random seeds for statistical relevance. We use cutout <ref type="bibr" target="#b8">[9]</ref> as data augmentation, as commonly used for these datasets. Please find in the supplementary the hyperparameters for this experiment. Discussion. We illustrate results for this experiment in Table <ref type="bibr" target="#b2">3</ref>. We notice that DE with cutout outperforms other methods on most of the metrics except ECE, cA, and cE on  <ref type="table">Table 1</ref>: Comparative results for image classification tasks. We evaluate on CIFAR-10 and CIFAR-100 for the tasks: in-domain classification, out-of-distribution detection with SVHN (CIFAR-10 only), robustness to distribution shift (CIFAR-10-C, CIFAR-100-C). We run all methods ourselves in similar settings using publicly available code for related methods. Results are averaged over three seeds. † : We did not manage to scale these methods to WRN-28-10 on CIFAR-100. A similar finding for EDL was reported in <ref type="bibr" target="#b31">[32]</ref>. ‡ DUQ does not scale on CIFAR-100 and it does not perfectly scale to WRN-28-10 on CIFAR-10 so we train it with Resnet 18 <ref type="bibr" target="#b21">[22]</ref> architecture like in the original paper.  CIFAR-10, and cA on CIFAR-100, where LP-BNN achieves state of the art results. This means that LP-BNN is competitive for aleatoric uncertainty estimation. In fact, ECE is calculated on the test set of CIFAR-10 and CIFAR-100, so it mostly measures the reliability of the confidence score in the training distribution. cA and cE are evaluated on corrupted versions of CIFAR-10 and CIFAR-100, which amounts to quantifying the aleatoric uncertainty. We can see that for this kind of uncertainty, LP-BNN achieves state of the art performance. On the other hand, for epistemic uncertainty, we can see that DE always attain best results. Overall, our LP-BNN is more computationally efficient while providing better results for the aleatoric uncertainty. Computation wise, DE takes 52 hours to train on CIFAR-10, while LP-BNN needs 2 times less, 26 hours and 30 minutes. In <ref type="figure" target="#fig_4">Figure 5</ref> and <ref type="table">Table S6</ref>, we observe that our method exhibits top ECE score on CIFAR-10-C, as well as for the stronger corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semantic segmentation</head><p>Next, we evaluate semantic segmentation, a task of interest for autonomous driving, where high capacity DNNs are used for processing high resolution images with complex urban scenery with strong class imbalance. StreetHazards <ref type="bibr" target="#b22">[23]</ref>. StreetHazards is a large-scale dataset that consists of different sets of synthetic images of street scenes. More precisely, this dataset is composed of 5, 125 images for training and 1, 500 test images. The training dataset contains pixel-wise annotations for 13 classes. The test dataset comprises 13 training classes and 250 OOD classes, unseen in the training set, making it possible to test the robustness of the algorithm when facing a diversity of possible scenarios. For this experiment, we used DeepLabv3+ <ref type="bibr" target="#b7">[8]</ref> with a ResNet-50 encoder <ref type="bibr" target="#b21">[22]</ref>. Following the implementation in <ref type="bibr" target="#b22">[23]</ref>, most papers use PSPNet <ref type="bibr" target="#b76">[77]</ref> that aggregates predictions over multiple scales, an ensembling that can obfuscate in the evaluation the uncertainty contribution of a method. This can partially explain the excellent performance of MCP on the original settings <ref type="bibr" target="#b22">[23]</ref>. We propose using DeepLabv3+ instead, as it enables a clearer evaluation of the predictive uncertainty. We propose two DeepLabv3+ variants as follows. DeepLabv3+ is composed of an encoder network and a decoder network; in the first version, we change the decoder by replacing all the convolutions with our new version of LP-BNN convolutions and leave the encoder unchanged. In the second variant we use weight standardization <ref type="bibr" target="#b56">[57]</ref> on the convolutional layers of the decoder, replacing batch normalization <ref type="bibr" target="#b29">[30]</ref> in the decoder with group normalization <ref type="bibr" target="#b72">[73]</ref>, to better balance mini-batch size and ensemble size. We denote the first version LP-BNN and the second one LP-BNN + GN. <ref type="bibr" target="#b22">[23]</ref>. BDD-Anomaly is a subset of the BDD100K dataset <ref type="bibr" target="#b73">[74]</ref>, composed of 6, 688 street scenes for training and 361 for the test set. The training set contains pixel-level annotations for 17 classes, and the test dataset is composed of the 17 training classes and 2 OOD classes: motor-cycle and train. For this experiment, we use DeepLabv3+ <ref type="bibr" target="#b7">[8]</ref> with the experimental protocol from <ref type="bibr" target="#b22">[23]</ref>. As previously we use ResNet50 encoder <ref type="bibr" target="#b21">[22]</ref>. For this experiment, we use the LP-BNN and LP-BNN + GN variants.  <ref type="table">Table 3</ref>: Comparative results on the OOD task for semantic segmentation. We run all methods ourselves in similar settings using publicly available code for related methods. Results are averaged over three seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BDD-Anomaly</head><p>Input image MCP BE LP-BNN <ref type="figure">Figure 6</ref>: Visual assessment on two BDD-Anomaly test images containing a motorcycle (OOD class). For each image: on the first row, input image and confidence maps from MCP <ref type="bibr" target="#b24">[25]</ref>, BE <ref type="bibr" target="#b69">[70]</ref>, and LP-BNN; on the second row, ground-truth segmentation and segmentation maps from MCP, BE, and LP-BNN. LP-BNN is less confident on the OOD objects.</p><p>Discussion. We emphasize that the semantic segmentation is more challenging than the CIFAR classification since images are bigger and their content is more complex. The larger input size constrains to use smaller mini-batches. This is crucial since the fast weights of the ensemble layers are trained just on one mini-batch slice. In this experiment, we could use mini-batches of size 4 and train the fast weights on slices of size 1. Yet, despite these computational difficulties, with our technique, we achieve state-of-the-art results for most metrics. We can see in <ref type="table">Table 3</ref> that our strategies achieve state-of-the-art performance in detecting OOD data and are well calibrated. We can also see in <ref type="figure">Figure 6</ref>, where the OOD class is the motorcycle, that our DNN is less confident on this class. Hence LP-BNN allows us to have a more reliable DNN which is essential for real-world applications. <ref type="table" target="#tab_2">Table 2</ref> shows the computational cost of LP-BNN and related methods. For training, LP-BNN takes only ×1.33 more time than a vanilla approach, in contrast to DE that take much longer, while their performances are equivalent in most cases. In the same time, LP-BNN enables implicit modeling of weight correlations at every layer with limited overhead as it does not explicitly computes the covariances. To the best of our knowledge, LP-BNN is the first approach with the posterior distribution computed with variational inference successfully trained and applied for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a new BNN framework able to quantify uncertainty in the context of deep learning. Owing to each layer of the network being tied to and regularized by a VAE, LP-BNNs are stable, efficient, and therefore easy to train compared to existing BNN models. The extensive empirical comparisons on multiple tasks show that LP-BNNs reach state-of-the-art levels with substantially lower computational cost. We hope that our work will open new research paths on effective training of BNNs. In the future we intend to explore new strategies for plugging more sophisticated VAEs in our models along with more in-depth theoretical studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding the latent posterior of Bayesian Neural Networks for uncertainty quantification (Supplementary material)</head><p>A. Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Covariance Priors of Bayesian Neural Network</head><p>We consider a data sample (x, y), with x ∈ R d and y ∈ R. We process the input data x with a MLP network f Θ (·) with parameters Θ composed of one hidden layer of h neurons. We detail the composing operations of the function f Θ (·) associated to this network: f Θ (x) = W 2 σ(W 1 x), where σ(·) is an element-wise activation function. For simplicity, we ignore the biases in this example. W 1 ∈ R d×h is the weight matrix associated with the first fully connected layer and W 2 ∈ R h×1 the weights of the second layer. In BNNs, W 1 and W 2 represent random variables, while for classic DNNs, they are simply singular realizations of the distribution sought by BNN inference. Most works exploiting in some way the statistics of the network parameters assume, for tractability reasons, that all weights of W 1 and W 2 follow independent Gaussian distributions. Hence this will lead to the following covariance matrix for W 1 : , respectively. In the case of LP-BNN, we consider that each coefficient of Z 1 and Z 2 represent an independent random variable. Thus, in contrast to approaches based on the mean-field approximation directly on the weights of the DNN, we can have for each layer a non-diagonal covariance matrix with the following variance and covariance terms for W 1 :</p><formula xml:id="formula_7">    </formula><formula xml:id="formula_8">var(W 1 [i, j]) = 3 k=1 (α [i,j] 1 [k]) 2 var(Z 1 [k]) (S1) cov(W 1 [i, j], W 1 [i , j ]) = 3 k=1 α [i,j] 1 [k]α [i ,j ] 1 [k]var(Z 1 [k])</formula><p>(S2) This allows us to leverage the lower-dimensional parameters of the distributions of Z 1 and Z 2 for estimating the higherdimensional distributions of W 1 and W 2 . In this manner, in LP-BNN we model an implicit covariance of weights at each layer.</p><p>We note that several approaches for modeling correlation between weights have been proposed under certain settings and assumptions. For instance, Karaletesos and Bui <ref type="bibr" target="#b33">[34]</ref> model correlations between weights within a layer and across layers thanks to a Gaussian process-based approach working in the function space via hierarchical priors instead of directly on the weights. Albeit elegant, this approach is still limited to relatively shallow MLPs (e.g., one hidden layer with 100 units <ref type="bibr" target="#b33">[34]</ref>) and cannot scale up yet to deep architectures considered in this work (e.g., ResNet-50). Other approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b64">65]</ref> model layer-level weight correlations through Matrix Variate Gaussian (MVG) prior distributions, increasing the expressiveness of the inferred posterior distribution at the cost of further increasing the computational complexity w.r.t. mean-field approximated BNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>. In contrast, LP-BNN does not explicitly model the covariance by conveniently leveraging fully connected layers to project weights in a low-dimensional latent space and performing the inference of the posterior distribution there. This strategy leads to a lighter BNN that is competitive in terms of computation and performance for complex computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. The utility of Rank-1 perturbations</head><p>One could ask why using the Rank-1 perturbation formalism from BE <ref type="bibr" target="#b69">[70]</ref>, instead of simply feeding the weights of a layer to the VAE to infer the latent distribution. Rank-1 perturbations significantly reduce the number of weights upon which we train the VAE, due to the decomposition of the fast weights into r and s. This further allows us to consider multiple such weights, J, at each forward pass enabling faster training of the VAE as its training samples are more numerous and more diverse.</p><p>Next, we establish connections between the cardinality J of the ensemble and the posterior covariance matrix. The mechanism of placing a prior distribution over the latent space enables an implicit modeling of correlations between weights in their original space. This is a desirable property due to its superior expressiveness <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref> but which can be otherwise computationally intractable or difficult to approximate. The covariance matrix of our prior in the original weight space is a Rank-1 matrix. Thanks to the Eckart-Young theorem (Theorem 5.1 in <ref type="bibr" target="#b67">[68]</ref>), we can quantify the error of approximating the covariance by a Rank-1 matrix, based on the second up to the last singular values.</p><p>Let us denote by Θ 1 , . . . , Θ J the J weights trained by our algorithm, Θ avg = 1 J J j=1 Θ j and ∆ j = Θ j − Θ avg . The differences and the sum in the previous equations are calculated element-wise on all the weights of the DNNs. Then, for each new data sample x, the prediction of the DNN f Θavg (·) is equivalent to the average of the DNNs f Θj (·) applied on x :</p><formula xml:id="formula_9">1 J J j=1 f Θj (x) = f Θavg (x) + O ∆ 2 (S3)</formula><p>with ∆ = max j ∆ j . The L 2 norm is computed over all weights. We refer the reader to the proof in §3.5 of <ref type="bibr" target="#b30">[31]</ref>. It follows that in fact we do not learn a Rank-1 matrix, but an up to Rank-J covariance matrix, if all the s j r j are independent. Hence the choice of J acts as an approximation factor of the covariance matrix. Wen et al. <ref type="bibr" target="#b69">[70]</ref> tested different values of J and found that J = 4 was the best compromise, which we also use here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Computational complexity</head><p>Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b71">72]</ref> studied the weight modes computed by Deep Ensembles under a BNN lens, yet these approaches are computationally prohibitive at the scale required for practical computer vision tasks. Recently, Dusenberry et al. <ref type="bibr" target="#b9">[10]</ref> proposed a more scalable approach for BNNs, which can still be subject to high instabilities as the ELBO loss is applied over a high-dimensional parameter space, all BatchEnsemble parameters. Increased stability can be achieved by leveraging large mini-batches that bring more robust feature and gradient statistics, at significantly higher computational cost (large virtual mini-batches are obtained through distributed training over multiple TPUs). In comparison, our approach has a smaller memory overhead since we encode r j in a lower dimensional space (we found empirically that a latent space of size only 32 provides an appealing compromise between accuracy and compactness). The ELBO loss here is applied over this lower-dimensional space which is easier to optimize. The only additional cost in terms of parameters and memory used w.r.t. BE is related to the compact VAEs associated with each layer.  In addition to the lower number of parameters, LP-BNN training is more stable than for Rank-1 BNN <ref type="bibr" target="#b9">[10]</ref> due to the reconstruction term r j −r j 2 2 which regularizes the L LP-BNN loss in Eq. (7) of the main paper by controlling the variances of the sampled weights. In practice, to train BNNs successfully, a range of carefully crafted heuristics are necessary, e.g., clipping, initialization from truncated Normal distributions, extra weight regularization to stabilize training <ref type="bibr" target="#b9">[10]</ref>. For LP-BNN, training is overall straightforward even on complex and deep models, e.g., DeepLabV3+, thanks to the VAE module that is stable and trains faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Stability of Bayesian Neural Networks</head><p>In this section, we experiment on CIFAR-10 to evaluate the stability of LP-BNN versus a classic BNN. For this experiment, we use the LeNet-5 architecture and choose a weight decay of 1e − 4 along with a mini-batch size of 128. Our goal is to see whether both techniques are stable when we vary the learning rate. Both DNNs were trained under the exact same conditions for 80 epochs. In <ref type="table" target="#tab_6">Table S1</ref>, we present two metrics for both DNNs. The first metric is the accuracy. The second metric is the epoch during which the training loss of the DNN explodes, i.e., is equal to infinity. This phenomenon may occur if the DNN is highly unstable to train. We argue that LP-BNN is visibly more stable and standard BNNs during training. We can see from <ref type="table" target="#tab_6">Table S1</ref> that LP-BNN is more stable than the standard BNN as it does not diverge for a wide range of learning rates. Moreover, its accuracy is higher than that of a standard BNN implemented on the same architecture, a property that we attribute to the VAE regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. LP-BNN diversity</head><p>At test-time, BNNs and DE aggregate the different predictions. For BNNs, these predictions come from the different realizations of the posterior distribution, while, for the DE, these predictions are provided by several DNNs trained in parallel. As proved in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b58">59]</ref>, the diversity among these different predictions is key to quantify the uncertainty of    <ref type="bibr" target="#b58">[59]</ref>, we evaluate the ratio-error introduced in <ref type="bibr" target="#b0">[1]</ref>. The ratio-error between two classifiers is the number of data samples on which only one classifier is wrong divided by the number of samples on which they are both wrong. A higher value means that the two classifiers are less likely to make the same errors. We also evaluated the Q-statistics <ref type="bibr" target="#b0">[1]</ref>, which measures the diversity between two classifiers. The value of the Q-statistics is between −1 and 1 and is defined as:</p><formula xml:id="formula_10">Q = N 11 N 00 − N 10 N 01 N 11 N 00 + N 10 N 01 (S4)</formula><p>where N 11 and N 00 are the numbers of data on which both classifiers are correct and incorrect, respectively. N 10 and N 01 are the number of data where just one of the two classifiers is wrong. If the two classifiers are always wrong or right for all data, then N 10 =N 01 =0 and Q=1, while if both classifiers always make errors on different inputs, then Q= − 1.</p><p>The maximum diversity comes when Q is minimum. Finally, we evaluated the correlation coefficient <ref type="bibr" target="#b0">[1]</ref>, which assesses the correlation between the error vectors of the two classifiers. <ref type="table" target="#tab_2">Tables S2 and S3</ref> illustrate that, for the normal case (CIFAR-10), LP-BNN displays similar diversity with DE, while in the corrupted case (CIFAR-10-C) LP-BNN achieves better diversity scores. We conclude that in terms of diversity metrics, LP-BNN has indeed the behavior that one would expect for uncertainty quantification purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>For our implementation, we use PyTorch <ref type="bibr" target="#b54">[55]</ref> and will release the code after the review in order to facilitate reproducibility and further progress. In the following we share the hyper-parameters for our experiments on image classification and semantic segmentation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Semantic segmentation experiments</head><p>In <ref type="table" target="#tab_11">Table S4</ref>, we summarize the hyper-parameters used in the StreetHazards <ref type="bibr" target="#b22">[23]</ref> and BDD-Anomaly <ref type="bibr" target="#b22">[23]</ref> experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Image classification experiments</head><p>In <ref type="table" target="#tab_12">Table S5</ref>, we summarize the hyper-parameters used in the CIFAR-10 <ref type="bibr" target="#b37">[38]</ref> and CIFAR-100 <ref type="bibr" target="#b37">[38]</ref> experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Notations</head><p>In <ref type="table">Table S6</ref>, we summarize the main notations used in the paper. <ref type="table">Table S6</ref> should facilitate the understanding of Section 2 (the preliminaries) and Section 3 (the presentation of our approach) of the main paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LP-BNN</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Diagram of a BatchEnsemble layer that generates for an ensemble of size J=2, the ensemble weights Wj from shared weights Wshare and fast weights Wj=rjs j , with j ∈ 1, J .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Diagram of a LP-BNN layer that generates for an ensemble of size J=2, ensemble weights Wj from shared weights Wshare and fast weights sj andrj, the latter sampled and decoded from the corresponding latent projection zj of rj, with j ∈ 1, J . empirical mean of the likelihoods of the ensemble:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Diversity of predictions of different ensemble methods. Col. 1: in top-down order images from the test set of CIFAR-10, CIFAR-10-C, and SVHN; Col. 2−4: outputs of different submodels of the three ensemble techniques: LP-BNN, BE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2), computational complexity ( §A.3), training stability of LP-BNN ( §A.4), and diversity of LP-BNN ( §A.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Calibration at different levels of corruption. We report ECE scores for LP-BNN, BE<ref type="bibr" target="#b69">[70]</ref>, and DE<ref type="bibr" target="#b38">[39]</ref> on CIFAR-10-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S1 :</head><label>S1</label><figDesc>Diversity of predictions of different ensemble methods. The first row contains in order two images from the test set of CIFAR-10, of CIFAR-10-C and of SVHN, respectively. The next three rows represent the corresponding outputs of the different sub models for the three ensembling algorithms being considered: LP-BNN, BatchEnsemble and Deep Ensembles.NotationsMeaningD = {(x i , y i )} n i=1the set of n data samples and the corresponding labels Θ the set of weights of a DNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>AUC ↑ AUPR ↑ FPR-95-TPR ↓ ECE ↓ cA ↑</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Acc ↑ cE ↓</cell><cell cols="2">Acc ↑ ECE ↓</cell><cell>cA ↑</cell><cell>cE ↓</cell></row><row><cell>MCP + cutout [25]</cell><cell>96.33 0.9600</cell><cell>0.9767</cell><cell>0.115</cell><cell>0.0207 32.98 0.6167</cell><cell cols="4">80.19 0.1228 19.33 0.7844</cell></row><row><cell>MC dropout [17]</cell><cell>95.95 0.9126</cell><cell>0.9511</cell><cell>0.282</cell><cell>0.0172 32.32 0.6673</cell><cell cols="4">75.40 0.0694 19.33 0.5830</cell></row><row><cell>MC dropout +cutout [17]</cell><cell>96.50 0.9273</cell><cell>0.9603</cell><cell>0.242</cell><cell>0.0117 32.35 0.6403</cell><cell cols="4">77.92 0.0672 27.66 0.5909</cell></row><row><cell>DUQ [67]  †</cell><cell>87.48 0.7083</cell><cell>0.8114</cell><cell>0.698</cell><cell>0.3983 64.89 0.2542</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DUQ Resnet18 [67]  ‡</cell><cell>93.36 0.8994</cell><cell>0.9213</cell><cell>0.1964</cell><cell>0.0131 69.01 0.5059</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDL [63]  †</cell><cell>85.73 0.9002</cell><cell>0.9198</cell><cell>0.247</cell><cell>0.0904 59.54 0.3412</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MIMO [21]</cell><cell>94.96 0.9387</cell><cell>0.9648</cell><cell>0.175</cell><cell cols="5">0.0300 69.99 0.1846 0.7869 0.1018 0.4735 0.2832</cell></row><row><cell cols="2">Deep Ensembles + cutout [39] 96.74 0.9803</cell><cell>0.9896</cell><cell>0.071</cell><cell>0.0093 68.75 0.1414</cell><cell cols="4">83.01 0.0673 47.35 0.2023</cell></row><row><cell cols="2">BatchEnsembles + cutout [70] 96.48 0.9540</cell><cell>0.9731</cell><cell>0.132</cell><cell>0.0167 71.67 0.1928</cell><cell cols="4">81.27 0.0912 47.44 0.2909</cell></row><row><cell>LP-BNN (ours) + cutout</cell><cell>95.02 0.9691</cell><cell>0.9836</cell><cell>0.103</cell><cell>0.0094 69.51 0.1197</cell><cell>79.3</cell><cell cols="3">0.0702 48.40 0.2224</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Runtime and memory analysis. Numbers correspond to StreetHazards images processed with DeepLabv3+ ResNet-50 with PyTorch on a PC: Intel Core i9-9820X and 1× GeForce RTX 2080Ti. Colored numbers are relative to vanilla approach. Mini- batch size for training is 4 and for testing 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>AUC ↑ AUPR ↑ FPR-95-TPR ↓ ECE ↓</figDesc><table><row><cell cols="3">Dataset mIoU ↑ StreetHazards OOD method Baseline (MCP) [25] 53.90 TRADI [15] 52.46 Deep Ensembles [39] 55.59 DeepLabv3+ MIMO [21] 55.44 ResNet50 BatchEnsemble [70] 56.16</cell><cell>0.8660 0.8739 0.8794 0.8738 0.8817</cell><cell>0.0691 0.0693 0.0832 0.0690 0.0759</cell><cell>0.3574 0.3826 0.3029 0.3266 0.3285</cell><cell>0.0652 0.0633 0.0533 0.0557 0.0609</cell></row><row><cell></cell><cell>LP-BNN (ours)</cell><cell>54.50</cell><cell>0.8833</cell><cell>0.0718</cell><cell>0.3261</cell><cell>0.0520</cell></row><row><cell></cell><cell>LP-BNN + GN (ours)</cell><cell>56.12</cell><cell>0.8908</cell><cell>0.0742</cell><cell>0.2999</cell><cell>0.0593</cell></row><row><cell></cell><cell>Baseline (MCP) [25]</cell><cell>47.63</cell><cell>0.8515</cell><cell>0.0450</cell><cell>0.2878</cell><cell>0.1768</cell></row><row><cell>BDD-Anomaly DeepLabv3+ ResNet50</cell><cell>TRADI [15] Deep Ensembles [39] MIMO [21] BatchEnsemble [70]</cell><cell>44.26 51.07 47.20 48.09</cell><cell>0.8480 0.8480 0.8438 0.8427</cell><cell>0.0454 0.0524 0.0432 0.0449</cell><cell>0.3687 0.2855 0.3524 0.3017</cell><cell>0.1661 0.1419 0.1633 0.1690</cell></row><row><cell></cell><cell>LP-BNN (ours)</cell><cell>49.01</cell><cell>0.8532</cell><cell>0.0452</cell><cell>0.2947</cell><cell>0.1716</cell></row><row><cell></cell><cell>LP-BNN + GN (ours)</cell><cell>47.15</cell><cell>0.8553</cell><cell>0.0577</cell><cell>0.2866</cell><cell>0.1623</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>[i, j]) is the variance of the coefficient [i, j] of matrix W 1 . Similarly, for W 2 we will have a diagonal matrix. Now, let us assume that W 1 and W 2 have a latent representationZ 1 = Z 1 [1], Z 1 [2], Z 1 [3] ∈ R 3 and Z 2 = Z 2 [1], Z 2 [2], Z 3 [3]∈ R 3 , respectively, such that for every coefficient [i, j] of W 1 and W 2 there exist real weights {α</figDesc><table><row><cell cols="2">var(W 1 [1, 1])</cell><cell>0</cell><cell cols="2">0 . . .</cell><cell>0</cell><cell></cell></row><row><cell>0 . . .</cell><cell></cell><cell cols="3">var(W 1 [2, 1]) 0 . . . . . . . . . . . .</cell><cell>0 . . .</cell><cell>   </cell></row><row><cell>0</cell><cell></cell><cell>0</cell><cell cols="3">0 . . . var(W 1 [d, h])</cell></row><row><cell cols="6">where var(W 1 [i,j] 1 [k]} 3 k=1</cell><cell>and</cell></row><row><cell cols="4">{α 2 [k]} 3 [i,j] k=1 such that: W 1 [i, j]=</cell><cell cols="2">3 k=1 α 1 [k]Z 1 [k] [i,j]</cell></row><row><cell>and W 2 =</cell><cell cols="2">3 k=1 α 2 [k]Z 2 [k][i,j]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>BNN accuracy 22.48 44.60 49.83 48.70 56.69</figDesc><table><row><cell>Learning Rate</cell><cell>0.2</cell><cell>0.1</cell><cell>0.05</cell><cell>0.01 0.005</cell></row><row><cell>BNN epoch div</cell><cell>3</cell><cell>25</cell><cell>65</cell><cell>None None</cell></row><row><cell>LP-BNN accuracy</cell><cell cols="4">20.02 55.04 59.68 63.73 64.41</cell></row><row><cell>LP-BNN epoch div</cell><cell>3</cell><cell cols="3">None None None None</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S1 :</head><label>S1</label><figDesc>Stability analysis of BNNs. Stability experiment with LeNet 5 architecture and 80 epochs on CIFAR-10. On the epoch divergence row, None means that the DNN does not diverge.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>ratio errors ↑ Q-statistic ↓ correlation coefficient ↓</figDesc><table><row><cell>DE</cell><cell>0.9825</cell><cell>0.9877</cell><cell>0.6583</cell></row><row><cell>BE</cell><cell>0.5915</cell><cell>0.9946</cell><cell>0.7634</cell></row><row><cell>LP-BNN</cell><cell>0.8390</cell><cell>0.9842</cell><cell>0.6601</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S2 :</head><label>S2</label><figDesc>Comparative results of diversity scores for image classification on the CIFAR-10 dataset.</figDesc><table><row><cell></cell><cell cols="3">ratio errors ↑ Q-statistic ↓ correlation coefficient ↓</cell></row><row><cell>DE</cell><cell>0.4193</cell><cell>0.9690</cell><cell>0.7568</cell></row><row><cell>BE</cell><cell>0.2722</cell><cell>0.9874</cell><cell>0.8352</cell></row><row><cell>LP-BNN</cell><cell>0.4476</cell><cell>0.9595</cell><cell>0.7332</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S3 :</head><label>S3</label><figDesc>Comparative results of diversity scores for image classification on the CIFAR-10-C dataset.</figDesc><table><row><cell>a DNN. Indeed, we want the different DNN predictions to</cell></row><row><cell>have a high variance when the model is not accurate. Fig-</cell></row><row><cell>ure S1 highlights this desirable property of high variance on</cell></row><row><cell>out-of-distribution samples exhibited by LP-BNN. Also, as</cell></row><row><cell>in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S4 :</head><label>S4</label><figDesc>Hyper-parameter configuration used in the semantic segmentation experiments ( §5.3).</figDesc><table><row><cell>Hyper-parameter</cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>Ensemble size J</cell><cell>4</cell><cell>4</cell></row><row><cell>initial learning rate</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>batch size</cell><cell>128</cell><cell>128</cell></row><row><cell>lr decay ratio</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>lr decay epochs</cell><cell cols="2">[80, 160, 200] [80, 160, 200]</cell></row><row><cell>number of train epochs</cell><cell>250</cell><cell>250</cell></row><row><cell>weight decay for θ slow weights</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>weight decay for θ fast weights</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>cutout</cell><cell>True</cell><cell>True</cell></row><row><cell>SyncEnsemble BN</cell><cell>False</cell><cell>False</cell></row><row><cell>Size of the latent space z</cell><cell>32</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S5 :</head><label>S5</label><figDesc>Hyper-parameter configuration used in the classification experiments ( §5.2).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparison of classifier selection methods for improving committee performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Aksela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multiple Classifier Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pitfalls of in-domain uncertainty estimation and ensembling in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lyzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uncertainty estimation via stochastic batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsenii</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umang</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Antorán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Fogliato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabrielle</forename><surname>Gauthier Melançon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranganath</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omesh</forename><surname>Tickoo</surname></persName>
		</author>
		<idno>arXiv, 2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient and scalable bayesian neural nets with rank-1 factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the expressiveness of approximate inference in bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep ensembles: A loss landscape perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mark van der Wilk, and Laurence Aitchison. Bayesian neural network priors revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Vincent Fortuin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AABI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tradi: Tracking deep neural network weight distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Franchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Aldea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Séverine</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Training independent subnetworks for robust prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marton</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><forename type="middle">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<idno>arXiv, 2020. 6</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. A benchmark for anomaly segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Miguel Hernández-Lobato</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hora</surname></persName>
		</author>
		<idno>1996. 5</idno>
	</analytic>
	<monogr>
		<title level="j">RESS</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2-3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In UAI</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Being bayesian about categorical probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taejong</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uijung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Gwan</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian process priors for bayesian neural network weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic meta-representations of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In UAI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Second opinion needed: communicating uncertainty in medical machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kompa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew L</forename><surname>Beam</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Why m heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heerad</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Structured and efficient variational deep learning with matrix gaussian posteriors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bayesian methods for adaptive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A practical bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A simple baseline for bayesian uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wesley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Chervontsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Regression prior networks</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Concrete problems for autonomous vehicle safety: Advantages of bayesian deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pep: Parameter ensembling by perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purang</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demian</forename><surname>Wassermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Slang: Fast structured covariance approximations for bayesian deep learning with natural gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Kunstner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Emtiyaz</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Structured weight priors for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brintrup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLW</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rethinking normalization and elimination singularity in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Weight standardization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dice: Diversity in deep ensembles via conditional redundancy adversarial estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melih</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning structured weight uncertainty in bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Information aware max-norm dirichlet networks for predictive uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Tsiligkaridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Uncertainty estimation using a single deep deterministic neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Geometric structure of high-dimensional data and dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Improving calibration of batchensemble with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassen</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLW</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Veeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linh</forename><surname>Swiątkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<title level="m">Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the bayes posterior in deep neural networks really? In ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bayesian deep learning and a probabilistic perspective of generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Noisy natural gradient as variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
