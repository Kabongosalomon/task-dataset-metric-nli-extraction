<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps with Accurate Object Boundaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
							<email>junjie.hu@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Advanced Intelligence project</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
							<email>mozay@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<email>zhang@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Advanced Intelligence project</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
							<email>okatani@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Advanced Intelligence project</orgName>
								<orgName type="institution">RIKEN</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps with Accurate Object Boundaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers the problem of single image depth estimation. The employment of convolutional neural networks (CNNs) has recently brought about significant advancements in the research of this problem. However, most existing methods suffer from loss of spatial resolution in the estimated depth maps; a typical symptom is distorted and blurry reconstruction of object boundaries. In this paper, toward more accurate estimation with a focus on depth maps with higher spatial resolution, we propose two improvements to existing approaches. One is about the strategy of fusing features extracted at different scales, for which we propose an improved network architecture consisting of four modules: an encoder, decoder, multi-scale feature fusion module, and refinement module. The other is about loss functions for measuring inference errors used in training. We show that three loss terms, which measure errors in depth, gradients and surface normals, respectively, contribute to improvement of accuracy in an complementary fashion. Experimental results show that these two improvements enable to attain higher accuracy than the current state-of-the-arts, which is given by finer resolution reconstruction, for example, with small objects and object boundaries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of estimating the depth map of a scene from its single image has attracted a lot of attention in the field of computer vision, since depth maps have a lot of applications, such as augmented reality <ref type="bibr" target="#b21">[22]</ref>, human computer interaction <ref type="bibr" target="#b32">[33]</ref>, human activity recognition <ref type="bibr" target="#b11">[12]</ref>, scene recognition <ref type="bibr" target="#b40">[41]</ref>, and segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref>. The recent employment of convolutional neural networks (CNNs) has accelerated the research of the problem <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>. In early studies, depth maps can be estimated only with much lower resolution than input images, which is mostly at- tributable to a series of downsampling operations performed in CNNs. To recover the degraded spatial resolution, a number of studies have been conducted so far. First, a new upsampling method called up-projection was proposed in <ref type="bibr" target="#b15">[16]</ref>. Integration of CRFs into CNNs for depth refinement, along with its end-to-end training method, was proposed in <ref type="bibr" target="#b39">[40]</ref>. Several works applied joint multi-task learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>, which provides a good amount of improvement of estimated depth maps. Recently, dilated convolution was employed in <ref type="bibr" target="#b9">[10]</ref>, updating the state-of-the-art accuracy.</p><p>In this paper, we argue that there is room for further improvements despite these previous efforts. In the depth maps estimated by the previous methods, including the one that has achieved the state-of-the-art accuracy, we can observe distortions in object shapes, missing small objects, and mosaic patterns, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. Depth maps with high spatial resolution, e.g., precise object boundaries, are especially important for some applications such as object recognition <ref type="bibr" target="#b34">[35]</ref> and depth-aware image re-rendering and editing <ref type="bibr" target="#b23">[24]</ref>.</p><p>The features learned in different layers of CNNs for depth estimation, which represent information at different scales, should contain different depth cues. For example, the lower layer features may have information on the details of object shapes, while the higher layer features represent global depth information without details of object shapes. Therefore, the former and latter should be used in a complementary fashion to estimate more accurate depth maps. Toward this end, we propose a feature fusion module that up-scales lower layer features to the same (high) level of resolution using skip connections, and then learns how to fuse the up-scaled different scale features with a convolutional layer. We also employ a decoder module to decode the high level feature extracted by the encoder. Then, the outputs of the two modules are integrated and further refined to provide final estimation.</p><p>To summarize, we propose an improved architectural design that consists of four modules, namely, i) an encoder (E) used for multi-scale feature extraction, ii) a decoder (D) used for feature decoding, iii) a multi-scale feature fusion (MFF) module to fuse features of different resolutions extracted by E, and iv) a refinement module (R) to refine and integrate features obtained from D and MFF for final prediction. The proposed framework is shown in <ref type="figure">Fig. 2</ref>. Note that the employment of symmetric skip connections was widely used in previous studies on many tasks, it needs pay attention to fix the channels of the same scale features between the encoder and decoder, while the proposed feature fusion module is more flexible to use and it can be used with different backbone networks employed for the decoder-encoder part. In our experiments, we show results obtained with several backbone networks, such as ResNet <ref type="bibr" target="#b12">[13]</ref>, DenseNet <ref type="bibr" target="#b10">[11]</ref>, and SENet <ref type="bibr" target="#b13">[14]</ref>. Our method is trained in an end to end fashion without employing any post-processing refinement.</p><p>We also propose an improved loss function for training the CNNs. It is motivated by the previous studies on statistical properties of range images of natural scenes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, the authors analyzed distribution of three-dimensional points using co-occurrence statistics, and two-and three-dimensional joint distributions of Haar filter reactions. The results indicate that the range images have much simpler structures than optical images, which is also known as the "random collage model", that is, the world can be broken down into piecewise smooth regions that depend little on each other and sharp discontinuities in between them. The sharp discontinuities typically emerge at the occluding boundaries of objects in scenes, which form step edges in depth maps. This structure is a key property of depth maps (of natural scenes). As mentioned above, however, previous methods often fail to recover such edges correctly.</p><p>We argue that distorted and blurry edges emerged in the depth maps estimated by previous methods are mostly attributable to the loss function employed by them. Many previous studies use the sum of differences in depth between an estimated depth map and its ground truth for their loss function, with different norms, i.e., 2 loss [9, 23, 26], 1 loss <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>, and a robust berhu loss <ref type="bibr" target="#b15">[16]</ref>). We point out that this type of losses is insensitive to errors emerging at step edges, such as shift in their positions, and difference between sharp and blurry edges. We then propose to use two additional loss functions, difference in gradients (l grad ) and difference in normals to scene surfaces (l normal ) between an estimated map and its ground truth. We describe that the three loss functions are complementary with each other, and show that their combination contributes to improve accuracy particularly around object edges. We show experimental results that confirm effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Skip connections Skip connections have been widely used for many purposes. In particular, they are employed in encoder-decoder networks to recover the degraded resolution due to downsampling, as in U-Net <ref type="bibr" target="#b6">[7]</ref>. In that case, skip connections are symmetrically inserted between the same level of layers in the encoder and decoder. This design is applied to image restoration <ref type="bibr" target="#b28">[29]</ref> , semantic segmentation <ref type="bibr" target="#b17">[18]</ref>, and others. In this paper, we use skip connections in a different way. Instead of symmetric application between the encoder and decoder, we directly merge the up-scaled feature maps from the different layers of the encoder to the final decoder output. This is done by the proposed multiscale feature fusion module.</p><p>Gradient-based loss Several existing studies also employ functions of depth gradients for losses <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>. Most of previous methods are built upon the framework of multi-task learning, such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> where semantic segmentation, surface normals etc. are simultaneously learned. On the other hand, we are interested in the following question: how far can we go in the minimal case where only ground truth depths are available for training? This question is important, because RGBD cameras have advanced greatly, making them easier to use, whereas segmentation labels are costly to obtain. Therefore, we revisit this minimal formulation. We analyze how the three different losses (i.e., l depth , l grad , and l normal ) can contribute to the improvement of accuracy. There are other works that use gradient-based losses including DeMon <ref type="bibr" target="#b35">[36]</ref>, which consider different problems from single image depth estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Improved Network Design</head><p>The proposed network architecture consists of four modules: an encoder (E), a decoder (D), a multi-scale feature fusion module (MFF), and a refinement module (R), as shown in <ref type="figure">Fig. 2</ref>. The encoder extracts features at multiple scales: 1/4, 1/8, 1/16, and 1/32. The decoder employs four upprojection modules <ref type="bibr" target="#b15">[16]</ref> to gradually up-scale the final feature from the encoder while decreasing the number of channels. Similar encoder-decoder networks were employed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>, but their outputs tend to lose spatial resolution and not to preserve object shapes.</p><p>The MFF module integrates four different scale features from the encoder using up-projection and channel-wise concatenation. To be specific, the outputs of the four encoder blocks (each having 16 channels) are upsampled by ×2, 4, 8, and 16, respectively, so as to have the same size as the fi- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Output <ref type="table" target="#tab_0">114×152  3  64  block1  57×76  64  256  block2  29×38  256  512  block3  15×19  512  1020  block4  8×10  1020  2040  conv2  8×10  2040  1020  up1  15×19  1020  512  up2  29×38  512  256  up3  57×76  256  128  up4  114×152  128  64  up5  114×152  256  16  up6  114×152  512  16  up7  114×152  1020  16  up8  114×152  2040  16  conv3  114×152  64  64  conv4  114×152  128  128  conv5  114×152  128  128  conv6  114×152  128  1</ref> nal output. This upsampling is done in a channel-wise manner. Then they are concatenated and further transformed by a convolutional layer to obtain the output of the MFF module, which has 64 channels. The main purpose of the MFF module is to merge different information at multiple scales into one. It is seen in <ref type="figure" target="#fig_2">Fig. 4</ref> that the lower layer outputs of the encoder retain information with finer spatial resolution, which should be utilized to restore the fine details lost due to the multiple applications of downsampling. The feature from the decoder and fused multi-scale features from MFF are then concatenated in their channels and fed to the refinement module having three convolutional layers to give the final prediction. MFF and the refinement module only add a small number of parameters as compared with the encoder-decoder part. For instance, if we use the ResNet-50 as the encoder, then the encoder-decoder part has 63.6M parameters, while the feature fusion and refinement modules have only 4M parameters.</p><formula xml:id="formula_0">Size Input/C Output/C conv1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Functions</head><p>Most of the previous studies employ the sum of the difference between the depth estimate d i and its ground truth g i for a loss:</p><formula xml:id="formula_1">l 1 = 1 n n i=1 e i ,<label>(1)</label></formula><p>where e i = d i − g i 1 . Some use its 2 norm or robustified version. There are two issues with this type of losses.</p><p>One is that a unit depth difference (e.g., 1cm) has an equal contribution to the loss between distant and nearby points in a scene. It should be more on nearby points and less on distant points. This is pointed out in <ref type="bibr" target="#b20">[21]</ref>, where a depthbalanced Euclidean loss is employed. We propose a simpler remedy here, which is to use the logarithm of depth errors as</p><formula xml:id="formula_2">l depth = 1 n n i=1 F (e i ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">F (x) = ln(x + α),<label>(3)</label></formula><p>where α(&gt; 0) is a parameter we set. The other issue, which is the main concern here, is that for a step edge structure of depth, while the above conventional loss is sensitive to shifts in depth direction, it is comparatively insensitive to shifts in x and y directions, as illustrated in the top and second rows of <ref type="figure">Fig. 5</ref>. It is similarly insensitive also to distortion and blur of edges.</p><p>The statistics of natural range images indicate that natural scenes consist of a lot of such step edge structures <ref type="bibr" target="#b14">[15]</ref>, which can easily be confirmed from examples of ground <ref type="figure">Figure 5</ref>. The three loss functions have orthogonal sensitivities to different types of errors of estimated depth maps. The solid and dotted lines depicted in the first column indicate two depth maps under comparison, where they are represented by one-dimensional depth images for the sake of explanation, and the vertical axis is depth and the horizontal axis is, say, the x axis of the images. truth depth maps in various datasets. We think that this insensitivity to small errors around edges must be a major reason for the phenomenon that the edges in depth maps estimated by CNNs trained using this loss tend to be distorted or blurry.</p><formula xml:id="formula_4">l depth l grad l normal ✗ ✓ ✗ ✗ ✓ ✓ ✓ ✗ ✓</formula><p>Thus, it is necessary to penalize such errors around edges more. Thus, we consider the following loss function of the gradients of depth;</p><formula xml:id="formula_5">l grad = 1 n n i=1 (F (∇ x (e i )) + F (∇ y (e i ))),<label>(4)</label></formula><p>where ∇ x (e i ) is the spatial derivative of e i computed at the i th pixel with respect to x, and so on. This loss is sensitive to the shift of edges in x and y directions, as shown in <ref type="figure">Fig. 5</ref>. Note that the proposed two loss functions l depth and l grad work in a complementary manner for different types of errors. Thus, we use the (weighted) sum of l depth and l grad to train our networks. Depth maps of natural scenes can roughly be modeled by a limited number of smooth surfaces and step edges in between them, according to the statistics of natural range images <ref type="bibr" target="#b14">[15]</ref>. For instance, depth will often be discontinuous at the boundary of an object. Errors around such strong edges are well penalized by l grad . However, since depth differences at such occluding boundaries of objects can sometimes be very large, we must choose a modest (i.e., not very large) weight λ &gt; 0 on l grad (Note that l grad is not upper bounded.). Then, the term λl grad cannot penalize small structural errors such as those of high-frequency undulation of a surface, as shown in the bottom row of <ref type="figure">Fig. 5</ref>.</p><p>To deal with such small depth structures and further improve fine details of depth maps, we consider yet another loss for training, which measures accuracy of the normal to the surface of an estimated depth map with respect to its ground truth. Denoting the surface normal of an estimated depth map and its ground truth by n d i ≡ [−∇ x (d i ), −∇ y (d i ), 1] and n g i ≡ [−∇ x (g i ), −∇ y (g i ), 1] respectively, we define the following loss measuring the difference between the two normals by</p><formula xml:id="formula_6">l normal = 1 n n i=1 1 − n d i , n g i n d i , n d i n g i , n g i ,<label>(5)</label></formula><p>where ·, · denotes the inner product of vectors. Although this loss is also computed from depth gradients, it measures the angle between two surface normals, and thus is sensitive to small depth structures illustrated in the bottom row of <ref type="figure">Fig. 5</ref>. Thus, we can say that l normal is also complementary with the other two losses. Finally, we define the loss by</p><formula xml:id="formula_7">L = l depth + λl grad + µl normal ,<label>(6)</label></formula><p>where λ, µ ∈ R are weighting coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Accuracy Measures for Depth Estimation</head><p>Denoting the total number of (valid) pixels used in all evaluated images by T , we use the following accuracy measures that are commonly employed in the previous studies:</p><p>• Root mean squared error (RMS):</p><formula xml:id="formula_8">1 T T i=1 (d i − g i ) 2 . • Mean relative error (REL): 1 T T i=1 di−gi 1 gi .</formula><p>• Mean log 10 error (log 10): 1 T T i=1 log 10 d i − log 10 g i 1 .</p><p>• Thresholded accuracy: Percentage of d i , such that max di gi , gi di = δ &lt; threshold.</p><p>These popular measures enable us to compare methods from multiple aspects to evaluate depth accuracy. However, a combination of these measures still has limitation. We argue that these measures are not good at detecting spatial distortion of object edges, because of the same reason as we discussed before. For instance, the method of Ma and Karaman <ref type="bibr" target="#b27">[28]</ref>, which leverages known depths at a few scene points to improve depth estimation, does outperform other methods that do not use such additional information, if we use the above measures. However, the outputs of their method tend to have spatially distorted or blurry object edges; local structures are often missing. The same tendency can be observed for others, particularly the recent ones; estimated depth maps showing small RMS errors tend to have apparent errors of this type.</p><p>Edge accuracy: In order to more properly evaluate accuracy of estimated depth maps, we propose an additional measure that is sensitive to positional errors of edges which will be overlooked by the above measures. For this purpose, we apply the Sobel operator <ref type="bibr" target="#b16">[17]</ref> to both of the estimated and the true depth maps, and then apply a threshold to them to identify pixels which satisfy f x (i) 2 + f y (i) 2 &gt; (threshold) by 'pixels on edges', where f x and f y are 3 × 3 horizontal and vertical Sobel operators, respectively. We used three different thresholds: 0.25, 0.5, and 1. Assuming those of the true depth map to be true, we measure precision (P), recall (R) and F1 score for those of the estimated map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We use the NYU-Depth V2 dataset <ref type="bibr" target="#b33">[34]</ref> which consists of a variety of indoor scenes, and is the most widely used for the task of single view depth prediction. In the previous studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40]</ref>, it is shown that data argumentation helps to improve accuracy as well as to avoid over-fitting. We employ the following data augmentation methods, which are individually applied to each sample (an RGB image and the corresponding depth map) in an online manner:</p><p>• Flip: The RGB and the depth image are both horizontally flipped with 0.5 probability. We follow the same procedure as the one employed in the previous studies. We use the official splits for 464 scenes, i.e., 249 scenes for training and 215 scenes for testing. Following previous methods, we downsample images from original size (640×480) to 320×240 pixels using bilinear interpolation, and then crop their central parts to obtain images with 304×228 pixels. For training, the depth maps are downsampled to 114×152 to fit the size of output. For testing, following the previous studies, we use the same small subset of 654 samples.</p><p>We train the proposed network for 20 epochs. The encoder module in the network is initialized by a model pretained with the ImageNet dataset <ref type="bibr" target="#b4">[5]</ref>. The other layers in the network are randomly initialized. We use Adam optimizer with an initial learning rate of 0.0001, and reduce it to 10% for every 5 epochs. We set β 1 = 0.9, β 2 = 0.999, and use weight decay of 0.0001. The weights λ of l grad and µ of l normal are set as λ = 1 and µ = 1, and α in the mapping function is set to 0.5 in all the experiments. We conducted all the experiments using PyTorch <ref type="bibr" target="#b30">[31]</ref> with batch size of 8. <ref type="table">Table 2</ref>. Comparisons of different methods on the NYU-Depth V2 dataset. The methods marked by * use partially known depths, and those with * * employ joint task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>RMS REL log 10 δ &lt; 1.25 δ &lt; 1.25 2 δ &lt; 1.25 3 Eigen et al. <ref type="bibr" target="#b8">[9]</ref> 0.907 0.215 -0.611 0.887 0.971 Liu et al. <ref type="bibr" target="#b24">[25]</ref> 0.824 0.230 0.095 0.614 0.883 0.971 Chakrabarti et al. <ref type="bibr" target="#b2">[3]</ref> 0.620 0.149 -0.806 0.958 0.987 Cao et al. <ref type="bibr" target="#b1">[2]</ref> 0.819 0.232 0.091 0.646 0.892 0.968 Li et al. <ref type="bibr" target="#b23">[24]</ref> 0.635 0.143 0.063 0.788 0.958 0.991 Ma and Karaman <ref type="bibr" target="#b27">[28]</ref> (-) 0.143 -0.810 0.959 0.989 Laina et al. <ref type="bibr" target="#b15">[16]</ref> 0.573 0.127 0.055 0.811 0.953 0.988 Xu et al. <ref type="bibr" target="#b39">[40]</ref> 0.586 0.121 0.052 0.811 0.954 0.987 Lee et al. <ref type="bibr" target="#b20">[21]</ref> 0.572 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>Table 2 1 shows the results of our method together with those of existing methods on the three basic measures. For the sake of references, the table includes the methods that use additional information other than the input RGB im-ages; those with * use relative depth between pairs or partially known depths <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>, and those with * * employ joint task learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39]</ref>. For the method of Ma and Karaman <ref type="bibr" target="#b27">[28]</ref>, we show two results that are obtained from single RGB images alone and with partially known depths (200 pixels). The methods denoted without a superscript <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref> and ours should be able to be compared in an equal condition.</p><p>Our method achieved the best performance on REL, log 10, and outperformed previous methods for δ &lt; 1.25, δ &lt; 1.25 2 , δ &lt; 1.25 3 with a large margin. It is observed that our method provided the second best performance for RMS, while the method of <ref type="bibr" target="#b9">[10]</ref> performed a little better on this measure. However, they used 120K training samples, and we used less number of training samples (50K samples). In addition, our method outperformed the methods proposed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>, which employed encoder-decoder networks with ResNet-50. <ref type="figure">Figure 6</ref> shows the depth maps estimated by different methods 2 , including joint multi-task learning method <ref type="bibr" target="#b7">[8]</ref>, encoder-decoder network <ref type="bibr" target="#b15">[16]</ref>, CRF-based refinement method <ref type="bibr" target="#b39">[40]</ref>, dilated ordinary regression network <ref type="bibr" target="#b9">[10]</ref> and our method. We show them in the ascending order of quality in traditional measures. It is seen that the methods of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref> suffer from heavy distortion of shapes, and the method of <ref type="bibr" target="#b9">[10]</ref> produces sharp discontinuities of shapes of Xu et al. <ref type="bibr" target="#b39">[40]</ref> Fu et al. <ref type="bibr" target="#b9">[10]</ref> Ours <ref type="figure">Figure 6</ref>. Results of different methods for six images. From the first to the last row; input RGB images, ground truth depth map, a multi-task learning method <ref type="bibr" target="#b7">[8]</ref>, encoder-decoder network <ref type="bibr" target="#b15">[16]</ref>, CRF-based method <ref type="bibr" target="#b39">[40]</ref>, dilated ordinary regression network <ref type="bibr" target="#b9">[10]</ref>, and our proposed network trained with the full loss function. We show them in the ascending order of quality in traditional measures.</p><p>objects, and mosaic effect is even observed. Although the method of <ref type="bibr" target="#b7">[8]</ref> shows relatively clear image boundaries, they produced many inaccurate weak edges which can be observed in the far side of each room. Our method shows significant performance by correctly recovering edges of objects and small structures, such as bottles in a kitchen and lamp shades on a desk.</p><p>We also observe a relationship between our proposed measure of edge accuracy and visual quality of estimated depth maps. For instance, our proposed method outperforms all the other methods given in <ref type="table" target="#tab_1">Table 3</ref>, and provides depth maps with the finest details in <ref type="figure">Fig.6</ref>. A similar relationship is also observed for the method of <ref type="bibr" target="#b7">[8]</ref> which shows the best scores among the previous works in <ref type="table" target="#tab_1">Table 3</ref>, and provides better visual results compared to these related work in <ref type="figure">Fig.6</ref>. <ref type="table">Table 4</ref>. Results of our method that is built on ResNet-50 trained with different loss functions on the NYU-Depth V2 dataset. For edge accuracy, we report the results for &gt;0.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMS REL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In order to compare and analyze the performance of the proposed loss functions, we train our model with different losses while using ResNet-50 as an encoder. In visual comparison of their estimated depth maps side by side, as shown in <ref type="figure" target="#fig_5">Figure 7</ref> with several samples, it can be observed that the results trained with l depth are more distorted and blurry. Moreover, the proposed loss function provides significant improvement especially for estimation of fine details and clear boundaries of objects in scenes. The numerical results given in <ref type="table">Table 4</ref>, show that finer details of objects in the scenes are gradually recovered using a model with l depth , l depth + λl grad and the full loss l depth + λl grad + µl normal , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented two improvements to existing methods for single image depth estimation. One is the improved design of network architecture. It consists of four modules: an encoder, a decoder, a multi-scale feature fusion module and a refinement module. Any base network can be used for the encoder, such as ResNet, DenseNet, and SENet. The overall network is trained in an end to end fashion without any post-processing refinement.</p><p>The previous methods fail to correctly estimate boundaries of objects in scenes. We explain that this failure may be attributable to the loss functions employed by the previous methods. The analyses of natural range image statistics indicate that the real world scene can be decomposed into smooth surfaces and sharp discontinuities in between them; the latter corresponds to object boundaries. Then, this makes it important to be able to accurately reconstruct those discontinuities, which appear as step edges in depth maps, and to deal with them appropriately during training of CNNs. We have made a simple analysis of how different loss functions affect measurement of estimation errors around step edges. Based on it, we argue that the loss of difference in depth is insensitive to positional shift and blurring of the edges, whereas the loss of difference in gradients tends to be sensitive to them. We further employ an additional loss of difference in surface normals, which is expected to be sensitive to small structures that tend to be neglected by the above two losses. We then propose to use a combined loss of the three loss functions.</p><p>Finally, we presented experimental results on the NYU Depth V2 dataset. We observed that existing measures, which make use of difference in depth, fail to correctly measure reconstruction error of step edges for evaluation of estimation accuracy. For more proper evaluation, we presented a simple measure of reconstruction accuracy of step edges. Our method outperforms previous methods using both traditional measures and the proposed measure, especially providing remarkable improvement on reconstruction of small objects and estimation of object boundaries. This agrees well with visual comparisons between the results of the proposed method and those of the previous ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example comparison of estimated depth maps; (a) RGB input, (b) ground truth depth, (c) the current state-of-the-art [10], and (d) our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>A diagram of the proposed network architecture. Given an input image, the encoder (E) extracts multi-scale features (1/4, 1/8, 1/16, and 1/32). The decoder (D) converts the last 1/32 scale feature to get a 1/2 scale feature. Each of the multi-scale features is up-scaled to 1/2 scale, and fused by the multi-scale feature fusion module (MFF). The outputs of D and MFF and are refined by the refinement module (R) to obtain the final depth map. Each box named "blockn" denotes a block of multiple convolutional layers, such as residual block of ResNet; each box named "upn" denotes a up-projection layer introduced in [16]. Batch normalization and ReLU nonlinearity are applied to the output of each convolutional layer except conv6. Diagrams of MFF, D and R. We employ the same upsampling strategy (up-projection) used in [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of outputs of different layers of the encoder network for the input image shown inFig. 2. Selected channels of (a) block1, and (b) block2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Rotation: The RGB and the depth image are both rotated by a random degree r ∈ [−5, 5]. • Color Jitter: Brightness, contrast, and saturation values of the RGB image are randomly scaled by c ∈ [0.6, 1.4].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Visual comparison of our method trained using ResNet-50 with different loss functions on the NYU-Depth V2 dataset. From top to bottom: Ground Truth, trained with l depth and the full loss, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Sizes of output features, and input/output channels of each layer when using a ResNet-50 as encoder.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Edge accuracy for the NYU-Depth V2. See text for details. The method marked by * * employs joint task learning.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>139</cell><cell>-</cell><cell>0.815</cell><cell>0.963</cell><cell>0.991</cell></row><row><cell></cell><cell>Fu et al. [10]</cell><cell></cell><cell cols="3">0.509 0.115 0.051 0.828</cell><cell>0.965</cell><cell>0.992</cell></row><row><cell></cell><cell>Qi et al. [32]</cell><cell></cell><cell cols="3">0.569 0.128 0.057 0.834</cell><cell>0.960</cell><cell>0.990</cell></row><row><cell></cell><cell cols="2">Ours (ResNet-50)</cell><cell cols="3">0.555 0.126 0.054 0.843</cell><cell>0.968</cell><cell>0.991</cell></row><row><cell></cell><cell cols="5">Ours (DenseNet-161) 0.544 0.123 0.053 0.855</cell><cell>0.972</cell><cell>0.993</cell></row><row><cell></cell><cell cols="2">Ours (SENet-154)</cell><cell cols="3">0.530 0.115 0.050 0.866</cell><cell>0.975</cell><cell>0.993</cell></row><row><cell></cell><cell cols="4">Ma and Karaman [28]  *  (-) 0.044</cell><cell>-</cell><cell>0.971</cell><cell>0.994</cell><cell>0.998</cell></row><row><cell></cell><cell>Li et al. [23]</cell><cell></cell><cell cols="3">0.821 0.232 0.094 0.621</cell><cell>0.886</cell><cell>0.968</cell></row><row><cell></cell><cell cols="4">Eigen and Fergus [8]  *  *  0.641 0.158</cell><cell>-</cell><cell>0.769</cell><cell>0.950</cell><cell>0.988</cell></row><row><cell></cell><cell cols="4">Dharmasiri et al. [6]  *  *  0.624 0.156</cell><cell>-</cell><cell>0.776</cell><cell>0.953</cell><cell>0.989</cell></row><row><cell></cell><cell>Xu et al. [39]  *  *</cell><cell></cell><cell cols="3">0.582 0.120 0.055 0.817</cell><cell>0.954</cell><cell>0.987</cell></row><row><cell cols="2">Threshold Method</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>&gt;0.25</cell><cell cols="4">Eigen and Fergus [8]  *  *  0.544 0.481 0.500</cell></row><row><cell></cell><cell>Laina et al [16]</cell><cell cols="3">0.489 0.435 0.454</cell></row><row><cell></cell><cell>Xu et al. [40]</cell><cell cols="3">0.516 0.400 0.436</cell></row><row><cell></cell><cell>Fu et al. [10]</cell><cell cols="3">0.320 0.583 0.402</cell></row><row><cell></cell><cell>Ours (SENet-154)</cell><cell cols="3">0.644 0.508 0.562</cell></row><row><cell>&gt;0.5</cell><cell cols="4">Eigen and Fergus [8]  *  *  0.587 0.456 0.501</cell></row><row><cell></cell><cell>Laina et al [16]</cell><cell cols="3">0.536 0.422 0.463</cell></row><row><cell></cell><cell>Xu et al. [40]</cell><cell cols="3">0.600 0.366 0.439</cell></row><row><cell></cell><cell>Fu et al. [10]</cell><cell cols="3">0.316 0.473 0.412</cell></row><row><cell></cell><cell>Ours (SENet-154)</cell><cell cols="3">0.668 0.505 0.568</cell></row><row><cell>&gt;1</cell><cell cols="4">Eigen and Fergus [8]  *  *  0.733 0.488 0.574</cell></row><row><cell></cell><cell>Laina et al [16]</cell><cell cols="3">0.670 0.479 0.548</cell></row><row><cell></cell><cell>Xu et al. [40]</cell><cell cols="3">0.794 0.407 0.525</cell></row><row><cell></cell><cell>Fu et al. [10]</cell><cell cols="3">0.483 0.512 0.485</cell></row><row><cell></cell><cell>Ours (SENet-154)</cell><cell cols="3">0.759 0.540 0.623</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">RMS values given in<ref type="bibr" target="#b27">[28]</ref> are omitted here (displayed as (-)), because we found an error in the authors' code, and believe that the numbers reported in their paper are miscalculated.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For<ref type="bibr" target="#b7">[8]</ref>, we show the results that are made publicly available by the authors. For<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10]</ref>, we use the authors' code to obtain the results and show them here.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The nonlinear statistics of high-contrast patches in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="83" to="103" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2658" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Joint prediction of depths, normals and surface curvature from rgb images using cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<idno>abs/1706.07593</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2650" to="2658" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K Q</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human activities recognition using depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y S</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistics of range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nassir. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Iro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vasileios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A 3x3 isotropic gradient operator for image processing. a talk at the Stanford Artificial Project in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="page" from="271" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rednet: Residual encoder-decoder network for indoor rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical analysis of local 3d structure in 2d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1114" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Depth-assisted real-time 3d object detection for augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICAT</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="126" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1119" to="1127" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3372" to="3380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Curve-structure segmentation from depth maps: A cnn-based approach and its application to exploring cultural heritage objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wilder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint estimation of camera pose, depth, deblurring, and super-resolution from a blurred image sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page" from="4623" to="4631" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Depth camera based hand gesture recognition and its applications in human-computerinteraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Information, Communications and Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Depth cnns for rgb-d scene recognition: Learning from scratch better than transferring from rgb-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4271" to="4277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Surge: Surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pad-net: Multitasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiscale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="161" to="169" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yüksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yüret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yemez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="894" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
