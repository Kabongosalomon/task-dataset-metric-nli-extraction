<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FIXUP INITIALIZATION: RESIDUAL LEARNING WITHOUT NORMALIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
							<email>hongyiz@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
							<email>tengyuma@stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FIXUP INITIALIZATION: RESIDUAL LEARNING WITHOUT NORMALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.</p><p>(i) without normalization, can a deep residual network be trained reliably? (And if so,) (ii) without normalization, can a deep residual network be trained with the same learning rate, converge at the same speed, and generalize equally well (or even better)?</p><p>Perhaps surprisingly, we find the answers to both questions are Yes. In particular, we show:</p><p>• Why normalization helps training. We derive a lower bound for the gradient norm of a residual network at initialization, which explains why with standard initializations, normalization techniques are essential for training deep residual networks at maximal learning rate. (Section 2) * Work done at Facebook. Equal contribution. † Work done at Facebook. Equal contribution. ‡ Work done at Facebook.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Artificial intelligence applications have witnessed major advances in recent years. At the core of this revolution is the development of novel neural network models and their training techniques. For example, since the landmark work of <ref type="bibr" target="#b13">He et al. (2016)</ref>, most of the state-of-the-art image recognition systems are built upon a deep stack of network blocks consisting of convolutional layers and additive skip connections, with some normalization mechanism (e.g., batch normalization <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref>) to facilitate training and generalization. Besides image classification, various normalization techniques <ref type="bibr">(Ulyanov et al., 2016;</ref><ref type="bibr" target="#b0">Ba et al., 2016;</ref><ref type="bibr" target="#b26">Salimans &amp; Kingma, 2016;</ref><ref type="bibr">Wu &amp; He, 2018)</ref> have been found essential to achieving good performance on other tasks, such as machine translation <ref type="bibr">(Vaswani et al., 2017)</ref> and generative modeling <ref type="bibr">(Zhu et al., 2017)</ref>. They are widely believed to have multiple benefits for training very deep neural networks, including stabilizing learning, enabling higher learning rate, accelerating convergence, and improving generalization. Despite the enormous empirical success of training deep networks with normalization, and recent progress on understanding the working of batch normalization <ref type="bibr" target="#b27">(Santurkar et al., 2018)</ref>, there is currently no general consensus on why these normalization techniques help training residual neural networks. Intrigued by this topic, in this work we study • Training without normalization. We propose Fixup, a method that rescales the standard initialization of residual branches by adjusting for the network architecture. Fixup enables training very deep residual networks stably at maximal learning rate without normalization. (Section 3) • Image classification. We apply Fixup to replace batch normalization on image classification benchmarks CIFAR-10 (with Wide-ResNet) and ImageNet (with ResNet), and find Fixup with proper regularization matches the well-tuned baseline trained with normalization. (Section 4.2) • Machine translation. We apply Fixup to replace layer normalization on machine translation benchmarks IWSLT and WMT using the Transformer model, and find it outperforms the baseline and achieves new state-of-the-art results on the same architecture.  <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref> layers are marked in red. Middle: A simple network block that trains stably when stacked together. Right: Fixup further improves by adding bias parameters. (See Section 3 for details.)</p><p>In the remaining of this paper, we first analyze the exploding gradient problem of residual networks at initialization in Section 2. To solve this problem, we develop Fixup in Section 3. In Section 4 we quantify the properties of Fixup and compare it against state-of-the-art normalization methods on real world benchmarks. A comparison with related work is presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM: RESNET WITH STANDARD INITIALIZATIONS LEAD TO EXPLODING GRADIENTS</head><p>Standard initialization methods <ref type="bibr" target="#b6">(Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b12">He et al., 2015;</ref><ref type="bibr">Xiao et al., 2018)</ref> attempt to set the initial parameters of the network such that the activations neither vanish nor explode. Unfortunately, it has been observed that without normalization techniques such as BatchNorm they do not account properly for the effect of residual connections and this causes exploding gradients. <ref type="bibr" target="#b1">Balduzzi et al. (2017)</ref> characterizes this problem for ReLU networks, and we will generalize this to residual networks with positively homogenous activation functions. A plain (i.e. without normalization layers) ResNet with residual blocks {F 1 , . . . , F L } and input x 0 computes the activations as</p><formula xml:id="formula_0">x l = x 0 + l−1 i=0 F i (x i ).<label>(1)</label></formula><p>ResNet output variance grows exponentially with depth. Here we only consider the initialization, view the input x 0 as fixed, and consider the randomness of the weight initialization. We analyze the variance of each layer x l , denoted by Var[x l ] (which is technically defined as the sum of the variance of all the coordinates of x l .) For simplicity we assume the blocks are initialized to be zero mean, i.e., This causes the output variance to explode exponentially with depth without normalization <ref type="bibr" target="#b10">(Hanin &amp; Rolnick, 2018)</ref> for positively homogeneous blocks (see Definition 1). This is detrimental to learning because it can in turn cause gradient explosion.</p><formula xml:id="formula_1">E[F l (x l ) | x l ] = 0. By x l+1 = x l + F l (x l ),</formula><p>As we will show, at initialization, the gradient norm of certain activations and weight tensors is lower bounded by the cross-entropy loss up to some constant. Intuitively, this implies that blowup in the logits will cause gradient explosion. Our result applies to convolutional and linear weights in a neural network with ReLU nonlinearity (e.g., feed-forward network, CNN), possibly with skip connections (e.g., ResNet, DenseNet), but without any normalization.</p><p>Our analysis utilizes properties of positively homogeneous functions, which we now introduce. Definition 1 (positively homogeneous function of first degree We study classification problems with c classes and the cross-entropy loss. We use f to denote a neural network function except for the softmax layer. Cross-entropy loss is defined as ), y (m) ), where we use (m) to index quantities referring to the m-th example. · denotes any valid norm. We only make the following assumptions about the network f :</p><formula xml:id="formula_2">(z, y) −y T (z − logsumexp(z)) where y is the one-hot label vector, z f (x) ∈ R c is</formula><formula xml:id="formula_3">1. f is a sequential composition of network blocks {f i } L i=1 , i.e. f (x 0 ) = f L (f L−1 (. . . f 1 (x 0 )))</formula><p>, each of which is composed of p.h. functions. 2. Weight elements in the FC layer are i.i.d. sampled from a zero-mean symmetric distribution.</p><p>These assumptions hold at initialization if we remove all the normalization layers in a residual network with ReLU nonlinearity, assuming all the biases are initialized at 0.</p><p>Our results are summarized in the following two theorems, whose proofs are listed in the appendix: Theorem 1. Denote the input to the i-th block by x i−1 . With Assumption 1, we have</p><formula xml:id="formula_4">∂ ∂x i−1 ≥ (z, y) − H(p) x i−1 ,<label>(2)</label></formula><p>where p is the softmax probabilities and H denotes the Shannon entropy.</p><p>Since H(p) is upper bounded by log(c) and x i−1 is small in the lower blocks, blowup in the loss will cause large gradient norm with respect to the lower block input. Our second theorem proves a lower bound on the gradient norm of a p.h. set in a network. Theorem 2. With Assumption 1, we have</p><formula xml:id="formula_5">∂ avg ∂θ ph ≥ 1 M θ ph M m=1 (z (m) , y (m) ) − H(p (m) ) G(θ ph ).<label>(3)</label></formula><p>Furthermore, with Assumptions 1 and 2, we have</p><formula xml:id="formula_6">EG(θ ph ) ≥ E[max i∈[c] z i ] − log(c) θ ph .<label>(4)</label></formula><p>It remains to identify such p.h. sets in a neural network. In <ref type="figure">Figure 2</ref> we provide three examples of p.h. sets in a ResNet without normalization. Theorem 2 suggests that these layers would suffer from the exploding gradient problem, if the logits z blow up at initialization, which unfortunately would occur in a ResNet without normalization if initialized in a traditional way. This motivates us to introduce a new initialization in the next section.</p><p>conv fc + conv conv <ref type="figure">Figure 2</ref>: Examples of p.h. sets in a ResNet without normalization: (1) the first convolution layer before max pooling; (2) the fully connected layer before softmax; (3) the union of a spatial downsampling layer in the backbone and a convolution layer in its corresponding residual branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FIXUP: UPDATE A RESIDUAL NETWORK Θ(η) PER SGD STEP</head><p>Our analysis in the previous section points out the failure mode of standard initializations for training deep residual network: the gradient norm of certain layers is in expectation lower bounded by a quantity that increases indefinitely with the network depth. However, escaping this failure mode does not necessarily lead us to successful training -after all, it is the whole network as a function that we care about, rather than a layer or a network block. In this section, we propose a top-down design of a new initialization that ensures proper update scale to the network function, by simply rescaling a standard initialization. To start, we denote the learning rate by η and set our goal:</p><formula xml:id="formula_7">f (x; θ) is updated by Θ(η) per SGD step after initialization as η → 0. That is, ∆f (x) = Θ(η) where ∆f (x) f (x; θ − η ∂ ∂θ (f (x), y)) − f (x; θ).</formula><p>Put another way, our goal is to design an initialization such that SGD updates to the network function are in the right scale and independent of the depth.</p><p>We define the Shortcut as the shortest path from input to output in a residual network. The Shortcut is typically a shallow network with a few trainable layers. <ref type="bibr">1</ref> We assume the Shortcut is initialized using a standard method, and focus on the initialization of the residual branches.</p><p>Residual branches update the network in sync. To start, we first make an important observation that the SGD update to each residual branch changes the network output in highly correlated directions. This implies that if a residual network has L residual branches, then an SGD step to each residual branch should change the network output by Θ(η/L) on average to achieve an overall Θ(η) update. We defer the formal statement and its proof until Appendix B.1.</p><p>Study of a scalar branch. Next we study how to initialize a residual branch with m layers so that its SGD update changes the network output by Θ(η/L). We assume m is a small positive integer (e.g., 2 or 3). As we are only concerned about the scale of the update, it is sufficiently instructive to study the scalar case, i.e., F (x) = ( m i=1 a i ) x where a 1 , . . . , a m , x ∈ R + . For example, the standard initialization methods typically initialize each layer so that the output (after nonlinear activation) preserves the input variance, which can be modeled as setting ∀i ∈ [m], a i = 1. In turn, setting a i to a positive number other than 1 corresponds to rescaling the i-th layer by a i .</p><p>Through deriving the constraints for F (x) to make Θ(η/L) updates, we will also discover how to rescale the weight layers of a standard initialization as desired. In particular, we show the SGD update to F (x) is Θ(η/L) if and only if the initialization satisfies the following constraint:</p><formula xml:id="formula_8">  i∈[m]\{j} a i   x = Θ 1 √ L , where j ∈ arg min k a k<label>(5)</label></formula><p>We defer the derivation until Appendix B.2.</p><p>Equation <ref type="formula" target="#formula_8">(5)</ref> suggests new methods to initialize a residual branch through rescaling the standard initialization of i-th layer in a residual branch by its corresponding scalar a i . For example, we could set ∀i ∈ [m], a i = L − 1 2m−2 . Alternatively, we could start the residual branch as a zero function by setting a m = 0 and ∀i ∈ [m − 1], a i = L − 1 2m−2 . In the second option, the residual branch does not need to "unlearn" its potentially bad random initial state, which can be beneficial for learning. Therefore, we use the latter option in our experiments, unless otherwise specified.</p><p>The effects of biases and multipliers. With proper rescaling of the weights in all the residual branches, a residual network is supposed to be updated by Θ(η) per SGD step -our goal is achieved. However, in order to match the training performance of a corresponding network with normalization, there are two more things to consider: biases and multipliers.</p><p>Using biases in the linear and convolution layers is a common practice. In normalization methods, bias and scale parameters are typically used to restore the representation power after normalization. 2 Intuitively, because the preferred input/output mean of a weight layer may be different from the preferred output/input mean of an activation layer, it also helps to insert bias terms in a residual network without normalization. Empirically, we find that inserting just one scalar bias before each weight layer and nonlinear activation layer significantly improves the training performance.</p><p>Multipliers scale the output of a residual branch, similar to the scale parameters in batch normalization. They have an interesting effect on the learning dynamics of weight layers in the same branch. Specifically, as the stochastic gradient of a layer is typically almost orthogonal to its weight, learning rate decay tends to cause the weight norm equilibrium to shrink when combined with L2 weight decay <ref type="bibr">(van Laarhoven, 2017)</ref>. In a branch with multipliers, this in turn causes the growth of the multipliers, increasing the effective learning rate of other layers. In particular, we observe that inserting just one scalar multiplier per residual branch mimics the weight norm dynamics of a network with normalization, and spares us the search of a new learning rate schedule.</p><p>Put together, we propose the following method to train residual networks without normalization:</p><p>Fixup initialization (or: How to train a deep residual network without normalization) 1. Initialize the classification layer and the last layer of each residual branch to 0. 2. Initialize every other layer using a standard method (e.g., <ref type="bibr" target="#b12">He et al. (2015)</ref>), and scale only the weight layers inside residual branches by L − 1 2m−2 . 3. Add a scalar multiplier (initialized at 1) in every branch and a scalar bias (initialized at 0) before each convolution, linear, and element-wise activation layer.</p><p>It is important to note that Rule 2 of Fixup is the essential part as predicted by Equation <ref type="formula" target="#formula_8">(5)</ref>. Indeed, we observe that using Rule 2 alone is sufficient and necessary for training extremely deep residual networks. On the other hand, Rule 1 and Rule 3 make further improvements for training so as to match the performance of a residual network with normalization layers, as we explain in the above text. <ref type="bibr">3</ref> We find ablation experiments confirm our claims (see Appendix C.1).</p><p>Our initialization and network design is consistent with recent theoretical work <ref type="bibr" target="#b11">Hardt &amp; Ma (2016)</ref>; <ref type="bibr" target="#b20">Li et al. (2018)</ref>, which, in much more simplified settings such as linearized residual nets and quadratic neural nets, propose that small initialization tend to stabilize optimization and help generalizaiton. However, our approach suggests that more delicate control of the scale of the initialization is beneficial. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TRAINING AT INCREASING DEPTH</head><p>One of the key advatanges of BatchNorm is that it leads to fast training even for very deep models <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref>. Here we will determine if we can match this desirable property by relying only on proper initialization. We propose to evaluate how each method affects training very deep nets by measuring the test accuracy after the first epoch as we increase depth. In particular, we use the wide residual network (WRN) architecture with width 1 and the default weight decay 5e−4 (Zagoruyko &amp; Komodakis, 2016). We specifically use the default learning rate of 0.1 because the ability to use high learning rates is considered to be important to the success of BatchNorm. We compare Fixup against three baseline methods -(1) rescale the output of each residual block by 1 √ 2 <ref type="bibr" target="#b1">(Balduzzi et al., 2017)</ref>, <ref type="formula" target="#formula_4">(2)</ref> post-process an orthogonal initialization such that the output variance of each residual block is close to 1 (Layer-sequential unit-variance orthogonal initialization, or LSUV) <ref type="bibr" target="#b22">(Mishkin &amp; Matas, 2015)</ref>, (3) batch normalization <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref>. We use the default batch size of 128 up to 1000 layers, with a batch size of 64 for 10,000 layers. We limit our budget of epochs to 1 due to the computational strain of evaluating models with up to 10,000 layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MACHINE TRANSLATION</head><p>To demonstrate the generality of Fixup, we also apply it to replace layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> in Transformer <ref type="figure" target="#fig_0">(Vaswani et al., 2017)</ref>, a state-of-the-art neural network for machine translation. Specifically, we use the fairseq library <ref type="bibr" target="#b5">(Gehring et al., 2017)</ref> and follow the Fixup template in Section 3 to modify the baseline model. We evaluate on two standard machine translation datasets, IWSLT German-English (de-en) and WMT English-German (en-de) following the setup of <ref type="bibr" target="#b23">Ott et al. (2018)</ref>. For the IWSLT de-en dataset, we cross-validate the dropout probability from {0.3, 0.4, 0.5, 0.6} and find 0.5 to be optimal for both Fixup and the LayerNorm baseline. For the WMT'16 en-de dataset, we use dropout probability 0.4. All models are trained for 200k updates.</p><p>It was reported (Chen et al., 2018) that "Layer normalization is most critical to stabilize the training process... removing layer normalization results in unstable training runs". However we find training with Fixup to be very stable and as fast as the baseline model. Results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Surprisingly, we find the models do not suffer from overfitting when LayerNorm is replaced by Fixup, thanks to the strong regularization effect of dropout. Instead, Fixup matches or supersedes the state-of-the-art results using Transformer model on both datasets.  Theoretical analysis of deep networks. Training very deep neural networks is an important theoretical problem. Early works study the propagation of variance in the forward and backward pass for different activation functions <ref type="bibr" target="#b6">(Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b12">He et al., 2015)</ref>.</p><p>Recently, the study of dynamical isometry (Saxe et al., 2013) provides a more detailed characterization of the forward and backward signal propogation at initialization <ref type="bibr" target="#b24">(Pennington et al., 2017;</ref><ref type="bibr" target="#b9">Hanin, 2018)</ref>, enabling training 10,000-layer CNNs from scratch <ref type="bibr">(Xiao et al., 2018)</ref>. For residual networks, activation scale <ref type="bibr" target="#b10">(Hanin &amp; Rolnick, 2018)</ref>, gradient variance <ref type="bibr" target="#b1">(Balduzzi et al., 2017)</ref> and dynamical isometry property (Yang &amp; Schoenholz, 2017) have been studied. Our analysis in Section 2 leads to the similar conclusion as previous work that the standard initialization for residual networks is problematic. However, our use of positive homogeneity for lower bounding the gradient norm of a neural network is novel, and applies to a broad class of neural network architectures (e.g., ResNet, DenseNet) and initialization methods (e.g., Xavier, LSUV) with simple assumptions and proof.</p><p>Hardt &amp; Ma (2016) analyze the optimization landscape (loss surface) of linearized residual nets in the neighborhood around the zero initialization where all the critical points are proved to be global minima. Yang &amp; Schoenholz (2017) study the effect of the initialization of residual nets to the test performance and pointed out Xavier or He initialization scheme is not optimal. In this paper, we give a concrete recipe for the initialization scheme with which we can train deep residual networks without batch normalization successfully.</p><p>Understanding batch normalization. Despite its popularity in practice, batch normalization has not been well understood. <ref type="bibr" target="#b16">Ioffe &amp; Szegedy (2015)</ref> attributed its success to "reducing internal covariate shift", whereas <ref type="bibr" target="#b27">Santurkar et al. (2018)</ref> argued that its effect may be "smoothing loss surface". Our analysis in Section 2 corroborates the latter idea of <ref type="bibr" target="#b27">Santurkar et al. (2018)</ref> by showing that standard initialization leads to very steep loss surface at initialization. Moreover, we empirically showed in Section 3 that steep loss surface may be alleviated for residual networks by using smaller initialization than the standard ones such as Xavier or He's initialization in residual branches. van Laarhoven (2017); <ref type="bibr" target="#b15">Hoffer et al. (2018)</ref> studied the effect of (batch) normalization and weight decay on the effective learning rate. Their results inspire us to include a multiplier in each residual branch.</p><p>ResNet initialization in practice. <ref type="bibr" target="#b5">Gehring et al. (2017)</ref>; <ref type="bibr" target="#b1">Balduzzi et al. (2017)</ref> proposed to address the initialization problem of residual nets by using the recurrence x l = 1 / 2 (x l−1 + F l (x l−1 )). <ref type="bibr" target="#b22">Mishkin &amp; Matas (2015)</ref> proposed a data-dependent initialization to mimic the effect of batch normalization in the first forward pass. While both methods limit the scale of activation and gradient, they would fail to train stably at the maximal learning rate for very deep residual networks, since they fail to consider the accumulation of highly correlated updates contributed by different residual branches to the network function (Appendix B.1). <ref type="bibr">Srivastava et al. (2015)</ref>; <ref type="bibr" target="#b11">Hardt &amp; Ma (2016)</ref>; <ref type="bibr" target="#b7">Goyal et al. (2017)</ref>; <ref type="bibr" target="#b17">Kingma &amp; Dhariwal (2018)</ref> found that initializing the residual branches at (or close to) zero helped optimization. Our results support their observation in general, but Equation <ref type="formula" target="#formula_8">(5)</ref> suggests additional subtleties when choosing a good initialization scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we study how to train a deep residual network reliably without normalization. Our theory in Section 2 suggests that the exploding gradient problem at initialization in a positively homogeneous network such as ResNet is directly linked to the blowup of logits. In Section <ref type="formula" target="#formula_5">3</ref>  </p><formula xml:id="formula_9">• f j−1 • · · · • f i , so that z = f i→L (x i−1 ) for all i ∈ [L]</formula><p>. Note that z is p.h. with respect to the input of each network block, i.e. f i→L ((1 + )x i−1 ) = (1 + )f i→L (x i−1 ) for &gt; −1. This allows us to compute the gradient of the cross-entropy loss with respect to the scaling factor at = 0 as</p><formula xml:id="formula_10">∂ ∂ (f i→L ((1 + )x i−1 ), y) =0 = ∂ ∂z ∂f i→L ∂ = −y T z + p T z = (z, y) − H(p)<label>(6)</label></formula><p>Since the gradient L 2 norm ∂ /∂xi−1 must be greater than the directional derivative ∂ ∂t (f i→L (x i−1 + t xi−1 xi−1 ), y), defining = t / xi−1 we have</p><formula xml:id="formula_11">∂ ∂x i−1 ≥ ∂ ∂ (f i→L (x i−1 + x i−1 ), y) ∂ ∂t = (z, y) − H(p) x i−1 .<label>(7)</label></formula><p>A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GRADIENT NORM LOWER BOUND FOR POSITIVELY HOMOGENEOUS SETS</head><p>Proof of Theorem 2. The proof idea is similar. Recall that if θ ph is a p.h. set, thenf (m) (θ ph ) f (x (m) ; θ \ θ ph , θ ph ) is a p.h. function. We therefore have</p><formula xml:id="formula_12">∂ ∂ avg (D M ; (1 + )θ ph ) =0 = 1 M M m=1 ∂ ∂z (m) ∂f (m) ∂ = 1 M M m=1 (z (m) , y (m) ) − H(p (m) ) (8)</formula><p>hence we again invoke the directional derivative argument to show</p><formula xml:id="formula_13">∂ avg ∂θ ph ≥ 1 M θ ph M m=1 (z (m) , y (m) ) − H(p (m) ) G(θ ph ).<label>(9)</label></formula><p>In order to estimate the scale of this lower bound, recall the FC layer weights are i.i.d. sampled from a symmetric, mean-zero distribution, therefore z has a symmetric probability density function with mean 0. We hence have</p><formula xml:id="formula_14">E (z, y) = E[−y T (z − logsumexp(z))] ≥ E[y T (max i∈[c] z i − z)] = E[max i∈[c] z i ] (10)</formula><p>where the inequality uses the fact that logsumexp(z) ≥ max i∈[c] z i ; the last equality is due to y and z being independent at initialization and Ez = 0. Using the trivial bound EH(p) ≤ log(c), we get A common theme in previous analysis of residual networks is the scale of activation and gradient <ref type="bibr" target="#b1">(Balduzzi et al., 2017;</ref><ref type="bibr">Yang &amp; Schoenholz, 2017;</ref><ref type="bibr" target="#b10">Hanin &amp; Rolnick, 2018)</ref>. However, it is more important to consider the scale of actual change to the network function made by a (stochastic) gradient descent step. If the updates to different layers cancel out each other, the network would be stable as a whole despite drastic changes in different layers; if, on the other hand, the updates to different layers align with each other, the whole network may incur a drastic change in one step, even if each layer only changes a tiny amount. We now provide analysis showing that the latter scenario more accurately describes what happens in reality at initialization.</p><formula xml:id="formula_15">EG(θ ph ) ≥ E[max i∈[c] z i ] − log(c) θ ph<label>(11)</label></formula><p>For our result in this section, we make the following assumptions:</p><p>• f is a sequential composition of network blocks</p><formula xml:id="formula_16">{f i } L i=1 , i.e. f (x 0 ) = f L (f L−1 (. . . f 1 (x 0 )))</formula><p>, consisting of fully-connected weight layers, ReLU activation functions and residual branches.</p><p>• f L is a fully-connected layer with weights i.i.d. sampled from a zero-mean distribution.</p><p>• There is no bias parameter in f .</p><p>For l &lt; L, let x l−1 be the input to f l and F l (x l−1 ) be a branch in f l with m l layers. Without loss of generality, we study the following specific form of network architecture:</p><formula xml:id="formula_17">F l (x l−1 ) = ( m l ReLU ReLU • W (m l ) l • · · · • ReLU • W (1) l )(x l−1 ), f l (x l−1 ) = x l−1 + F l (x l−1 ).</formula><p>For the last block we denote m L = 1 and f L (x L−1 ) = F L (x L−1 ) = W (1) L x L−1 . Furthermore, we always choose 0 as the gradient of ReLU when its input is 0. As such, with input x, the output and gradient of ReLU(x) can be simply written as D 1[x&gt;0] x, where D 1[x&gt;0] is a diagonal matrix with diagonal entries corresponding to 1[x &gt; 0]. Denote the preactivation of the i-th layer (i.e. the input to the i-th ReLU) in the l-th block by x (i) l . We define the following terms to simplify our presentation:</p><formula xml:id="formula_18">F (i−) l D 1[x (i−1) l &gt;0] W (i−1) l · · · D 1[x (1) l &gt;0] W (1) l x l−1 , l &lt; L, i ∈ [m l ] F (i+) l D 1[x (m l ) l &gt;0] W (m l ) l · · · D 1[x (i) l &gt;0] , l &lt; L, i ∈ [m l ] F (1−) L x L−1 F (1+) L I</formula><p>We have the following result on the gradient update to f : Theorem 3. With the above assumptions, suppose we update the network parameters by ∆θ = −η ∂ ∂θ (f (x 0 ; θ), y), then the update to network output ∆f (</p><formula xml:id="formula_19">x 0 ) f (x 0 ; θ + ∆θ) − f (x 0 ; θ) is ∆f (x 0 ) = −η L l=1       m l i=1 J i l F (i−) l 2 ∂f ∂x l T F (i+) l F (i+) l T ∂f ∂x l       ∂ ∂z + O(η 2 ),<label>(12)</label></formula><p>where z f (x 0 ) ∈ R c is the logits.</p><p>Let us discuss the implecation of this result before delving into the proof. As each J i l is a c × c real symmetric positive semi-definite matrix, the trace norm of each J i l equals its trace. Similarly, the trace norm of J l i J i l equals the trace of the sum of all J i l as well, which scales linearly with the number of residual branches L. Since the output z has no (or little) correlation with the target y at the start of training, ∂ ∂z is a vector of some random direction. It then follows that the expected update scale is proportional to the trace norm of J, which is proportional to L as well as the average trace of J i l . Simply put, to allow the whole network be updated by Θ(η) per step independent of depth, we need to ensure each residual branch contributes only a Θ(η/L) update on average.</p><p>Proof. The first insight to prove our result is to note that conditioning on a specific input x 0 , we can replace each ReLU activation layer by a diagonal matrix and does not change the forward and backward pass. (In fact, this is valid even after we apply a gradient descent update, as long as the learning rate η &gt; 0 is sufficiently small so that all positive preactivation remains positive. This observation will be essential for our later analysis.) We thus have the gradient w.r.t. the i-th weight layer in the l-th block is ∂</p><formula xml:id="formula_20">∂Vec(W (i) l ) = ∂x l ∂Vec(W (i) l ) · ∂f ∂x l · ∂ ∂z = F (i−) l ⊗ I (i) l F (i+) l T ∂f ∂x l · ∂ ∂z .<label>(13)</label></formula><p>where ⊗ denotes the Kronecker product. The second insight is to note that with our assumptions, a network block and its gradient w.r.t. its input have the following relation:</p><formula xml:id="formula_21">f l (x l−1 ) = ∂f l ∂x l−1 · x l−1 .<label>(14)</label></formula><p>We then plug in Equation <ref type="formula" target="#formula_0">(13)</ref> to the gradient update ∆θ = −η ∂ ∂θ (f (x 0 ; θ), y), and recalculate the forward pass f (x 0 ; θ +∆θ). The theorem follows by applying Equation <ref type="formula" target="#formula_0">(14)</ref> and a first-order Taylor series expansion in a small neighborhood of η = 0 where f (x 0 ; θ + ∆θ) is smooth w.r.t. η.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 WHAT SCALAR BRANCH HAS Θ(η/L) UPDATES?</head><p>For this section, we focus on the proper initialization of a scalar branch F (x) = ( m i=1 a i )x. We have the following result: Theorem 4. Assuming ∀i, a i ≥ 0, x = Θ(1) and ∂ ∂F (x) = Θ(1), then ∆F (</p><formula xml:id="formula_22">x) F (x; θ − η ∂ ∂θ ) − F (x; θ) is Θ(η/L) if and only if   k∈[m]\{j} a k   x = Θ 1 √ L , where j ∈ arg min k a k<label>(15)</label></formula><p>Proof. We start by calculating the gradient of each parameter:</p><formula xml:id="formula_23">∂ ∂a i = ∂ ∂F   k∈[m]\{i} a k   x<label>(16)</label></formula><p>and a first-order approximation of ∆F (x):</p><formula xml:id="formula_24">∆F (x) = −η ∂ ∂F (x) (F (x)) 2 m i=1 1 a 2 i<label>(17)</label></formula><p>where we conveniently abuse some notations by defining</p><formula xml:id="formula_25">F (x) 1 a i   k∈[m]\{i} a k   x, if a i = 0.<label>(18)</label></formula><formula xml:id="formula_26">Denote m i=1 1 a 2 i</formula><p>as M and min k a k as A, we have</p><formula xml:id="formula_27">(F (x)) 2 · 1 A 2 ≤ (F (x)) 2 M ≤ (F (x)) 2 · m A 2<label>(19)</label></formula><p>and therefore by rearranging Equation <ref type="formula" target="#formula_0">(17)</ref> and letting ∆F (x) = Θ(η/L) we get</p><formula xml:id="formula_28">(F (x)) 2 · 1 A 2 = Θ ∆F (x) η ∂ ∂F (x) = Θ 1 L<label>(20)</label></formula><p>i.e. F (x)/A = Θ(1/ √ L). Hence the "only if" part is proved. For the "if" part, we apply Equation (19) to Equation <ref type="formula" target="#formula_0">(17)</ref> and observe that by Equation <ref type="formula" target="#formula_0">(15)</ref> ∆F</p><formula xml:id="formula_29">(x) = Θ η(F (x)) 2 · 1 A 2 = Θ η L<label>(21)</label></formula><p>The result of this theorem provides useful guidance on how to rescale the standard initialization to achieve the desired update scale for the network function.</p><p>C ADDITIONAL EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 ABLATION STUDIES OF FIXUP</head><p>In this section we present the training curves of different architecture designs and initialization schemes. Specifically, we compare the training accuracy of batch normalization, Fixup, as well as a few ablated options: (1) removing the bias parameters in the network;</p><p>(2) use 0.1x the suggested initialization scale and no bias parameters; (3) use 10x the suggested initialization scale and no bias parameters; and (4) remove all the residual branches. The results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>. We see that initializing the residual branch layers at a smaller scale (or all zero) slows down learning, whereas training fails when initializing them at a larger scale; we also see the clear benefit of adding bias parameters in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 CIFAR AND SVHN WITH BETTER REGULARIZATION</head><p>We perform additional experiments to validate our hypothesis that the gap in test error between Fixup and batch normalization is primarily due to overfitting. To combat overfitting, we use Mixup <ref type="bibr">(Zhang et al., 2017)</ref> and Cutout (DeVries &amp; Taylor, 2017) with default hyperparameters as additional regularization. On the CIFAR-10 dataset, we perform experiments with WideResNet-40-10 and on SVHN we use <ref type="bibr">WideResNet-16-12 (Zagoruyko &amp; Komodakis, 2016)</ref>, all with the default hyperparameters. We observe in <ref type="table" target="#tab_9">Table 4</ref> that models trained with Fixup and strong regularization are competitive with state-of-the-art methods on CIFAR-10 and SVHN, as well as our baseline with batch normalization.    <ref type="figure">Figure 5</ref> shows that without additional regularization Fixup fits the training set very well, but overfits significantly. We see in <ref type="figure">Figure 6</ref> that Fixup is competitive with networks trained with normalization when the Mixup regularizer is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHODS</head><p>The first use of normalization in neural networks appears in the modeling of biological visual system and dates back at least to Heeger (1992) in neuroscience and to <ref type="bibr" target="#b25">Pinto et al. (2008)</ref>; <ref type="bibr" target="#b21">Lyu &amp; Simoncelli (2008)</ref> in computer vision, where each neuron output is divided by the sum (or norm) of all of the outputs, a module called divisive normalization. Recent popular normalization methods, such as local response normalization <ref type="bibr" target="#b18">(Krizhevsky et al., 2012)</ref>, batch normalization <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref> and layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> mostly follow this tradition of dividing the neuron activations by their certain summary statistics, often also with the activation mean subtracted. An exception is weight normalization <ref type="bibr" target="#b26">(Salimans &amp; Kingma, 2016)</ref>, which instead divides the weight parameters by their statistics, specifically the weight norm; weight normalization also adopts the idea of activation normalization for weight initialization. The recently proposed actnorm <ref type="bibr" target="#b17">(Kingma &amp; Dhariwal, 2018)</ref> removes the normalization of weight parameters, but still use activation normalization to initialize the affine transformation layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: ResNet basic block. Batch normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the logits where z i denotes its i-th element, and logsumexp(z) log i∈[c] exp(z i ) . Consider a minibatch of training examples D M = {(x (m) , y (m) )} M m=1 and the average cross-entropy loss avg (D M )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Minibatch training accuracy of ResNet-110 on CIFAR-10 dataset with different configurations in the first 3 epochs. We use minibatch size of 128 and smooth the curves using 10-step moving average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Training and test errors on ImageNet using ResNet-50 without additional regularization. We observe that Fixup is able to better fit the training data and that leads to overfitting -more regularization is needed. Results of BatchNorm and GroupNorm reproduced from(Wu &amp; He, 2018). Test error of ResNet-50 on ImageNet withMixup (Zhang et al., 2017). Fixup closely matches the final results yielded by the use of GroupNorm, without any normalization.D ADDITIONAL REFERENCES: A BRIEF HISTORY OF NORMALIZATION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Var[F l (x l )|x l ] will be about the same as its input variance Var[x l ], and thus Var[x l+1 ] ≈ 2Var[x l ].</figDesc><table /><note>and the law of total variance, we have Var[x l+1 ] = E[Var[F (x l )|x l ]] + Var(x l ). Resnet structure prevents x l from vanishing by forcing the variance to grow with depth, i.e. Var[x l ] &lt; Var[x l+1 ] if E[Var[F (x l )|x l ]] &gt; 0. Yet, combined with initialization methods such as He et al. (2015), the output variance of each residual branch</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Depth of residual networks versus test accuracy at the first epoch for various methods on CIFAR-10 with the default BatchNorm learning rate. We observe that Fixup is able to train very deep networks with the same learning rate as batch normalization. (Higher is better.)Figure 3shows the test accuracy at the first epoch as depth increases. Observe that Fixup matches the performance of BatchNorm at the first epoch, even with 10,000 layers. LSUV and 1 / 2 -scaling are not able to train with the same learning rate as BatchNorm past 100 layers.4.2 IMAGE CLASSIFICATIONIn this section, we evaluate the ability of Fixup to replace batch normalization in image classification applications. On the CIFAR-10 dataset, we first test on ResNet-110<ref type="bibr" target="#b13">(He et al., 2016)</ref> with default hyper-parameters; results are shown inTable 1. Fixup obtains 7% relative improvement in test error compared with standard initialization; however, we note a substantial difference in the difficulty of training. While network with Fixup is trained with the same learning rate and converge as fast as BatchNorm rather than difficulty in optimization; when we train Fixup networks with better regularization, the test error gap disappears and we obtain state-of-the-art results on CIFAR-10 and SVHN without normalization layers (see Appendix C.2). Results on CIFAR-10 with ResNet-110 (mean/median of 5 runs; lower is better).On the ImageNet dataset, we benchmark Fixup with the ResNet-50 and ResNet-101 architectures<ref type="bibr" target="#b13">(He et al., 2016)</ref>, trained for 100 epochs and 200 epochs respectively. Similar to our finding on the CIFAR-10 dataset, we observe that (1) training with Fixup is fast and stable with the default hyperparameters, (2) Fixup alone significantly improves the test error of standard initialization, and</figDesc><table><row><cell>First Epoch Test Accuracy (%)</cell><cell>25 30 35 40 45 50 55</cell><cell>10</cell><cell>100</cell><cell>Depth</cell><cell>1000</cell><cell>10000</cell><cell>1 / 2 -scaling LSUV BatchNorm Fixup</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>network with batch normalization, we fail to train a Xavier initialized ResNet-110 with 0.1x maximal learning rate.5 The test error gap in Table 1 is likely due to the regularization effect of(3) there is a large test error gap between Fixup and BatchNorm. Further inspection reveals that Fixup initialized models obtain significantly lower training error compared with BatchNorm models (see Appendix C.3), i.e., Fixup suffers from overfitting. We therefore apply stronger regularization to the Fixup models using Mixup (Zhang et al., 2017). We find it is beneficial to reduce the learning rate of the scalar multiplier and bias by 10x when additional large regularization is used. Best Mixup coefficients are found through cross-validation: they are 0.2, 0.1 and 0.7 for BatchNorm, GroupNorm (Wu &amp; He, 2018) and Fixup respectively. We present the results in Table 2, noting that with better regularization, the performance of Fixup is on par with GroupNorm.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>ImageNet test results using the ResNet architecture. (Lower is better.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparing Fixup vs. LayerNorm for machine translation tasks. (Higher is better.)</figDesc><table><row><cell>5 RELATED WORK</cell></row><row><cell>Normalization methods. Normalization methods have enabled training very deep residual net-</cell></row><row><cell>works, and are currently an essential building block of the most successful deep learning architec-</cell></row><row><cell>tures. All normalization methods for training neural networks explicitly normalize (i.e. standardize)</cell></row><row><cell>some component (activations or weights) through dividing activations or weights by some real num-</cell></row><row><cell>ber computed from its statistics and/or subtracting some real number activation statistics (typically</cell></row><row><cell>the mean) from the activations.</cell></row></table><note>6 In contrast, Fixup does not compute statistics (mean, variance or norm) at initialization or during any phase of training, hence is not a normalization method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>we develop Fixup initialization to ensure the whole network as well as each residual branch gets updates of proper scale, based on a top-down analysis. Extensive experiments on real world datasets demonstrate that Fixup matches normalization techniques in training deep residual networks, and achieves state-of-the-art test performance with proper regularization.Our work opens up new possibilities for both theory and applications. Can we analyze the training dynamics of Fixup, which may potentially be simpler than analyzing models with batch normalization is? Could we apply or extend the initialization scheme to other applications of deep learning? It would also be very interesting to understand the regularization benefits of various normalization methods, and to develop better regularizers to further improve the test performance of Fixup.Wenling Shang, Justin Chiu, and Kihyuk Sohn. Exploring normalization in deep residual networks with concatenated rectified linear units. In AAAI, pp. 1509-1516, 2017. Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015. Masakazu Iwamura, and Koichi Kise. Shakedrop regularization. arXiv preprint arXiv:1802.02375, 2018. Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances in neural information processing systems, pp. 7103-7114, 2017. GRADIENT NORM LOWER BOUND FOR THE INPUT TO A NETWORK BLOCKProof of Theorem 1. We use f i→j to denote the composition f j</figDesc><table><row><cell cols="3">Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing</cell></row><row><cell cols="2">ingredient for fast stylization. CoRR, abs/1607.08022, 2016.</cell><cell></cell></row><row><cell cols="3">Twan van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint</cell></row><row><cell>arXiv:1706.05350, 2017.</cell><cell></cell><cell></cell></row><row><cell cols="3">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</cell></row><row><cell cols="3">Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-</cell></row><row><cell cols="2">mation Processing Systems, pp. 5998-6008, 2017.</cell><cell></cell></row><row><cell cols="3">Yuxin Wu and Kaiming He. Group normalization. In The European Conference on Computer Vision</cell></row><row><cell>(ECCV), September 2018.</cell><cell></cell><cell></cell></row><row><cell cols="3">Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Penning-</cell></row><row><cell cols="3">ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla</cell></row><row><cell cols="2">convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.</cell><cell></cell></row><row><cell>Yoshihiro Yamada, Sergey Zagoruyko and Nikos Komodakis.</cell><cell>Wide residual networks.</cell><cell>arXiv preprint</cell></row><row><cell>arXiv:1605.07146, 2016.</cell><cell></cell><cell></cell></row><row><cell cols="3">Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical</cell></row><row><cell cols="2">risk minimization. arXiv preprint arXiv:1710.09412, 2017.</cell><cell></cell></row><row><cell cols="3">Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation</cell></row><row><cell cols="3">using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE Interna-</cell></row><row><cell>tional Conference on, 2017.</cell><cell></cell><cell></cell></row><row><cell>A PROOFS FOR SECTION 2</cell><cell></cell><cell></cell></row><row><cell>A.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Additional results on CIFAR-10, SVHN datasets.C.3 TRAINING AND TEST CURVES ON IMAGENET</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, in the ResNet architecture (e.g., ResNet-50, ResNet-101 or ResNet-152) for ImageNet classification, the Shortcut is always a 6-layer network with five convolution layers and one fully-connected layer, irrespective of the total depth of the whole network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, in batch normalization gamma and beta parameters are used to affine-transform the normalized activations per each channel.3 It is worth noting that the design of Fixup is a simplification of the common practice, in that we only introduce O(K) parameters beyond convolution and linear weights (since we remove bias terms from convolution and linear layers), whereas the common practice includes O(KC)<ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b26">Salimans &amp; Kingma, 2016)</ref> or O(KCW H)<ref type="bibr" target="#b0">(Ba et al., 2016)</ref> additional parameters, where K is the number of layers, C is the max number of channels per layer and W, H are the spatial dimension of the largest feature maps.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For example, learning rate smaller than our choice would also stabilize the training, but lead to lower convergence rate.5  Personal communication with the authors of (Shang et al., 2017) confirms our observation, and reveals that the Xavier initialized network need more epochs to converge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For reference, we include a brief history of normalization methods in Appendix D.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Yuxin Wu, Kaiming He, Aleksander Madry and the anonymous reviewers for their helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennox</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt Wan-Duo</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcwilliams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08591</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09849</idno>
		<title level="m">The best of both worlds: Combining recent advances in neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<title level="m">Latent alignment and variational attention. Thirty-second Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Fractional max-pooling. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03744</idno>
		<title level="m">Which neural net architectures give rise to exploding and vanishing gradients</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01719</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Identity matters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04231</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Normalization of cell responses in cat striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="197" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Norm matters: efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01814</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithmic regularization in over-parameterized matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonlinear image representation using divisive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<title level="m">All you need is a good init</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4785" to="4795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Why is real-world visual object recognition hard?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">How does batch normalization help optimization?(no, it is not about internal covariate shift)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11604</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
