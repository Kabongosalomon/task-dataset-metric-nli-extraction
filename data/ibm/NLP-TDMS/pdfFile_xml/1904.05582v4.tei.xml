<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Space-time Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Nicolicioiu</surname></persName>
							<email>anicolicioiu@bitdefender.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Duta</surname></persName>
							<email>iduta@bitdefender.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><forename type="middle">Leordeanu</forename><surname>Bitdefender</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romania</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Bitdefender</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Mathematics of the Romanian Academy University &quot;Politehnica&quot; of Bucharest</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Space-time Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning in the space-time domain remains a very challenging problem in machine learning and computer vision. Current computational models for understanding spatio-temporal visual data are heavily rooted in the classical single-image based paradigm. It is not yet well understood how to integrate information in space and time into a single, general model. We propose a neural graph model, recurrent in space and time, suitable for capturing both the local appearance and the complex higher-level interactions of different entities and objects within the changing world scene. Nodes and edges in our graph have dedicated neural networks for processing information. Nodes operate over features extracted from local parts in space and time and over previous memory states. Edges process messages between connected nodes at different locations and spatial scales or between past and present time. Messages are passed iteratively in order to transmit information globally and establish long range interactions. Our model is general and could learn to recognize a variety of high level spatio-temporal concepts and be applied to different learning tasks. We demonstrate, through extensive experiments and ablation studies, that our model outperforms strong baselines and top published methods on recognizing complex activities in video. Moreover, we obtain state-of-the-art performance on the challenging Something-Something human-object interaction dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video data is available almost everywhere. While image level recognition is better understood, visual learning in space and time is far from being solved. The main challenge is how to model interactions between objects and higher level concepts, within the large spatio-temporal context. For such a difficult learning task it is important to efficiently model the local appearance, the spatial relationships and the complex interactions and changes that take place over time.</p><p>Often, for different learning tasks, different models are preferred, such that they capture the specific domain priors and biases of the problem <ref type="bibr" target="#b0">[1]</ref>. Convolutional neural networks (CNNs) are preferred on tasks involving strong local and stationary assumptions about the data. Recurrent models are chosen when data is sequential in nature. Fully connected models could be preferred when there is no known structure in the data. Our recurrent neural graph efficiently processes information in both space and time and can be applied to different learning tasks in video.</p><p>We propose Recurrent Space-time Graph (RSTG) neural networks, in which each node receives features extracted from a specific region in space-time using a backbone deep neural network. <ref type="figure">Figure 1</ref>: The RSTG-to-map architecture: the input to RSTG is a feature volume, extracted by a backbone network, down-sampled according to each scale. Each node receives input from a cell, corresponding to a region of interest in space. The edges between different nodes represent messages in space, the red links are spatial updates, while the purple links represent messages in time. All the extracted (input to graph) and up-sampled features (output from graph) have the same spatial and temporal dimension T × H × W × C and are only represented at different scales for a better visualisation.</p><p>Global processing is achieved through iterative message passing in space and time. Spatio-temporal processing is factorized, into a space processing stage and a time processing stage, which are alternated within each iteration. We aim to decouple, conceptually, the data from the computational machine that processes the data. Thus, our nodes are processing units that receive inputs from several sources: local regions in space at the present time, their neighbor spatial nodes as well as their past memory states ( <ref type="figure">Fig. 6</ref>).</p><p>Main contributions. We sum up our contributions into the following three main ideas:</p><p>1. We propose a novel computational model for learning in spatio-temporal domain. Space and time are treated differently, while they function together in complementary ways. Our model is general and could be applied to various learning problems. It could also be used as a processing block in combination with other powerful models. 2. We factorize space and time and process them differently within a unified neural graph model from an unstructured video. In extensive ablation studies we show the importance of each graph component and also demonstrate that different temporal and spatial processing is crucial for learning in space-time domain. Through recurrent and factorized space-time processing our model achieves a relatively low computational complexity. 3. We introduce a new synthetic dataset, with complex interactions, to analyse and evaluate different spatio-temporal models. We obtain a performance that is superior to several powerful baselines and top published methods. More importantly, we obtain state-of-theart results on the challenging Something-Something, real world dataset.</p><p>Relation to previous work: Iterative graph based methods have a long history in machine learning and are currently enjoying a fast-growing interest <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Their main paradigm is the following: at each iteration, messages are passed between nodes, information is updated at each node and the process continues until convergence or a stopping criterion is met. Such ideas trace back to work on image denoising, restoration and labeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, with many inference methods, graphical models and mathematical formulations being proposed over time for various tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Current approaches combine the idea of message passing between graph nodes, from graphical models, with convolution operations. Thus, the idea of graph convolutions was born. Initial methods generalizing conv nets to the case of graph structured data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> learn in the spectral domain of the graph. They are approximated <ref type="bibr" target="#b16">[17]</ref> by message passing based on linear operations <ref type="bibr" target="#b17">[18]</ref> or MLPs <ref type="bibr" target="#b18">[19]</ref>. Aggregation of messages needs permutation invariant operators such as max or sum, the last one being proved superior in <ref type="bibr" target="#b19">[20]</ref>, with attention mechanism <ref type="bibr" target="#b20">[21]</ref> as an alternative.</p><p>Recurrence in graph models has been proposed for sequential tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> or for iteratively processing the input <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Recurrence is used in graph neural nets <ref type="bibr" target="#b21">[22]</ref> to tackle symbolic tasks with single input and sequential language output. Different from them, we have two types of recurrent stages, with distinct functionality, one over space and the other over time.</p><p>The idea of modeling complex, higher order and long range spatial relationships by the spatial recurrence relates to more classical work using pictorial structures <ref type="bibr" target="#b25">[26]</ref> to model object parts and their relationships and perform inference through iterative optimization algorithms. The idea of combining information at different scales also relates to classic approaches in object recognition, such as the well-known spatial pyramid model <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Long-range dependencies in sequential language are captured in <ref type="bibr" target="#b28">[29]</ref> with a self-attention model. It has a stack of attention layers, each with different parameters. It is improved in <ref type="bibr" target="#b23">[24]</ref> by performing operations recurrently. This is similar to our recurrent spatial processing stage. As mentioned before, our model is different by adding another complementary dimension -the temporal one. In <ref type="bibr" target="#b24">[25]</ref> new information is incorporated into the existing memory by self-attention using a temporary new node. Then each node is updated by an LSTM <ref type="bibr" target="#b29">[30]</ref>. Their method is applied on program evaluation, simulated environments used in reinforcement learning and language modeling where they do not have a spatial dimension. Their nodes act as a set of memories. Different from them, we receive new information for each node and process them in multiple interleaved iterations of our two stages.</p><p>Initial node information could come from each local spatio-temporal point in convolutional feature maps <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> or from features corresponding to entities detected by external methods, such as objects <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> or skeletons <ref type="bibr" target="#b35">[36]</ref>. Also, the approach in <ref type="bibr" target="#b36">[37]</ref> is to extract objects and form relations between objects from pairs of time steps randomly chosen. Different from that methods, our nodes are not attached to specific volumes in time and space. Also, we do not need pre-trained higher-level detectors, our model working on unstructured videos.</p><p>While the above methods need access to the whole video at test time, ours is recurrent and can function in an online, continuous manner in time. All space-time positions in the input volume are connected in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>. In contrast, we treat space and time differently and prove the effectiveness of our choice in experiments. A 1D convolution is used in <ref type="bibr" target="#b35">[36]</ref> to temporally connect only the nodes corresponding to the same skeleton joint and recently <ref type="bibr" target="#b33">[34]</ref> send messages between nodes corresponding to the same entities, while we recurrently update in time the state of each node. We could see our different handling of time and space as an efficient factorization into simpler mechanisms that function together along different dimensions. The work in <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> confirm our hypothesis that features could be more efficiently processed by factorization into simpler operations. The models in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> factorize 3D convolutions into 2D spatial and 1D temporal convolutions.</p><p>For spatio-temporal processing, some methods, which do not use explicit graph modeling, encode frames individually using 2D convolutions and aggregate them in different ways <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>; others form relations as functions (MLPs) over sets of frames <ref type="bibr" target="#b46">[47]</ref> or use 3D convolution inflated from existing 2D convolutional networks <ref type="bibr" target="#b47">[48]</ref> . Optical flow could be used as input to a separate branch of a 2D ConvNet <ref type="bibr" target="#b48">[49]</ref> or used as part of the model to guide the kernel of 3D convolutions <ref type="bibr" target="#b49">[50]</ref>. To cover both spatial and temporal dimensions simultaneously, Convolutional LSTM <ref type="bibr" target="#b50">[51]</ref> can be used, augmented with additional memory <ref type="bibr" target="#b51">[52]</ref> or self-attention in order to update LSTM hidden states <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent Space-time Graph Model</head><p>The Recurrent Space-time Graph (RSTG) model is designed to process data in both space and time, to capture both local and long range spatio-temporal interactions ( <ref type="figure">Fig. 6</ref>). RSTG takes into consideration local information by computing over features extracted from specific locations and scales at each moment in time. Then it integrates long range spatial and temporal information by iterative message passing at the spatial level between connected nodes and by recurrence in time, respectively. The space and time message passing is coupled with the two stages succeeding one after another.</p><p>Our model takes a video and process it using a backbone function into a features volume F ∈ R T ×H×W ×C , where T is the time dimension and H,W the spatial ones. The backbone function could be modeled by any deep neural network that operates over single frames or over Algorithm 1 Space-time processing in RSTG model. space-time volumes. Thus, we extract local spatio-temporal information from the video volume and we process it using our graph, sequentially, time step after time step. This approach makes it possible for our graph to also process a continuous flow of spatio-temporal data and function in an online manner.</p><formula xml:id="formula_0">Input: Time-space features F ∈ R T ×H×W ×C repeat v i ← extract_f eatures(F t , i) ∀i for k = 0 to K − 1 do v i = h t,k i = f time (v i , h t−1,k i ) ∀i m j,i = f send (v j , v i ) ∀i, ∀j ∈ N (i) g i = f gather (v i , {m j,i } j∈N (i) ) ∀i v i = f space (v i , g i ) ∀i end for h t,K i = f time (v i , h t−1,K i ) ∀i t = t + 1 until end-of-video v f inal = f aggregate ({h 1:T,K i } ∀i )</formula><p>Instead of fully connecting all positions in time and space, which is costly, we establish long range interactions through recurrent and complementary Space and Time Processing Stages. Thus, in the temporal processing stage, each node receives a message from the previous time step. Then, at the spatial stage, the graph nodes, which now have information from both present and past, start exchanging information through message passing. Space and time are coupled and performed alternatively: after each space iteration iter, another time iteration follows, with a message coming from past memory associated with the same space iteration iter. The processing stages of our algorithm are succinctly presented in Alg. 1 and <ref type="figure" target="#fig_0">Fig. 2</ref>. They are detailed below. The code for the full model can be found in our repository 2 .</p><p>Graph Creation. We create N nodes connected in a graph structure and use them to process a features volume F ∈ R T ×H×W ×C . Each node receives input from a specific region (a window defined by a location and scale) of the features volume at each time step t ( <ref type="figure">Fig. 6</ref>). At each scale we downsample the H × W feature maps into h × w grids, each cell corresponding to one node. Two nodes are connected if they are neighbours in space or if their regions at different scales intersect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Space Processing Stage</head><p>Spatial interactions are established by exchanging messages between nodes. The process involves 3 steps: send messages between all connected nodes, gather information at node level from the received messages and update internal nodes representations. Each step has its own dedicated MLP. Message passing is iterated K times, with time processing steps followed by space processing steps, at each iteration.</p><p>Message sending function. A given message between two nodes should represent relevant information about their pairwise interaction. Thus, the message is a function of both the source and destination nodes j and i, respectively. The function, f send (v j , v i ) is modeled as a multilayer perceptron (MLP) applied on the concatenation of the two node features:</p><formula xml:id="formula_1">f send (v j , v i ) = MLP s ([v j |v i ]) ∈ R D . (1) MLP a (x) = σ(W a2 σ(W a1 (x) + b a1 ) + b a2 ).</formula><p>(2)</p><p>Position-aware messages. The pairwise interactions between nodes should have positional awareness -each node should be aware of the position of the neighbor that sends a particular message. Therefore we include the position information as a (linearized) low-resolution 6 × 6 map in the message body sent with f send , by concatenating the map to the rest of the message. The actual map is formed by putting ones for the cells corresponding to the region of interest of the sending nodes and zeros for the remaining cells, and then applying filtering with a Gaussian kernel.</p><p>Gather function. Each node receives a message from each of its neighbours and aggregates them using the f gather function, which could be a simple sum of all messages or an attention mechanism that gives a different weight to each message, according to its importance. In this way, a node could choose what information to receive. In our implementation, the attentional weight function α is computed as the dot product between features of the two nodes, measuring their similarity.</p><formula xml:id="formula_2">f gather (v i ) = j∈N (i) α(v j , v i )f send (v j , v i ) ∈ R D . (3) α(v j , v i ) = (W α1 v j ) T (W α2 v i ) ∈ R.<label>(4)</label></formula><p>Update function. We update the representation of each node with the information gathered from its neighbours, using function f space modeled as a multilayer perceptron (MLP). We want each node to be capable of taking into consideration global information while also maintaining its local identity. The MLP is able to combine efficiently new information received from neighbours with the local information from the node's input features.</p><formula xml:id="formula_3">f space (v i ) = MLP u ([v i |f gather (v i )]) ∈ R D .<label>(5)</label></formula><p>In general, the parameters W u , b u could be shared among all nodes at all scales or each set could be specific to the actual scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Time Processing Stage</head><p>Each node updates its state in time by aggregating the current spatial representation f space (v i ) with its time representation from the previous step using a recurrent function. In order to model more expressive spatio-temporal interactions and to give it the ability to reason about all the information in the scene, with knowledge about past states, we put a Time Processing Stage before each Space Processing Stage, at each iteration, and another Time Processing Stage after the last spatial processing.</p><p>Thus messages are passed iteratively in both space and time, alternatively. The Time Processing Stage at iteration k updates each node's internal state v t,k i with information from its corespondent state v t−1,k i , at iteration k, in the previous time t − 1, resulting in features that take into account both spatial interactions and history <ref type="figure" target="#fig_0">(Fig. 2)</ref>.</p><formula xml:id="formula_4">h t,k i,time = f time (v k i,space , h t−1,k i,time ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Aggregation step</head><p>The aggregation f aggregate function could produce two types of final representations, a 1D vector or a 3D map. In the first case, denoted RSTG-to-vec, we obtain the vector encoding by summing the representation of all the nodes from the last time step. In the second case, denoted RSTG-to-map, we create the inverse operation of the node creation, by sending the processed information contained in each node back to the original region in the space-time volume as shown in <ref type="figure">Figure 6</ref>. For each scale, we have h * w nodes with C-channel features, that we arrange in a h × w grid resulting in a volume of size h × w × C. We up-sample the grid map for each scale into H × W × C maps and sum all maps for all scales for the final H × W × C representation.  <ref type="figure">Figure 3</ref>: On each row we present frames from videos of 5SyncM-NIST dataset. In each video sequence two digits follow the exact same pattern of movement. The correct classes: "3-9" "6-7" and "9-1".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Computational complexity</head><p>We analyse the computational complexity of the RSTG model. If N is the number of nodes in a frame and E the number of edges, we have O(2E) messages per space-processing stage, as there are two different spatial messages in each edge direction. With a total of T time steps and K (=3) spatiotemporal message passing iterations, each of the K spatial message passing iterations is preceded by a temporal iteration, resulting in a total complexity of</p><formula xml:id="formula_5">O(T × ( 2E) × K + T × N × (K + 1)).</formula><p>Note that E is upper-bounded by N (N − 1)/2. Without the factorisation, with messages between all the nodes in time and space (similar to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>), we would arrive at a complexity of O(T 2 × N 2 × K) in the number of messages, which is quadratic in time. Note that our lower complexity is due to the recurrent nature of our model and the space-time factorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We perform experiments on two video classification tasks, which involve complex object interactions. We experiment on a video dataset that we create synthetically, containing complex patterns of movements and shapes, and on the challenging Something-Something-v1 dataset, involving interactions between a human and other objects <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning patterns of movements and shapes</head><p>There are not many available video datasets that require modeling of difficult object interactions. Improvements are often made by averaging the final predictions over space and time <ref type="bibr" target="#b37">[38]</ref>. The complex interactions and the structure of the space-time world still seem to escape the modeling capabilities. For this reason, and to better understand the role played by each component of our model in relation to some very strong baselines, we introduce a novel dataset, named SyncMNIST.</p><p>We make several MNIST digits move in complex ways. We designed the dataset such that the relationships involved are challenging in both space and time. The dataset contains 600K videos showing multiple digits, where all of them move randomly, apart from a pair of digits that moves synchronously -that specific pair determines the class of the activity pattern, for a total of 45 unique digit pairs (classes) plus one extra class (no pair is synchronous).</p><p>In order to recognize the pattern, a given model has to reason about the location in space of each digit, track them across the entire time in order to learn the association between a label and a pair of digits that moves synchronously. The data has 18 × 18 size digits moving on a black 64 × 64 background for 10 frames. In <ref type="figure">Fig. 3</ref> we present frames from three different videos used in our experiments. We trained and evaluated our models first on an easier 3 digits (3SyncMNIST) dataset and then, only the best models were trained and tested on the harder 5 digits dataset (5SyncMNIST).</p><p>We compared against four strong baseline models that are often used on video understanding tasks. For all tested models we used a convolutional network as a backbone. It is a small CNN with 3 layers, pre-trained to classify a digit randomly placed in a frame of the video. It is important to notice that published models such as MeanPooling+LSTM, Conv+LSTM, I3D and Non-Local, have the same ranking on our SyncMNIST dataset as on other datasets such as UCF-101 <ref type="bibr" target="#b54">[55]</ref>, HMDB-51 <ref type="bibr" target="#b55">[56]</ref>, Kinetics (see <ref type="bibr" target="#b47">[48]</ref>) and Something-Something (see <ref type="bibr" target="#b32">[33]</ref>). The available performance of these models on all datasets can be found in Section A of the Appendix.</p><p>It is also important that the performance of different models seems to be well correlated with the ability of a specific model to incorporate and process time axis. This aspect, combined with the fact that, by design, on SyncMNIST the temporal dimension is important, make the tests on SyncMNIST relevant.</p><p>Mean pooling + LSTM: Use backbone for feature extraction, spatial mean pool and temporally aggregate them using an LSTM. This model is capable of processing information from distant time-steps but it has poor understanding of spatial information.</p><p>ConvNet + LSTM: Replace the mean pooling with convolutional layers that are able to capture fine spatial relationships between different parts of the scene. Thus, it is fully capable of analysing the entire video, both in space and in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I3D:</head><p>We adapt the I3D model <ref type="bibr" target="#b47">[48]</ref> with a smaller ResNet <ref type="bibr" target="#b56">[57]</ref> backbone to maintain the number of parameters comparable to our model. 3D convolutions are capable of capturing some of the longer range relationships both spatially and temporally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Local:</head><p>We used the previous I3D architecture as a backbone for a Non-Local [32] model. We obtained best results with one non-local block in the second residual block.</p><p>Implementation details for RSTG: Our recurrent neural graph model (RSTG) uses the initial 3-layer CNN as backbone, an LSTM with 512 hidden state size for the f time and RSTG-to-vec as aggregation. We use 3 scales with 1 × 1, 2 × 2 and 3 × 3 grids with nodes of dimension 512. We implement our model in Tensorflow framework <ref type="bibr" target="#b57">[58]</ref>. We use cross-entropy as loss function and trained the model end-to-end with SGD with Nesterov Momentum with value 0.9 for momentum, starting from a learning rate of 0.0001 and decreasing by a factor of 10 when performance saturates.</p><p>In <ref type="table" target="#tab_2">Table 3</ref> results show that RSTG is significantly more powerful than the competitors. Note that the graph model runs on single-image based features, without any temporal processing at the backbone level. The only temporal information is transmitted between nodes at the higher graph level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Ablation study</head><p>Solving the moving digits task requires a model capable of capturing pairwise interactions both in space and time. RSTG is able to accomplish that, through spatial connections between nodes and the temporal updates of their state. In order to prove the benefits of each element, we perform experiments that shows the contributions brought by each one and present them in <ref type="table" target="#tab_2">Table 3</ref>. We observed the efficiently transfer capabilities of our model between the two versions of the SyncMNIST dataset. When pretrained on 3SyncMNIST, our best model RSTG-all-temp-stages achieves 90% of its maximum performance in a number of steps in which an uninitialized model only attains 17% of its maximum performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Space-Only RSTG:</head><p>We create this model in order to prove the necessity of having powerful time modeling. It performs the Space Processing Stage on each frame, but ignores the temporal sequence, replacing the recurrence with an average pool across time dimension, applied for each node. As expected, this model obtains the worst results because the task is based on the movement of each digit, an information that could not be inferred only from spatial exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-Only RSTG:</head><p>This model performs just the Time Processing Stage, without any messagepassing between nodes. The features used in the recurrent step are the initial features extracted from the backbone neural network, which takes as input single frames.</p><p>Homogeneous Space-time RSTG: This model allows the graph to interact both spatially and temporally, but learn the same set of parameters for the MLPs that compute messages in time and space. Thus, time and space are computed in the same way. Positional All-temp RSTG: This is the previous all-temp RSTG model, but enriched with positional embeddings used in f send function as explained in Section 2. This model, which is our best and final model, is also able to reason about global locations of the entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning human-object interaction</head><p>In order to evaluate our method in a real world scenario involving complex interactions, we use the Something-Something-v1 dataset <ref type="bibr" target="#b53">[54]</ref>. It consists of a collection of 108499 videos with 86017, 11522 and 10960 videos for train, validation and test splits respectively. It has 174 classes for fine-grained interactions between humans and objects. It is designed such that classes can be discriminated not by some global context or background but from the actual specific interactions.</p><p>For this task we investigate the performance of our graph model combined with two backbones, a 2D convolutional one (C2D <ref type="bibr" target="#b31">[32]</ref>), based on ResNet-50 architecture and an I3D <ref type="bibr" target="#b47">[48]</ref> model inflated also from the ResNet-50. We start with backbones pretrained on Kinetics-400 <ref type="bibr" target="#b47">[48]</ref> dataset as provided by <ref type="bibr" target="#b31">[32]</ref> and train the whole model end-to-end.</p><p>We analyse our both aggregation types, described in Section 2.3. For RSTG-to-vec we use the last convolutional features given by the I3D backbone as input to our graph model and obtain a vector representation. To facilitate the optimisation process we use residual connections in RSTG, by adding the results of the graph processing to the pooled features of the backbone. For the second case we use intermediate features of I3D as input to the graph and also add them to the graph output by a residual connection and continue the I3D model. For this purpose we need both the input and the output of the graph to have the same dimension. Thus we use RSTG-to-map to obtain a 3D map at each time step.</p><p>Training and evaluation. For training, we uniformly sample 32 frames from each video resized such that the height is 256, preserving the aspect ratio and randomly cropped to a 224 × 224 clip. For inference, we apply the backbone fully convolutional on a 256 × 256 crop with the graph taking features from larger activation maps. We use 11 square clips uniformly sampled on the width of the frames for covering the entire spatial size of the video, and use 2 samplings along the time dimension. We mean pool the clips output for the final prediction.  Results. We analyse how our graph model could be used to improve I3D by applying RSTG-to-map at different layers in the backbone and RSTG-to-vec after the last convolutional layer. In all cases the model achieves competitive results, and the best performance is obtained using the graph in the res3 and res4 blocks of the I3D as shown in <ref type="table" target="#tab_4">Table 5</ref>. We compare against recent methods on the Something-Something-v1 dataset and show the results in <ref type="table" target="#tab_1">Table 2</ref>. Among the models using 2D ConvNet backbones, ours obtains the best results (with a significant improvement of more than 8% over all methods using a 2D backbone, for the Top-1 setup). When using the I3D backbone, RSTG reaches state-of-the-art results, with 1% improvement over all methods (Top-1 case) and 3.1% improvement over top methods (Top-1 case) with the same 3D-ResNet-50 backbone.</p><p>Computational requirements We show the compute times for different variants of our model and for the Non-Local model using the Resnet-50 backbone on Something-Something videos running on one Nvidia GTX 1080 Ti GPU in <ref type="figure" target="#fig_1">Figure 4</ref>. We observe that our RSTG-to-vec model is faster, while having better accuracy than the Non-Local model, whereas our top performing model RSTG-to-map res3-4 further increase the results at the cost of being about 2x slower than RSTG-to-vec. Our RSTG-to-vec requires 6.95 GB for training and 1.23 GB for inference, while RSTG-to-map res3-res4 requires 7.50 GB and 1.93 GB respectively, with a batch of 2 clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper we introduce the Recurrent Space-time Graph (RSTG) neural network model, which is specifically designed to learn efficiently in space and time. The graph, at each moment in time, starts by receiving local space-time information from features produced by a given backbone network.</p><p>Then it moves towards global understanding by passing messages over space between different locations and scales and recurrently in time, by having a different past memory for each space-time iteration. Our model is unique in the literature in the way it processes space and time, with several main contributions: 1) it treats space and time differently; 2) it factorizes them and uses recurrent connections within a unified neural graph model from an unstructured video, with relatively low computational complexity; 3) it is flexible and general, being relatively easy to adapt to various learning tasks in the spatio-temporal domain; 4) our ablation study justifies the structure and different components of our model, which obtains state-of-the-art results on the challenging Something-Something dataset. In future work we plan to further study and extend our model to other higher-level tasks such as semantic segmentation in spatio-temporal data and vision-to-language translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Models ranking on 5SyncMNIST vs other video datasets</head><p>We present in <ref type="figure" target="#fig_2">Figure 5</ref> the available results of our RSTG model and the published models Mean-Pooling+LSTM, Conv+LSTM, I3D and Non-Local on UCF-101 <ref type="bibr" target="#b54">[55]</ref>, HMDB-51 <ref type="bibr" target="#b55">[56]</ref>, Kinetics (see <ref type="bibr" target="#b47">[48]</ref>), Something-Something (see <ref type="bibr" target="#b32">[33]</ref>) and on our 5SyncMNIST dataset. There is one curve per dataset, with one point on the curve per method, shown in increasing order of performance, which is preserved across datasets. As seen by the strictly increasing lines, the same rank order of all the models is maintained on several datasets, including ours. This affirms the consistent behaviour of the methods as well as the relevance of the datasets. Feature extraction: In the following section we give some additional technical details of the way the RSTG model is designed in order to work in conjunction with a backbone. More specifically, we discuss how we combine RSTG with I3D as the backbone model. Our model is suited for being inserted at multiple layers of any backbone network. We test the model when it is included after a single I3D stage or after multiple such stages. This insertion is done as follows (as seen in <ref type="figure">Figure 1</ref> in the main paper): we take the output from a specific backbone layer, having dimension R T ×H×W ×C and use it as an input to our graph model. For example, if we give as video input 32 frames of size 224 × 224 frames to I3D and use the features given by the res4 stage (as input to our RSTG model), we obtain a 16 × 14 × 14 × 2048 input feature map as shown in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>Node creation: As shown in <ref type="figure">Figure 1</ref> in the main paper, we pool features at S different scales, each corresponding to regions in the original image, ranging from areas covering parts of the image to areas covering the entire image. For each of the S scales, we down-sample the features into increasingly smaller maps (one for each scale). Each of these maps form an M × M grid, each point representing a node. Then, at each time step, each node receives a temporal slice from the features corresponding to its cell in the grid.</p><p>Output creation: We process the graph with our interleaved Space and Time Processing Stages. The recurrent Time Processing Stage runs for T steps, with each node internal state h t i at each time step having an increasingly better temporal information during this process.</p><p>By using a residual connection, we add the graph features to the backbone features, thus they must have the same number of channels and temporal dimension. For this, we first project each node back to the initial 2048 dimension and aggregate together the graph and backbone features.</p><p>In the case of RSTG-to-vec models, we sum all the nodes and add them to the global spatial mean pooling of the backbone and average them across time.</p><p>In the case of RSTG-to-map models, we also need to have the same spatial dimension for graph and backbone features. For each scale, the node features, arranged as grids, are up-sampled to the original input spatial size. This is done for each of the S scales, resulting in S 16 × 14 × 14 × 2048 maps  <ref type="figure">Figure 1</ref> of the main paper) is then fed into the remaining stages of the I3D model to obtain the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on the graph position inside the backbone:</head><p>We conduct several experiments showing different ways of combining our RSTG model with the I3D backbone, by varying which I3D layers are used as input to the graph (res2, res3, res4), the final aggregation methods (RSTG-to-vec, RSTG-to-map) and the number of graphs used between different I3D layers.</p><p>We observe that our graph model obtains the best results with higher level features from res4 stage, while it obtains the wors results using lower level res2 stage features. The best results are achieved when two graphs are stacked with input features from different layers of the backbone (i.e. res3 and res4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional experiments regarding the form of adjacency matrix</head><p>In order to validate our choice of connectivity used in the spatial processing, we experimentally test two types of adjacency matrix. The first is formed as stated in the main paper by connecting two nodes if they are neighbours in space or if their regions at different scales intersect. The second is a matrix full of ones, forming a completely connected graph. We observe that the sparser version used in the rest of the paper obtains better results.  <ref type="figure">Figure 6</ref>: Sampled frames from our 5SyncMNIST dataset. With green arrows we show the path of the pair of digits that move synchronously and determine the class label of the video (for a total of 45+1 possible class labels, where the is no pair of synchronously moving digits); with red arrows we show the paths of other digits that move randomly, independently from the pair that move synchronously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details about the baselines used in SyncMNIST experiments</head><p>We offer a more detailed description of the baselines used in the experiments on SyncMNIST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean pooling + LSTM:</head><p>We use as backbone a 3-layer CNN to independently extract features from each frame. Each convolutional layer uses 3 × 3 filters followed by ReLU non-linearity and 2 × 2 max-pooling. We aggregate the spatial information using a final global average pooling, and use an LSTM with 512 hidden state dimension to process all frame features into a feature vector used to make the final prediction. This model is capable of processing information from distant time-steps but it has poor understanding of spatial information because of the lost information due to the pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvNet + LSTM:</head><p>We include an additional ConvNet on top of the previous CNN backbone. It consists of a three convolutional layers, with stride 1, without pooling, with ReLU non-linearity and batch normalization <ref type="bibr" target="#b60">[61]</ref> between them. For the second baseline, the extra convolutional layers (without spatial pooling and no down-sizing of the activation maps) are able to capture fine spatial relationships between different parts of the scene. The features from the last layer are also passed through the same LSTM model. Thus, the second baseline is fully capable of analyzing the entire video, both in space and in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I3D:</head><p>We used a version of I3D <ref type="bibr" target="#b47">[48]</ref> with inflated 3D convolutions adapted from ResNet-50. We considered only 3 residual stages (res2, res3, res5) with fewer number of blocks (2 for each residual stage) and fewer filters such that the number of parameters became comparable with our models (around 10M). While the 3D convolutions process local space-time volumes at a time, they are very powerful and capable to capture some of the longer range relationships through repeated convolutions at multiple layers of depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Local:</head><p>We used the previous I3D architecture (described as I3D above) as a backbone for a Non-Local <ref type="bibr" target="#b31">[32]</ref> model. We tried adding multiple blocks at different stages, and obtained the best results with one non-local block in the second residual stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E An intuitive view of Recurrent Space-time Graph Neural Networks</head><p>The experiments presented in the paper strongly indicate that the RSTG graph structure, which operates iteratively and recurrently over space and time brings an important additional value to the convolutional network backbone, whether that backbone has 3D or 2D convolutions. We believe that the main reason for this fact is hidden in the way objects and events, which are more distant in space and time, interact and influence each other to determine a specific action or activity. Complex activities are often composed of many events, which in turn are defined by several interactions between objects that take place at different positions, scales and moments in time. It is also important that the arrangement of events in time is different, conceptually, than the arrangements of objects in space.</p><p>The ideas above suggest that there is a need for a computational structure that is able to process information locally but is also able to quickly send the results of such computations to distant regions in space and time. What the recurrent space-time graph model has over the more uniform and local convolutional backbone networks is, first and foremost, its ability to separate conceptually the local computation at the level of nodes from the passing of messages between nodes at the level of space-time edges. Then, the message passing routine, which is iterative, can quickly send information globally and reach a convergent state that puts in agreement the local computations.</p><p>In the case of convolutional networks, the spreading of information from the local to the global levels seems to be done less efficiently, through many layers of processing, in a more continuous and local manner, in which time and space are treated more or less in the same uniform fashion. The RSTG graph structure treats from the start time and space differently. It encourages the computation at the node level to reach an agreement by passing messages between nodes for several iterations, in space and also in time (from past state to the current one). We believe that this iterative process is suited for such higher levels of abstraction in order to learn efficiently about how objects interact to form first simpler and more local events and then more complex and more global activities.</p><p>These concluding remarks are supported by our experiments in which we show that RSTG brings a stronger boost over a powerful backbone model when operating over the higher level features provided by this model. The results suggest that the RSTG graph adds complementary capabilities to the input network, by being able to capture, perhaps more efficiently, the discrete-continuous and complex structure of the space-time world at higher levels of abstraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Two Space Processing Stages (K = 2) from top to bottom, each one preceded by a Temporal Processing Stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>We show running time (clips / s) on the left axis and final accuracy on the right axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Performance of different models on several datasets B Details about using the RSTG module together with a backbone</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy on SyncMNIST dataset, showing the capabilities of different parts of our model.</figDesc><table><row><cell>Model</cell><cell cols="2">3 SyncMNIST 5 SyncMNIST</cell></row><row><cell>Mean + LSTM</cell><cell>77.0</cell><cell>-</cell></row><row><cell>Conv + LSTM</cell><cell>95.0</cell><cell>39.7</cell></row><row><cell>I3D</cell><cell>-</cell><cell>90.6</cell></row><row><cell>Non-Local</cell><cell>-</cell><cell>93.5</cell></row><row><cell>RSTG: Space-Only</cell><cell>61.3</cell><cell>-</cell></row><row><cell>RSTG: Time-Only</cell><cell>89.7</cell><cell>-</cell></row><row><cell>RSTG: Homogenous</cell><cell>95.7</cell><cell>58.3</cell></row><row><cell>RSTG: 1-temp-stage</cell><cell>97.0</cell><cell>74.1</cell></row><row><cell>RSTG: All-temp-stages</cell><cell>98.9</cell><cell>94.5</cell></row><row><cell>RSTG: Positional All-temp</cell><cell>-</cell><cell>97.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art models on Something-Something-v1 dataset showing Top-1 and Top-5 accuracy.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell cols="2">Val Top-1 Val Top-5</cell></row><row><cell>C2D</cell><cell>2D ResNet-50</cell><cell>31.7</cell><cell>64.7</cell></row><row><cell>TRN [47]</cell><cell>2D Inception</cell><cell>34.4</cell><cell>-</cell></row><row><cell>ours C2D + RSTG</cell><cell>2D ResNet-50</cell><cell>42.8</cell><cell>73.6</cell></row><row><cell>MFNet-C50 [59]</cell><cell>3D ResNet-50</cell><cell>40.3</cell><cell>70.9</cell></row><row><cell>I3D [33]</cell><cell>3D ResNet-50</cell><cell>41.6</cell><cell>72.2</cell></row><row><cell>NL I3D [33]</cell><cell>3D ResNet-50</cell><cell>44.4</cell><cell>76.0</cell></row><row><cell>NL I3D + Joint GCN [33]</cell><cell>3D ResNet-50</cell><cell>46.1</cell><cell>76.8</cell></row><row><cell>ECOLite-16F [60]</cell><cell>2D Inc+3D Res-18</cell><cell>42.2</cell><cell>-</cell></row><row><cell>MFNet-C101 [59]</cell><cell>3D ResNet-101</cell><cell>43.9</cell><cell>73.1</cell></row><row><cell>I3D [42]</cell><cell>3D Inception</cell><cell>45.8</cell><cell>76.5</cell></row><row><cell>S3D-G [42]</cell><cell>3D Inception</cell><cell>48.2</cell><cell>78.7</cell></row><row><cell>ours I3D + RSTG</cell><cell>3D ResNet-50</cell><cell>49.2</cell><cell>78.8</cell></row></table><note>Heterogeneous Space-time RSTG: We developed different schedulers for our spatial and temporal stages. In the first scheduler, used in the 1-temp RSTG model, for each time step, we performed 3 successive spatial iteration, followed by a single final temporal update. The second scheduler, the all-temp RSTG model, alternates between the spatial and temporal stages (as presented in Alg.1). We use one Time Processing Stage before each of the three Space-Processing Stages, and a last Time Processing Stage to obtain the final nodes representation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study showing where to place the graph inside the ResNet-50 I3D backbone. For our best model we use two different graphs after the res3 and res4 stages of the I3D.</figDesc><table><row><cell>Model</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>RSTG-to-vec</cell><cell>47.7</cell><cell>77.9</cell></row><row><cell>RSTG-to-map res2</cell><cell>46.9</cell><cell>76.8</cell></row><row><cell>RSTG-to-map res3</cell><cell>47.7</cell><cell>77.8</cell></row><row><cell>RSTG-to-map res4</cell><cell>48.4</cell><cell>78.1</cell></row><row><cell>RSTG-to-map res3-4</cell><cell>49.2</cell><cell>78.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Architecture of our RSTG-to-map-res4 model, with 4 scales, that processes features from the res4 stage of the I3D ResNet-50 model. × 1 × 174 that are then summed together into a single map of the same size 16 × 14 × 14 × 2048. The resulting map is further summed with the backbone input features map using a residual connection. The final map thus obtained (which is represented by the up-sampled output features in</figDesc><table><row><cell>model</cell><cell>layer</cell><cell>output size</cell></row><row><cell></cell><cell>input</cell><cell>32 × 224 × 224 × 3</cell></row><row><cell></cell><cell>conv1</cell><cell>32 × 112 × 112 × 64</cell></row><row><cell></cell><cell>pool1</cell><cell>32 × 56 × 56 × 64</cell></row><row><cell>I3D</cell><cell>res2 pool2</cell><cell>32 × 56 × 56 × 256 16 × 56 × 56 × 256</cell></row><row><cell></cell><cell>res3</cell><cell>16 × 28 × 28 × 512</cell></row><row><cell></cell><cell>res4</cell><cell>16 × 14 × 14 × 1024</cell></row><row><cell></cell><cell></cell><cell>16 × 4 × 4 × 512</cell></row><row><cell></cell><cell>Graph creation</cell><cell>16 × 3 × 3 × 512 16 × 2 × 2 × 512</cell></row><row><cell></cell><cell></cell><cell>16 × 1 × 1 × 512</cell></row><row><cell>RSTG</cell><cell>Temporal Processing Stage Spatial Processing Stage</cell><cell>×3</cell></row><row><cell></cell><cell>Temporal Processing Stage</cell><cell>16 × 30 × 512</cell></row><row><cell></cell><cell>Up-sample each grid</cell><cell>16 × 14 × 14 × 512</cell></row><row><cell></cell><cell>1 × 1 × 1 conv</cell><cell>16 × 14 × 14 × 2048</cell></row><row><cell>I3D</cell><cell>res5</cell><cell>16×14×14×2048</cell></row><row><cell></cell><cell>mean pool, fc</cell><cell>1 × 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of RSTG-to-vec model obtained by varying the adjacency matrix.</figDesc><table><row><cell>Model</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>RSTG-to-vec -sparse</cell><cell>47.7</cell><cell>77.9</cell></row><row><cell>RSTG-to-vec -full</cell><cell>46.9</cell><cell>76.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/IuliaDuta/RSTG</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work has been supported in part by Bitdefender and UEFISCDI, through projects EEA-RO-2018-0496 and TE-2016-2182.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh, editors</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the foundations of relaxation labeling processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">W</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="287" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Markov random field image models and their applications to computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Graffigne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international congress of mathematicians</title>
		<meeting>the international congress of mathematicians<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="201" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems: networks of plausible inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quadratic programming relaxations for metric labeling and markov random field map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Satu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer science review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="64" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="45" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6203</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1506.05163</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relational recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7310" to="7321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked spatio-temporal graph convolutional networks for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallabi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
		<idno>abs/1811.10575</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video relationship reasoning using gated spatio-temporal energy graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10424" to="10433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aˆ2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Trajectory convolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2204" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Eidetic 3d LSTM: A model for video prediction and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg, Dandelion Mané</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Sung Joon Son, Gyutae Park, and Nojun Kwak</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
