<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Learning with Normalizing Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kirichenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
						</author>
						<title level="a" type="main">Semi-Supervised Learning with Normalizing Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi-supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The discriminative approach to classification models the probability of a class label given an input p(y|x) directly. The generative approach, by contrast, models the class conditional density for the data p(x|y), and then uses Bayes rule to find p(y|x). In principle, generative modelling has long been more alluring, for the effort is focused on creating an interpretable object of interest, and "what I cannot create, I do not understand". In practice, discriminative approaches typically outperform generative methods, and thus are far more widely used.</p><p>The challenge in generative modelling is that standard approaches to density estimation are often poor descriptions of high-dimensional natural signals. For example, a Gaussian mixture directly over images, while highly flexible for estimating densities, would specify similarities between images as related to Euclidean distances of pixel intensities, which would be a poor inductive bias for handling translations or * Equal contribution 1 New York University. Correspondence to: Pavel Izmailov &lt;pi390@nyu.edu&gt;, Andrew Gordon Wilson &lt;andrewgw@cims.nyu.edu&gt;.</p><p>representing other salient statistical properties. Recently, generative adversarial networks <ref type="bibr" target="#b11">(Goodfellow et al., 2014)</ref>, variational autoencoders <ref type="bibr" target="#b16">(Kingma &amp; Welling, 2013)</ref>, and normalizing flows <ref type="bibr" target="#b8">(Dinh et al., 2014)</ref>, have led to great advances in unsupervised generative modelling, by leveraging the inductive biases of deep convolutional neural networks.</p><p>Normalizing flows are a pleasingly simple approach to generative modelling, which work by transforming a distribution through an invertible neural network. Since the transformation is invertible, it is possible to exactly express the likelihood over the observed data, to train the neural network mapping. The network provides both useful inductive biases, and a flexible approach to density estimation. Normalizing flows also admit controllable latent representations and can be sampled efficiently, unlike auto-regressive models <ref type="bibr" target="#b27">(Papamakarios et al., 2017;</ref><ref type="bibr" target="#b26">Oord et al., 2016)</ref>. Moreover, recent work <ref type="bibr" target="#b9">(Dinh et al., 2016;</ref><ref type="bibr" target="#b15">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b2">Behrmann et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2019;</ref><ref type="bibr" target="#b29">Song et al., 2019)</ref> demonstrated that normalizing flows can produce high-fidelity samples for natural image datasets.</p><p>Advances in unsupervised generative modelling, such as normalizing flows, are particularly compelling for semisupervised learning, where we wish to share structure over labelled and unlabelled data, to make better predictions of class labels on unseen data. In this paper, we introduce an approach to semi-supervised learning with normalizing flows, by modelling the density in the latent space as a Gaussian mixture, with each mixture component corresponding to a class represented in the labelled data. This Flow Gaussian Mixture Model (FlowGMM) is to the best of our knowledge the first approach to semi-supervised learning with normalizing flows that provides an exact joint likelihood over both labelled and unlabelled data, for end-to-end training. <ref type="bibr">*</ref> We illustrate FlowGMM with a simple example in <ref type="figure" target="#fig_0">Figure 1</ref>. We are solving a binary semi-supervised classification problem on the dataset shown in panel (a): the labeled data are shown with triangles colored according to their class, and unlabeled data are shown with blue circles. We introduce a Gaussian mixture with two components corresponding to each of the classes, shown in panel (c) in the latent space Z and an invertible transformation f . The transformation f is then trained to map the data distribution in the data space X to the latent Gaussian mixture in the Z space, mapping the labeled data to the corresponding mixture component. We visualize the learned transformation in panel (b), showing the positions of the images f (x) for all of the training data points. The inverse f −1 of this mapping serves as a class-conditional generative model, that we visualize in panel <ref type="bibr">(d)</ref>. To classify a data point x in the input space we compute its image f (x) in the latent space, and pick the class corresponding to the Gaussian that is closest to f (x). We visualize the decision boundary of the learned classifier with a dashed line in panel (a).</p><formula xml:id="formula_0">f f −1 X , Data Z, Latent Z, Latent X , Data (a) (b) (c) (d)</formula><p>FlowGMM naturally encodes the clustering principle: the decision boundary between classes must lie in the lowdensity region in the data space. Indeed, in the latent space the decision boundary between two classes coincides with the hyperplane perpendicular to the line segment connecting means of the corresponding mixture components and passing through the midpoint of this line segment (assuming the components are normal distributions with identity covariance matrices); in panel (b) of <ref type="figure" target="#fig_0">Figure 1</ref> we show the decision boundary in the latent space with a dashed line. The density of the latent distribution near the decision boundary is low. As the flow is trained to represent data as a transformation of this latent distribution, the density near the decision boundary should also be low. In panel (a) of <ref type="figure" target="#fig_0">Figure 1</ref> the decision boundary indeed lies in the low-density region.</p><p>The contributions of this work include:</p><p>• We propose FlowGMM, a new probabilistic classification model based on normalizing flows that can be naturally applied to semi-supervised learning.</p><p>• We show that FlowGMM has good performance on a broad range of semi-supervised tasks, including image, text and tabular data classification.</p><p>• We propose a new type of probabilistic consistency regularization that significantly improves FlowGMM on image classification problems.</p><p>• To demonstrate the interpretability of FlowGMM, we visualize the learned latent space representations for the proposed semi-supervised model and show that interpolations between data points from different classes pass through low-density regions. We also show how FlowGMM can be used for feature visualization in real-time, without requiring gradients.</p><p>• We show that the predictive uncertainties of FlowGMM can be naturally calibrated by scaling the variances of mixture components.</p><p>• We provide code for FlowGMM at: https:// github.com/izmailovpavel/flowgmm 2. Related Work  represents one of the earliest works on semi-supervised deep generative modelling, demonstrating how the likelihood model of a variational autoencoder <ref type="bibr" target="#b16">(Kingma &amp; Welling, 2013</ref>) could be used for semisupervised image classification. <ref type="bibr">Xu et al. (2017)</ref> later extended this framework to semi-supervised text classification.</p><p>Many generative models for classification <ref type="bibr" target="#b28">(Salimans et al., 2016;</ref><ref type="bibr" target="#b23">Nalisnick et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2019)</ref> have relied upon multitask learning, where a shared latent representation is learned for the generative model and the classifier. With the method of <ref type="bibr" target="#b5">Chen et al. (2019)</ref>, hybrid modeling is observed to reduce performance for both tasks in the supervised case. While GANs have achieved promising performance on semi-supervised tasks, <ref type="bibr" target="#b6">Dai et al. (2017)</ref> showed that classification performance and generative performance are in direct conflict: a perfect generator provides no benefit to classification performance.</p><p>Some works on normalizing flows, such as RealNVP <ref type="bibr" target="#b9">(Dinh et al., 2016)</ref>, have used class-conditional sampling, where the transformation is conditioned on the class label. These approaches pass the class label as an input to coupling layers, conditioning the output of the flow on the class.</p><p>Deep Invertible Generalized Linear Model <ref type="bibr">(DIGLM, Nalisnick et al., 2019)</ref>, most closely related to our work, trains a classifier on the latent representation of a normalizing flow to perform supervised or semi-supervised image classification. Our approach is principally different, as we use a mixture of Gaussians in the latent space Z and perform classification based on class-conditional likelihoods (see (5)), rather than training a separate classifier. One of the key advantages of our approach is the explicit encoding of clustering principle in the method and a more natural probabilistic interpretation.</p><p>Indeed, many approaches to semi-supervised learn from the labelled and unlabelled data using different (and possibly misaligned) objectives, often also involving two step procedures where the unsupervised model is used as pre-processing for a supervised approach. In general, FlowGMM is distinct in that the generative model is used directly as a Bayes classifier, and in the limit of a perfect generative model the Bayes classifier achieves a provably optimal misclassification rate (see e.g. <ref type="bibr" target="#b21">Mohri et al., 2018)</ref>. Moreover, approaches to semi-supervised classification, such as consistency regularization <ref type="bibr" target="#b18">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b20">Miyato et al., 2018;</ref><ref type="bibr" target="#b31">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b1">Athiwaratkun et al., 2019;</ref><ref type="bibr" target="#b32">Verma et al., 2019;</ref><ref type="bibr" target="#b3">Berthelot et al., 2019)</ref>, typically focus on image modelling. We instead focus on showcasing the broad applicability of FlowGMM on text, tabular, and image data, as well as the ability to conveniently discover interpretable structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background: Normalizing Flows</head><p>The normalizing flow <ref type="bibr" target="#b9">(Dinh et al., 2016</ref>) is an unsupervised model for density estimation defined as an invertible mapping f : X → Z from the data space X to the latent space Z. We can model the data distribution as a transformation f −1 : Z → X applied to a random variable from the latent distribution z ∼ p Z , which is often chosen to be Gaussian. The density of the transformed random variable x = f −1 (z) is given by the change of variables formula</p><formula xml:id="formula_1">p X (x) = p Z (f (x)) · det ∂f ∂x .<label>(1)</label></formula><p>The mapping f is implemented as a sequence of invertible functions, parametrized by a neural network with architecture that is designed to ensure invertibility and efficient computation of log-determinants, and a set of parameters θ that can be optimized. The model can be trained by maximizing the likelihood in Equation (1) of the training data with respect to the parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Flow Gaussian Mixture Model</head><p>We introduce the Flow Gaussian Mixture Model (FlowGMM), a probabilistic generative model for semisupervised learning with normalizing flows. In FlowGMM, we introduce a discrete latent variable y for the class label, y ∈ {1 . . . C}. Our latent space distribution, conditioned on a label k, is Gaussian with mean µ k and covariance Σ k :</p><formula xml:id="formula_2">p Z (z|y = k) = N (z|µ k , Σ k ).<label>(2)</label></formula><p>The marginal distribution of z is then a Gaussian mixture. When the classes are balanced, this distribution is</p><formula xml:id="formula_3">p Z (z) = 1 C C k=1 N (z|µ k , Σ k ).<label>(3)</label></formula><p>Combining equations <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_1">(1)</ref>, the likelihood for labeled data is</p><formula xml:id="formula_4">p X (x|y = k) = N (f (x)|µ k , Σ k ) · det ∂f ∂x ,</formula><p>and the likelihood for data with unknown label is p X (x) = k p X (x|y = k)p(y = k). If we have access to both a labeled dataset D and an unlabeled dataset D u , then we can train our model in a semi-supervised way to maximize the joint likelihood of the labeled and unlabeled data</p><formula xml:id="formula_5">p X (D , D u |θ) = (xi,yi)∈D p X (x i , y i ) xj ∈Du p X (x j ), (4)</formula><p>over the parameters θ of the bijective function f , which learns a density model for a Bayes classifier. In particular, given a test point x, the model predictive distribution is given by</p><formula xml:id="formula_6">p X (y|x) = p X (x|y)p(y) p(x) = N (f (x)|µ y , Σ y ) C k=1 N (f (x)|µ k , Σ k ) .<label>(5)</label></formula><p>We can then make predictions for a test point x with the Bayes decision rule y = arg max i∈{1,...,C} p X (y = i|x).</p><p>As an alternative to direct likelihood maximization, we can adapt the Expectation Maximization algorithm for model training as discussed in Appendix A.</p><formula xml:id="formula_7">(a) Labeled + Unlabeled (b) Labeled Only (c) Labeled + Unlabeled (d) Labeled Only Figure 2.</formula><p>Illustration of FlowGMM performance on synthetic datasets. Labeled data are shown with colored triangles, and unlabeled data are shown with blue circles. Colors represent different classes. We compare the classifier decision boundaries when only using labeled data (panels b, d) and when using both labeled and unlabeled data (panels a, c) on two circles (panels a, b) and pinwheel (panels c, d) datasets. FlowGMM leverages unlabeled data to push the decision boundary to low-density regions of the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Consistency Regularization</head><p>Most of the existing state-of-the-art approaches to semisupervised learning on image data are based on consistency regularization <ref type="bibr" target="#b18">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b20">Miyato et al., 2018;</ref><ref type="bibr" target="#b31">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b1">Athiwaratkun et al., 2019;</ref><ref type="bibr" target="#b32">Verma et al., 2019;</ref><ref type="bibr">Xie et al., 2020;</ref><ref type="bibr" target="#b4">Berthelot et al., 2020)</ref>. These methods penalize changes in network predictions with respect to input perturbations, such as random translations and horizontal flips, with an additional loss term that can be computed on unlabeled data,</p><formula xml:id="formula_8">cons (x) = g(x ) − g(x ) 2 ,<label>(6)</label></formula><p>where x , x are random perturbations of x, and g is the vector of probabilities over the classes.</p><p>Motivated by these methods, we introduce a new consistency regularization term for FlowGMM. Let y be the label predicted on image x by FlowGMM according to (5). We then define our consistency loss as the negative log likelihood of the input x given the label y :</p><formula xml:id="formula_9">L cons (x , x ) = − log p(x |y ) = − log N (f (x )|µ y , Σ y ) − log det ∂f ∂x .<label>(7)</label></formula><p>This loss term encourages the model to map small perturbations of the same unlabeled inputs to the same components of the Gaussian mixture distribution in the latent space. Unlike the standard consistency loss of (6), the proposed loss in (7) takes values on the same scale as the data log likelihood (4), and indeed we find it to work better in practice. We refer to FlowGMM with the consistency term as FlowGMM-cons. The final loss for FlowGMM-cons is then the weighted sum of the consistency loss (7) and the negative log likelihood of both labeled and unlabeled data (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate FlowGMM on a wide range of datasets across different application domains including low-dimensional synthetic data (Section 5.1), text and tabular data (Section 5.2), and image data (Section 5.3). We show that FlowGMM outperforms the baselines on tabular and text data. FlowGMM is also state-of-the-art as an end-to-end generative approach to semi-supervised image classification, conditioned on architecture. However, FlowGMM is constrained by the RealNVP architecture, and thus does not outperform the most powerful approaches in this setting, which involve discriminative classifiers.</p><p>In all experiments, we use the RealNVP normalizing flow architecture. Throughout training, Gaussian mixture parameters are fixed: the means are initialized randomly from the standard normal distribution and the covariances are set to I. See Appendix B for further discussion on GMM initialization and training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic Data</head><p>We first apply FlowGMM to a range of two-dimensional synthetic datasets, in order to gain a better visual intuition for the method. We use the RealNVP architecture with 5 coupling layers, defined by fully-connected shift and scale networks, each with 1 hidden layer of size 512. In addition to the semi-supervised setting, we also trained the method only using the labeled data. In <ref type="figure">Figure 2</ref> we visualize the decision boundaries of the classifier corresponding to FlowGMM for both of these settings on the two circles and pinwheel datasets. On both datasets, FlowGMM is able to benefit from the unlabeled data to push the decision boundary to a low-density region, as expected. On the two circles problem the method is unable to fit the data perfectly as flows are homeomorphisms, and the disk is topologically distinct from an annulus. However, FlowGMM still produces a reasonable decision boundary and improves over the case when only labeled data are available. We provide additional visualizations in Appendix C, <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Text and Tabular Data</head><p>FlowGMM can be especially useful for semi-supervised learning on tabular data. Consistency-based semisupervised methods have mostly been developed for image classification, where the predictions of the method are regularized to be invariant to random flips and translations of the image. On tabular data, desirable invariances are less obvious, finding suitable transformations to apply for consistency-based methods is not-trivial. Similarly, approaches based on GANs have mostly been developed for images. We evaluate FlowGMM on the Hepmass and Miniboone UCI classification datasets (previously used in <ref type="bibr" target="#b27">Papamakarios et al. (2017)</ref> for density estimation).</p><p>Along with standard tabular UCI datasets, we also consider text classification on AG-News and Yahoo Answers datasets. Using the recent advances in transfer learning for NLP, we construct embeddings for input texts using the BERT transformer model <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> trained on a corpus of Wikipedia articles, and then train FlowGMM and other baselines on the embeddings.</p><p>We compare FlowGMM to the graph based label spreading method from Zhou et al. <ref type="formula" target="#formula_2">(2004)</ref>, a Π-Model <ref type="bibr" target="#b18">(Laine &amp; Aila, 2016)</ref> that uses dropout perturbations, as well as supervised logistic regression, k-nearest neighbors, and a neural network trained on the labeled data only. We report the results in <ref type="table">Table 1</ref>, where FlowGMM outperforms the alternative semi-supervised learning methods on each of the considered datasets. Implementation details for FlowGMM, the baselines, and data preprocessing details are in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image Classification</head><p>We next evaluate the proposed method on semi-supervised image classification benchmarks on CIFAR-10, MNIST and SVHN datasets. For all the datasets, we use the RealNVP <ref type="bibr" target="#b9">(Dinh et al., 2016)</ref> architecture. Exact implementation details are listed in the Appendix E. The supervised model is trained using the same loss (4), where all the data points are labeled (n u = 0).</p><p>We present the results for FlowGMM and FlowGMM-cons in <ref type="table">Table 2</ref>. We also report results from DIGLM <ref type="bibr" target="#b23">(Nalisnick et al., 2019)</ref>, supervised only performance on MNIST and SVHN, and the M1+M2 VAE model . FlowGMM outperforms the M1+M2 model and performs better or on par with DIGLM. Furthermore, FlowGMMcons improves over FlowGMM on all three datasets, suggesting that our proposed consistency regularization is helpful for performance.</p><p>Following <ref type="bibr" target="#b25">Oliver et al. (2018)</ref>, we evaluate FlowGMM-cons varying the number of labeled data points. Specifically, we follow the setup of  and train FlowGMM-cons on MNIST with 100, 600, 1000 and 3000 labeled data points. We present the results in <ref type="table" target="#tab_1">Table 3</ref>. FlowGMM-cons outperforms the M1+M2 model of  in all the considered settings.</p><p>We note that the results presented in this Section are not directly comparable with the state-of-the-art methods using GANs or consistency regularization (see e.g. <ref type="bibr" target="#b18">Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b6">Dai et al., 2017;</ref><ref type="bibr" target="#b1">Athiwaratkun et al., 2019;</ref><ref type="bibr" target="#b3">Berthelot et al., 2019)</ref>, as the architecture we employ is much less powerful for classification than the ConvNet and ResNet architectures that have been designed for classification without the constraint of invertibility. We believe that invertible architectures with better inductive biases for classification may help bridge this gap; invertible residual networks <ref type="bibr" target="#b2">(Behrmann et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2019)</ref> and invertible CNNs  are some of the early examples of this class of architectures.</p><p>In general, it is difficult to directly compare FlowGMM with most existing approaches, because the types of architectures available for fully generative normalizing flows are very different than what is available to (partially) discriminative approaches or even other generative methods like VAEs. This difference is due to the invertibility requirement for normalizing flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Model Analysis</head><p>We empirically analyze different aspcects of FlowGMM and highlight some useful features of this model. In Section 6.1 we discuss the calibration of predictive uncertainties produced by the model. In Section 6.2, we study the latent representations learned by FlowGMM. Finally, in Section 6.3, we discuss a feature visualization technique that can be used to interpret the features learned by FlowGMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Uncertainty and Calibration</head><p>In many applications, particularly where decision making is involved, it is crucial to have reliable confidences associated with predictions. In classification problems, well-calibrated models are expected to output accurate probabilities of belonging to a particular class. Reliable uncertainty estimation is especially relevant in semi-supervised learning since label information is limited during training. <ref type="bibr" target="#b12">Guo et al. (2017)</ref>, showed that modern deep learning models are highly overconfident, but could be easily recalibrated with temperature scaling. In this Section, we analyze the predictive uncertainties produced by FlowGMM. In Appendix Section F, we also consider out-of-domain data detection.</p><p>When using FlowGMM for classification, the class predic- </p><formula xml:id="formula_10">p(y|x) = N (f (x)|µ y , Σ y ) C k=1 N (f (x)|µ k , Σ k )</formula><p>.</p><p>Since we initialize Gaussian mixture means randomly from the standard normal distribution and do not train them along with the flow parameters (see Appendix B), FlowGMM predictions become inherently overconfident due to the curse of dimensionality. For example, consider two Gaussians with means sampled independently from the standard normal µ 1 , µ 2 ∼ N (0, I) in D-dimensional space. If s 1 ∼ N (µ 1 , I) is a sample from the first Gaussian, then its expected squared distances to both mixture means are E s 1 − µ 1 2 = D and E s 1 − µ 2 2 = 3D (for a detailed derivation see Appendix Section G). In high dimensional spaces, such logits would lead to hard label assignment in FlowGMM (p(y|x) ≈ 1 for exactly one class). In fact, in the experiments we observe that FlowGMM is overconfident and performs hard label assignment: predicted class probabilities are all close to either 1 or 0.</p><p>We address this problem by learning a single scalar parameter σ 2 for all components in the Gaussian mixture (the component k will be N (µ k , σ 2 I)) by minimizing the negative log likelihood on a validation set. This way we can naturally re-calibrate the variance of the latent GMM. This procedure is also equivalent to applying temperature scaling <ref type="bibr" target="#b12">(Guo et al., 2017)</ref> to logits log N (x|µ k , Σ k ). We test FlowGMM calibration on MNIST and CIFAR datasets in the supervised setting. On MNIST we restricted the training set size to 1000 objects, since on the full dataset the model makes too few mistakes which makes evaluating calibration harder. In <ref type="table">Table 4</ref>, we report negative log likelihood and expected calibration error (ECE, see <ref type="bibr" target="#b12">Guo et al. (2017)</ref> for a description of this metric). We can see that re-calibrating variances of the Gaussians in the mixture significantly improves both metrics and mitigates overconfidence. The effectiveness of this simple rescaling procedure suggests that the latent space distances learned by the flow model are correlated with the probabilities of belonging to a particular class: the closer a datapoint is to the mean of a Gaussian in the latent space, the more likely it belongs to the corresponding class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Learned Latent Representations</head><p>We next analyze the latent representation space learned by FlowGMM. We examine latent interpolations between members of the same class in <ref type="figure" target="#fig_1">Figure 3</ref>(a) and between different classes in <ref type="figure" target="#fig_1">Figure 3</ref>(b) for our MNIST FlowGMMcons model trained with n = 1k labels. As expected, interclass interpolations pass through regions of low-density, leading to low quality samples but intra-class interpolations do not. These observations suggest that, as expected, the model learns to put the decision boundary in the low-density region of the data space.</p><p>In Appendix section H, we present images corresponding to the means of the Gaussian mixture and class-conditional samples from FlowGMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance to Decision Boundary</head><p>To explicitly test this conclusion, we compute the distribution of distances from unlabeled data to the decision boundary for FlowGMM-cons and FlowGMM Sup trained on labeled data only. In order to compute this distance exactly for an image x, we find the two closest means µ , µ to the corresponding latent variable z = f (x), and evaluate the expression</p><formula xml:id="formula_11">d(x) = µ −f (x) 2 − µ −f (x) 2 2 µ −µ .</formula><p>We visualize the distributions of the distances for the supervised and semi-supervised method in <ref type="figure" target="#fig_1">Figure 3(c)</ref>. While most of the unlabeled data are far from the decision boundary for both methods, the supervised method puts a substantially larger fraction of data close to the decision boundary. For example, the distance to the decision boundary is smaller than 5 for 1089 unlabeled data points with supervised model, but only 143 data points with FlowGMM-cons. This increased separation suggests that FlowGMM-cons indeed pushes the decision boundary away from the data distribution as would be desired from the clustering principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Feature Visualization</head><p>Feature visualization has become an important tool for increasing the interpretability of neural networks in supervised learning. The majority of methods rely on maximizing the activations of a given neuron, channel, or layer over a parametrization of an input image with different kinds of image regularization <ref type="bibr" target="#b30">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b24">Olah et al., 2017;</ref><ref type="bibr" target="#b19">Mahendran &amp; Vedaldi, 2015)</ref>. These methods, while effective, require iterative optimization too costly for real time interactive exploration. In this Section we discuss a simple and efficient feature visualization technique that leverages the invertibility of FlowGMM. This technique can be used with any invertible model but is especially relevant for FlowGMM, where we can use feature visualization to gain insights into the classification decisions made by the model. Since our classification model uses a flow which is a sequence of invertible transformations f (x) = f :L (x) := f L • f L−1 • · · · • f 1 (x), intermediate activations can be inverted directly. This means that we can combine the methods of feature inversion and feature maximization directly by feeding in a set of input images, modifying intermediate activations arbitrarily, and inverting the representation. Given a set of activations in the th layer a [c, i, j] = f : (x) cij with channels c and spatial extent i, j, we may perturb a single neuron with</p><formula xml:id="formula_12">x(α) = f −1 : (f : (x) + ασ c δ c ),<label>(8)</label></formula><p>where δ c is a one hot vector at channel c; and σ c is the standard deviation of the activations in channel c over the the training set and spatial locations. This procedure can be performed at real-time rates to explore the activation parametrized by α and the location (c, i, j) without any optimization or hyper-parameters. We show the feature visualization for intermediate layers on CIFAR-10 test images in <ref type="figure" target="#fig_1">Figure 3(d)</ref>. The channel being visualized appears to activate on the zeroed pixels from random translations as well as the green channel. Analyzing the features learned by FlowGMM we can gain insight into the workings of the model.</p><formula xml:id="formula_13">x x( ) vs (a [c, : , : ] c )/ c (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>We proposed a simple and interpretable approach for end-toend generative semi-supervised prediction with normalizing flows. While FlowGMM does not yet outperform the most powerful discriminative approaches for semi-supervised image classification <ref type="bibr" target="#b1">(Athiwaratkun et al., 2019;</ref><ref type="bibr" target="#b32">Verma et al., 2019)</ref>, we believe it is a promising step towards making fully generative approaches more practical for semi-supervised tasks. As we develop improved invertible architectures, the performance of FlowGMM will also continue to improve.</p><p>Moreover, FlowGMM does outperform graph-based and consistency-based baselines on tabular data including semisupervised text classification with BERT embeddings. We believe that the results show promise for generative semisupervised learning based on normalizing flows, especially for tabular tasks where consistency-based methods struggle.</p><p>We view interpretability and broad applicability as a strong advantage of FlowGMM. The access to latent space representations and the feature visualization technique discussed in Section 6 as well as the ability to sample from the model can be used to obtain insights into the performance of the model in practical applications.</p><p>Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsupervised data augmentation for consistency training, 2020. URL https://openreview.net/forum? id=ByeL1R4FvS.</p><p>Xu, W., Sun, H., Deng, C., and Tan, Y. Variational autoencoder for semi-supervised text classification. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.</p><p>Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and Schölkopf, B. Learning with local and global consistency.</p><p>In Advances in neural information processing systems, pp. 321-328, 2004.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Expectation Maximization</head><p>As an alternative to direct optimization of the likelihood (4), we consider Expectation-Maximization algorithm (EM). EM is a popular approach for finding maximum likelihood estimates in mixture models.</p><formula xml:id="formula_14">Suppose X = {x i } n i=1 is the observed dataset, T = {t i } n i=1</formula><p>are corresponding unobserved latent variables (often denoting the component in mixture model) and θ is a vector of model parameters. EM algorithm consists of the two alternating steps: on E-step, we compute posterior probabilities of latent variables for each data point q(t i |x i ) = P (t i |x i , θ); and on M-step, we fix q and maximize the expected log likelihood of the data and latent variables with respect to θ: E q log P (X, T |θ) → max θ . The algorithm can be easily adapted to the semi-supervised setting where a subset of data is labeled with {y l i } n l i=1 : then, on E-step we have hard assignment to the true mixture component q(t i |x i ) = I[t i = y l i ] for labeled data points.</p><p>EM is applicable to fitting the transformed mixture of Gaussians. We can perform the exact E-step for unlabeled data in the model since</p><formula xml:id="formula_15">q(t|x) = p(x|t, θ) p(x|θ) = N (f (x)|µ t , Σ t ) · det ∂f ∂x C k=1 N (f (x)|µ k , Σ k ) · det ∂f ∂x = N (f (x)|µ t , Σ t ) C k=1 N (f (x)|µ k , Σ k )</formula><p>which coincides with the E-step of EM algorithm on Gaussian mixture model. On M-step, the objective has the following form:</p><formula xml:id="formula_16">n l i=1 log N (f θ (x l i )|µ y l i , Σ y l i ) ∂f θ ∂x l i + nu i=1 E q(ti|x u i ,θ) log N (f θ (x u i )|µ ti , Σ ti ) ∂f θ ∂x u i .</formula><p>Since the exact solution is not tractable due to complexity of the flow model, we perform a stochastic gradient step to optimize the expected log likelihood with respect to flow parameters θ.</p><p>Note that unlike regular EM algorithm for mixture models, we have Gaussian mixture parameters {(µ k , Σ k )} C k=1 fixed in our experiments, and on M-step the update of θ induces the change of z i = f θ (x i ) latent space representations.</p><p>Using EM algorithm for optimization in the semi-supervised setting on MNIST dataset with 1000 labeled images, we obtain 98.97% accuracy which is comparable to the result for FlowGMM with regular SGD training. However, in our experiments, we observed that on E-step, hard label assignment happens for unlabeled points (q(t|x) ≈ 1 for one of the classes) because of the high dimensionality of the problem (see section 6.1) which affects the M-step objective and hinders training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latent Distribution Mean and Covariance Choices</head><p>Initialization In our experiments, we draw the mean vectors µ i of Gaussian mixture model randomly from the standard normal distribution µ i <ref type="figure">∼ N (0, I)</ref>, and set the covariance matrices to identity Σ i = I for all classes; we fixed GMM parameters throughout training. However, one could potentially benefit from data-dependent placing of means in the latent space. We experimented with different initialization methods, in particular, initializing means using the mean point of latent representations of labeled data in each class:</p><formula xml:id="formula_17">µ i = (1/n i l ) n i l m=1 f (x i m )</formula><p>where x i m represents labeled data points from class i and n i l is the total number of labeled points in that class. In addition, we can scale all means by a scalar valueμ i = rµ i to increase or decrease distances between them. We observed that such initialization leads to much faster convergence of FlowGMM on semi-supervised classification on MNIST dataset, however, the final performance of the model was worse compared to the one with random mean placing. We hypothesize that it becomes easier for the flow model to warm up faster with data-dependent initialization because Gaussian means are closer to the initial latent representations, but afterwards the model gets stuck in a suboptimal solution.</p><p>GMM training FlowGMM would become even more flexible and expressive if we could learn Gaussian mixture parameters in a principled way. In the current setup where means are sampled from the standard normal distribution, the distances between mixture components are about √ 2D where D is the dimensionality of the data (see Appendix G). Thus, classes are quite far apart from each other in the latent space, which, as observed in Section 6.1, leads to model miscalibration. Training GMM parameters can further increase interpretability of the learned latent space representations: we can imagine a scenario in which some of the classes are very similar or even intersecting, and it would be useful to represent it in the latent space. We could train GMM by directly optimizing likelihood (4), or using expectation maximization (see Section A), either jointly with the flow parameters or iteratively switching between training flow parameters with the fixed GMM and training GMM with the fixed flow. In our initial experiments on semi-supervised classification on MNIST, training GMM jointly with the flow parameters did not improve performance or lead to substantial change of the latent representations. Further improvements require careful hyper-parameter choice which we leave for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Synthetic Experiments</head><p>In <ref type="figure">Figure 4</ref> we visualize the classification decision boundaries of FlowGMM as well as the learned mapping to the latent space and generated samples for three different synthetic datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tabular data preparation and hyperparameters</head><p>The AG-News and Yahoo Answers were constructed by applying BERT embeddings to the text input, yielding a 768 dimensional vector for each data point. AG-News has 4 classes while Yahoo Answers has 10. The UCI datasets Hepmass and Miniboone were constructed using the data preprocessing from <ref type="bibr" target="#b27">Papamakarios et al. (2017)</ref>, but with the inclusion of the removed background process class so that the two problems can be used for binary classification. We then subsample the fraction of background class examples so that the dataset is balanced. For each of the datasets, a separate validation set of size 5k was used to tune hyperparameters. All neural network models use the ADAM optimizer <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2014)</ref>.</p><p>k-Nearest Neighbors: We tested both using both L2 distance and L2 with inputs normalized to unit norm, (sin 2 distance), and the latter performed the best. The value k chosen in the method was found sweeping over 1 − 20, and the optimal values for each of the datasets are shown in 5.</p><p>3 Layer NN + Dropout: The 3-Layer NN + Dropout baseline network has three fully connected hidden layers with inner dimension k = 512, ReLU nonlinearities, and dropout with p = 0.5. We use the learning rate 3e−4 for training the supervised baseline across all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Π-Model:</head><p>The Π-Model uses the same network architecture, and dropout for the perturbations. The additional consistency loss per unlabeled data point is computed as L Unlab = ||g(x ) − g(x )|| 2 , where g is are the output probabilities after the softmax layer of the neural network and the consistency weight λ = 30 which worked the best across the datasets. The model was trained for 50 epochs with labeled and unlabeled batch size n for AG-News and Yahoo Answers, and labeled and unlabeled batch sizes n and 2000 for Hepmass and Miniboone. </p><formula xml:id="formula_18">(x i , x j )) where sin 2 (x i , x j ) := 1 − xi,xj</formula><p>xi xj . This is equivalent to L2 distance on the inputs normalized to unit magnitude. Because the algorithm scales poorly with number of unlabeled points for dense affinity matrices, O(n 3 u ), we we subsampled the number of unlabeled data points to 10k and test data points to 5k for this graph method. However, we also evaluate the label spreading algorithm with a sparse kNN affinity matrix on using a larger subset 20k of unlabeled data. The two hyperparameters for label spreading (γ/k and α) were tuned by separate grid search for each of the datasets. In both cases, we use the inductive variant of the algorithm where the test data is not included in the unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FlowGMM:</head><p>We train our FlowGMM model with a Real-NVP normalizing flow, similar to the architectures used in <ref type="bibr" target="#b27">Papamakarios et al. (2017)</ref>. Specifically, the model uses 7 coupling layers, with 1 hidden layer each and 256 hidden units for the UCI datasets but 512 for text classification. UCI models were trained for 50 epochs of unlabeled data and the text datasets were trained for 30 epochs of unlabeled data. The labeled and unlabeled batch sizes are the same as in the Π-Model.</p><p>The tuned learning rates for each of the models that we used for these experiments are shown in <ref type="table" target="#tab_2">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Image data preparation and hyperparameters</head><p>We use the CIFAR-10 multi-scale architecture with 2 scales, each containing 3 coupling layers defined by 8 residual blocks with 64 feature maps. We use Adam optimizer <ref type="bibr" target="#b14">(Kingma &amp; Ba, 2014)</ref> with learning rate 10 −3 for CIFAR-10 and SVHN and 10 −4 for MNIST. We train the supervised model for 100 epochs, and semi-supervised models for 1000 passes through the labeled data for CIFAR-10 and SVHN and 3000 passes for MNIST. We use a batch size of 64 and sample 32 labeled and 32 unlabeled data points in each mini-batch. For the consistency loss term <ref type="formula" target="#formula_9">(7)</ref>, we linearly Two Circles</p><p>8 Gaussians</p><p>Pinwheel increase the weight from 0 to 1 for the first 100 epochs following <ref type="bibr" target="#b1">Athiwaratkun et al. (2019)</ref>. For FlowGMM and FlowGMM-cons, we re-weight the loss on labeled data by λ = 3 (value tuned on validation  on CIFAR-10), as otherwise, we observed that the method underfits the labeled data.</p><formula xml:id="formula_19">f f f f −1 f −1 f −1 X , Data Z, Latent Z, Latent X , Data (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Out-of-domain data detection</head><p>Density models have held promise for being able to detect out-of-domain data, an especially important task for robust machine learning systems <ref type="bibr" target="#b23">(Nalisnick et al., 2019)</ref>.</p><p>Recently, it has been shown that existing flow and autoregressive density models are not as apt at this task as previously thought, yielding high likelihood on images coming from other (simpler) distributions. The conclusion put forward is that datasets like SVHN are encompassed by, or have roughly the same mean but lower variance than, more complex datasets like CIFAR-10 <ref type="bibr" target="#b22">(Nalisnick et al., 2018)</ref>. We examine this hypothesis in the context of our flow model which has a multi-modal latent space distribution unlike methods considered in <ref type="bibr" target="#b22">Nalisnick et al. (2018)</ref>.</p><p>Using a fully supervised model trained on MNIST, we evaluate the log likelihood for data points coming from the NotMNIST dataset, consisting of letters instead of digits, and the FashionMNIST dataset. We then train a supervised model on the more complex dataset FashionMNIST and evaluate on MNIST and NotMNIST. The distribution of the log likelihood log p X (·) = log p Z (f (·)) + log det ∂f ∂x on these datasets is shown in <ref type="figure">Figure 5</ref>. For the model trained on MNIST we see that the data from Fashion MNIST and NotMNIST is assigned lower likelihood, as expected. However, the model trained on FashionMNIST predicts higher likelihoods for MNIST images. The majority (≈ 75%) of the MNIST data points get mapped into the mode of the FashionMNIST model corresponding to sandals, which is the class with the largest fraction of pixels that are zero.</p><p>Similarly, for the model trained on MNIST the image of all zeros has very high likelihood and gets mapped to the mode corresponding to the digit 1 which has the largest fraction of empty space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Expected Distances between Gaussian Samples</head><p>Consider two Gaussians with means sampled independently from the standard normal µ 1 , µ 2 ∼ N (0, I) in Ddimensional space. If s 1 ∼ N (µ 1 , I) is a sample from the first Gaussian, then its expected squared distances to both mixture means are:</p><formula xml:id="formula_20">E s 1 − µ 1 2 = E E s 1 − µ 1 2 |µ 1 = E D i=1 E (s 1,i − µ 1,i ) 2 |µ 1,i = E D i=1 E[s 2 1,i ] − 2µ 2 1,i + µ 2 1,i = E D i=1 1 + µ 2 1,i − µ 2 1,i = D E s 1 − µ 2 2 = E E s 1 − µ 2 2 |µ 1 , µ 2 = E D i=1 E (s 1,i − µ 2,i ) 2 |µ 1,i , µ 2,i = E D i=1</formula><p>1 + µ 2 1,i − 2µ 1,i µ 2,i + µ 2 2,i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>= 3D</head><p>For high-dimensional Gaussians the random variables s 1 − µ 1 2 and s 1 − µ 2 2 will be concentrated around their expectations. Since the function exp(−x) decreases rapidly to zero for positive x, the probability of s 1 belonging to the first Gaussian exp(− s 1 − µ 1 2 )/ exp(− s 1 − µ 1 2 ) + exp(− s 1 − µ 2 2 ) ≈ exp(−D)/(exp(−D) + exp(−3D)) = 1/(1 + exp(−2D)) saturates at 1 with the growth of dimensionality D.</p><p>H. FlowGMM as generative model In <ref type="figure" target="#fig_3">Figure 6a</ref> we show the images f −1 (µ i ) corresponding to the means of the Gaussians representing each class. We see that the flow correctly learns to map the means to samples from the corresponding classes. Next, in <ref type="figure" target="#fig_3">Figure 6b</ref> we show class-conditional samples from the model. To produce a sample from class i, we first generate z ∼ N (µ i , T I), where T is a temperature parameter that controls trade-off between sample quality and diversity; we then compute the samples as f −1 (z). We set T = 0.25 2 to produce samples in <ref type="figure" target="#fig_3">Figure 6b</ref>. As we can see, FlowGMM can produce reasonable class-conditional samples simultaneously with achieving a high classification accuracy (99.63%) on the MNIST dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of semi-supervised learning with FlowGMM on a binary classification problem. Colors represent the two classes or the corresponding Gaussian mixture components. Labeled data are shown with triangles, colored by the corresponding class label, and blue dots represent unlabeled data. (a): Data distribution and the classifier decision boundary. (b): The learned mapping of the data to the latent space. (c): Samples from the Gaussian mixture in the latent space. (d): Samples from the model in the data space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualizations of the latent space representations learned by supervised FlowGMM on MNIST. (a): Latent space interpolations between test images from the same class and (b): from different classes. Observe that interpolations between objects from different classes pass through low-density regions. (c): Histogram of distances from unlabeled data to the decision boundary for FlowGMM-cons trained on 1k labeled and 59k unlabeled data and FlowGMM Sup trained on 1k labeled data only. FlowGMM-cons is able to push the decision boundary away from the data distribution using unlabeled data. (d): Feature visualization for CIFAR-10: four test reconstructions are shown as an intermediate feature is perturbed. The value of the perturbation α is shown in red vs the distribution of the channel activations. Observe that the channel visualized activates on zeroed out pixels to the left of the image mimicking the random translations applied to the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Illustration of FlowGMM on synthetic datasets: two circles (top row), eight Gaussians (middle row) and pinwheel (bottom row). (a): Data distribution and classification decision boundaries. Unlabeled data are shown with blue circles and labeled data are shown with colored triangles, where color represents the class. Background color visualizes the classification decision boundaries of FlowGMM. (b): Mapping of the data to the latent space. (c): Gaussian mixture in the latent space. (d): Samples from the learned generative model corresponding to different classes, as shown by their color. Left: Log likelihoods on in-and out-of-domain data for our model trained on MNIST. Center: Log likelihoods on in-and out-of-domain data for our model trained on FashionMNIST. Right: MNIST digits get mapped onto the sandal mode of the FashionMNIST model 75% of the time, often being assigned higher likelihood than elements of the original sandal class. Representative elements are shown above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Visualizations of the latent space representations learned by supervised FlowGMM on MNIST. (a): Images corresponding to means of the Gaussians corresponding to different classes. (b): Class-conditional samples from the model at a reduced temperature T = 0.25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Accuracy on BERT embedded text classification datasets and UCI datasets with a small number of labeled examples. The kNN baseline, logistic regression, and the 3-Layer NN + Dropout were trained on the labeled data only. Numbers reported for each method are the best of 3 runs (ranked by performance on the validation set). n l and nu are the number of labeled and unlabeled data points.Dataset (n l / nu, classes) Accuracy of the FlowGMM, VAE model (M1+M2 VAE,, DIGLM<ref type="bibr" target="#b23">(Nalisnick et al., 2019)</ref> in supervised and semi-supervised settings on MNIST, SVHN, and CIFAR-10.</figDesc><table><row><cell>Method</cell><cell>AG-News</cell><cell cols="2">Yahoo Answers</cell><cell cols="2">Hepmass</cell><cell>Miniboone</cell></row><row><cell></cell><cell>(200 / 200k, 4)</cell><cell cols="2">(800 / 50k, 10)</cell><cell cols="2">(20 / 140k, 2) (20 / 65k, 2)</cell></row><row><cell>kNN</cell><cell>51.3</cell><cell>28.4</cell><cell></cell><cell></cell><cell>84.6</cell><cell>77.7</cell></row><row><cell>Logistic Regression</cell><cell>78.9</cell><cell>54.9</cell><cell></cell><cell></cell><cell>84.9</cell><cell>75.9</cell></row><row><cell>3-Layer NN + Dropout</cell><cell>78.1</cell><cell>55.6</cell><cell></cell><cell></cell><cell>84.4</cell><cell>77.3</cell></row><row><cell>RBF Label Spreading</cell><cell>54.6</cell><cell>30.4</cell><cell></cell><cell></cell><cell>87.1</cell><cell>78.8</cell></row><row><cell>kNN Label Spreading</cell><cell>56.7</cell><cell>25.6</cell><cell></cell><cell></cell><cell>87.2</cell><cell>78.1</cell></row><row><cell>Π-model</cell><cell>80.6</cell><cell>56.6</cell><cell></cell><cell></cell><cell>87.9</cell><cell>78.3</cell></row><row><cell>FlowGMM</cell><cell>84.8</cell><cell>57.4</cell><cell></cell><cell></cell><cell>88.8</cell><cell>80.6</cell></row><row><cell>Method</cell><cell></cell><cell>MNIST</cell><cell cols="2">SVHN</cell><cell>CIFAR-10</cell></row><row><cell></cell><cell></cell><cell cols="3">(1k/59k) (1k/72k)</cell><cell>(4k/46k)</cell></row><row><cell cols="2">DIGLM Sup (All labels)</cell><cell>99.27</cell><cell cols="2">95.74</cell><cell>-</cell></row><row><cell cols="2">FlowGMM Sup (All labels)</cell><cell>99.63</cell><cell cols="2">95.81</cell><cell>88.44</cell></row><row><cell cols="2">M1+M2 VAE SSL</cell><cell>97.60</cell><cell cols="2">63.98</cell><cell>-</cell></row><row><cell>DIGLM SSL</cell><cell></cell><cell>99.0</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">FlowGMM Sup (n l labels)</cell><cell>97.36</cell><cell cols="2">78.26</cell><cell>73.13</cell></row><row><cell>FlowGMM</cell><cell></cell><cell>98.94</cell><cell cols="2">82.42</cell><cell>78.24</cell></row><row><cell cols="2">FlowGMM-cons</cell><cell>99.0</cell><cell cols="2">86.44</cell><cell>80.9</cell></row><row><cell>BadGAN</cell><cell></cell><cell>-</cell><cell cols="2">95.75</cell><cell>85.59</cell></row><row><cell>Π-Model</cell><cell></cell><cell>-</cell><cell cols="2">94.57</cell><cell>87.64</cell></row><row><cell>tive probabilities are</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>FlowGMM Sup (All labels) as well as DIGLM Sup (All labels) were trained on full train datasets with all labels to demonstrate general capacity of these models. FlowGMM Sup (n l labels) was trained on n l labeled examples (and no unlabeled data). For reference, at the bottom we list the performance of the Π-Model (Laine &amp; Aila, 2016) and BadGAN (Dai et al., 2017) as representative consistency-based and GAN-based state-of-the-art methods. Both of these methods use non-invertible architectures with substantially higher base performance and, thus, are not directly comparable. Dataset (n l / nu)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Semi-supervised classification accuracy for FlowGMM-cons and VAE M1 + M2 model on MNIST for different number of labeled data points n l .</figDesc><table><row><cell>Method</cell><cell></cell><cell>n l = 100</cell><cell cols="2">n l = 600</cell><cell>n l = 1000</cell><cell>n l = 3000</cell></row><row><cell cols="2">M1+M2 VAE SSL (n l labels)</cell><cell>96.67</cell><cell cols="3">97.41 ± 0.05 97.60 ± 0.02 97.82 ± 0.04</cell></row><row><cell cols="2">FlowGMM-cons (n l labels)</cell><cell>98.2</cell><cell>98.7</cell><cell></cell><cell>99</cell><cell>99.2</cell></row><row><cell cols="6">Table 4. Negative log-likelihood and Expected Calibration Error for supervised FlowGMM trained on MNIST (1k train, 1k validation, 10k</cell></row><row><cell cols="6">test) and CIFAR-10 (50k train, 1k validation, 9k test). FlowGMM-temp stands for tempered FlowGMM where a single scalar parameter</cell></row><row><cell cols="3">σ 2 was learned on a validation set for variances in all components.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">MNIST (test acc 97.3%)</cell><cell cols="2">CIFAR-10 (test acc 89.3%)</cell></row><row><cell></cell><cell cols="5">FlowGMM FlowGMM-temp FlowGMM FlowGMM-temp</cell></row><row><cell>NLL ↓</cell><cell>0.295</cell><cell>0.094</cell><cell></cell><cell>2.98</cell><cell>0.444</cell></row><row><cell>ECE ↓</cell><cell>0.024</cell><cell>0.004</cell><cell></cell><cell>0.108</cell><cell>0.038</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Tuned learning rates for 3-Layer NN + Dropout, Π-model and method on text and tabular tasks. For kNN we report the number of neighbours. All hyper-parameters were tuned via cross-validation.</figDesc><table><row><cell>Method Learning Rate</cell><cell cols="4">AG-News Yahoo Answers Hepmass Miniboone</cell></row><row><cell>3-Layer NN + Dropout</cell><cell>3e-4</cell><cell>3e-4</cell><cell>3e-4</cell><cell>3e-4</cell></row><row><cell>Π-model</cell><cell>1e-3</cell><cell>1e-4</cell><cell>3e-3</cell><cell>1e-4</cell></row><row><cell>FlowGMM</cell><cell>1e-4</cell><cell>1e-4</cell><cell>3e-3</cell><cell>3e-4</cell></row><row><cell>kNN</cell><cell>k = 4</cell><cell>k = 18</cell><cell>k = 9</cell><cell>k = 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Label Spreading: We use the local and global consistency method fromZhou et al. (2004), Y * = (I − αS) −1 Y where in our case Y is the matrix of labels for the labeled, unlabeled, and test data but filled with zeros for unlabeled and test. S = D −1/2 W D −1/2 computed from the affinity matrix W ij = exp (−γ sin 2</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-conditional normalizing flows for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Volokhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00505</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgKBhA5Y7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00995</idno>
		<title level="m">Invertible residual networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixmatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<title level="m">A holistic approach to semi-supervised learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HklkeR4KPB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02735</idno>
		<title level="m">Residual flows for invertible generative modeling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6510" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Density estimation using Real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Invertible convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Invertible Neural Nets and Normalizing Flows, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On calibration of modern neural networks. CoRR, abs/1706.04599</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.04599" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Invertible Neural Nets and Normalizing Flows, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative flow with invertible 1×1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational Bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09136</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">t know? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02767</idno>
		<title level="m">Hybrid models with deep and invertible features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00007</idno>
		<ptr target="https://distill.pub/2017/feature-visualization" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building invertible neural networks with masked convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mintnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11002" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
