<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Performance Large-Scale Image Recognition Without Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
						</author>
						<title level="a" type="main">High-Performance Large-Scale Image Recognition Without Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7× faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The vast majority of recent models in computer vision are variants of deep residual networks <ref type="bibr" target="#b22">(He et al., 2016b;</ref><ref type="bibr">a)</ref>, trained with batch normalization <ref type="bibr" target="#b34">(Ioffe &amp; Szegedy, 2015)</ref>. The combination of these two architectural innovations has enabled practitioners to train significantly deeper networks which can achieve higher accuracies on both the training set and the test set. Batch normalization also smoothens the loss landscape <ref type="bibr" target="#b52">(Santurkar et al., 2018)</ref>, which enables stable training with larger learning rates and at larger batch sizes <ref type="bibr" target="#b7">(Bjorck et al., 2018;</ref>, and it can have a regularizing effect <ref type="bibr" target="#b27">(Hoffer et al., 2017;</ref><ref type="bibr" target="#b42">Luo et al., 2018)</ref>. All numbers are single-model, single crop. Our NFNet-F1 model achieves comparable accuracy to an EffNet-B7 while being 8.7× faster to train. Our NFNet-F5 model has similar training latency to EffNet-B7, but achieves a state-of-the-art 86.0% top-1 accuracy on ImageNet. We further improve on this using Sharpness Aware Minimization <ref type="bibr" target="#b14">(Foret et al., 2021)</ref> to achieve 86.5% top-1 accuracy.</p><p>However, batch normalization has three significant practical disadvantages. First, it is a surprisingly expensive computational primitive, which incurs memory overhead <ref type="bibr" target="#b48">(Rota Bulò et al., 2018)</ref>, and significantly increases the time required to evaluate the gradient in some networks . Second, it introduces a discrepancy between the behaviour of the model during training and at inference time <ref type="bibr" target="#b61">(Summers &amp; Dinneen, 2019;</ref><ref type="bibr" target="#b56">Singh &amp; Shrivastava, 2019)</ref>, introducing hidden hyper-parameters that have to be tuned. Third, and most importantly, batch normalization breaks the independence between training examples in the minibatch.</p><p>This third property has a range of negative consequences. For instance, practitioners have found that batch normalized networks are often difficult to replicate precisely on different hardware, and batch normalization is often the cause of subtle implementation errors, especially during distributed training <ref type="bibr">(Pham et al., 2019)</ref>. Furthermore, batch normalization cannot be used for some tasks, since the interaction between training examples in a batch enables the network to 'cheat' certain loss functions. For example, batch normalization requires specific care to prevent information leakage in arXiv:2102.06171v1 [cs.CV] 11 Feb 2021 some contrastive learning algorithms <ref type="bibr" target="#b10">(Chen et al., 2020;</ref><ref type="bibr" target="#b23">He et al., 2020)</ref>. This is a major concern for sequence modeling tasks as well, which has driven language models to adopt alternative normalizers <ref type="bibr" target="#b1">(Ba et al., 2016;</ref><ref type="bibr" target="#b70">Vaswani et al., 2017)</ref>. The performance of batch-normalized networks can also degrade if the batch statistics have a large variance during training <ref type="bibr" target="#b54">(Shen et al., 2020)</ref>. Finally, the performance of batch normalization is sensitive to the batch size, and batch normalized networks perform poorly when the batch size is too small <ref type="bibr" target="#b27">(Hoffer et al., 2017;</ref><ref type="bibr" target="#b33">Ioffe, 2017;</ref><ref type="bibr" target="#b71">Wu &amp; He, 2018)</ref>, which limits the maximum model size we can train on finite hardware. We expand on the challenges associated with batch normalization in Appendix B.</p><p>Therefore, although batch normalization has enabled the deep learning community to make substantial gains in recent years, we anticipate that in the long term it is likely to impede progress. We believe the community should seek to identify a simple alternative which achieves competitive test accuracies and can be used for a wide range of tasks. Although a number of alternative normalizers have been proposed <ref type="bibr" target="#b1">(Ba et al., 2016;</ref><ref type="bibr" target="#b71">Wu &amp; He, 2018;</ref><ref type="bibr" target="#b32">Huang et al., 2020)</ref>, these alternatives often achieve inferior test accuracies and introduce their own disadvantages, such as additional compute costs at inference. Fortunately, in recent years two promising research themes have emerged. The first studies the origin of the benefits of batch normalization during training <ref type="bibr" target="#b4">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b52">Santurkar et al., 2018;</ref><ref type="bibr" target="#b7">Bjorck et al., 2018;</ref><ref type="bibr" target="#b42">Luo et al., 2018;</ref><ref type="bibr" target="#b74">Yang et al., 2019;</ref><ref type="bibr" target="#b35">Jacot et al., 2019;</ref>, while the second seeks to train deep ResNets to competitive accuracies without normalization layers <ref type="bibr" target="#b19">(Hanin &amp; Rolnick, 2018;</ref><ref type="bibr" target="#b79">Zhang et al., 2019a;</ref><ref type="bibr" target="#b53">Shao et al., 2020;</ref><ref type="bibr">Brock et al., 2021)</ref>.</p><p>A key theme in many of these works is that it is possible to train very deep ResNets without normalization by suppressing the scale of the hidden activations on the residual branch. The simplest way to achieve this is to introduce a learnable scalar at the end of each residual branch, initialized to zero <ref type="bibr" target="#b17">(Goyal et al., 2017;</ref><ref type="bibr" target="#b79">Zhang et al., 2019a;</ref><ref type="bibr" target="#b3">Bachlechner et al., 2020)</ref>. However this trick alone is not sufficient to obtain competitive test accuracies on challenging benchmarks. Another line of work has shown that ReLU activations introduce a 'mean shift', which causes the hidden activations of different training examples to become increasingly correlated as the network depth increases <ref type="bibr" target="#b31">(Huang et al., 2017;</ref><ref type="bibr" target="#b35">Jacot et al., 2019)</ref>. In a recent work, <ref type="bibr">Brock et al. (2021)</ref> introduced "Normalizer-Free" ResNets, which suppress the residual branch at initialization and apply Scaled Weight Standardization <ref type="bibr">(Qiao et al., 2019)</ref> to remove the mean shift. With additional regularization, these unnormalized networks match the performance of batch-normalized ResNets <ref type="bibr" target="#b21">(He et al., 2016a)</ref> on ImageNet <ref type="bibr" target="#b49">(Russakovsky et al., 2015)</ref>, but they are not stable at large batch sizes and do not match the performance of EfficientNets <ref type="bibr" target="#b66">(Tan &amp; Le, 2019)</ref>, the current state of the art <ref type="bibr" target="#b16">(Gong et al., 2020)</ref>. This paper builds on this line of work and seeks to address these central limitations. Our main contributions are as follows:</p><p>• We propose Adaptive Gradient Clipping (AGC), which clips gradients based on the unit-wise ratio of gradient norms to parameter norms, and we demonstrate that AGC allows us to train Normalizer-Free Networks with larger batch sizes and stronger data augmentations.</p><p>• We design a family of Normalizer-Free ResNets, called NFNets, which set new state-of-the-art validation accuracies on ImageNet for a range of training latencies (See <ref type="figure" target="#fig_0">Figure 1</ref>). Our NFNet-F1 model achieves similar accuracy to EfficientNet-B7 while being 8.7× faster to train, and our largest model sets a new overall state of the art without extra data of 86.5% top-1 accuracy.</p><p>• We show that NFNets achieve substantially higher validation accuracies than batch-normalized networks when fine-tuning on ImageNet after pre-training on a large private dataset of 300 million labelled images. Our best model achieves 89.2% top-1 after fine-tuning.</p><p>The paper is structured as follows. We discuss the benefits of batch normalization in Section 2, and recent work seeking to train ResNets without normalization in Section 3. We introduce AGC in Section 4, and we describe how we developed our new state-of-the-art architectures in Section 5. Finally, we present our experimental results in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Understanding Batch Normalization</head><p>In order to train networks without normalization to competitive accuracy, we must understand the benefits batch normalization brings during training, and identify alternative strategies to recover these benefits. Here we list the four main benefits which have been identified by prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch normalization downscales the residual branch:</head><p>The combination of skip connections <ref type="bibr" target="#b60">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b22">He et al., 2016b;</ref><ref type="bibr">a)</ref> and batch normalization <ref type="bibr" target="#b34">(Ioffe &amp; Szegedy, 2015)</ref> enables us to train significantly deeper networks with thousands of layers <ref type="bibr" target="#b79">(Zhang et al., 2019a)</ref>. This benefit arises because batch normalization, when placed on the residual branch (as is typical), reduces the scale of hidden activations on the residual branches at initialization . This biases the signal towards the skip path, which ensures that the network has well-behaved gradients early in training, enabling efficient optimization <ref type="bibr" target="#b4">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b19">Hanin &amp; Rolnick, 2018;</ref><ref type="bibr" target="#b74">Yang et al., 2019)</ref>.</p><p>Batch normalization eliminates mean-shift: Activation functions like ReLUs or GELUs <ref type="bibr" target="#b25">(Hendrycks &amp; Gimpel, 2016)</ref>, which are not anti-symmetric, have non-zero mean activations. Consequently, the inner product between the activations of independent training examples immediately after the non-linearity is typically large and positive, even if the inner product between the input features is close to zero. This issue compounds as the network depth increases, and introduces a 'mean-shift' in the activations of different training examples on any single channel proportional to the network depth , which can cause deep networks to predict the same label for all training examples at initialization <ref type="bibr" target="#b35">(Jacot et al., 2019)</ref>. Batch normalization ensures the mean activation on each channel is zero across the current batch, eliminating mean shift <ref type="bibr">(Brock et al., 2021)</ref>.</p><p>Batch normalization has a regularizing effect: It is widely believed that batch normalization also acts as a regularizer enhancing test set accuracy, due to the noise in the batch statistics which are computed on a subset of the training data <ref type="bibr" target="#b42">(Luo et al., 2018)</ref>. Consistent with this perspective, the test accuracy of batch-normalized networks can often be improved by tuning the batch size, or by using ghost batch normalization in distributed training <ref type="bibr" target="#b27">(Hoffer et al., 2017)</ref>.</p><p>Batch normalization allows efficient large-batch training: Batch normalization smoothens the loss landscape <ref type="bibr" target="#b52">(Santurkar et al., 2018)</ref>, and this increases the largest stable learning rate <ref type="bibr" target="#b7">(Bjorck et al., 2018)</ref>. While this property does not have practical benefits when the batch size is small , the ability to train at larger learning rates is essential if one wishes to train efficiently with large batch sizes. Although large-batch training does not achieve higher test accuracies within a fixed epoch budget , it does achieve a given test accuracy in fewer parameter updates, significantly improving training speed when parallelized across multiple devices <ref type="bibr" target="#b17">(Goyal et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Towards Removing Batch Normalization</head><p>Many authors have attempted to train deep ResNets to competitive accuracies without normalization, by recovering one or more of the benefits of batch normalization described above. Most of these works suppress the scale of the activations on the residual branch at initialization, by introducing either small constants or learnable scalars <ref type="bibr" target="#b19">(Hanin &amp; Rolnick, 2018;</ref><ref type="bibr" target="#b79">Zhang et al., 2019a;</ref><ref type="bibr" target="#b53">Shao et al., 2020)</ref>. Additionally, <ref type="bibr" target="#b79">Zhang et al. (2019a)</ref> and  observed that the performance of unnormalized ResNets can be improved with additional regularization. However only recovering these two benefits of batch normalization is not sufficient to achieve competitive test accuracies on challenging benchmarks .</p><p>In this work, we adopt and build on "Normalizer-Free ResNets" (NF-ResNets) <ref type="bibr">(Brock et al., 2021)</ref>, a class of preactivation ResNets <ref type="bibr" target="#b21">(He et al., 2016a)</ref> which can be trained to competitive training and test accuracies without normalization layers. NF-ResNets employ a residual block of the form</p><formula xml:id="formula_0">h i+1 = h i + αf i (h i /β i ),</formula><p>where h i denotes the inputs to the i th residual block, and f i denotes the function computed by the i th residual branch. The function f i is parameterized to be variance preserving at initialization, such that Var(f i (z)) = Var(z) for all i. The scalar α specifies the rate at which the variance of the activations increases after each residual block (at initialization), and is typically set to a small value like α = 0.2. The scalar β i is determined by predicting the standard deviation of the inputs to the i th residual block, β i = Var(h i ), where Var(h i+1 ) = Var(h i ) + α 2 , except for transition blocks (where spatial downsampling occurs), for which the skip path operates on the downscaled input (h i /β i ), and the expected variance is reset after the transition block to h i+1 = 1 + α 2 . The outputs of squeezeexcite layers <ref type="bibr" target="#b29">(Hu et al., 2018)</ref> are multiplied by a factor of 2. Empirically, <ref type="bibr">Brock et al. (2021)</ref> found it was also beneficial to include a learnable scalar initialized to zero at the end of each residual branch ('SkipInit' ).</p><p>In addition, <ref type="bibr">Brock et al. (2021)</ref> prevent the emergence of a mean-shift in the hidden activations by introducing Scaled Weight Standardization (a minor modification of Weight Standardization <ref type="bibr" target="#b31">(Huang et al., 2017;</ref><ref type="bibr">Qiao et al., 2019)</ref>). This technique reparameterizes the convolutional layers as:</p><formula xml:id="formula_1">W ij = W ij − µ i √ N σ i ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">µ i = (1/N ) j W ij , σ 2 i = (1/N ) j (W ij − µ i ) 2</formula><p>, and N denotes the fan-in. The activation functions are also scaled by a non-linearity specific scalar gain γ, which ensures that the combination of the γ-scaled activation function and a Scaled Weight Standardized layer is variance preserving. For ReLUs, γ = 2/(1 − (1/π)) <ref type="bibr" target="#b0">(Arpit et al., 2016)</ref>. We refer the reader to <ref type="bibr">Brock et al. (2021)</ref> for a description of how to compute γ for other non-linearities.</p><p>With additional regularization (Dropout <ref type="bibr" target="#b59">(Srivastava et al., 2014)</ref> and Stochastic Depth <ref type="bibr" target="#b30">(Huang et al., 2016)</ref>), Normalizer-Free ResNets match the test accuracies achieved by batch normalized pre-activation ResNets on ImageNet at batch size 1024. They also significantly outperform their batch normalized counterparts when the batch size is very small, but they perform worse than batch normalized networks for large batch sizes (4096 or higher). Crucially, they do not match the performance of state-of-the-art networks like EfficientNets <ref type="bibr" target="#b66">(Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b16">Gong et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adaptive Gradient Clipping for Efficient Large-Batch Training</head><p>To scale NF-ResNets to larger batch sizes, we explore a range of gradient clipping strategies <ref type="bibr">(Pascanu et al., 2013)</ref>. Gradient clipping is often used in language modeling to stabilize training <ref type="bibr">(Merity et al., 2018)</ref>, and recent work shows that it allows training with larger learning rates compared to gradient descent, accelerating convergence <ref type="bibr" target="#b81">(Zhang et al., 2020)</ref>. This is particularly important for poorly conditioned loss landscapes or when training with large batch sizes, since in these settings the optimal learning rate is constrained by the maximum stable learning rate . We therefore hypothesize that gradient clipping should help scale NF-ResNets efficiently to the large-batch setting.</p><p>Gradient clipping is typically performed by constraining the norm of the gradient <ref type="bibr">(Pascanu et al., 2013)</ref>. Specifically, for gradient vector G = ∂L/∂θ, where L denotes the loss and θ denotes a vector with all model parameters, the standard clipping algorithm clips the gradient before updating θ as:</p><formula xml:id="formula_3">G → λ G G if G &gt; λ, G otherwise.<label>(2)</label></formula><p>The clipping threshold λ is a hyper-parameter which must be tuned. Empirically, we found that while this clipping algorithm enabled us to train at higher batch sizes than before, training stability was extremely sensitive to the choice of the clipping threshold, requiring fine-grained tuning when varying the model depth, the batch size, or the learning rate.</p><p>To overcome this issue, we introduce "Adaptive Gradient Clipping" (AGC), which we now describe. Let W ∈ R N ×M denote the weight matrix of the th layer, G ∈ R N ×M denote the gradient with respect to W , and · F denote the Frobenius norm, i.e.,</p><formula xml:id="formula_4">W F = N i M j (W i,j ) 2 .</formula><p>The AGC algorithm is motivated by the observation that the ratio of the norm of the gradients G to the norm of the weights W of layer , G F W F , provides a simple measure of how much a single gradient descent step will change the original weights W . For instance, if we train using gradient descent without momentum, then</p><formula xml:id="formula_5">∆W W = h G F W F ,</formula><p>where the parameter update for the th layer is given by ∆W = −hG , and h is the learning rate.</p><p>Intuitively, we expect training to become unstable if ( ∆W / W ) is large, which motivates a clipping strategy based on the ratio G F W F . However in practice, we clip gradients based on the unit-wise ratios of gradient norms to parameter norms, which we found to perform better empirically than taking layer-wise norm ratios. Specifically, in our AGC algorithm, each unit i of the gradient of the -th layer G i (defined as the i th row of matrix G ) is clipped as:</p><formula xml:id="formula_6">G i → λ W i F G i F G i if G i F W i F &gt; λ, G i otherwise.<label>(3)</label></formula><p>The clipping threshold λ is a scalar hyperparameter, and we define W i F = max( W i F , ), with default = 10 −3 , which prevents zero-initialized parameters from always having their gradients clipped to zero. For parameters in convolutional filters, we evaluate the unit-wise norms over the fan-in extent (including the channel and spatial dimensions).</p><p>Using AGC, we can train NF-ResNets stably with larger batch sizes (up to 4096), as well as with very strong data augmentations like RandAugment <ref type="bibr" target="#b11">(Cubuk et al., 2020)</ref> for which NF-ResNets without AGC fail to train <ref type="bibr">(Brock et al., 2021)</ref>. Note that the optimal clipping parameter λ may depend on the choice of optimizer, learning rate and batch size.</p><p>Empirically, we find λ should be smaller for larger batches.</p><p>AGC is closely related to a recent line of work studying "normalized optimizers" <ref type="bibr" target="#b75">(You et al., 2017;</ref><ref type="bibr" target="#b6">Bernstein et al., 2020;</ref><ref type="bibr" target="#b76">You et al., 2019)</ref>, which ignore the scale of the gradient by choosing an adaptive learning rate inversely proportional to the gradient norm. In particular, <ref type="bibr" target="#b75">You et al. (2017)</ref> propose LARS, a momentum variant which sets the norm of the parameter update to be a fixed ratio of the parameter norm, completely ignoring the gradient magnitude. AGC can be interpreted as a relaxation of normalized optimizers, which imposes a maximum update size based on the parameter norm but does not simultaneously impose a lower-bound on the update size or ignore the gradient magnitude. Although we are also able to stably train at high batch sizes with LARS, we found that doing so degrades performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablations for Adaptive Gradient Clipping (AGC)</head><p>We now present a range of ablations designed to test the efficacy of AGC. We performed experiments on pre-activation NF-ResNet-50 and NF-ResNet-200 on ImageNet, trained using SGD with Nesterov's Momentum for 90 epochs at a range of batch sizes between 256 and 4096. As in <ref type="bibr" target="#b17">Goyal et al. (2017)</ref> we use a base learning rate of 0.1 for batch size 256, which is scaled linearly with the batch size. We consider a range of λ values [0.01, 0.02, 0.04, 0.08, 0.16].</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>(a), we compare batch-normalized ResNets to NF-ResNets with and without AGC. We show test accuracy at the best clipping threshold λ for each batch size. We find that AGC helps scale NF-ResNets to large batch sizes while maintaining performance comparable or better than batchnormalized networks on both ResNet50 and ResNet200. As anticipated, the benefits of using AGC are smaller when the batch size is small. In <ref type="figure" target="#fig_1">Figure 2</ref>(b), we show performance  <ref type="figure">Figure 3</ref>. Summary of NFNet bottleneck block design and architectural differences. See <ref type="figure" target="#fig_2">Figure 5</ref> in Appendix C for more details.</p><p>for different clipping thresholds λ across a range of batch sizes on ResNet50. We see that smaller (stronger) clipping thresholds are necessary for stability at higher batch sizes. We provide additional ablation details in Appendix D.</p><p>Next, we study whether or not AGC is beneficial for all layers. Using batch size 4096 and a clipping threshold λ = 0.01, we remove AGC from different combinations of the first convolution, the final linear layer, and every block in any given set of the residual stages. For example, one experiment may remove clipping in the linear layer and all the blocks in the second and fourth stages. Two key trends emerge: first, it is always better to not clip the final linear layer. Second, it is often possible to train stably without clipping the initial convolution, but the weights of all four stages must be clipped to achieve stability when training at batch size 4096 with the default learning rate of 1.6. For the rest of this paper (and for our ablations in <ref type="figure" target="#fig_1">Figure 2</ref>), we apply AGC to every layer except for the final linear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Normalizer-Free Architectures with Improved Accuracy and Training Speed</head><p>In the previous section we introduced AGC, a gradient clipping method which allows us to train efficiently with large batch sizes and strong data augmentations. Equipped with this technique, we now seek to design Normalizer-Free architectures with state-of-the-art accuracy and training speed.</p><p>The current state of the art on image classification is generally held by the EfficientNet family of models <ref type="bibr" target="#b66">(Tan &amp; Le, 2019)</ref>, which are based on a variant of inverted bottleneck blocks <ref type="bibr" target="#b50">(Sandler et al., 2018)</ref> with a backbone and model scaling strategy derived from neural architecture search. These models are optimized to maximize test accuracy while minimizing parameter and FLOP counts, but their low theoretical compute complexity does not translate into improved training speed on modern accelerators. Despite having 10x fewer FLOPS than a ResNet-50, an EffNet-B0 has similar training latency and final performance when trained on GPU or TPU.</p><p>The choice of which metric to optimize-theoretical FLOPS, inference latency on a target device, or training latency on an accelerator-is a matter of preference, and the nature of each metric will yield different design requirements. In this work we choose to focus on manually designing models which are optimized for training latency on existing accelerators, as in <ref type="bibr" target="#b44">Radosavovic et al. (2020)</ref>. It is possible that future accelerators may be able to take full advantage of the potential training speed that largely goes unrealized with models like EfficientNets, so we believe this direction should not be ignored <ref type="bibr" target="#b28">(Hooker, 2020)</ref>, however we anticipate that developing models with improved training speed on current hardware will be beneficial for accelerating research. We note that accelerators like GPU and TPU tend to favor dense computation, and while there are differences between these two platforms, they have enough in common that models designed for one device are likely to train fast on the other.</p><p>We therefore explore the space of model design by manually searching for design trends which yield improvements to the pareto front of holdout top-1 on ImageNet against actual training latency on device. This section describes the changes which we found to work well to this end (with more details in Appendix C), while the ideas which we found to work poorly are described in Appendix E. A summary of these modifications is presented in <ref type="figure">Figure 3</ref>, and the effect they have on holdout accuracy is presented in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We begin with an SE-ResNeXt-D model <ref type="bibr" target="#b73">(Xie et al., 2017;</ref><ref type="bibr" target="#b29">Hu et al., 2018;</ref><ref type="bibr" target="#b24">He et al., 2019)</ref> with GELU activations <ref type="bibr" target="#b25">(Hendrycks &amp; Gimpel, 2016)</ref>, which we found to be a surprisingly strong baseline for Normalizer-Free Networks. We make the following changes. First, we set the group width (the number of channels each output unit is connected to) in the 3 × 3 convs to 128, regardless of block width. Smaller group widths reduce theoretical FLOPS, but the reduction in compute density means that on many modern accelerators no actual speedup is realized. On TPUv3 for example, an SE-ResNeXt-50 with a group width of 8 trains at the same speed as an SE-ResNeXt-50 with a group width of 128 unless the per-device batch size is 128 or larger (Google, 2021), which is often not realizable due to memory constraints.</p><p>Next, we make two changes to the model backbone. First, we note that the default depth scaling pattern for ResNets (e.g., the method by which one increases depth to construct a ResNet101 or ResNet200 from a ResNet50) involves nonuniformly increasing the number of layers in the second and third stages, while maintaining 3 blocks in the first and fourth stages, where 'stage' refers to a sequence of residual blocks whose activations are the same width and have the same resolution. We find that this strategy is suboptimal. Layers in early stages operate at higher resolution, require more memory and compute, and tend to learn localized, taskgeneral features <ref type="bibr" target="#b38">(Krizhevsky et al., 2012)</ref>, while layers in later stages operate at lower resolutions, contain most of the model's parameters, and learn more task-specific features <ref type="bibr" target="#b45">(Raghu et al., 2017a)</ref>. However, being overly parsimonious with early stages (such as through aggressive downsampling) can hurt performance, since the model needs enough capacity to extract good local features <ref type="bibr" target="#b46">(Raghu et al., 2017b)</ref>. It is also desirable to have a simple scaling rule for constructing deeper variants <ref type="bibr" target="#b66">(Tan &amp; Le, 2019)</ref>. With these principles in mind, we explored several choices of backbone for our smallest model variant, named F0, before settling on the simple pattern [1, 2, 6, 3] (indicating how many bottleneck blocks to allocate to each stage). We construct deeper variants by multiplying the depth of each stage by a scalar N , so that, for example, variant F1 has a depth pattern <ref type="bibr">[2,</ref><ref type="bibr">4,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>, and variant F4 has a depth pattern <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">30,</ref><ref type="bibr">15]</ref>.</p><p>In addition, we reconsider the default width pattern in ResNets, where the first stage has 256 channels which are doubled at each subsequent stage, resulting in a pattern <ref type="bibr">[256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref>. Employing our depth patterns described above, we considered a range of alternative patterns (taking inspiration from <ref type="bibr" target="#b44">Radosavovic et al. (2020)</ref>) but found that only one choice was better than this default: <ref type="bibr">[256,</ref><ref type="bibr">512,</ref><ref type="bibr">1536,</ref><ref type="bibr">1536]</ref>. This width pattern is designed to increase capacity in the third stage while slightly reducing capacity in the fourth stage, roughly preserving training speed. Consistent with our chosen depth pattern and the default design of ResNets, we find that the third stage tends to be the best place to add capacity, which we hypothesize is due to this stage being deep enough to have a large receptive field and access to deeper levels of the feature hierarchy, while having a slightly higher resolution than the final stage.</p><p>We also consider the structure of the bottleneck residual block itself. We considered a variety of pre-existing and novel modifications (see Appendix E) but found that the best improvement came from adding an additional 3 × 3 grouped conv after the first (with accompanying nonlinearity). This additional convolution minimally impacts FLOPS and has almost no impact on training time on our target accelerators.</p><p>Finally, we establish a scaling strategy to produce model variants at different compute budgets. The EfficientNet scaling strategy <ref type="bibr" target="#b66">(Tan &amp; Le, 2019)</ref> is to jointly scale model width, depth, and input resolution, which works extremely well for base models with very slim MobileNet-like backbones. However we find that width scaling is ineffective for ResNet backbones, consistent with <ref type="bibr" target="#b5">Bello (2021)</ref>, who attain strong performance when only scaling depth and input resolution. We therefore also adopt the latter strategy, using the fixed width pattern mentioned above, scaling depth as described above, and scaling training resolution such that each variant is approximately half as fast to train as its predecessor. Following <ref type="bibr" target="#b68">Touvron et al. (2019)</ref>, we evaluate images at inference at a slightly higher resolution than we train at, chosen for each variant as approximately 33% larger than the train resolution. We do not fine-tune at this higher resolution.</p><p>We also find that it is helpful to increase the regularization strength as the model capacity rises. However modifying the weight decay or stochastic depth rate was not effective, and instead we scale the drop rate of Dropout <ref type="bibr" target="#b59">(Srivastava et al., 2014)</ref>, following <ref type="bibr" target="#b66">Tan &amp; Le (2019)</ref>. This step is particularly important as our models lack the implicit regularization of batch normalization, and without explicit regularization tend to dramatically overfit. Our resulting models are highly performant and, despite being optimized for training latency, remain competitive with larger EfficientNet variants in terms of FLOPs vs accuracy (although not in terms of parameters vs accuracy), as shown in <ref type="figure">Figure 4</ref> in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Summary</head><p>Our training recipe can be summarized as follows: First, apply the Normalizer-Free setup of <ref type="bibr">Brock et al. (2021)</ref> to an SE-ResNeXt-D, with modified width and depth patterns, and a second spatial convolution. Second, apply AGC to every parameter except for the linear weight of the classifier layer. For batch size 1024 to 4096, set λ = 0.01, and make use of strong regularization and data augmentation. See <ref type="table" target="#tab_1">Table 1</ref> for additional information on each model variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluating NFNets on ImageNet</head><p>We now turn our attention to evaluating our NFNet models on ImageNet, beginning with an ablation of our architectural modifications when training for 360 epochs at batch size 4096. We use Nesterov's Momentum with a momentum coefficient of 0.9, AGC as described in Section 4 with a <ref type="table" target="#tab_4">Table 3</ref>. ImageNet Accuracy comparison for NFNets and a representative set of models, including SENet <ref type="bibr" target="#b29">(Hu et al., 2018)</ref>, LambdaNet, <ref type="bibr" target="#b5">(Bello, 2021)</ref>, BoTNet <ref type="bibr" target="#b58">(Srinivas et al., 2021)</ref>, and DeIT <ref type="bibr" target="#b69">(Touvron et al., 2020)</ref>. Except for results using SAM, our results are averaged over three random seeds. Latencies are given as the time in milliseconds required to perform a single full training step on TPU or GPU (V100). clipping threshold of 0.01, and a learning rate which linearly increases from 0 to 1.6 over 5 epochs, before decaying to zero with cosine annealing <ref type="bibr" target="#b41">(Loshchilov &amp; Hutter, 2017)</ref>. From the first three rows of <ref type="table" target="#tab_2">Table 2</ref>, we can see that the two changes we make to the model each result in slight improvements to performance with only minor changes in training latency (See <ref type="table" target="#tab_9">Table 6</ref> in the Appendix for latencies).</p><p>Next, we evaluate the effects of progressively adding stronger augmentations, combining MixUp <ref type="bibr" target="#b78">(Zhang et al., 2017)</ref>, RandAugment (RA, <ref type="bibr" target="#b11">(Cubuk et al., 2020)</ref>) and Cut-Mix <ref type="bibr" target="#b77">(Yun et al., 2019)</ref>. We apply RA with 4 layers and scale the magnitude with the resolution of the images, following <ref type="bibr" target="#b11">Cubuk et al. (2020)</ref>. We find that this scaling is particularly important, as if the magnitude is set too high relative to the image size (for example, using a magnitude of 20 on images of resolution 224) then most of the augmented images will be completely blank. See Appendix A for a complete description of these magnitudes and how they are selected. We show in <ref type="table" target="#tab_2">Table 2</ref> that these data augmentations substantially improve performance. Finally, in the last row of <ref type="table" target="#tab_2">Table 2</ref>, we additionally present the performance of our full model ablated to use the default ResNet stage widths, demonstrating that our slightly modified pattern in the third and fourth stages does yield improvements under direct comparison.</p><p>For completeness, in <ref type="table" target="#tab_9">Table 6</ref> of the Appendix we also report the performance of our model architectures when trained with batch normalization instead of the NF strategy. These models achieve slightly lower test accuracies than their NF counterparts and they are between 20% and 40% slower to train, even when using highly optimized batch normalization implementations without cross-replica syncing. Furthermore, we found that the larger model variants F4 and F5</p><p>were not stable when training with batch normalization, with or without AGC. We attribute this to the necessity of using bfloat16 training to fit these larger models in memory, which may introduce numerical imprecision that interacts poorly with the computation of batch normalization statistics.</p><p>We provide a detailed summary of the size, training latency (on TPUv3 and V100 with tensorcores), and ImageNet validation accuracy of six model variants, NFNet-F0 through F5, along with comparisons to other models with similar training latencies, in Our models also benefit from the recently proposed Sharpness-Aware Minimization (SAM, <ref type="bibr" target="#b14">(Foret et al., 2021)</ref>). SAM is not part of our standard training pipeline, as by default it doubles the training time and typically can only be used for distributed training. However we make a small modification to the SAM procedure to reduce this cost to 20-40% increased training time (explained in Appendix A) and employ it to train our two largest model variants, resulting in an NFNet-F5 that attains 86.3% top-1, and an NFNet-F6 that attains 86.5% top-1, substantially improving over the existing state of the art on ImageNet without extra data.</p><p>Finally, we also evaluated the performance of our data augmentation strategy on EfficientNets. We find that while RA strongly improves EfficientNets' performance over baseline augmentation, increasing the number of layers beyond 2 or adding MixUp and CutMix does not further improve their performance, suggesting that our performance improvements are difficult to obtain by simply using stronger data augmentations. We also find that using SGD with cosine annealing instead of RMSProp <ref type="bibr" target="#b67">(Tieleman &amp; Hinton, 2012)</ref> with step decay severely degrades EfficientNet performance, indicating that our performance improvements are also not simply due to the selection of a different optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluating NFNets under Transfer</head><p>Unnormalized networks do not share the implicit regularization effect of batch normalization, and on datasets like ImageNet <ref type="bibr" target="#b49">(Russakovsky et al., 2015)</ref> they tend to overfit unless explicitly regularized <ref type="bibr" target="#b79">(Zhang et al., 2019a;</ref><ref type="bibr">Brock et al., 2021)</ref>. However when pre-training on extremely large scale datasets, such regularization may not only be unnecessary, but also harmful to performance, reducing the model's ability to devote its full capacity to the training set. We hypothesize that this may make Normalizer-Free networks naturally better suited to transfer learning after large-scale pre-training, and investigate this via pre- training on a large dataset of 300 million labeled images.</p><p>We pre-train a range of batch normalized and NF-ResNets for 10 epochs on this large dataset, then fine-tune all layers on ImageNet simultaneously, using a batch size of 2048 and a small learning rate of 0.1 with cosine annealing for 15,000 steps, for input image resolutions in the range <ref type="bibr">[224,</ref><ref type="bibr">320,</ref><ref type="bibr">384]</ref>. As shown in <ref type="table" target="#tab_5">Table 4</ref>, Normalizer-Free networks outperform their Batch-Normalized counterparts in every single case, typically by a margin of around 1% absolute top-1. This suggests that in the transfer learning regime, removing batch normalization can directly benefit final performance.</p><p>We perform this same experiment using our NFNet models, pre-training an NFNet-F4 and a slightly wider variant which we denote NFNet-F4+ (see Appendix C). As shown in <ref type="table" target="#tab_7">Table 5</ref> of the appendix, with 20 epochs of pre-training our NFNet-F4+ attains an ImageNet top-1 accuracy of 89.2%. This is the second highest validation accuracy achieved to date with extra training data, second only to a strong recent semi-supervised learning baseline <ref type="bibr">(Pham et al., 2020)</ref>, and the highest accuracy achieved using transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We show for the first time that image recognition models, trained without normalization layers, can not only match the classification accuracies of the best batch normalized models on large-scale datasets but also substantially exceed them, while still being faster to train. To achieve this, we introduce Adaptive Gradient Clipping, a simple clipping algorithm which stabilizes large-batch training and enables us to optimize unnormalized networks with strong data augmentations. Leveraging this technique and simple architecture design principles, we develop a family of models which attain state-of-the-art performance on ImageNet without extra data, while being substantially faster to train than competing approaches. We also show that Normalizer-Free models are better suited to fine-tuning after pre-training on very large scale datasets than their batch-normalized counterparts. For ImageNet experiments <ref type="bibr" target="#b49">(Russakovsky et al., 2015)</ref>, we train on the standard ILSVRC2012 training split, which comprises 1281167 images from 1000 classes. Our baseline training preprocessing follows <ref type="bibr" target="#b65">Szegedy et al. (2016b)</ref>, with distorted bounding box crops and random horizontal flips <ref type="bibr" target="#b55">(Simonyan &amp; Zisserman, 2015)</ref>, with all other augmentations being applied in addition to this. We train using the categorical softmax cross-entropy loss with label smoothing of 0.1 <ref type="bibr" target="#b65">(Szegedy et al., 2016b)</ref>, and optimize our networks using stochastic gradient descent <ref type="bibr" target="#b47">(Robbins &amp; Monro, 1951)</ref> with Nesterov's momentum <ref type="bibr">(Nesterov, 1983;</ref><ref type="bibr" target="#b63">Sutskever et al., 2013)</ref>, using a momentum coefficient of 0.9. Our training code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets, and is written using numpy <ref type="bibr" target="#b20">(Harris et al., 2020)</ref>, JAX <ref type="bibr" target="#b8">(Bradbury et al., 2018)</ref>, Haiku , and the DeepMind JAX Ecosystem .</p><p>We employ weight decay in the standard style (not decoupled as in <ref type="bibr" target="#b41">Loshchilov &amp; Hutter (2017)</ref>), with a weight decay coefficient of 2 × 10 −5 for NFNets. Critically, weight decay is not applied to the affine gains or biases in the weightstandardized convolutional layers, or to the SkipInit gains. We apply a Dropout rate specific to each NFNet variant as in <ref type="bibr" target="#b66">Tan &amp; Le (2019)</ref>, and use Stochastic Depth with a rate of 0.25 for all variants, again similar to <ref type="bibr" target="#b66">Tan &amp; Le (2019)</ref>.</p><p>We use a learning rate which warms up from 0 to its maximal value over the first 5 epochs, where the maximal value is chosen as 0.1 × B/256, with B the batch size, following <ref type="bibr" target="#b17">Goyal et al. (2017)</ref>. After warmup, the learning rate is annealed to zero with cosine decay over the rest of training <ref type="bibr" target="#b40">(Loshchilov &amp; Hutter, 2016)</ref>. We employ AGC with λ = 0.01 and = 10 −3 for every parameter except the fullyconnected weight of the linear classifier layer.</p><p>By default, we train with a batch size of 4096 for 360 epochs, a common training schedule which has the same number of total training steps (roughly 112,000) as training with a batch size of 1024 for 90 epochs. We found that training for longer sometimes improved results, but that this was not always consistent across models or training settings; all results reported in this work employ the 360 epoch schedule.</p><p>Unlike <ref type="bibr" target="#b66">Tan &amp; Le (2019)</ref> we do not perform early stopping.</p><p>We employ an exponential moving average of the model parameters (similar to Polyak averaging (Polyak, 1964)), with a decay rate of 0.99999 which, following <ref type="bibr" target="#b66">Tan &amp; Le (2019)</ref>, follows a warmup schedule where the decay is equal to min(0.99999, 1+t 10+t ). We train on TPU using bfloat16 activations to save memory and improve speed. This means that we keep the parameters and optimizer state (the momentum buffer) in float32, but compute activations and gradients in bfloat16 during forward-and backpropagation. We cast the logits to float32 before computing the loss to aid numerical stability. We cast gradients back to float32 before summing them across devices, which helps prevent compounding accumulation error and ensures the parameter update is computed in float32.</p><p>For evaluation we follow the most common style of singlecrop preprocessing: we resize the raw image (with bicubic interpolation) to be 32 pixels larger than the target resolution, then crop to the target resolution <ref type="bibr" target="#b55">(Simonyan &amp; Zisserman, 2015)</ref>. While this is the most commonly employed variant, we note that an alternative method exists where a padded center crop is taken and then resized to the target resolution <ref type="bibr" target="#b64">(Szegedy et al., 2016a;</ref><ref type="bibr" target="#b66">Tan &amp; Le, 2019)</ref>. We find this alternative to work marginally worse than the standard choice of resizing before cropping. No test time augmentation, multi-crop evaluation, or model ensembling is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Measuring Training Latency</head><p>We measure training latency as the actual observed wallclock time required to perform a training step at a given per-device batch size. To accomplish this, we run the full training loop for 5000 steps, then take the median time required to perform a single training step. We choose the median as the mean would also incorporate the initial speed ramp-up at the beginning of training, so the median is more  <ref type="bibr" target="#b13">Dosovitskiy et al. (2021)</ref>, BiT results are from <ref type="bibr" target="#b37">Kolesnikov et al. (2019)</ref>. Noisy Student results  are taken from the improved versions reported in <ref type="bibr" target="#b14">Foret et al. (2021)</ref> which employ SAM. IG-940M <ref type="bibr" target="#b43">(Mahajan et al., 2018)</ref> results are taken from the improved versions reported in <ref type="bibr" target="#b68">Touvron et al. (2019)</ref> For measuring speed on TPUv3, we run on 32 devices with a batch size of 32 per device, and sync gradients between replicas, meaning that our training latency is representative of the actual speed we can obtain in practice with distributed training. We employ bfloat16 training for all models, as described above. For some of our larger models, this batch size of 32 per device does not fit into the 16GB of device memory, so we allow the compiler to engage automatic rematerialization (also known as gradient checkpointing). Additional speed may be obtainable by careful tuning of manual rematerialization.</p><p>For measuring speed on GPU, we run on a single V100 GPU using float16 training to engage the card's tensorcores, which strongly accelerates training. Unlike TPUv3, we do not consider the cost of cross-device communication for GPU, which will vary substantially depending on the hardware configuration of the interlinks available to the user. As with TPUv3, some of our models do not fit in memory at this batch size, but we instead employ gradient accumulation to mimic the full batch size. This appears to be less efficient than rematerialization for large models (specifically for our F5 variant and for EfficientNet-B7), so we expect that manually applying rematerialization would potentially yield GPU speedups in this case, but require extra engineering effort.</p><p>We report results from our own measurements for all models except for SENets <ref type="bibr" target="#b29">(Hu et al., 2018)</ref>, BoTNets <ref type="bibr" target="#b58">(Srinivas et al., 2021)</ref>, and DeIT <ref type="bibr" target="#b69">(Touvron et al., 2020)</ref>, which we instead borrow from <ref type="bibr" target="#b58">Srinivas et al. (2021)</ref>. We report slightly differ-ent training latencies for small EfficientNet variants because we report the wallclock time, whereas <ref type="bibr" target="#b58">Srinivas et al. (2021)</ref> report the "compute time" which will ignore cross-device communication. For very small models the inter-device communication costs can be non-negligible relative to the compute time, especially for EfficientNets which employ cross-replica batch normalization. For larger models this cost is generally negligible on hardware like TPUv3 with very fast interconnects, so in practice one can expect that the compute time for models like BoTNets will be the same regardless of the reporting methodology used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Augmentations</head><p>Our full NFNet training recipe applies "baseline" preprocessing (sampling distorted bounding boxes and applying random horizontal flips), RandAugment (RA, <ref type="bibr" target="#b11">Cubuk et al. (2020)</ref>), which we apply to all images in a batch, MixUp <ref type="bibr" target="#b78">(Zhang et al., 2017)</ref>, which we apply to half the images in a batch with α = 0.2, and CutMix <ref type="bibr" target="#b77">(Yun et al., 2019)</ref>, which we apply to the other half of the images in the batch.</p><p>Following Qin et al. (2020) we apply RandAugment after applying MixUp or CutMix. We apply RA with 4 layers (meaning 4 augmentations are chosen), which is substantially stronger than the common default of 2 layers, and following <ref type="bibr" target="#b11">Cubuk et al. (2020)</ref> we pick the magnitude of the RA augmentation based on the training resolution of the images. If the augmentation magnitude is set too high relative to the image resolution, then certain operations (such as shearing) can result in many images being completely blank, which will impede training. For NFNet variants F0 through F6, the chosen RA magnitudes are <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">15,</ref><ref type="bibr">15,</ref><ref type="bibr">15]</ref>, respectively.</p><p>The combination of MixUp, CutMix, and RA results in an intense level of augmentation which progressively benefits NFNets, but does not appear to benefit other models like EfficientNets over a baseline of just using well-tuned RA.</p><p>We hypothesize that this is because our models lack the implicit regularization of batch normalization, and similar to how they are more amenable to large-scale pre-training, they are accordingly also more amenable to stronger data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Accelerating Sharpness-Aware Minimization</head><p>Sharpness-Aware Minimization (SAM, <ref type="bibr" target="#b14">Foret et al. (2021)</ref>) has been shown to improve the performance of various classifier models by seeking flat minima which are hypothesized to generalize better. However, by default it is expensive to apply as it requires two evaluations of the gradient: one for a step of gradient ascent to attain "noised" parameters, and then one to attain the gradients with respect to the noised parameters, which are used to update the actual parameters. We experimented with ameliorating this cost by only employing 20% of the batch to compute the gradients for the ascent step, which we found to result in equivalent performance while only increasing the training latency by 20%-40% instead of by 100%. We also tried using SAM where the batch of data used to compute the ascent step was a different batch from the one used to compute the descent step, but found that this destroyed all the benefits of SAM. This indicates that it is necessary for the ascent step to be computed using the same batch (or a subset thereof) as is used to compute the descent step. As noted in <ref type="bibr" target="#b14">Foret et al. (2021)</ref>, we found that SAM worked best in a distributed setup where the gradients used for the ascent step are not synced between replicas (meaning a separate copy of the "noised" parameters is kept on each replica and used to compute the local descent gradients). We note that this phenomenon can also be mimicked on fewer devices, or a single device, by employing gradient accumulation (iteratively computing noised parameters and then accumulating the gradients to be used for descent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Large Scale Pre-Training Details</head><p>Our large scale pre-training is performed on JFT-300m <ref type="bibr" target="#b62">(Sun et al., 2017)</ref>, a dataset of 300 million labeled images spanning roughly 18,000 classes. We pre-train all models at resolution 224 (regardless of the native model resolution for a given NFNet variant) using the same optimizer settings as for our ImageNet experiments (as described in Appendix A.1) with the exception of using a smaller weight decay (10 −5 for BN and NF-ResNets, and 10 −6 for all NFNet models). We briefly tried pre-training at larger image resolutions and found that this was not worth the added pre-trainining expense. We do not use any augmentations except for baseline random crops and flips, nor do we use any exponential moving averages during pre-training.</p><p>For ResNet models, we pre-train with a batch size of 1024 for 10 epochs using a learning rate of 0.4 following <ref type="bibr" target="#b17">Goyal et al. (2017)</ref>, which is warmed up over 5,000 steps and then decayed to zero with cosine annealing through the rest of training. We fine-tune ResNets on ImageNet with a batch size of 2048 for 15,000 steps using a learning rate of 0.1 (again employing a 5000 step warmup and cosine decay, but not applying the batch size scaling of <ref type="bibr" target="#b17">Goyal et al. (2017)</ref>), no weight decay, no DropOut, and no Stochastic Depth. For fine-tuning we apply EMA with decay 0.9999 and the decay warmup described above. Due to the expense of this experiment we only run a single random seed for each model (fine-tuning three separate times at each of the fine-tune resolutions of 224, 320, and 384 pixels).</p><p>We find, contrary to <ref type="bibr" target="#b13">(Dosovitskiy et al., 2021)</ref>, that a large weight decay is harmful during pre-training, and that instead very small weight decays are important so that the models are not constrained when trying to capture the information in a large scale dataset. Contrary to <ref type="bibr" target="#b13">Dosovitskiy et al. (2021)</ref> we also find that Adam is not as performant as SGD in this setting. We believe this reflects in the fact that our baseline batch-normalized ResNets substantially outperform the baselines reported in <ref type="bibr" target="#b13">Dosovitskiy et al. (2021)</ref> despite otherwise similar pre-training and fine-tuning configurations. For reference, <ref type="bibr" target="#b13">Dosovitskiy et al. (2021)</ref> report a ResNet-50 transfer accuracy of 77.54% when fine-tuned at 384px resolution, whereas we obtain an accuracy of 79.9% in the same setting for BN-ResNet-50 and 81.1% for NF-ResNet-50. The full set of accuracies for these ResNet models is available in <ref type="table" target="#tab_5">Table 4</ref>. We recommend future work on large-scale pre-training to begin with a weight decay of zero and consider lightly increasing it, rather than starting with a large value of weight decay and experimenting with decreasing it.</p><p>For NFNet models, we pre-train with a batch size of 4096. For NFNet-F4, we pre-train for 40 epochs, and for NFNet-F4+ we pre-train for 20 epochs. The F4+ model is a wider variant, constructed from the F4 model by using a channel pattern of <ref type="bibr">[384,</ref><ref type="bibr">768,</ref><ref type="bibr">2048,</ref><ref type="bibr">2048]</ref> instead of <ref type="bibr">[256,</ref><ref type="bibr">512,</ref><ref type="bibr">1536,</ref><ref type="bibr">1536]</ref> and keeping all other hyperparameters the same. We find that both models obtain about the same training latency (around 830ms per step when training with a per-core batch size of 32), but that the F4 model needs the additional pre-training time to reach the same final performance as the F4+ model. This indicates that (given sufficient pre-training data) it is more efficient to train larger models with a shorter epoch budget than to train smaller models for longer, consistent with the observations in <ref type="bibr" target="#b36">(Kaplan et al., 2020)</ref>.</p><p>We fine-tune NFNet models for 15,000 steps at a batch size of 2048 using a learning rate of 0.1, which is warmed up from zero over 5000 steps, then annealed to zero with cosine decay through the rest of training. We use SAM with ρ = 0.05, weight decay of 10 −5 , a DropOut rate of 0.25, and a stochastic depth rate of 0.1. We found that we could obtain similar results using the same regularization setup as for ResNets (no weight decay, DropOut, or Stochastic Depth) but that this mild degree of augmentation was slightly more performant. As with our ResNet fine-tuning we employ an exponential moving average of the parameters with EMA decay warmup. The results of this experiment, compared against other models which are pre-trained on large scale datasets, are available in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Downsides of Batch Normalization</head><p>Batch normalization provides a range of benefits, which we discussed in Section 2 of the main text, but it also has a number of disadvantages that motivated this work on normalizerfree networks. We discussed some of the disadvantages of batch normalization in Section 1. In addition, here we enumerate some documented errors and challenges in the implementation of batch normalization in popular frameworks and published work. A number of these errors are identified by <ref type="bibr">Pham et al. (2019)</ref>, an academic paper on automated testing which discovers two such implementation errors in Keras and one in the CNTK toolkit.</p><p>One example is a long-standing bug in certain versions of Keras, whose consequence is that even if a user sets the batch normalization layers to testing mode (as is common when freezing the layers for fine-tuning for downstream tasks) the batch normalization statistics will continue to update, contrary to user expectations. This implementation error is raised in in this github issue and this github issue.</p><p>The discrepancy between batch normalization train and test behavior has had direct impact several times in previous work. For examples, both <ref type="bibr">DCGAN (Radford et al., 2016)</ref> and SAGAN <ref type="bibr" target="#b80">(Zhang et al., 2019b)</ref> reported results and released code where batch normalization was run in training mode at test time as noted here and here, 3 and consequently their reported results depend on the batch size used to generate samples.</p><p>Subtle differences in batch normalization implementations can also hamper reproducibility. For example, the Efficient-Net training code uses a form of cross-replica BatchNorm where the number of devices used to compute statistics varies nonlinearly with the total number of devices (as seen here), and consequently, even given the same code, exact reproduction can be difficult without access to the same hardware. Additionally, the EfficientNet code takes a moving average of the running batch normalization statistics, which in practice means that it takes a moving average of a moving average, compounding the averaging horizon in a way that may be unexpected.</p><p>As discussed in the main text, breaking the independence between training examples causes issues in contrastive learning setups like SimCLR <ref type="bibr" target="#b10">(Chen et al., 2020)</ref> and MoCo .Both models have to deal with the potential for intra-batch information leakage negatively impacting the contrastive objective. MoCo seeks to resolve this by shuffling examples between devices when computing batch statistics, which introduces implementation complexity and makes it challenging to exactly reproduce their results on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Details</head><p>Our NFNet model is a modified SE-ResNeXt-D <ref type="bibr" target="#b22">(He et al., 2016b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b73">Xie et al., 2017;</ref><ref type="bibr" target="#b29">Hu et al., 2018;</ref><ref type="bibr" target="#b24">He et al., 2019)</ref>.</p><p>The input to the model is an H × W RGB image which has been normalized by the per-channel mean / standard deviation from the entire ImageNet <ref type="bibr" target="#b49">(Russakovsky et al., 2015)</ref> training set, as is standard in most image classifiers. The model has an initial "stem" comprised of a 3 × 3 stride 2 convolution with 16 channels, two 3 × 3 stride 1 convolutions with 32 channels and 64 channels respectively, and a final 3 × 3 stride 2 convolution with 128 channels. A nonlinearity is placed in between each convolution in the stem, but importantly not after the final convolution in the stem. By default we use GELU <ref type="bibr" target="#b25">(Hendrycks &amp; Gimpel, 2016)</ref>, although most common nonlinearities like ReLU or SiLU appear to have similar performance. All our nonlinearities are rescaled to be approximately variance-preserving following <ref type="bibr">Brock et al. (2021)</ref> using a fixed scalar gain, for which we provide reference values in our source code.</p><p>Following the stem are four residual "stages", where the number of blocks per stage is <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">6,</ref><ref type="bibr">3]</ref> for our baseline F0 variant, and each subsequent variant has this number multiplied by N (where N = 1 for F0). The residual stages begin with a "transition" block (as shown in <ref type="figure" target="#fig_2">Figure 5</ref>) followed by standard residual blocks (as shown in <ref type="figure">Figure 6</ref>). In all but the first stage, the transition block downsamples (with 2 × 2 average pooling on the skip path and by striding the first 3 × 3 convolution on the main path) and changes the output channel count (via a 1×1 shortcut convolution on the skip path). <ref type="bibr" target="#b24">He et al. (2019)</ref> identified that the use of a 2 × 2 average pooling improves performance over using a strided 1×1 convolution on the skip path (which merely subsamples the activation). Note that this is slightly different from <ref type="bibr" target="#b5">Bello (2021)</ref>, which uses a 3 × 3 average pooling kernel with stride 2.</p><p>All blocks employ the pre-activation ResNe(X)t bottleneck pattern with an added 3 × 3 grouped convolution inside the bottleneck. This means that the main path comprises a 1 × 1 convolution whose output channels are equal to 0.5× the output channel count for the block, two 3 × 3 grouped convolution with group width 128 (with the first strided in transition blocks), and a final 1 × 1 convolution whose output channel count is equal to the block output channel count.</p><p>Following the last 1 × 1 convolution is a Squeeze &amp; Excite layer <ref type="bibr" target="#b29">(Hu et al., 2018)</ref>, which globally average pools the activation, applies two linear layers with an interleaved scaled nonlinearity to the pooled activation, applies a sigmoid, then rescales the tensor channel-wise by twice the value of this sigmoid. Concretely, the output of this layer is 2σ(F C(GELU (F C(pool(h))))) × h. The non-standard scalar multiplier of 2 is used following <ref type="bibr">Brock et al. (2021)</ref> to maintain signal variance.</p><p>After all of the residual stages, we apply a 1 × 1 expansion convolution that doubles the channel count, similar to the final expansion convolution in EfficientNets <ref type="bibr" target="#b66">(Tan &amp; Le, 2019)</ref>, then global average pooling. This layer is primarily helpful when using very thin networks, as it is typically desirable to have the dimensionality of the final activation vectors (which the classifier layer receives) be greater than or equal to the number of classes, but we retain it in our wider networks to benefit future work which might seek to train very thin networks based on our backbones. We tried replacing this convolution with a fully connected layer after the average pooling but found that this was not helpful.</p><p>The final layer is a fully-connected classifier layer with learnable biases which outputs a 1000-way class vector (which can be softmaxed in order to obtain normalized class probabilities). We initialize this layer's weight with a standard deviation of 0.01 following <ref type="bibr" target="#b17">Goyal et al. (2017)</ref>. We found that initializing the weight with zeros as is sometimes done could sometimes lead to instabilities when training with very large numbers of output classes.</p><p>No activation normalization layers are used anywhere in our residual blocks. Instead, we employ the Normalizer-Free variance downscaling strategy <ref type="bibr">(Brock et al., 2021)</ref>. This means that the input to the main path of the residual block is multiplied by 1/β, where β is the analytically predicted value of the variance at that block at initialization, and the output of the block is multiplied by a scalar hyperparameter α, typically set to a small value like α = 0.2. As in <ref type="bibr">Brock et al. (2021)</ref>, we compute the expected empirical variance at residual block analytically using Var(x ) = Var(x −1 ) + α 2 , with Var(x 0 ) = 1, resulting in β = Var(x ). We also mimic the variance reset that happens in the transition blocks of batch-normalized networks, by having the shortcut convolution in transition layers operate on (x /β ) rather than x (see <ref type="figure" target="#fig_2">Figure 5</ref>). This ensures unit signal variance at the start of each stage (Var(x +1 ) = 1 + α 2 ).</p><p>Additionally following <ref type="bibr">Brock et al. (2021)</ref>, we also employ SkipInit , a learnable zero-initialized scalar gain in addition to α which results in the residual block being initialized to the identity (except in transition layers), similar to <ref type="bibr" target="#b17">Goyal et al. (2017)</ref>; <ref type="bibr" target="#b79">Zhang et al. (2019a)</ref>; <ref type="bibr" target="#b3">Bachlechner et al. (2020)</ref>, which we find to improve stability for very deep networks. While this will result in the signal propagation at initialization not actually following the expected variance as computed above, we find that the variance downscaling and α scalar are still beneficial for stability.</p><p>All convolutions employ Scaled Weight Standardization <ref type="bibr">(Brock et al., 2021)</ref>, with a learnable affine gain applied to the standardized weight and a learnable affine bias ap-High-Performance Normalizer-Free ResNets   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional AGC Ablations</head><p>In <ref type="figure">Figure 7</ref>, we show performance for different clipping thresholds λ across a range of batch sizes on ResNet200, using the same training setup described in Section 4.1. In both <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_1">Figure 2</ref>, we run NF-ResNets with AGC for 5 independent runs, and report the average of the best 4 of these 5 runs. This ensures that our results are robust to outliers and failed training runs.</p><p>As in <ref type="figure" target="#fig_1">Figure 2(b)</ref>, we see that smaller clipping thresholds are necessary for stability at higher batch sizes on the ResNet200. For all our experiments in Section 6 where we use batch size 4096, we use a clipping threshold λ = 0.01. In the course of developing the NFNet architecture we experimented with strategies impacting a range of model design aspects, including rules for picking backbone width and depth, bottleneck compression or expansion ratios, choice of group width, the placement of Squeeze &amp; Excite (S&amp;E) layers, and more. In this section we present select insights from what we found not to work well. As in Section 5, our goal here was to improve the pareto front of top-1 holdout accuracy versus training speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Negative Results</head><p>First, we considered patterns where, for a given choice of backbone, we allowed the group width or number of groups in the 3 × 3 convolutions to be different in different stages, or similarly allowed the bottleneck ratio to vary in different stages. We also considered varying whether the transition blocks would have their bottleneck ratios be a function of the number of block output channels (as in ResNet models) or as the number of block input channels (as in many mobile models). For example, one model family variant used a group width of <ref type="bibr">[8,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">16]</ref> in each of the four stages, with a bottleneck ratio of 0.25 (with the transition blocks using the bottleneck width based on the input channel count) in the first two stages and 0.5 in the latter two stages (with the transition blocks here using the bottleneck width based on the output channel count).</p><p>While we occasionally found that some of this variance could be helpful (for example, using inverted bottleneck blocks in the first stage yielded occasional but inconsistent improvements), we broadly found such heterogeneity to be unnecessary, and to confound attempts to reason out interpretable design patterns. We expect that these design aspects could yield better models if incorporated into largescale architecture search to get individual models at given compute budget targets, but to be less useful for manual design. Our final NFNet designs are largely homogenous with respect to these parameters, with only width and stage depth varying between stages, and ResNet style bottleneck widths (where the channel count of the 3 × 3 convolutions is the number of output channels times the bottleneck ratio).</p><p>We explored aggressive downsampling strategies, such as operating on 8 × 8 DCT coefficients as in <ref type="bibr" target="#b18">Gueguen et al. (2018)</ref> instead of using the standard ResNet stem. While this is an effective way to improve model speed, we found that any improvements in model speed came at the cost of model accuracy. This appears to hold true even when this downsampling is done with an invertible operation (e.g. an orthogonal strided transform like the DCT) such that no information is lost. This is arguably consistent with the observations in <ref type="bibr" target="#b51">Sandler et al. (2019)</ref>, suggesting that model "internal resolution" is a more important quantity to consider in this respect, but we did not explore this direction in further detail.</p><p>We next considered trying to improve speed by making our 1 × 1 dense convolutions into grouped convolutions. This normally causes sharp performance degradation, as these layers are responsible for the flow of information across all channels (as the other convolutions are grouped), and removing their full connectivity substantially reduces model expressivity. To ameliorate this we considered applying straddled Squeeze &amp; Excite layers, where the input to the S&amp;E is the input to the convolution, but the output of the S&amp;E multiplies the output of the convolution. This is in constrast to the normal Squeeze &amp; Excite formulation, which simply operates directly on an activation (i.e. it takes in a value, and its output is used to multiply that same value). Both forms of S&amp;E block help to restore cross-channel connectivity (albeit not as strongly as using a fully connected convolution) more cheaply than fully-connected layers as they operate on globally average-pooled activations.</p><p>Employing grouped 1 × 1 convs with a small number of groups (2 or 4) paired with S&amp;E layers slightly reduces accuracy, and improves theoretical FLOPS and reduces parameter counts, with the straddled S&amp;E block resulting in slightly improved accuracy relative to the standard S&amp;E block. However, we found there was no choice of 1 × 1 group width or group count which maintained comparable accuracy while reducing training latency. Using a high group count (and therefore a small group width) substantially reduces FLOPS and parameter counts, but also substantially reduces model performance, indicating that incorporating these S&amp;E layers helps but does not fully recover the expressivity of dense 1 × 1 convolutions.</p><p>Finally, we did not experiment with any attention variants <ref type="bibr" target="#b5">(Bello, 2021;</ref><ref type="bibr" target="#b58">Srinivas et al., 2021)</ref>, and we expect that our results could likely be improved by adopting these strategies into our models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>ImageNet Validation Accuracy vs Training Latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) AGC efficiently scales NF-ResNets to larger batch sizes. (b) The performance across different clipping thresholds λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Detailed view of an NFNet transition block. The bottleneck ratio is 0.5, while the group width (the number of channels per group, C/G) in the 3 × 3 convolutions is fixed at 128 regardless of the number of channels. Note that in this block, the skip path takes in the signal after the variance downscaling with β and the scaled nonlinearity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Detailed view of an NFNet non-transition block. The bottleneck ratio is 0.5, while the group width (the number of channels per group, C/G) in the 3 × 3 convolutions is fixed at 128 regardless of the number of channels. Note that in this block, the skip path takes in the signal before the variance downscaling with β. Performance across different clipping thresholds λ of AGC for different batch sizes on ResNet200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Comparison of standard (left) and "straddling" (right) Squeeze &amp; Excite blocks. Both forms of S&amp;E block allow for full cross-channel connectivity, but only in the form of a scalar multiplier per channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>NFNet family depths, drop rates, and input resolutions.</figDesc><table><row><cell>Variant</cell><cell>Depth</cell><cell cols="2">Dropout Train</cell><cell>Test</cell></row><row><cell>F0</cell><cell>[1, 2, 6, 3]</cell><cell>0.2</cell><cell cols="2">192px 256px</cell></row><row><cell>F1</cell><cell>[2, 4, 12, 6]</cell><cell>0.3</cell><cell cols="2">224px 320px</cell></row><row><cell>F2</cell><cell>[3, 6, 18, 9]</cell><cell>0.4</cell><cell cols="2">256px 352px</cell></row><row><cell>F3</cell><cell>[4, 8, 24, 12]</cell><cell>0.4</cell><cell cols="2">320px 416px</cell></row><row><cell>F4</cell><cell>[5, 10, 30, 15]</cell><cell>0.5</cell><cell cols="2">384px 512px</cell></row><row><cell>F5</cell><cell>[6, 12, 36, 18]</cell><cell>0.5</cell><cell cols="2">416px 544px</cell></row><row><cell>F6</cell><cell>[7, 14, 42, 21]</cell><cell>0.5</cell><cell cols="2">448px 576px</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The effect of architectural modifications and data augmentation on ImageNet Top-1 accuracy (averaged over 3 seeds).</figDesc><table><row><cell></cell><cell>F0</cell><cell>F1</cell><cell>F2</cell><cell>F3</cell></row><row><cell>Baseline</cell><cell cols="4">80.4 81.7 82.0 82.3</cell></row><row><cell>+ Modified Width</cell><cell cols="4">80.9 81.8 82.0 82.3</cell></row><row><cell>+ Second Conv</cell><cell cols="4">81.3 82.2 82.4 82.7</cell></row><row><cell>+ MixUp</cell><cell cols="4">82.2 82.9 83.1 83.5</cell></row><row><cell>+ RandAugment</cell><cell cols="4">83.2 84.6 84.8 85.0</cell></row><row><cell>+ CutMix</cell><cell cols="4">83.6 84.7 85.1 85.7</cell></row><row><cell cols="5">Default Width + Augs 83.1 84.5 85.0 85.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>. Our NFNet-F5 model attains a</cell></row><row><cell>top-1 validation accuracy of 86.0%, improving over the pre-</cell></row><row><cell>vious state of the art, EfficientNet-B8 with MaxUp (Gong</cell></row><row><cell>et al., 2020) by a small margin, and our NFNet-F1 model</cell></row><row><cell>matches the 84.7% of EfficientNet-B7 with RA (Cubuk</cell></row><row><cell>et al., 2020), while being 8.7 times faster to train. See</cell></row><row><cell>Appendix A for details of how we measure training latency.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>ImageNet Transfer Top-1 accuracy after pre-training.</figDesc><table><row><cell></cell><cell cols="3">224px 320px 384px</cell></row><row><cell>BN-ResNet-50</cell><cell>78.1</cell><cell>79.6</cell><cell>79.9</cell></row><row><cell>NF-ResNet-50</cell><cell>79.5</cell><cell>80.9</cell><cell>81.1</cell></row><row><cell>BN-ResNet-101</cell><cell>80.8</cell><cell>82.2</cell><cell>82.5</cell></row><row><cell>NF-ResNet-101</cell><cell>81.4</cell><cell>82.7</cell><cell>83.2</cell></row><row><cell>BN-ResNet-152</cell><cell>81.8</cell><cell>83.1</cell><cell>83.4</cell></row><row><cell>NF-ResNet-152</cell><cell>82.7</cell><cell>83.6</cell><cell>84.0</cell></row><row><cell>BN-ResNet-200</cell><cell>81.8</cell><cell>83.1</cell><cell>83.5</cell></row><row><cell>NF-ResNet-200</cell><cell>82.9</cell><cell>84.1</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Merity, S., Keskar, N. S., and Socher, R. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations, 2018.Nesterov, Y. A method for unconstrained convex minimization problem with the rate of convergence o(1/k 2 ).</figDesc><table><row><cell>A. Experiment Details</cell><cell></cell></row><row><cell>A.1. ImageNet Experiment Settings</cell><cell></cell></row><row><cell>87</cell><cell></cell></row><row><cell>Doklady AN USSR, pp. (269), 543-547, 1983. 86 F3</cell><cell>F4 NFNet-F5</cell></row><row><cell cols="2">Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In International conference on machine learning, pp. 1310-1318, 2013. Pham, H., Xie, Q., Dai, Z., and Le, Q. V. Meta pseudo labels. arXiv preprint arXiv:2003.10580, 2020. Pham, H. V., Lutellier, T., Qi, W., and Tan, L. Cradle: cross-backend validation to detect and localize bugs in deep learning libraries. In 2019 IEEE/ACM 41st International 82 83 84 85 F0 EffNet-B5 ImageNet Top-1 Accuracy (%) F2 F1 EffNet-B7 BoTNet-128-T7</cell></row><row><cell cols="2">Conference on Software Engineering (ICSE), pp. 1027-BoTNet-59</cell></row><row><cell>1038. IEEE, 2019. 81</cell><cell></cell></row><row><cell cols="2">Polyak, B. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, pp. 4(5):1-17, 1964. 10 0 10 1 Test GFLOPS 10 2 80 EffNet-B2</cell></row><row><cell></cell><cell>Wang,</cell></row><row><cell cols="2">X. Resizemix: Mixing data with preserved object infor-</cell></row><row><cell cols="2">mation and true labels. arXiv preprint arXiv:2012.11101,</cell></row><row><cell>2020.</cell><cell></cell></row><row><cell cols="2">Radford, A., Metz, L., and Chintala, S. Unsupervised rep-</cell></row><row><cell cols="2">resentation learning with deep convolutional generative</cell></row><row><cell cols="2">adversarial networks. In 4th International Conference on</cell></row><row><cell>Learning Representations, ICLR, 2016.</cell><cell></cell></row></table><note>Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A. Weight standardization. arXiv preprint arXiv:1903.10520, 2019. Qin, J., Fang, J., Zhang, Q., Liu, W., Wang, X., andFigure 4. ImageNet Validation Accuracy vs. Test GFLOPs. All numbers are single-model, single crop. Our NFNet models are competitive with large EfficientNet variants for a given FLOPs budget, despite being optimized for training latency.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Comparing ImageNet transfer performance for models which use extra data for large-scale pre-training. Meta-Psuedo-Labels results are from Pham et al. (2020), ViT results are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Detailed Model ablation table. Each entry reports ImageNet Top-1 on the left, and TPUv3 training latency on the right. 4% 58.0ms 81.7% 116.0ms 82.0% 211.7ms 82.3% 369.5ms + Modified Width 80.9% 64.1ms 81.8% 133.9ms 82.0% 252.2ms 82.3% 441.5ms + Second Conv 81.3% 73.3ms 82.2% 158.5ms 82.4% 295.8ms 82.7% 532.2ms + MixUp 82.2% 73.3ms 82.9% 158.5ms 83.1% 295.8ms 83.5% 532.2ms + RandAugment 83.2% 73.3ms 84.6% 158.5ms 84.8% 295.8ms 85.0% 532.2ms + CutMix 83.6% 73.3ms 84.7% 158.5ms 85.1% 295.8ms 85.7% 532.2ms Default Width + Augs 83.1% 65.9ms 84.5% 137.4ms 85.0% 248.8ms 85.5% 452.2ms -NF, + BN 83.4% 111.7ms 84.4% 258.0ms 85.1% 396.3ms 85.5% 617.7ms</figDesc><table><row><cell>F0</cell><cell>F1</cell><cell>F2</cell><cell>F3</cell></row><row><cell>Baseline 80.plied to the output of the convolution operation. Critically,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>weight decay is not applied to the affine gains or biases or</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the SkipInit gains. The S&amp;E layers do not employ weight</cell><cell></cell><cell></cell><cell></cell></row><row><cell>standardization on their fully connected layers, nor does the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>fully-connected classifier layer's weight. We initialize the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>underlying weights for these layers using LeCun initializa-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tion (LeCun et al., 2012).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">DeepMind, London, United Kingdom. Correspondence to: Andrew Brock &lt;ajbrock@google.com&gt;. 2 Code available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that no 'u' or 's' values are passed into the batch normalization op here, meaning that running statistics are not accumulated. different hardware. SimCLR seeks to resolve this via the use of cross-replica batch normalization.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Aäron van den Oord, Sander Dieleman, Erich Elsen, Guillaume Desjardins, Michael Figurnov, Nikolay Savinov, Omar Rivasplata, Relja Arandjelović, and Rishub Jain for helpful discussions and guidance. Additionally, we would like to thank Blake Hechtman, Tim Shen, Peter Hawkins, and James Bradbury for assistance with developing highly performant JAX code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Normalization propagation: A parametric technique for removing internal covariate shift in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1168" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baumli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buchlovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fantacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sezener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind" />
	</analytic>
	<monogr>
		<title level="j">The DeepMind JAX Ecosystem</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rezero is all you need: Fast convergence at large depth</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-D</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=xTJEN-ggl1b" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03432</idno>
		<title level="m">On the distance between two neural networks and the stability of learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7694" to="7705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jax</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<title level="m">composable transformations of Python+NumPy programs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=6Tm1mposlrM" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Comparison of batch normalization and weight normalization algorithms for the large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08145</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Google. Cloud TPU Performance Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maxup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09024</idno>
		<ptr target="https://cloud.google.com/tpu/docs/performance-guide" />
	</analytic>
	<monogr>
		<title level="m">A simple way to improve generalization of neural network training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster neural networks straight from jpeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gueguen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosinski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in High-Performance Normalizer-Free ResNets Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3933" to="3944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Array programming with numpy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Van Kerkwijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Del Río</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gérard-Marchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<idno>1476-4687</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (GELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haiku</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/dm-haiku" />
		<title level="m">Sonnet for JAX</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1731" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The hardware lottery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06489</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Centered weight normalization in accelerating training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2803" to="2811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12836</idno>
		<title level="m">Normalization techniques in training dnns: Methodology, analysis and application</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03275</idno>
		<title level="m">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Freeze and chaos for dnns: an ntk view of batch normalization, checkerboard and boundary effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05715</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Large scale learning of general visual representations for transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficient</surname></persName>
		</author>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards understanding regularization in batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00846</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision ECCV</title>
		<meeting>the European Conference on Computer Vision ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Singular vector canonical correlation analysis for deep learning dynamics and interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svcca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6076" to="6085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
	<note>In international conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The Annals of Mathematical Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Non-discriminative data or weak model? on the relative importance of data and model resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baccash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Is normalization indispensable for training deep neural network?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Powernorm: Rethinking batch normalization in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8741" to="8751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Estimating batch normalization statistics for evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evalnorm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3633" to="3641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the generalization benefit of noise in stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9058" to="9067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent High-Performance Normalizer-Free ResNets neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Four things everyone should know to improve batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Dinneen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03548</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Divide the gradient by a running average of its recent magnitude. COURS-ERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rmsprop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8252" to="8262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08129</idno>
		<title level="m">A mean field theory of batch normalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09321</idno>
		<title level="m">Fixup initialization: Residual learning without normalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Why gradient clipping accelerates training: A theoretical justification for adaptivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jadbabaie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJgnXpVYwS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
