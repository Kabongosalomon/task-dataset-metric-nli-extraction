<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangning</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>He</surname></persName>
							<email>rfhe@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<email>yongliu@iipc.zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<email>yingtai@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised learning of optical flow, which leverages the supervision from view synthesis, has emerged as a promising alternative to supervised methods. However, the objective of unsupervised learning is likely to be unreliable in challenging scenes. In this work, we present a framework to use more reliable supervision from transformations. It simply twists the general unsupervised learning pipeline by running another forward pass with transformed data from augmentation, along with using transformed predictions of original data as the self-supervision signal. Besides, we further introduce a lightweight network with multiple frames by a highly-shared flow decoder. Our method consistently gets a leap of performance on several benchmarks with the best accuracy among deep unsupervised methods. Also, our method achieves competitive results to recent fully supervised methods while with much fewer parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow, as a motion description of images, has been widely used in high-level video tasks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref>. Benefitting from the growth of deep learning, learningbased optical flow methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30]</ref> with considerable accuracy and efficient inference are gradually replacing the classical variational-based approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b43">44]</ref>. However, it is tough to collect the ground truth of dense optical flow in reality, which makes most supervised methods heavily dependent on the large-scale synthetic datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, and the domain difference leads to an underlying degradation when the model is transferred to the real-world.</p><p>In another point of view, many works proposed to learn optical flow in an unsupervised way <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24]</ref>, in which the ground truth is not necessary. These works aim to train networks with objective from view synthesis <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b48">49]</ref>, <ref type="bibr">Figure 1</ref>. Timeline of average end-point error (AEPE) advances in deep optical flow. Marker size indicates network size, and oversized markers have been adjusted. Our method outperforms all of the previous unsupervised methods, also yields comparable accuracy to supervised methods while with fewer parameters. † indicates the model using more than two frames.</p><p>i.e. optimizing the difference between reference images and the flow warped target images. This objective is based on the assumption of brightness constancy, which will be violated for challenging scenes, e.g. with extreme brightness or partial occlusion. Hence, proper regularization such as occlusion handling <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b16">17]</ref> or local smooth <ref type="bibr" target="#b26">[27]</ref> is required. Recent studies have focused on more complicate regularizations such as 3D geometry constraints <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b21">22]</ref> and global epipolar constraints <ref type="bibr" target="#b49">[50]</ref>. As shown in <ref type="figure">Fig. 1</ref>, there is still a large gap between these works and supervised methods. In this paper, we do not rely on the geometrical regularizations but rethink the task itself to improve accuracy.</p><p>Interestingly, we notice that almost all of the unsupervised works, such as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref>, avoid using a heavy combination of augmentations, even if it has been proven effective in supervised flow works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14]</ref>. The reason we conclude is two-fold: (i) Data augmentation is essentially a trade-off between diversity and validity. It can improve the model by increasing the diversity of data, while also leads to a shift of data distribution which decreases the accuracy.</p><p>In unsupervised learning, the benefit of diversity is limited since the abundant training data is easy to access. (ii) Data augmentation will generate challenging samples, for which view synthesis is more likely to be unreliable, so the objective cannot guide networks for a correct solution.</p><p>More recently, there are some works based on knowledge distillation that alleviate the problem of unreliable objective in occluded regions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. The training of these methods is split into two stages. In the first stage, a teacher model is trained to make predictions on original data, and offline creating occluded samples with random crop or mask out. In the second stage, these artificial samples from the teacher model are used to update a student model. However, these methods were designed for the case of partial occluded only. Hence we ask: Can we generalize the distillation of occlusion to other transformation cases? Moreover, the distillation method has a bottleneck due to the frozen teacher model. We thus ask: Can we jointly optimize teacher model and student model, or just training a single network?</p><p>In this work, we address the above two questions with a novel unsupervised learning framework of optical flow. Specifically, for the first question, diverse transformations are used to generate challenging scenes such as low-light, overexposed, with large displacement or partial occlusion. For the second question, instead of optimizing two models with distillation, we simply twist the training step in the regular learning framework by running an additional forward with the input of transformed images, and the transformed flow from the first forward pass is treated as reliable supervision. Since the self-supervision from transformations avoids the unsupervised objective to be ambiguous in challenging scenes, our framework allows the network to learn by analogy with the original samples, and gradually mastering the ability to handle challenging samples.</p><p>In summary, our contributions are: (i) We propose a novel way to make use of the self-supervision signal from abundant augmentations for unsupervised optical flow by only training a single network; (ii) We demonstrate the applicability of our method for various augmentation methods. In addition to occlusion, we develop a general form for more challenging transformations. (iii) Our method leads in a leap of performance among deep unsupervised methods. It also achieves a comparable performance w.r.t. previous supervised methods, but with much fewer parameters and excellent cross dataset generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised Optical Flow. Starting from FlowNet <ref type="bibr" target="#b6">[7]</ref>, various networks for optical flow with supervised learning have been proposed, e.g. FlowNet2 <ref type="bibr" target="#b14">[15]</ref>, PWC-Net <ref type="bibr" target="#b37">[38]</ref>, IRR-PWC <ref type="bibr" target="#b13">[14]</ref>. These methods are comparable in accuracy to well-designed variational methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25]</ref>, and are more effective during inference. However, the success of super-vised methods heavily dependent on the large scale synthetic datasets <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7]</ref>, which leads to an underlying degradation when transferring to real-world applications. As an alternative, we dig into the unsupervised method to alleviate the need for ground truth of dense optical flow.</p><p>Unsupervised Optical Flow. Yu et al. <ref type="bibr" target="#b17">[18]</ref> first introduced a method for learning optical flow with brightness constancy and motion smoothness, which is similar to the energy minimization in conventional methods. Further researches improve accuracy through occlusion reasoning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b26">27]</ref>, multi-frame extension <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref>, epipolar constraint <ref type="bibr" target="#b49">[50]</ref>, 3D geometrical constraints with monocular depth <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34]</ref> and stereo depth <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b21">22]</ref>. Although these methods have become complicated, there is still a large gap with state-of-the-art supervised methods. Recent works improve the performance by learning the flow of occluded pixels in a knowledge distillation manner <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, while the two-stage training in these works is trivial. Instead of studying the complicated geometrical constraints, our approach focuses on the basic training strategy. It generalizes the case of occlusion distillation to more kinds of challenging scenes with a straightforward single-stage learning framework.</p><p>Learning with Augmentation. Data augmentation is one of the easiest ways to improve training. Recently, there has been something new about integrating augmentation into the learning frameworks. Mounsaveng et al. <ref type="bibr" target="#b28">[29]</ref> and Xiao et al. <ref type="bibr" target="#b44">[45]</ref> suggested learning data augmentation with a spatial transformer network <ref type="bibr" target="#b15">[16]</ref> to generate more complex samples. Xie et al. <ref type="bibr" target="#b45">[46]</ref> proposed to use augmentation in the semi-supervised tasks by consistency training. Peng et al. <ref type="bibr" target="#b32">[33]</ref> introduced to optimize data augmentation with the training of task-specific networks jointly. As a new trend in AutoML, several efforts to automatically search for the best policy of augmentations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref> are proposed. All these methods aimed at supervised or semi-supervised learning. In this work, we present a simple yet effective approach to integrate abundant augmentations with unsupervised optical flow. We propose to use reliable predictions of original samples as a self-supervision signal to guide the predictions of augmented samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>This work aims to learn optical flow from images without the need for ground truth. For completeness, we first briefly introduce the general framework for unsupervised optical flow methods, which is shown in the left part of <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Given a dataset of image sequences I, our goal is to train a network f (.) to predict dense optical flow U 12 for two consecutive RGB frames {I 1 , I 2 } ∈ I,</p><formula xml:id="formula_0">U 12 = f (I 1 , I 2 ; Θ),<label>(1)</label></formula><p>where Θ is the set of learnable parameters in the network.</p><p>Despite the lack of direct supervision from ground truth, the network can be trained implicitly with view synthesis. Specifically, image I 2 can be warped to synthesize the view of I 1 with the prediction of optical flow U 12 ,</p><formula xml:id="formula_1">I 1 (p) = I 2 (p + U 12 (p)) ,<label>(2)</label></formula><p>where p denotes pixel coordinates in the image, and bilinear sampling is used for the continuous coordinates. Then, the objective of view synthesis, also known as photometric loss L ph , can be formulated as:</p><formula xml:id="formula_2">L ph ∼ p ρ(Î(Θ), I),<label>(3)</label></formula><p>where ρ(.) is a pixel-wise similarity measurement, e.g. 1 distance or structural similarities (SSIM). Nevertheless, the photometric loss is violated when pixels are occluded or moved out of view so that there are no corresponding pixels in I 2 . As a common practice in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref>, we denote these pixels by a binary occlusion map O 12 . This map is obtained by the classical forwardbackward checking method, where the backward flow is estimated by swapping the order of input images. The photometric loss in the occluded region will be discarded.</p><p>Furthermore, supervision solely based on the photometric loss is ambiguous for somewhere textureless or with repetitive patterns. One of the most common ways to reduce ambiguity is named smooth regularization,</p><formula xml:id="formula_3">L sm ∼ d∈x,y p ∇ d U 12 1 e −|∇ d I| ,<label>(4)</label></formula><p>which constrains the prediction similar to the neighbors in x and y directions when no significant image gradient exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Since the general pipeline suffers from unreliable supervision for challenging cases, previous unsupervised works avoid using heavy augmentations. In this section, we introduce a novel framework to reuse existing heavy augmentations that have been proven effective in the supervised scenario, but with different forms. The pipeline is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, and we will explain in detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Augmentation as a Regularization</head><p>Formally, we define an augmentation parameterized by a random vector θ as T img θ : I t → I t , from which one can sample augmented images {I 1 , I 2 } based on original images {I 1 , I 2 } in the dataset. In the general pipeline, the network is trained with the data sampled from the augmented dataset. In contrast, we train the network on original data, but leverage augmented samples as a regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I1</head><p>I2 More specifically, after a regular forward pass for original images, we additionally run another forward for transformed images to predict the optical flow U * 12 . Meanwhile, the prediction of optical flow in the first forward is transformed consistently by T flo θ : U 12 → U 12 . The basic assumption of our method is that augmentation brings challenging scenes in which the unsupervised loss will be unreliable, while the transformed predictions of original data can provide reliable self-supervision. Therefore, we optimize the consistency for the transformed samples instead of the objective of view synthesis. We follow the generalized Charbonnier function that commonly used in the supervised learning of optical flow as:</p><formula xml:id="formula_4">U 12 U 21 O 21Î 1 Lsm L ph T img θ Eq. (6) T flo θ Eq. (7) T occ θ Eq. (8) I1 I2 U * 12 Laug U 12 O 12</formula><formula xml:id="formula_5">L aug ∼ p S U 12 (p) − U * 12 (p) + q ,<label>(5)</label></formula><p>where S(.) stands for stop-gradient, and the same setting as supervised work <ref type="bibr" target="#b37">[38]</ref> with q = 0.4 and = 0.01 gives less penalty to outliers. For stability, we stop the gradients of L aug propagating to the transformed original flow U 12 . Also, only the loss in the non-occluded region is considered. After twice forwarding, the photometric loss Eq. (3), the smooth regularization Eq. (4), and the augmentation regularization Eq. (5) are backward at once to update the model.</p><p>Our learning framework can be integrated with almost all types of augmentation methods. In the following, we summarize three kinds of transformations, which compose the common augmentations for the optical flow task. Some examples are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><formula xml:id="formula_6">Target Image I2 Predicted Flow U12 Ground Truth (Unused) Target Image I2 Predicted Flow U * 12</formula><p>Transformed Flow U12 Spatial Transformation. We assume the transformation that results in a change in the location of pixels is called spatial transformation, which includes random crop, flip, zoom, affine transform, or more complicated transformations such as thin-plate-spline or CPAB transformations <ref type="bibr" target="#b7">[8]</ref>.</p><p>Here we show a general form for these transformations. Let τ θ be a transformation of pixel coordinates. The transformation of image T img θ : I t → I t can be formulated as:</p><formula xml:id="formula_7">I t (p) = I t (τ θ (p)) ,<label>(6)</label></formula><p>which can be implemented by a differentiable warping process, same as the one used in Eq. (2). Since changing pixel locations will lead to a change in optical flow, we should warp on an intermediate flow field U 12 instead of the original flow. The transformation of optical flow is T flo θ : U 12 → U 12 can be formulated as:</p><formula xml:id="formula_8">U 12 (p) = τ θ (p + U 12 (p)) − τ θ (p) , U 12 (p) = U τ −1 θ (p) .<label>(7)</label></formula><p>Additionally, the spatial transformation brings new occlusions. As we mentioned above, we explicitly reasoning occlusion from the predictions of bi-directional optical flow. Since predictions of transformed samples are noisy, we infer the transformed occlusion map from original predictions instead. The transformation T occ  (p) for pixels whose correspondences are out of the boundary Ω. The former can be obtained by the same warping process as T img θ but with nearest-neighbor interpolation, and the latter can be explicitly estimated from the flow U 12 by checking the boundary:</p><formula xml:id="formula_9">O new 12 (p) = p + U 12 (p) / ∈ Ω.<label>(8)</label></formula><p>The final transformed occlusion O 12 is a union of these two parts. Note that, the non-occluded pixels in O Besides, since we formulate the spatial transformation as a warping process, there might be pixels out of boundary after transformation. The common solution, such as padding with zero or the value of boundary pixels, will lead to severe artifacts. Therefore, we repeat sampling the transformations until all transformed pixels are in the region of the original view. On the other hand, this strategy increases the displacement of the pixel in general.</p><p>Occlusion Transformation. The spatial transformation provides reliable supervision for the flow with large displacement or occlusion around the boundary. As a complementary, recent work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> proposed to learn optical flow in arbitrary occluded regions with knowledge distillation. The general learning process of these methods consists of training a teacher model, offline creating occluded samples, and distilling to a student model. We argue that the way of model distillation is too trivial, and there is a performance bottleneck due to the frozen teacher model.</p><p>We integrate the occlusion hallucination into our onestage training framework and named as occlusion transformation. Specifically, there are two steps: (i) Random crop. Actually, random crop is a kind of spatial transformation, but it efficiently creates new occlusion in the boundary. We crop the pair of images as a preprocess of occlusion transformation. (ii) Random mask out. We randomly mask out some superpixels in the target images with Gaussian noise, which will introduce new occlusion for the source image.</p><p>Note that, we adopt a strategy consistent with the spatial transformation that only the pixels not occluded in O old 12 contribute to L aug . It is different from the previous distillation works, in which they reasoning a new occlusion map from the noisy prediction of transform images. Besides, in order to avoid creating transformed samples offline, we adopt a fast method of superpixel segmentation similar to <ref type="bibr" target="#b34">[35]</ref>. The occlusion transformation in our framework simplifies the way of model distillation by optimizing a single model in one-stage with end-to-end learning.</p><p>Appearance Transformation. More transformations only change the appearance of images, such as random color jitter, random brightness, random blur, random noise. As a relatively simple case, appearance transformation does not change the location of pixels, nor introduce new occlusion. Still, the transformations lead to a risk for general methods, e.g. the photometric loss is meaningless when the image is overexposed, blurred, or in extremely low light. Instead, our method can exploit these transformations since the pre-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Overall Objective and Convergence Analysis</head><p>Our framework assumes that transformed predictions are generally more accurate than the predictions of transformed samples, but what if samples are in the opposite case? In fact, we ensure convergence with the scope of each loss, i.e., which pixels affect each loss.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the overall objective for a training step consists of three loss terms in twice forwarding,</p><formula xml:id="formula_10">L all = L ph (U 12 ) + λ 1 L sm (U 12 ) 1st forward + λ 2 L aug (S(U 12 ), U * 12 ) 2nd forward ,<label>(9)</label></formula><p>in which the first two terms propagate gradients for the original sample, and the last term is for the transformed sample. The original data and the augmented data are treated differently. By setting a minor weight of λ 2 , we can ensure that the original data is always dominant, so the effects of bad cases are limited. Moreover, the scope of the photometric loss L ph is the non-occluded pixels in O 12 . Thus the augmentation consistency loss becomes dominant for the new occluded pixels, which leads the network to learn the optical flow with occlusion effectively. Besides, the scope of augmentation loss L aug avoids the network to be misguided from the original occluded predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Lightweight Network Architecture</head><p>The learning framework we proposed can be applied to any flow networks. However, optical flow often plays a role as a sub-module in high-level video tasks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b30">31]</ref> where the model size should be concerned. Hence, we introduce a lightweight architecture and extend it to multiple frames.</p><p>We start from a well-known network for the optical flow task named PWC-Net <ref type="bibr" target="#b37">[38]</ref>. The original network shares a feature encoder with a siamese feature pyramid network for the images. For the level l in the pyramid, the feature maps of target image x l 2 are aligned by warping operation with the flow prediction U l+1 12 from the higher level. Then the cost volume cv l 12 is constructed with correlation operation. The input for flow decoder F l 12 is organized by concatenating the feature maps of source image x l 1 , the upsampled flow from the higher level U l+1 12 , and the cost volume cv l 12 . Finally, the specific flow decoder of level l predicts the optical flow U l 12 . By iterating over the pyramid, the network predicts optical flow at different scales.</p><p>Our method follows the main pipeline of the original PWC-Net but with some modifications. The flowchart of our multi-frame extension is shown in <ref type="figure" target="#fig_6">Fig. 4</ref>. We notice that the majority of learnable parameters of PWC-Net is in the flow decoder of each feature level, so we take several steps to reduce the parameters: (i) The original implementation adopts a fully dense connection in each decoder, while we reduce the connections that only connections in the nearest two layers are retained. (ii) We share the flow decoder for all of the levels across the pyramid, with an additional convolution layer for each level to align the feature maps. (iii) We extend the model to multiple frames by repeating the warping and correlation to the backward features. The flow decoder is shared for both forward flow and backward flow in the multi-frame extension by changing the sign of optical flow and the order in feature concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We implement our end-to-end approach in PyTorch <ref type="bibr" target="#b31">[32]</ref>. All models are trained by Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with β 1 = 0.9, β 2 = 0.99, batch size of 4. The learning rate is 10 −4 without adjustment during training. The loss weights for regularizations are set to λ 1 = 60 and λ 2 = 0.01 for all datasets. In addition, an optional pre-training can be used for better results, which is under almost the same setting above, but with λ 2 = 0, i.e. a regular training step without the transformed pass in forward 1 .</p><p>Only random flip and random time order switch are performed as the regular data augmentation. The heavy combination of augmentations in supervised works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b12">13]</ref> are used as the appearance transformation and spatial transformation in our framework, including random rotate, translate, zoom in, as well as additive Gaussian noise, Gaussian blur and random jitter in brightness, color, and contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets</head><p>We first evaluate our method on three well-established optical flow benchmarks, MPI Sintel <ref type="bibr" target="#b0">[1]</ref>, KITTI 2012 <ref type="bibr" target="#b9">[10]</ref>, and KITTI 2015 <ref type="bibr" target="#b27">[28]</ref>. Then, we conduct a cross dataset experiment with another optical flow dataset FlyingChairs <ref type="bibr" target="#b6">[7]</ref> and a segmentation dataset CityScapes <ref type="bibr" target="#b3">[4]</ref>.</p><p>We follow a similar data setting in previous unsupervised works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. For the MPI Sintel benchmark, we extract all frames from the raw movie and manually group frames by shots for pre-training, which consists of 14,570 image pairs. Then, the model is fine-tuned on the standard training set, which provides 1,041 image pairs with two different rendering passes ("Clean" and "Final"). For the KITTI 2012 and KITTI 2015, we pre-train the model on the KITTI raw dataset <ref type="bibr" target="#b8">[9]</ref>, but discard scenes that contain images appeared in the optical flow benchmarks. The pre-training set consists of 28,058 image pairs. Then the model is fine-tuned on the multi-view extension data, but discards samples containing frames related to validation, i.e. numbers 9-12. The final training set consists of 6,000 samples for our basic model and 3,600 samples for the multi-frame model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-art</head><p>We compare our method with both supervised and unsupervised methods on optical flow benchmarks. Standard metrics for optical flow are used, including average endpoint error (AEPE), and percentage of erroneous pixels (Fl). <ref type="table">Table 1</ref> reports the results on MPI Sintel benchmark. Our basic two-frame model "ARFlow" outperforms all previous unsupervised works with the least parameters. Furthermore, our multi-frame model "ARFlow-MV" reduces <ref type="bibr" target="#b0">1</ref> Code available at https://github.com/lliuz/ARFlow. the previous best AEPE from 6.18 <ref type="bibr" target="#b22">[23]</ref> to 4.49 on the clean pass, with 27.3% improvement, and from 6.57 <ref type="bibr" target="#b23">[24]</ref> to 5.67 on the final pass, with 13.7% improvement. As for KITTI benchmarks, <ref type="table">Table 2</ref> shows a significant improvement. On the training set, we achieve AEPE=1.26 with 25.4% relative improvement on KITTI 2012 and AEPE=2.85 with 41.2% improvement on KITTI 2015 w.r.t. the previous best unsupervised method <ref type="bibr" target="#b23">[24]</ref>. On the test set, our method reaches the best AEPE=1.5 and F1-all=11.79% among unsupervised methods, respectively.</p><p>Several representative supervised methods are also reported as a reference. As a result, our unsupervised models firstly reach or approach some powerful fully supervised methods such as LiteFlowNet <ref type="bibr" target="#b12">[13]</ref>, PWC-Net <ref type="bibr" target="#b37">[38]</ref>, even with 27.1% parameters of PWC-Net.</p><p>Samples on MPI Sintel and KITTI are shown in <ref type="figure" target="#fig_7">Fig. 5</ref>. Compared with the state-of-the-art competitor <ref type="bibr" target="#b23">[24]</ref>, for the low light and large displacement scenes in MPI Sintel, our method maintains better performance in general and is more   <ref type="table">Table 3</ref>. Ablation study of our learning framework with multiple model architectures. AEPE in specific regions of the scene and the number of CNN parameters are reported. AR: Training with augmentation as a regularization framework.</p><p>accurate around the boundaries. For KITTI results, the shapes in our optical flow are more structured for objects and more accurate in texture-less regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>To further analyze the capability of each component, we conduct four groups of ablation studies. We randomly resplit the Sintel training set into a new training set and a validation set by the scene. We evaluate AEPE in different regions over all pixels (ALL), non-occluded pixels (NOC), occluded pixels (OCC), and according to speed (s0-10, s10-40 and s40+ are pixels that move less than 10 pixels, between 10 and 40, and more than 40, respectively) Main Ablation. <ref type="table">Table 3</ref> assesses the overall improvement of our augmentation as a regularization learning framework under multiple model architectures. Our framework consistently improves the accuracy of optical flow over 10% for all architectures, whether for occluded or non-occluded pixels.</p><p>For the consideration of the number of model parameters, we start from the original PWC-Net and a variant named PWC-Net-small without dense connections in the flow decoders <ref type="bibr" target="#b37">[38]</ref>. Although removing dense connections can reduce half parameters, it leads to severe performance <ref type="bibr">ST</ref>     <ref type="table">Table 6</ref>. Comparison of different augmentation transformations integrated with our framework. AT: appearance transformation, ST: spatial transformation. evaluate the same transformations with different usages. <ref type="table" target="#tab_4">Table 5</ref> reports the results of (i) training without heavy augmentation, (ii) using transformation as a regular data augmentation and training directly, (iii) training with data distillation that similar in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, (iv) training with the learning framework we proposed. The results show that directly augmentation makes all metrics worse. Instead of applying transformations directly, distillation alleviates the problem of unreliable supervision. However, the frozen teacher model is still a bottleneck for the student model. Also, the tedious multi-stage training process of knowledge distillation is undesired. Our framework avoids the unreliable photometric loss for the transformed samples. It achieves the best results with a single-stage optimization.</p><p>Integrate Complicated Augmentation. By implementing the corresponding transformation of optical flow and occlusion map, our framework can be integrated with almost all types of augmentation. We assess a complicated spatial transformation called CPAB <ref type="bibr" target="#b7">[8]</ref> and a recent work in AutoML on searching for the best augmentation policy called AutoAugment <ref type="bibr" target="#b4">[5]</ref>. Note that random zoom in is applied first to avoid invalid coordinate values of transformations. <ref type="table">Table 6</ref> shows that both strategies integrated with our framework can improve accuracy. Note that AutoAugment is too time consuming for our task, therefore we adopt the final policy searched from ImageNet <ref type="bibr" target="#b5">[6]</ref> classification task. It is promising that our framework with AutoAugment will be further improved with policy fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Cross Dataset Generalization</head><p>Although deep optical flow methods have been far ahead of the most popular classical variational method TV-L1 <ref type="bibr" target="#b42">[43]</ref> on optical flow benchmarks, the latter has not gone away. One possible reason is that supervised learning methods are prone to overfitting, which results in poor generalization when transferring to high-level video tasks.  <ref type="table">Table 7</ref>. Generalization performance of cross datasets evaluation.</p><p>The numbers indicate AEPE on each dataset. For KITTI and Sintel, the results are evaluated on the training set. () indicates the results of a dataset that the method has been trained on.</p><p>Hence, we report the cross dataset accuracy in <ref type="table">Table 7</ref>, in which our unsupervised method is compared with a fully supervised method PWC-Net <ref type="bibr" target="#b37">[38]</ref>. The supervised PWC-Net consistently outperforms for the dataset that the model is trained on, while our unsupervised method works much better when transferring to other datasets. In addition, we train a model on an urban street dataset named CityScapes <ref type="bibr" target="#b3">[4]</ref>, in which 50,625 image pairs are used for training without the ground truth. This model performs best on the KITTI 2012 and KITTI 2015 than any other model trained on the synthetic dataset. Our method makes it possible to fit the domain of high-level video tasks by training a model on the unlabeled videos from that domain.</p><p>Remarkably, despite the lack of cross dataset results from other unsupervised methods, the accuracy of our model trained on CityScapes is even better than most of the previous works trained on KITTI (c.f . <ref type="table">Table 2</ref>), which shows the superiority of our method. The results demonstrate a significant improvement of our method for unsupervised optical flow task with an excellent generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a novel framework that learns optical flow from unlabeled image sequences with the self-supervision from augmentations. To avoid the objective of view synthesis being unreliable on transformed data, we twist the basic learning framework by adding another forward pass for transformed images, where the supervision is from the transformed prediction of original images. Besides, a lightweight network and its multi-frame extension were presented. Extensive experiments have shown that our methods significantly improve accuracy, with high compatibility and generalization ability. We believe that our learning framework can be further combined with other geometrical constraints or transferred to other visual geometry tasks, such as depth or scene flow estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The pipeline of our proposed method. A complete training step includes two forwards: (i) The left side shows the first forward with original samples by the regular pipeline introduced in Section 3. Then, we perform transformations on images, predicted flow, and occlusion map respectively to construct an augmented sample. (ii) The right side shows an additional forward with the input of transformed images, and the output flow is supervised by the flow prediction of original samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Some examples of the main idea. The same network is used to predict the optical flow of original images and transformed images, respectively. (a) Spatial transformation and appearance transformation generate a scene with large displacement and low brightness. (b) Occlusion transformation introduces additional occlusions. The pseudo label U12 that transformed from the original predictions U12 can provide reliable supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>θ:</head><label></label><figDesc>O 12 → O 12 consists of two parts: the old occlusion O old 12 (p) in the new view and the new occlusion O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>new 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>12 .</head><label>12</label><figDesc>It provides an effective way to learn the optical flow in occluded regions. For stability, only the non-occluded pixels in O old 12 contribute to the loss L aug .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Network architecture for our lightweight multi-frame extension of PWC-Net<ref type="bibr" target="#b37">[38]</ref>. It shares a semi-dense flow decoder for all of the levels across the pyramid with both forward flow and backward flow. For simplicity and completeness, the pipeline of two levels in the feature pyramid is displayed. Different line colors represent different levels of the process.diction of the original sample provides a way to learn the optical flow in challenging transformed scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative visualization comparing with unsupervised SelFlow<ref type="bibr" target="#b23">[24]</ref>. The first two rows are from the Sintel Final pass, where the errors are visualized in gray. The last two rows are from KITTI 2015, in which the correct predictions are depicted in blue and the wrongs in red for the error visualization. More samples will be available on the website of corresponding benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>MPI Sintel Flow: AEPE and the number of CNN parameters are reported. Missing entry (-) means that the results are not reported for the respective method, and † indicates the model using more than two frames. KITTI Optical Flow 2012 and 2015: AEPE and Fl are reported. For unsupervised methods, only the works published in 2019 are shown. Missing entry (-) means that the results are not reported for the respective method. † indicates the model using more than two frames. § indicates training with geometrical constraints.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Sintel Training</cell><cell cols="2">Sintel Test</cell><cell># Param.</cell></row><row><cell></cell><cell></cell><cell>Clean</cell><cell>Final</cell><cell>Clean</cell><cell>Final</cell><cell></cell></row><row><cell>Supervised</cell><cell>FlowNetS-ft [7] LiteFlowNet-ft[13] PWC-Net-ft[38] IRR-PWC-ft [14] SelFlow-ft  † [24]</cell><cell>(3.66) (1.64) (2.02) (1.92) (1.68)</cell><cell>(4.44) (2.23) (2.08) (2.51) (1.77)</cell><cell>6.96 4.86 4.39 3.84 3.74</cell><cell>7.76 6.09 5.04 4.58 4.26</cell><cell>32.07 M 5.37 M 8.75 M 6.36 M 4.79 M</cell></row><row><cell></cell><cell>UnFlow-CSS [27]</cell><cell>-</cell><cell>(7.91)</cell><cell>9.38</cell><cell>10.22</cell><cell>116.58 M</cell></row><row><cell>Unsupervised</cell><cell>OccAwareFlow [42] MFOccFlow  † [17] EpiFlow train-ft [50] DDFlow [23] SelFlow  † [24] Ours (ARFlow)</cell><cell>(4.03) (3.89) (3.54) (2.92) (2.88) (2.79)</cell><cell>(5.95) (5.52) (4.99) (3.98) (3.87) (3.73)</cell><cell>7.95 7.23 7.00 6.18 6.56 4.78</cell><cell>9.15 8.81 8.51 7.40 6.57 5.89</cell><cell>5.12 M 12.21 M 8.75 M 4.27 M 4.79 M 2.24 M</cell></row><row><cell></cell><cell>Ours (ARFlow-MV  † )</cell><cell>(2.73)</cell><cell>(3.69)</cell><cell>4.49</cell><cell>5.67</cell><cell>2.37 M</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">KITTI 2012</cell><cell cols="2">KITTI 2015</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell>training</cell><cell>test</cell><cell>training</cell><cell>test (F1)</cell></row><row><cell>Supervised</cell><cell>FlowNet2-ft [15] LiteFlowNet-ft [13] PWC-Net-ft [38] SelFlow-ft  † [24]</cell><cell></cell><cell>(1.28) (1.26) (1.45) (0.76)</cell><cell>1.8 1.7 1.7 1.5</cell><cell>(2.30) (2.16) (2.16) (1.18)</cell><cell>11.48% 11.48% 9.60% 8.42%</cell></row><row><cell></cell><cell>BridgeDepthFlow  § [20]</cell><cell></cell><cell>2.56</cell><cell>-</cell><cell>7.02</cell><cell>-</cell></row><row><cell>Unsupervised</cell><cell>CCFlow  § [34] UnOS-stereo  § [41] EpiFlow-train-ft  § [50] DDFlow [23] SelFlow  † [24]</cell><cell></cell><cell>-1.64 (2.51) 2.35 1.69</cell><cell>-1.8 3.4 3.0 2.2</cell><cell>5.66 5.58 (5.55) 5.72 4.84</cell><cell>25.27% 18.00% 16.95% 14.29% 14.19%</cell></row><row><cell></cell><cell>Ours (ARFlow)</cell><cell></cell><cell>1.44</cell><cell>1.8</cell><cell>2.85</cell><cell>11.80%</cell></row><row><cell></cell><cell>Ours (ARFlow-MV  † )</cell><cell></cell><cell>1.26</cell><cell>1.5</cell><cell>3.46</cell><cell>11.79%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of combinations of transformations. AEPE in specific regions are reported. ST: Spatial transformation, AT: Appearance transformation, OT: Occlusion transformation.Combination of Transformations. Furthermore, we delve into the type of transformations in our framework.Table 4shows the performance of the model trained with several combinations of the three kinds of transformations. There are some critical observations: (i) Each transformation can improve the performance individually. (ii) Spatial transformation is the most helpful to all measurements, especially for large displacement estimation. (iii) The accuracy in the occluded region can be significantly improved by occlusion transformation or spatial transformation. All these observations are consistent with our assumption that the transformation will introduce new challenging scenes, and our approach can provide reliable supervision.<ref type="bibr" target="#b18">19</ref>.90 3.31 0.86 3.50 30.18 Ours(aug. as reg.) 2.04 0.61 2.55 17.05 2.97 0.77 3.40 27.25</figDesc><table><row><cell>degradation. In contrast, our reduced dense variant main-</cell></row><row><cell>tains the performance while reducing 39.2% parameters.</cell></row><row><cell>Sharing decoder across feature pyramid yields an improve-</cell></row><row><cell>ment on flow with only 25.6% parameters of the original</cell></row><row><cell>model. The multi-frame extension reaches the best perfor-</cell></row><row><cell>mance with the minimal extra overhead of parameters.</cell></row><row><cell>Usage of Augmentation. As we mentioned above, almost</cell></row><row><cell>all of the unsupervised learning approaches avoid using a</cell></row><row><cell>heavy combination of augmentations. As a reference, we</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of our learning framework with direct data augmentation and the data distillation framework used in<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</figDesc><table><row><cell></cell><cell cols="2">Sintel Clean</cell><cell cols="2">Sintel Final</cell></row><row><cell>Method</cell><cell cols="4">ALL s0-10 s10-40 s40+ ALL s0-10 s10-40 s40+</cell></row><row><cell>Without Aug.</cell><cell>2.53 0.61</cell><cell cols="2">2.74 24.30 3.47 0.82</cell><cell>3.77 33.48</cell></row><row><cell>CPAB [8] + AT</cell><cell>2.38 0.61</cell><cell cols="2">2.78 21.60 3.32 0.81</cell><cell>3.59 31.09</cell></row><row><cell cols="2">AutoAugment [5] 2.30 0.62</cell><cell cols="2">2.59 21.18 3.29 0.81</cell><cell>3.53 30.11</cell></row><row><cell>Ours(ST + AT)</cell><cell>2.09 0.59</cell><cell cols="2">2.65 18.03 3.01 0.75</cell><cell>3.48 28.48</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment We thank anonymous reviewers for their constructive comments, and LL would like to thank Pengpeng Liu for helpful suggestions. This work is partially supported by the National Natural Science Foundation of China (NSFC) under Grant No. 61836015 and Key R&amp;D Program Project of Zhejiang Province (2019C01004).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coherent online video style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonn W</forename><surname>Fisher</surname></persName>
		</author>
		<title level="m">Transformations based on continuous piecewise-affine velocity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning for optical flow estimation using pyramid convolution lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxin</forename><surname>Shuosen Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Population based augmentation: Efficient learning of augmentation policy schedules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition(CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Yu Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bridging stereo matching and optical flow via spatiotemporal correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ying</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of scene flow estimation fusing with local rigidity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ddflow: Learning optical flow with unlabeled data distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Proflow: Learning to predict optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial learning of general transformations for data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saypraseuth</forename><surname>Mounsaveng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continual occlusion and optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Neoral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janšochman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">gSLICr: SLIC superpixels at over 250Hz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno>ArXiv pre-prints:1509.04232</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fusion approach for multiframe optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Models matter, so does training: An empirical study of cnns for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unos: Unified unsupervised opticalflow and stereo-depth estimation by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An improved algorithm for tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dagstuhl Motion Workshop</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatially transformed adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
	<note>ArXiv pre-prints</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep flow-guided video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised moving object detection via contextual information separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised deep epipolar flow for stationary or dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
